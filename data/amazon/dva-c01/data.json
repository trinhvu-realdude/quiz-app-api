[ {
  "id" : 1,
  "question" : "An organization deployed their static website on Amazon S3\nNow, the Developer has a requirement to serve dynamic content using a serverless solution.\nWhich combination of services should be used to implement a serverless application for the dynamic content? Select 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ea23496812a142f88c2f875850eaa77f",
    "option" : "Amazon API Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "28a133748b7843a5b6c42add773abac3",
    "option" : "Amazon EC2",
    "isCorrect" : "false"
  }, {
    "id" : "d41381403bf04e30af07738e5154e2b7",
    "option" : "AWS ECS",
    "isCorrect" : "false"
  }, {
    "id" : "bba74ffe28ec40f68ae01c643d0ac67d",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "baf6ec7c5e3a49fd867c185178eed5ab",
    "option" : "Amazon kinesis.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and D.\nOut of the above list, Given the scenario, API Gateway and AWS Lambda are the best two choices to build this serverless application.\nThe AWS Documentation mentions the following.\nAWS Lambda lets you run code without provisioning or managing servers.\nYou pay only for the compute time you consume - there is no charge when your code is not running.\nFor more information on AWS Lambda, please refer to the below Link-\nhttps://aws.amazon.com/lambda/\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\nFor more information on the API gateway, please refer to the below Link-\nhttps://aws.amazon.com/api-gateway/\nOption B is incorrect because EC2 is not a serverless offering.\nTo use serverless compute on Cloud, you can use Lambda.\nOption C is incorrect because ECS is primarily used for hosting containers and serverless offering for the same solution is Fargate.\nOption E is incorrect because Kinesis Data Streams is a scalable and durable real-time data streaming service that can continuously capture gigabytes of data per second from hundreds of thousands of sources.\nIt is not required in this scenario.\n\nThe organization has already deployed their static website on Amazon S3, which is a highly scalable, reliable, and low-cost storage service. Now, the requirement is to serve dynamic content using a serverless solution. Therefore, two services that can be used to implement a serverless application for the dynamic content are:\nAmazon API Gateway AWS Lambda\nHere's a detailed explanation of why these two services are suitable for the requirement:\nAmazon API Gateway: Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It acts as a front door for applications to access data, business logic, or functionality from backend services, such as AWS Lambda. In this case, API Gateway can be used to expose Lambda functions that generate dynamic content as RESTful APIs. These APIs can be called from the static website hosted on S3 to fetch the required dynamic content. AWS Lambda: AWS Lambda is a serverless compute service that lets developers run code without provisioning or managing servers. It can be used to execute code in response to events, such as changes to data in Amazon S3, DynamoDB, or Kinesis. In this case, Lambda can be used to write functions that generate dynamic content based on certain input parameters or events. These functions can be called from the static website hosted on S3 using API Gateway.\nOther options, such as Amazon EC2, AWS ECS, and Amazon Kinesis, are not suitable for this requirement because:\nAmazon EC2: Amazon EC2 is a web service that provides resizable compute capacity in the cloud. It requires developers to provision and manage virtual machines to run their applications. This approach is not serverless and adds overheads in terms of management and cost. AWS ECS: AWS ECS is a fully managed container orchestration service that lets developers run applications on a cluster of Amazon EC2 instances. It requires developers to package their application code and dependencies into containers and manage the underlying infrastructure. This approach is not serverless and adds overheads in terms of management and cost. Amazon Kinesis: Amazon Kinesis is a fully managed service that makes it easy to collect, process, and analyze real-time streaming data at any scale. It is suitable for use cases where data needs to be processed in real-time, such as IoT devices or social media feeds. However, it is not suitable for serving dynamic content on a static website hosted on S3, as it is not designed for that purpose.\n\n"
}, {
  "id" : 2,
  "question" : "A Developer has been asked to create an AWS Elastic Beanstalk environment for a production web application that needs to handle thousands of requests.\nCurrently, the dev environment is running on a t1.micro instance. What is the best way for the developer to provision a new production environment with a m4.large instance instead of a t1.micro?\n",
  "answers" : [ {
    "id" : "03622e7646884a388ac15736fd865c2e",
    "option" : "Use CloudFormation to migrate the Amazon EC2 instance type from t1.micro to m4.large.",
    "isCorrect" : "false"
  }, {
    "id" : "55474737721b4a1ba73938df9fde871e",
    "option" : "Create a new configuration file with the instance type as m4.large and reference this file when provisioning the new environment.",
    "isCorrect" : "true"
  }, {
    "id" : "6e8445cb60e64a468022999f3518b65b",
    "option" : "Provision a m4.large instance directly in the dev environment and deploy to the new production environment.",
    "isCorrect" : "false"
  }, {
    "id" : "2182ee24871d46d79a89b61f76938026",
    "option" : "Change the instance type value in the configurations file to m4.large by using the update autoscaling group CLI command.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe Elastic Beanstalk console and EB CLI set configuration options when you create an environment.\nYou can also set configuration options in saved configurations and configuration files.\nIf the same option is set in multiple locations, the value used is determined by order of precedence.\nConfiguration option settings can be composed in text format and saved before environment creation, applied during environment creation using any supported client, and added, modified, or removed after environment creation.\nDuring environment creation, configuration options are applied from multiple sources with the following precedence, from highest to lowest:\nSettings applied directly to the environment - Settings specified during a create environment or update environment operation on the Elastic Beanstalk API by any client, including the AWS Management Console, EB CLI, AWS CLI, and SDKs.\nThe AWS Management Console and EB CLI also apply recommended values for some options that apply at this level unless overridden.\nSaved Configurations - Settings for any options that are not applied directly to the environment are loaded from a saved configuration, if specified.\nConfiguration Files (.ebextensions) - Settings for any options that are not applied directly to the environment and not specified in a saved configuration are loaded from configuration files in the .ebextensions folder at the root of the application source bundle.\nConfiguration files are executed in alphabetical order.\nFor example, .ebextensions/01run.config is executed before .ebextensions/02do.config.\nDefault Values - If a configuration option has a default value, it only applies when the option is not set at any of the above levels.\nIf the same configuration option is defined in more than one location, the setting with the highest precedence is applied.\nWhen a setting is applied from a saved configuration or settings applied directly to the environment, the setting is stored as part of the environment's configuration.\nThese settings can be removed with the AWS CLI or with the EB CLI.\n.\nSettings in configuration files are not applied directly to the environment and cannot be removed without modifying the configuration files and deploying a new application version.\nIf a setting applied with one of the other methods is removed, the same setting will be loaded from configuration files in the source bundle.\nOption A is incorrect since the Elastic Beanstalk service already manages the environment, and we don't need Cloudformation for this.\nOption C is incorrect since the changes need to be done for the current configuration.\nFor more information on making this change, please refer to the below Link-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.ec2.html\n\nThe correct answer is B. Create a new configuration file with the instance type as m4.large and reference this file when provisioning the new environment.\nExplanation:\nElastic Beanstalk is a service that allows developers to quickly deploy web applications in multiple languages, such as Java, Python, .NET, and more, on a scalable infrastructure without worrying about the underlying infrastructure management. Elastic Beanstalk provisions and manages the AWS resources that are needed to run the application, such as Amazon EC2 instances, load balancers, and auto-scaling groups. Elastic Beanstalk also handles the deployment, monitoring, and health management of the application.\nIn this scenario, the developer needs to provision a new production environment that can handle thousands of requests, which requires a larger instance type than the t1.micro instance used in the dev environment.\nOption A is incorrect because CloudFormation is not needed to change the instance type of an Elastic Beanstalk environment. CloudFormation is a service that provides a way to model and provision AWS resources as code, using templates. While it's possible to use CloudFormation to provision an Elastic Beanstalk environment, it's not necessary in this scenario, and it would require additional setup.\nOption C is incorrect because it's not recommended to use the dev environment as a production environment. The dev environment is typically used for testing and development purposes, and it's not designed to handle production traffic. Also, provisioning a new instance directly in the dev environment doesn't ensure that the production environment is set up correctly, and it can cause issues with the production environment's configuration.\nOption D is incorrect because updating the auto-scaling group configuration directly using the AWS CLI is not recommended. Elastic Beanstalk provides a way to configure instances through configuration files, which is the recommended way of managing instances. Directly modifying the auto-scaling group can cause inconsistencies in the configuration and lead to issues with the environment.\nOption B is the correct answer because Elastic Beanstalk allows developers to use configuration files to define the environment's configuration, including the instance type. By creating a new configuration file with the instance type as m4.large, the developer can reference this file when provisioning the new environment, which ensures that the new environment is provisioned with the correct instance type. The developer can create the new configuration file based on the existing configuration file used in the dev environment and modify the instance type value to m4.large.\n\n"
}, {
  "id" : 3,
  "question" : "You are using AWS SAM templates to deploy a serverless application.\nWhich of the following resource will embed nested applications from Amazon S3 buckets?\n",
  "answers" : [ {
    "id" : "4da46cb2f6f54348980b471d777939cb",
    "option" : "AWS::Serverless::API",
    "isCorrect" : "false"
  }, {
    "id" : "21fc38e07bd54eb8a2424c3414c58a95",
    "option" : "AWS::Serverless::Application",
    "isCorrect" : "true"
  }, {
    "id" : "bf5e5f286b7c46e0a2a3adf88c8d7ec3",
    "option" : "AWS::Serverless::LayerVersion",
    "isCorrect" : "false"
  }, {
    "id" : "af0921793a0049979e96bbb241bb0e28",
    "option" : "AWS::Serverless::Function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS::Serverless::Application resource in AWS SAM template is used to embed application from Amazon S3 buckets.\nOption A is incorrect as AWS::Serverless::API is used for creating API Gateway resources &amp; methods that can be invoked through HTTPS endpoints.\nOption C is incorrect as AWS::Serverless::LayerVersion resource type creates Lambda layered function.\nOption D is incorrect as AWS::Serverless::Function resource describes the configuration for creating Lambda function.\nFor more information on using AWS SAM templates, refer to the following URLs-\nhttps://aws.amazon.com/blogs/compute/announcing-nested-applications-for-aws-sam-and-the-aws-serverless-application-repository/\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-template.html\n\nThe correct answer is B. AWS::Serverless::Application.\nAWS SAM (Serverless Application Model) templates are used to define the AWS resources required for a serverless application. The AWS::Serverless::Application resource type in a SAM template is used to deploy a nested serverless application from an S3 bucket.\nThe AWS::Serverless::API resource type is used to create an Amazon API Gateway REST API that integrates with AWS Lambda functions. It does not embed nested applications from Amazon S3 buckets.\nThe AWS::Serverless::LayerVersion resource type is used to create a layer that contains code that can be reused across multiple functions. It does not embed nested applications from Amazon S3 buckets.\nThe AWS::Serverless::Function resource type is used to create an AWS Lambda function that runs code in response to an event. It does not embed nested applications from Amazon S3 buckets.\nTherefore, the correct answer is B. AWS::Serverless::Application, as it is the resource type that is used to embed nested applications from Amazon S3 buckets.\n\n"
}, {
  "id" : 4,
  "question" : "An application hosted in AWS has been configured to use a DynamoDB table.\nSeveral items are written to the DynamoDB table.\nThese items are only accessed in a particular time frame, after which they can be deleted.\nWhich of the following is an ideal way to manage the deletion of the stale items?\n",
  "answers" : [ {
    "id" : "48418b7c6a184d06a11002f8917113ff",
    "option" : "Perform a scan on the table for the stale items and issue the Delete operation.",
    "isCorrect" : "false"
  }, {
    "id" : "5944b2497e9e46649b06974daa31e2bc",
    "option" : "Create an additional column to store the date. Perform a query for the stale objectsand the perform the Delete operation.",
    "isCorrect" : "false"
  }, {
    "id" : "f9f0f62187004411bdad8f16d3243955",
    "option" : "Enable versioning for the items in DynamoDB and delete the last accessed version.",
    "isCorrect" : "false"
  }, {
    "id" : "9ca07d77ea82419891da402c638ba577",
    "option" : "Enable TTL for the items in DynamoD.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nTime To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database.\nTTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput.\nWith TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.\nOptions A and B are incorrect since these would not be cost-effective and have a performance issue on the underlying DynamoDB table.\nOption C is incorrect since versioning is not possible in DynamoDB.For more information on Time to Live for items in DynamoDB, please refer to the below Link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n\nThe ideal way to manage the deletion of stale items from a DynamoDB table is to enable Time-to-Live (TTL) feature for the items in the table. The correct answer is D.\nTTL is a feature that allows you to specify a time period for the items in a DynamoDB table to live. When an item expires, DynamoDB automatically deletes it from the table. This feature is useful for scenarios where data becomes stale after a certain period of time and is no longer needed.\nTo enable TTL for items in a DynamoDB table, you need to create a new attribute in the table that stores the expiration time for each item. This attribute must be a Unix timestamp, which represents the number of seconds since January 1, 1970, UTC. Once you have created this attribute, you can enable TTL for the table by configuring it in the table settings.\nWhen TTL is enabled for a DynamoDB table, expired items are automatically deleted from the table. DynamoDB checks the expiration time of each item and deletes it if the current time is greater than or equal to the expiration time. This process is performed asynchronously and does not affect the performance of the table.\nOption A, performing a scan on the table for the stale items and issuing the Delete operation, is not an ideal way to manage the deletion of stale items because it can be slow and expensive for large tables. Scans are not efficient for large tables as they read every item in the table, which can result in a high read capacity consumption and increased cost.\nOption B, creating an additional column to store the date and performing a query for the stale objects and then performing the Delete operation, can work, but it requires additional management and maintenance of the table. You need to ensure that the expiration date attribute is updated correctly and that the query for stale objects is performed efficiently.\nOption C, enabling versioning for the items in DynamoDB and deleting the last accessed version, is not an ideal way to manage the deletion of stale items because it does not guarantee that the deleted version of the item is the stale one. Versioning in DynamoDB allows you to keep track of multiple versions of an item, but it does not provide a way to determine which version is stale or expired.\n\n"
}, {
  "id" : 5,
  "question" : "You have created the following stages in CodePipeline. What happens if there is a failure detected in the \"Build\" stage?\n\n",
  "answers" : [ {
    "id" : "5bc8a157e2234a5bbe9cd577afc51788",
    "option" : "A rollback will happen at the \"Source\" stage.",
    "isCorrect" : "false"
  }, {
    "id" : "090495aead6a47e09507458393559e8d",
    "option" : "The \"Build\" step will be attempted again.",
    "isCorrect" : "false"
  }, {
    "id" : "41f0f2e2b13248b096f2a1d93ed967a1",
    "option" : "The \"Build\" step will be skipped and the \"Staging\" step will start.",
    "isCorrect" : "false"
  }, {
    "id" : "7b86a03275ee4987b91000b40d41023a",
    "option" : "The entire process will halt.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nIn AWS CodePipeline, an action is a task performed on an artifact in a stage.\nIf an action or a set of parallel actions is not completed successfully, the pipeline stops running.\nOptions A, B and C are incorrect since the default action will be that the entire pipeline will be stopped if the build does not succeed.\nFor more information on Actions retry, please refer to the below Link-\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-retry.html\n"
}, {
  "id" : 6,
  "question" : "Your team has just finished developing a new version of an existing application.\nThis is a web-based application hosted on AWS.\nCurrently, Route 53 is being used to point the company's DNS name to the web site.\nYour Management has instructed you to deliver the new application to a portion of the users for testing.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "a46de79b32ea4d1f8ccfd6cfdfd45855",
    "option" : "Port the application onto Elastic beanstalk and use the Swap URL feature.",
    "isCorrect" : "false"
  }, {
    "id" : "e11efd65298f4004afb58229581d25c0",
    "option" : "Use Route 53 weighted Routing policies.",
    "isCorrect" : "true"
  }, {
    "id" : "a34efbd3b6494d8e91f7cac921d234b0",
    "option" : "Port the application onto Opswork by creating a new stack.",
    "isCorrect" : "false"
  }, {
    "id" : "bbc2be2c23bb4d25a9ad5e9b4e5c8010",
    "option" : "Use Route 53 failover Routing policies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following to support this.\nWeighted Routing.\nWeighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource.\nThis can be useful for various purposes, including load balancing and testing new versions of software.\nTo configure weighted routing, you create records that have the same name and type for each of your resources.\nYou assign each record a relative weight that corresponds with how much traffic you want to send to each resource.\nAmazon Route 53 sends traffic to a resource based on the weight that you assign to the record as a proportion of the total weight for all records in the group:\nFormula for how much traffic is routed to a given resource:\nweight for a specified record/sum of the weights for all records.\nFor example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255\nThe resource with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource gets 255/256ths (255/1+255)\nYou can gradually change the balance by changing the weights.\nIf you want to stop sending traffic to a resource, you can change the weight for that record to 0.\nOptions A and C is incorrect since this would cause a full flown deployment of the new app and is just a maintenance overhead to port the application to a new service environment.\nOption D is incorrect since this should only be used for failover conditions.\nFor more information on the weighted routing policy, please refer to the below Link-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\n\nThe best answer to achieve this task is B. Use Route 53 weighted routing policies.\nExplanation:\nWeighted Routing policies allow you to distribute traffic across multiple resources, such as web servers or Amazon EC2 instances, in proportions that you specify. You can use this feature to perform A/B testing of your new application by routing a portion of the traffic to the new application while keeping the rest of the traffic on the existing application.\nTo do this, you need to follow these steps:\nDeploy the new version of the application to a new set of servers or EC2 instances. Create a new record set in Route 53 for the new version of the application. Create a weighted routing policy in Route 53 that points to both the old and new record sets. Specify the proportion of traffic that should be sent to each record set.\nFor example, you can configure the weighted routing policy to send 80% of the traffic to the old record set and 20% of the traffic to the new record set.\nWait for DNS propagation to complete. Test the new version of the application using the new record set's DNS name.\nNote: It's important to keep in mind that Route 53 doesn't support session affinity, so users may end up switching between the old and new versions of the application if they refresh their browser or use multiple tabs.\nOption A. Elastic Beanstalk with Swap URL feature: The Swap URL feature of Elastic Beanstalk allows you to deploy a new version of an application to a separate environment, test it, and then swap the URLs of the environments to make the new version live. However, this is not the best solution in this case because it involves creating a separate environment and can take a longer time to deploy the new version. Additionally, it requires you to manage two separate environments, which may not be ideal for testing purposes.\nOption C. Port the application onto Opswork by creating a new stack: Opswork is a configuration management service that allows you to create stacks of AWS resources, such as EC2 instances, load balancers, and databases. While it is possible to create a new stack in Opswork for the new version of the application, it is not the best solution for testing purposes. This is because creating a new stack is time-consuming and can be complicated to set up. Additionally, it may require you to duplicate existing resources, which can be expensive.\nOption D. Use Route 53 failover Routing policies: Failover routing policies are used to route traffic to a secondary resource, such as a backup server or site, in case the primary resource becomes unavailable. While it is possible to use failover routing policies to test a new version of an application, it is not the best solution in this case. This is because failover routing policies are designed for high availability rather than testing purposes. Additionally, failover routing policies can take time to propagate, which may not be ideal for testing purposes.\n\n"
}, {
  "id" : 7,
  "question" : "You're developing an application that will be hosted on an EC2 Instance.\nThis will be part of an Autoscaling Group.\nThe application needs to get the private IP of the instance to send it across to a controller-based application.\nWhich of the following can be done to achieve this?\n",
  "answers" : [ {
    "id" : "b367033b703c442d98a3f42ae5009e4b",
    "option" : "Query the Instance Meta Data",
    "isCorrect" : "true"
  }, {
    "id" : "1b7c2be0186d4e25b296c1b3e452d100",
    "option" : "Query the Instance User Data",
    "isCorrect" : "false"
  }, {
    "id" : "bfea025c1f4e4e5f9139b4c079b8ee97",
    "option" : "Have an Admin get the IP address from the console",
    "isCorrect" : "false"
  }, {
    "id" : "60aeb6f478e24b38bbafee266aa79a39",
    "option" : "Make the application run IFConfig.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe application can use the application metadata to get the private IP address.\nThe below snapshot from the AWS Documentation shows the information that you can get from the Instance metadata.\nOption B is invalid because this cannot be used to get the IP address of the instance.\nOption C is invalid because this is not an automated approach.\nOption D is invalid because we don't know the type of instance the application is running on.\nFor more information on AWS Instance Metadata, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\n\n\nThe correct option for this scenario is A. Query the Instance Meta Data.\nHere's why:\nWhen an EC2 instance is launched, it can get metadata about itself and its environment. This metadata is available at a special URL that can be accessed from within the instance. The metadata includes information such as the instance ID, instance type, and security groups assigned to the instance.\nOne piece of metadata that's particularly useful for this scenario is the private IP address of the instance. To retrieve this information, you can query the instance metadata service (IMDS) using the following URL:\nhttp://169.254.169.254/latest/meta-data/local-ipv4\nThis URL will return the private IP address of the instance.\nOption B, querying the Instance User Data, is not relevant to this scenario as it is a feature that allows you to pass data to an EC2 instance when it's launched. This data can be used to perform various tasks, such as configuring software on the instance or running scripts. However, it doesn't provide information about the instance itself, such as its IP address.\nOption C, having an Admin get the IP address from the console, is not a scalable or automated solution. It requires manual intervention and is prone to errors.\nOption D, making the application run IFConfig, is not a recommended approach as it involves running a command within the instance. This requires the application to have permissions to execute commands on the instance, which can be a security risk. Additionally, this approach is not portable and may not work on all operating systems.\nTherefore, the recommended approach for retrieving the private IP address of an EC2 instance that's part of an Autoscaling Group is to query the instance metadata service (IMDS) using the appropriate URL.\n\n"
}, {
  "id" : 8,
  "question" : "Your current log analysis application takes more than four hours to generate a report of the top 10 users of your web application.\nYou have been asked to implement a system that can report this information in real-time, ensure that the report is always up to date, and handle increases in the number of requests to your web application.\nChoose the option that is cost-effective and can fulfill the requirements.\n",
  "answers" : [ {
    "id" : "2e1e9817020d4ec7b28731dd3b7253fb",
    "option" : "Publish your data to CloudWatch Logs, and configure your application to Autoscale to handle the load on demand.",
    "isCorrect" : "false"
  }, {
    "id" : "d90bbaeff92b4d8581cb7fe94b118457",
    "option" : "Publish your log data to an Amazon S3 bucket.Use AWS CloudFormation to create an Auto Scaling group to scale your post-processing application which is configured to pull down your log files stored an Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "478bcfcdb1574f92b7b3943a7f82c243",
    "option" : "Post your log data to an Amazon Kinesis data stream, and subscribe your log-processing application so that is configured to process your logging data.",
    "isCorrect" : "true"
  }, {
    "id" : "242830c93bdc4ac4b02eeb21b18028ac",
    "option" : "Create a multi-AZ Amazon RDS MySQL cluster, post the logging data to MySQL, and run a map reduce job to retrieve the required information on user counts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nWhen you see Amazon Kinesis as an option, this becomes the ideal option to process data in real-time.\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to process streaming data cost-effectively at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, data lakes, and data warehouses, or build your own real-time applications using this data.\nFor more information on AWS Kinesis, please refer to the below link-\nhttps://aws.amazon.com/kinesis/\nOption A is incorrect because logs in CloudWatch Logs are not in real-time.\nOption B is incorrect because you have to pay for S3 buckets and EC2 instances cost, which is not a cost-effective solution.\nAnd the logs may not be in real-time.\nOption D is incorrect because it is not cost-efficient to store the logs in RDS MySQL.\n\nThe requirement is to generate real-time reports of the top 10 users of a web application and ensure that the report is always up to date, and handle increases in the number of requests to your web application. The solution should be cost-effective as well.\nOption A: Publish data to CloudWatch Logs, and configure your application to Autoscale to handle the load on demand.\nCloudWatch Logs is a managed service provided by AWS for log management. It allows you to collect, monitor, and store log files from AWS resources such as EC2 instances, Lambda functions, and VPC flow logs. By publishing log data to CloudWatch Logs, you can easily analyze log data in real-time using CloudWatch Logs Insights. However, CloudWatch Logs does not provide a built-in way to analyze log data for specific use cases like identifying the top 10 users of a web application. Therefore, it is not a suitable option for this requirement. Autoscaling can help to handle load on demand, but it does not directly address the requirement of generating real-time reports of the top 10 users of a web application.\nOption B: Publish log data to an Amazon S3 bucket. Use AWS CloudFormation to create an Auto Scaling group to scale your post-processing application which is configured to pull down your log files stored an Amazon S3.\nAmazon S3 is a highly scalable object storage service provided by AWS. It is suitable for storing large amounts of data, including log files. By publishing log data to an S3 bucket, you can easily analyze log data using other AWS services such as Amazon Athena, AWS Glue, or Amazon EMR. CloudFormation is an AWS service that allows you to create and manage AWS resources using templates. By using CloudFormation to create an Auto Scaling group, you can ensure that your post-processing application scales automatically based on the demand. The post-processing application can be configured to pull down log files stored in an S3 bucket and analyze the log data to generate real-time reports of the top 10 users of your web application. This solution is suitable for the requirement, and it is cost-effective as well since you only pay for the S3 storage and the resources used by the post-processing application.\nOption C: Post your log data to an Amazon Kinesis data stream, and subscribe your log-processing application so that is configured to process your logging data.\nAmazon Kinesis is a managed service provided by AWS for real-time data streaming. It allows you to collect, process, and analyze data streams such as log data, clickstreams, and social media feeds. By posting log data to an Amazon Kinesis data stream, you can process log data in real-time using Kinesis Data Analytics or other AWS services. The log-processing application can be subscribed to the Kinesis data stream, and it can process the log data to generate real-time reports of the top 10 users of your web application. This solution is suitable for the requirement of generating real-time reports, but it may not be cost-effective since you are charged based on the number of Kinesis data stream shards used and the amount of data ingested.\nOption D: Create a multi-AZ Amazon RDS MySQL cluster, post the logging data to MySQL, and run a map reduce job to retrieve the required information on user counts.\nAmazon RDS is a managed database service provided by AWS. It allows you to easily deploy and manage relational databases such as MySQL, PostgreSQL, and Oracle. By creating a multi-AZ Amazon RDS MySQL cluster, you can ensure high availability and durability for your database. However, posting log data to a relational database like MySQL may not be suitable for real-time log analysis since relational databases are optimized for transactional processing rather than analytical processing. Additionally, running a map reduce job to retrieve the required information on user counts may take\n\n"
}, {
  "id" : 9,
  "question" : "An application has been making use of AWS DynamoDB for its back-end data store.\nThe size of the table has now grown to 20 GB, and the scans on the table are causing throttling errors.\nWhich of the following should now be implemented to avoid such errors?\n",
  "answers" : [ {
    "id" : "5d4ee28e4a4c4c75bb27865148d21424",
    "option" : "Large Page size",
    "isCorrect" : "false"
  }, {
    "id" : "d7df3be135b346899f1c24e5e2c19685",
    "option" : "Reduced page size",
    "isCorrect" : "true"
  }, {
    "id" : "d8a37104697f4e73b9c15e3f648a04b9",
    "option" : "Parallel Scans",
    "isCorrect" : "false"
  }, {
    "id" : "5e7e80a7b506496fbf64770e9a33176a",
    "option" : "Sequential scans.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nWhen you scan your table in Amazon DynamoDB, you should follow the DynamoDB best practices for avoiding sudden bursts of read activity.\nYou can use the following technique to minimize the impact of a scan on a table's provisioned throughput.\nReduce page size.\nBecause a Scan operation reads an entire page (by default, 1 MB), you can reduce the scan operation's impact by setting a smaller page size.\nThe Scan operation provides a Limit parameter that you can use to set the page size for your request.\nEach Query or Scan request with a smaller page size uses fewer read operations and creates a \"pause\" between each request.\nFor example, suppose that, each item is 4 KB, and you set the page size to 40 items.\nA Query request would then consume only 20 eventually consistent read operations or 40 strongly consistent read operations.\nA larger number of smaller Query or Scan operations would allow your other critical requests to succeed without throttling.\nOption A is incorrect because the page size should be reduced rather than enlarged.\nOption C is incorrect because a parallel scan with many workers can easily consume all of the provisioned read capacity.\nOption D is incorrect because the sequential scan processes data sequentially.\nIt does not help to avoid the throttling errors.\nFor more information, please check below AWS Docs-\nhttps://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html\n\nThe correct answer is C. Parallel Scans.\nExplanation:\nDynamoDB is a NoSQL database that can handle large datasets and provide consistent, low-latency performance. However, when the size of the table grows beyond a certain point, scans on the table can cause throttling errors.\nTo understand how to solve the problem, we need to know how DynamoDB scans work. DynamoDB scans operate on partitions, which are the basic unit of data distribution in DynamoDB. Each partition contains a subset of the table's data. When a scan is performed, DynamoDB reads all the items in a partition sequentially until it has scanned all the items in the partition.\nSo, what can cause throttling errors? DynamoDB provides a certain level of read and write capacity for each table or partition. If the scan operation consumes more capacity than is available, DynamoDB will throttle the operation. Throttling can cause delays in accessing data and can affect the performance of the application.\nNow, let's look at the answer options:\nA. Large Page size: This option is not relevant to the problem. Page size is a parameter that determines how many items DynamoDB reads at a time during a scan operation. Increasing page size can reduce the number of requests needed to scan a table, but it will not solve the throttling problem.\nB. Reduced page size: This option is also not relevant. As mentioned above, reducing page size can increase the number of requests needed to scan a table, which can actually make the throttling problem worse.\nC. Parallel Scans: This option is the correct answer. Parallel scans involve dividing the table into multiple partitions and scanning them simultaneously. This allows for more efficient use of read capacity and can reduce the chance of throttling errors. Parallel scans can be implemented by using multiple threads in the application or by using the parallel scan feature in the AWS SDK.\nD. Sequential scans: This option is not recommended. As mentioned earlier, DynamoDB scans operate on partitions, and scanning a partition sequentially can consume a lot of read capacity. Sequential scans can cause throttling errors and should be avoided whenever possible.\nIn conclusion, the correct answer is C. Parallel Scans. This will help avoid throttling errors by distributing the read capacity across multiple partitions, allowing for more efficient use of DynamoDB resources.\n\n"
}, {
  "id" : 10,
  "question" : "You've developed a set of scripts using AWS Lambda.\nThese scripts need to access EC2 Instances in a VPC.\nWhich of the following needs to be done to ensure that the AWS Lambda function can access the resources in the VPC? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "3cb045b3d03d477eb1d66e15329b40f8",
    "option" : "Ensure that the subnet ID’s are configured in the Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "2f6b6c1df6854ca78e091c0be90b172a",
    "option" : "Ensure that the NACL ID’s are configured in the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "bb4f0642f72a408284a5cdb0b5e1e6bf",
    "option" : "Ensure that the Security Group ID’s are configured in the Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "fc1869dcb990425a8fba5d8c84a174e0",
    "option" : "Ensure that the VPC Flow Log ID’s are configured in the Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nOptions B and D are incorrect since you have to mention the Subnet and Security IDs for the Lambda function to access the resources in the VPC.The AWS Documentation mentions the following.\nYou can enable AWS Lambda to access resources in a Virtual Private Cloud (VPC)\nYour Lambda functions can now access Amazon RDS databases, Amazon Redshift data warehouses, Amazon ElasticCache nodes, and other endpoints that are accessible only from within a particular VPC (e.g., web service running on EC2)\nYou must provide additional VPC-specific configuration information such as VPC subnet IDs and security group IDs in order to enable your Lambda functions to access resources in an Amazon VPC.\nFor more information on configuring a lambda function to access resources in a VPC, please refer to the below link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/vpc.html\nhttps://aws.amazon.com/about-aws/whats-new/2016/02/access-resources-within-a-vpc-using-aws-lambda/\n\nTo ensure that an AWS Lambda function can access resources in a VPC, you need to configure the following:\nSubnet IDs - When you create a Lambda function, you need to specify the VPC and subnet IDs that it should run in. The subnet IDs allow the Lambda function to access resources within the VPC. Security Group IDs - You also need to specify the security group IDs that the Lambda function should use. Security groups act as a virtual firewall that controls inbound and outbound traffic to the resources within the VPC. You can configure the security groups to allow or restrict access based on IP addresses, protocols, and ports.\nIn summary, to ensure that AWS Lambda functions can access EC2 instances in a VPC, you need to configure the subnet IDs and security group IDs in the Lambda function.\n\n"
}, {
  "id" : 11,
  "question" : "You are developing an application that is going to make use of Amazon Kinesis.\nDue to the high throughput, you decide to have multiple shards for the streams.\nWhich of the following is TRUE when it comes to processing data across multiple shards?\n",
  "answers" : [ {
    "id" : "dd3de7260ff942478ed11777b5dfa3d4",
    "option" : "You cannot guarantee the order of data across multiple shards. It`s possible only within a shard.",
    "isCorrect" : "true"
  }, {
    "id" : "93768766647b4aa48b7ac16c49f8e00d",
    "option" : "Order of data is possible across all shards in a stream.",
    "isCorrect" : "false"
  }, {
    "id" : "66686732776847228b774791c6cb8940",
    "option" : "Order of data is not possible at all in Kinesis streams.",
    "isCorrect" : "false"
  }, {
    "id" : "15ca862d769f4e7caa958b64e5e22853",
    "option" : "You need to use Kinesis firehose to guarantee the order of data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nKinesis Data Streams lets you order records and read and replay records in the same order to many Kinesis Data Streams applications.\nTo enable write ordering, Kinesis Data Streams expects you to call the PutRecord API to write serially to a shard while using the sequenceNumberForOrdering parameter.\nSetting this parameter guarantees strictly increasing of sequence numbers for puts from the same client and to the same partition key.\nOption A is correct as it cannot guarantee the ordering of records across multiple shards.\nOption B, C and D are incorrect becauseKinesis Data Streams can order records on a single shard.\nEach data record has a sequence number that is unique within its shard.\nKinesis Data Streams assigns the sequence number after you write to the stream with putRecords or client.putRecord.\nFor more information, please refer to:\nhttps://aws.amazon.com/blogs/database/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/\nhttps://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\n\nThe correct answer is A. You cannot guarantee the order of data across multiple shards. It's possible only within a shard.\nAmazon Kinesis is a managed service that allows you to collect, process, and analyze real-time streaming data at a large scale. Kinesis streams are made up of one or more shards. Each shard is a fixed unit of capacity that can ingest data records at a certain rate and store data records up to a certain size.\nWhen processing data across multiple shards in a Kinesis stream, it's important to keep in mind that data ordering is only guaranteed within a shard. This means that records ingested into a shard will be processed in the order they were received, and records ingested into different shards will be processed in parallel, which may result in records being processed out of order.\nTo ensure that records are processed in order across multiple shards, you need to include a sequence number in each record and use this sequence number to reorder the records during processing. Alternatively, you can use a partition key to ensure that records with the same key are always processed by the same shard, which can help maintain ordering.\nIt's worth noting that Kinesis Firehose is a different service that is used to deliver data from Kinesis streams to destinations such as S3, Redshift, or Elasticsearch. Firehose does not support ordering guarantees, and records delivered to destinations may be out of order. If ordering is important for your use case, you should use Kinesis Streams instead of Firehose.\n\n"
}, {
  "id" : 12,
  "question" : "Your company has asked you to maintain an application using Elastic Beanstalk.\nAt times, you normally hit the application version limit when deploying new versions of the application.\nWhich of the following is the most effective way to manage this issue?\n",
  "answers" : [ {
    "id" : "0df027f068714209be346705ac276693",
    "option" : "Create multiple environments and deploy the different versions to different environments.",
    "isCorrect" : "false"
  }, {
    "id" : "f31abb4c71ae4908811e96f999939a81",
    "option" : "Create an Application Version Lifecycle policy.",
    "isCorrect" : "true"
  }, {
    "id" : "098c66f0906a4f35a983402b93ad9ecc",
    "option" : "Create multiple applications and deploy the different versions to different applications.",
    "isCorrect" : "false"
  }, {
    "id" : "0f15d556a1ce4e06b333783cd76f4a06",
    "option" : "Delete the application versions manually.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nEach time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version.\nIf you don't delete versions that you no longer use, you will eventually reach the application version limit and be unable to create new versions of that application.\nYou can avoid hitting the limit by applying an application version lifecycle policy to your applications.\nA lifecycle policy tells Elastic Beanstalk to delete application versions that are old or delete application versions when the total number of versions for an application exceeds a specified number.\nOptions A and C are invalid because they are not the right approaches when managing the deployment of application versions.\nOption D, even though possible, is not the most effective way.\nFor more information on the application lifecycle, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\n\nThe correct answer is A: Create multiple environments and deploy different versions to different environments.\nElastic Beanstalk provides a platform to deploy, manage, and scale applications in the AWS cloud easily. An application environment is a logical deployment group within Elastic Beanstalk that contains versions of an application. Elastic Beanstalk provides an environment limit of 1000 versions per environment by default. When you hit the version limit, you need to delete some versions to make space for new ones.\nHowever, deleting older versions may not always be desirable as you may need to roll back to an older version for debugging purposes or because of an unexpected issue with the newer version. Therefore, creating multiple environments is the most effective way to manage this issue as it provides a way to maintain different versions of your application while allowing you to manage them separately.\nCreating multiple environments allows you to deploy different versions of your application to different environments, ensuring that you can manage and test each version separately without interfering with the others. You can also use Elastic Beanstalk's blue-green deployment feature to deploy new versions to a temporary environment for testing before promoting it to production.\nOption B, creating an Application Version Lifecycle policy, allows you to define rules for deleting application versions automatically. However, this approach may not always be desirable, as it can lead to the loss of important versions that you may need for debugging or rollbacks.\nOption C, creating multiple applications and deploying different versions to different applications, is not an optimal solution as it may lead to increased management overhead and potentially higher costs.\nOption D, deleting application versions manually, is not a sustainable solution as it can be time-consuming and error-prone, and you may lose important versions that you need for debugging or rollbacks.\nTherefore, the most effective way to manage this issue is to create multiple environments and deploy different versions to different environments.\n\n"
}, {
  "id" : 13,
  "question" : "As a developer, you have enabled server logging on an S3 bucket.\nYou have a simple static web page with CSS pages uploaded to the bucket which is 1 MB in total size.\nAfter a duration of 2 weeks, you come back and see that the size of the bucket has increased to 50MB.\nWhich of the following could be a reason for this?\n",
  "answers" : [ {
    "id" : "9dce6434deaf4f86afc2f5ffcedd3bc4",
    "option" : "You have enabled CRR on the bucket as well. That is why the space is being consumed.",
    "isCorrect" : "false"
  }, {
    "id" : "49270d31cbbc47efb99dd52131fc9cfb",
    "option" : "You have enabled Encryption on the bucket as well. That is why the space is being consumed.",
    "isCorrect" : "false"
  }, {
    "id" : "9bf8236159c74c4ab4f3ca259afe391d",
    "option" : "Server access logs are configured to be delivered to the same bucket as the source bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "62922047e56449bba335c4edee680a12",
    "option" : "Monitoring has been enabled for the bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAn S3 bucket with server access logging enabled can accumulate many server log objects over time.\nYour application might need these access logs for a specific period after creation, and after that, you might want to delete them.\nYou can use the Amazon S3 lifecycle configuration to set rules so that Amazon S3 automatically queues these objects for deletion at the end of their life.\nThe correct answer is.\nC.\nThis is normal behavior since the logs are being delivered to the same bucket.\nOption A is incorrect because CRR (Cross-region replication) does not consume space in the S3 bucket.\nOption B is incorrect because encryption does not consume space.\nOption D is incorrect because monitoring and logging are two different things.\nMonitoring deals with metrics and does not add data to the S3 bucket.\nFor more information on deleting logs files, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/deleting-log-files-lifecycle.html\n\nThe most likely reason for the increase in size of the S3 bucket from 1 MB to 50 MB is option C, \"Server access logs are configured to be delivered to the same bucket as the source bucket.\"\nWhen server access logging is enabled on an S3 bucket, AWS will automatically generate log files that record all requests made to that bucket. These logs are then saved to a separate bucket or a subfolder within the same bucket.\nIf server access logs are configured to be delivered to the same bucket as the source bucket, it is possible that these log files are being saved in the same location as the original files, leading to an increase in the overall size of the bucket. The size of these logs can add up over time, especially if the bucket receives a large number of requests.\nOption A, \"You have enabled CRR on the bucket as well,\" is less likely to be the reason for the increase in bucket size because Cross-Region Replication (CRR) does not directly affect the size of the bucket. CRR only replicates objects from one S3 bucket to another in a different region.\nOption B, \"You have enabled Encryption on the bucket as well,\" is also less likely to be the reason for the increase in bucket size. Enabling server-side encryption on an S3 bucket does not increase the size of the stored objects.\nOption D, \"Monitoring has been enabled for the bucket,\" is not a likely reason for the increase in bucket size because monitoring does not directly affect the size of the bucket. Monitoring simply tracks metrics such as the number of requests made to the bucket, the amount of data transferred, and so on.\nIn summary, option C is the most likely reason for the increase in the size of the S3 bucket from 1 MB to 50 MB. Server access logs configured to be delivered to the same bucket as the source bucket can cause an increase in overall size, especially if the bucket receives a large number of requests.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 15,
  "question" : "An application has a database in an AWS RDS Instance.\nWhen the traffic is high, the application's response time grows as there are many read queries to the RDS database.\nWhich of the following can be used to decrease the response time for the application?\n",
  "answers" : [ {
    "id" : "bee54e951ef14f92a7328a0908489976",
    "option" : "Place a CloudFront distribution in front of the database.",
    "isCorrect" : "false"
  }, {
    "id" : "ec9e1acff17f4081b58efcfe635fc5be",
    "option" : "Enable Read Replicas for the database.",
    "isCorrect" : "true"
  }, {
    "id" : "2e00b29b42e44f31877529112acd663c",
    "option" : "Change the database from RDS to DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "b76d158bb19e45b4a73bf1d099feb253",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nYou can reduce the load on your source DB instance by routing read queries from your applications to the read replica.\nRead replicas allow you to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads.\nOption A is incorrect because the RDS database cannot be the origin of a CloudFront distribution.\nOption C is incorrect because changing the database type is not the ideal approach.\nOption D is incorrect because Multi-AZ is used for fault-tolerant scenarios for the database.\nFor more information on Read Replicas, please refer to the below URL-\nhttps://aws.amazon.com/rds/details/read-replicas/\n\nThe best solution to decrease response time for the application is to enable Read Replicas for the database. Read Replicas allow for read-only copies of the primary RDS instance, which can be used to offload read traffic from the primary instance. This can decrease response times by reducing the load on the primary instance, which can then focus on serving write requests.\nCloudFront distribution is a content delivery network (CDN) service that can cache static and dynamic content to reduce the latency of content delivery. However, it is not a suitable solution for reducing response time for database read queries as it does not interact with databases.\nDynamoDB is a NoSQL database service that can handle high-velocity workloads, and it is ideal for applications with large amounts of unstructured data. While DynamoDB can be a solution to handle high read queries, it is not a direct solution to the problem, as it requires changing the database architecture.\nMulti-AZ is a feature that provides high availability for RDS databases. When Multi-AZ is enabled, a standby instance is provisioned in a different availability zone, which is kept in sync with the primary instance. In case of a primary instance failure, the standby instance can take over. Although it improves availability, it does not directly decrease response time for read queries.\nIn summary, the best solution to decrease the response time for the application with a high volume of read queries is to enable Read Replicas for the RDS database.\n\n"
}, {
  "id" : 16,
  "question" : "You have deployed an application on an EC2 Instance.\nThis application makes calls to a DynamoDB service.\nThere are numerous performance issues present in the application.\nYou decide to use the XRay service to debug the performance issues.\nYou are not able to see the trails in the XRay service.\nWhich of the following could be the underlying issue? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "7b6f194159f34147b9afef01996b2440",
    "option" : "The X-Ray daemon is not installed on the EC2 Instance.",
    "isCorrect" : "true"
  }, {
    "id" : "d8bc3c1321514f638bc177309bdb3192",
    "option" : "The right AMI is not chosen for the EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "6ead238c9fb54e7e860813038133437d",
    "option" : "Ensure that the IAM Role attached to the Instance has permission to upload data onto X-Ray.",
    "isCorrect" : "true"
  }, {
    "id" : "b691f7eb72ba403bade4f5a9b1d856a1",
    "option" : "Ensure that the IAM Role attached to the Instance has permission to upload data onto Cloudwatch.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nYou need to have the daemon service running on the EC2 Instance.\nAnd a role needs to be attached to the EC2 Instance.\nRunning the X-Ray Daemon on Amazon EC2\nYou can run the X-Ray daemon on the following operating systems on Amazon EC2.\nAmazon Linux.\nUbuntu.\nWindows Server (2012 R2 and newer)\nUse an instance profile to grant the daemon permission to upload trace data to X-Ray.\nFor more information, see Giving the Daemon Permission to Send Data to X-Ray.\nOption B is incorrect since the agent can be installed on different types of instances.\nOption D is incorrect since the traces need to be sent to the X-Ray service.\nFor more information on the X-Ray daemon service, please refer to the below URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ec2.html\n\nThe X-Ray service is an AWS service that helps developers analyze and debug distributed applications, such as those that use EC2 instances and DynamoDB. The service provides an end-to-end view of requests as they travel through an application and enables developers to identify performance bottlenecks and errors.\nIn this scenario, the application deployed on an EC2 instance is experiencing performance issues, and the developer has decided to use the X-Ray service to debug these issues. However, they are unable to see the trails in the X-Ray service, which indicates that there may be an underlying issue preventing the service from functioning properly.\nLet's review the given options to understand what could be causing the problem:\nA. The X-Ray daemon is not installed on the EC2 Instance.\nThe X-Ray daemon is a software component that runs on EC2 instances and collects data about requests that pass through the application. If the X-Ray daemon is not installed on the EC2 instance running the application, then the X-Ray service will not receive any data about the application's performance. This could be a possible reason why the trails are not visible in the X-Ray service.\nB. The right AMI is not chosen for the EC2 Instance.\nAn Amazon Machine Image (AMI) is a pre-configured virtual machine image that is used to create EC2 instances. If the AMI used to create the EC2 instance does not support the X-Ray daemon, then it will not be possible to collect performance data and send it to the X-Ray service. This could be another possible reason why the trails are not visible in the X-Ray service.\nC. Ensure that the IAM Role attached to the Instance has permission to upload data onto X-Ray.\nThe IAM Role attached to the EC2 instance determines what permissions the instance has to access AWS resources, including the X-Ray service. If the IAM Role does not have permission to upload data onto the X-Ray service, then it will not be possible to send performance data to the service. This could be another possible reason why the trails are not visible in the X-Ray service.\nD. Ensure that the IAM Role attached to the Instance has permission to upload data onto Cloudwatch.\nCloudwatch is another AWS service that is often used to monitor and debug applications. However, it is not directly related to the X-Ray service. If the IAM Role attached to the EC2 instance does not have permission to upload data onto Cloudwatch, then it will not be possible to send performance data to Cloudwatch, but this will not directly impact the ability to see trails in the X-Ray service.\nIn conclusion, options A and C could be the underlying issues that are preventing the trails from being visible in the X-Ray service. It is important to ensure that the X-Ray daemon is installed on the EC2 instance, and that the IAM Role attached to the instance has permission to upload data onto the X-Ray service.\n\n"
}, {
  "id" : 17,
  "question" : "You are planning on deploying an application to the worker role in Elastic Beanstalk.\nMoreover, this worker application is going to run the periodic tasks.\nWhich of the following is a must-have as part of the deployment?\n",
  "answers" : [ {
    "id" : "2d88287863914664a36f2000abb3375c",
    "option" : "An appspec.yaml file",
    "isCorrect" : "false"
  }, {
    "id" : "acd1e9cb771148b6b270e567e4027079",
    "option" : "A cron.yaml file",
    "isCorrect" : "true"
  }, {
    "id" : "9b0d02210d5d46af9475f0718faed2aa",
    "option" : "A cron.config file",
    "isCorrect" : "false"
  }, {
    "id" : "6bab80f0993741f0a0d96c55077cb001",
    "option" : "An appspec.json file.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is also given in the AWS Documentation.\nCreate an Application Source Bundle.\nWhen you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle.\nYour source bundle must meet the following requirements.\nConsist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)\nNot exceed 512 MB.\nNot include a parent folder or top-level directory (subdirectories are fine)\nIf you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file.\nFor more information, see Periodic Tasks.\nBecause of the exact requirement given in the AWS Documentation, all other options are invalid.\nFor more information on creating an application source bundle for Elastic beanstalk, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html\n\nThe correct answer is B. A cron.yaml file.\nElastic Beanstalk is an AWS service that simplifies the deployment and management of web applications. Elastic Beanstalk provides different environments, including web server environments and worker environments. The web server environment is used for hosting web applications, while the worker environment is used for running background tasks.\nIn Elastic Beanstalk, a cron job is a task that runs periodically at a specified interval. A cron job can be used for different purposes, such as sending emails, updating databases, or running backups.\nTo run a periodic task in a worker environment, you need to create a cron.yaml file. The cron.yaml file specifies the schedule for the cron job and the command to execute. The syntax for the cron.yaml file is similar to the syntax used in Linux systems.\nThe cron.yaml file must be placed in the root directory of your application. When Elastic Beanstalk deploys your application to the worker environment, it reads the cron.yaml file and creates a cron job based on the schedule and command specified in the file.\nAn appspec.yaml file is used for the deployment of web server environments in Elastic Beanstalk. It specifies the instructions for deploying your application, such as copying files, running scripts, and creating resources. It is not required for worker environments.\nA cron.config file does not exist in Elastic Beanstalk. It is not a valid file name for any configuration file.\nAn appspec.json file is an alternative to the appspec.yaml file. It is also used for the deployment of web server environments, but it is not required for worker environments.\nIn summary, if you want to run periodic tasks in an Elastic Beanstalk worker environment, you need to create a cron.yaml file in the root directory of your application.\n\n"
}, {
  "id" : 18,
  "question" : "You're developing an application that is going to be hosted in AWS Lambda.\nThe function will make calls to a database.\nA requirement is that all database connection strings should be kept secure.\nWhich of the following is the MOST secure way to implement this?\n",
  "answers" : [ {
    "id" : "d9201dffe7484cb78b1c15e01cf21bac",
    "option" : "Put the connection strings values in a CloudFormation template.",
    "isCorrect" : "false"
  }, {
    "id" : "8aa9e886882b44c589f8722c84b97b8a",
    "option" : "Put the database connection string in the app.json file and store it in a Git repository.",
    "isCorrect" : "false"
  }, {
    "id" : "5a22e7cd39214061bb0a4af1c87563df",
    "option" : "Lambda needs to reference the AWS Systems Manager Parameter Store for the encrypted database connection string.",
    "isCorrect" : "true"
  }, {
    "id" : "5b41d593997e46c0b9537f7623c1217f",
    "option" : "Place the database connection string in the AWS Lambda function itself since all Lambda functions are encrypted at rest.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management.\nYou can store data such as passwords, database strings, and license codes as parameter values.\nYou can store values as plain text or encrypted data.\nYou can then reference values by using the unique name that you specified when you created the parameter.\nHighly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.\nParameter Store is offered at no additional charge.\nOption A is incorrect because the connection strings values are exposed in the CloudFormation template.\nOption B is incorrect because the string is stored in the code repository and is unsecure.\nOption D is incorrect because the string is stored together with the Lambda function code, which is not secure.\nFor more information on the Systems Manager Parameter Store, please refer to the below URL-\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\n\nThe most secure way to implement database connection strings in AWS Lambda is option C, which involves using the AWS Systems Manager Parameter Store to store and retrieve the encrypted database connection string.\nExplanation of each option:\nA. Putting the connection strings values in a CloudFormation template is not the most secure way to store sensitive data because CloudFormation templates are stored in plain text and can be accessed by anyone who has access to the AWS Management Console.\nB. Putting the database connection string in the app.json file and storing it in a Git repository is not secure because Git repositories can be accessed by anyone who has access to the repository, and the database connection string will be stored in plain text.\nC. Lambda needs to reference the AWS Systems Manager Parameter Store for the encrypted database connection string. The Parameter Store provides a centralized location for storing and managing secure information such as database connection strings. When using the Parameter Store, the connection string is encrypted using an AWS KMS key, and only authorized AWS Identity and Access Management (IAM) users and roles can access the Parameter Store.\nD. Placing the database connection string in the AWS Lambda function itself is not secure because the Lambda function code is stored in plaintext and can be accessed by anyone who has access to the AWS Management Console. It is not recommended to store sensitive data in the Lambda function code.\nIn summary, option C, using the AWS Systems Manager Parameter Store, is the most secure way to implement database connection strings in AWS Lambda.\n\n"
}, {
  "id" : 19,
  "question" : "A company currently has an application that works with DynamoDB.\nThe application is a high revenue generating application for the company.\nTheir current response time for their read workloads is in the order of milliseconds.\nBut to bump up hits to their pages, they want to reduce the response time to microseconds.\nWhich of the following would you suggest to be most preferably used with DynamoDB to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "d2e53be5f80949e393868a00e529c0ab",
    "option" : "Consider deploying an ElastiCache in front of DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "d9e6bb7fe84240bc9858ec989b24c44f",
    "option" : "Consider using DynamoDB global tables.",
    "isCorrect" : "false"
  }, {
    "id" : "f4b1b52f28ac4cf084dac1969d2e7ac5",
    "option" : "Consider using DynamoDB accelerator.",
    "isCorrect" : "true"
  }, {
    "id" : "0a8eb9d8c2fe44cab6103c19210db9f8",
    "option" : "Consider using a higher throughput for the tables.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nDynamoDB Accelerator(DAX) is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications.\nDAX addresses three core scenarios.\nAs an in-memory cache, DAX reduces the response times of eventually-consistent read workloads by order of magnitude, from single-digit milliseconds to microseconds.\nDAX reduces operational and application complexity by providing a managed service that is API-compatible with Amazon DynamoDB.\nThus it requires only minimal functional changes to use with an existing application.\nOption A is invalid because ElastiCache would not guarantee for certain such a great reduction in response times.\nThe question also asks for a \"preferred\" service to be used along with DynamoDB, for which DAX would be a better answer than Elasticache.\nOption B is invalid because it is used when you want to make replicas of the tables in different regions.\nOption D is invalid because this option is used only when you have throttling errors for the table.\nFor more information on DAX, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\n\nTo reduce the response time of DynamoDB read workloads from milliseconds to microseconds, the most preferable option would be to consider using DynamoDB Accelerator (DAX).\nDynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers microsecond response times for read-intensive workloads. DAX is designed to seamlessly integrate with existing DynamoDB applications without requiring any application code changes.\nOption A - Deploying an ElastiCache in front of DynamoDB may improve performance, but it is primarily used for caching frequently accessed data from the database to reduce the number of database calls. However, this would not provide the level of performance improvement required by the application, as it may not provide the sub-millisecond latency required by the application.\nOption B - Using DynamoDB Global Tables can help with scalability and high availability by replicating data across multiple AWS Regions, but it is not directly related to improving read response times in the order of microseconds.\nOption D - Using a higher throughput for the tables can help to increase the read and write capacity units, but it may not provide the required reduction in response time from milliseconds to microseconds.\nTherefore, the best option to reduce response time from milliseconds to microseconds is to use DynamoDB Accelerator (DAX), which provides a highly available, in-memory cache for read-intensive workloads with sub-millisecond response times.\n\n"
}, {
  "id" : 20,
  "question" : "Which of the following is true with respect to strongly consistent read requests from an application to a DynamoDB with a DAX cluster?\n",
  "answers" : [ {
    "id" : "517e52c542004fb4992e6aa516f2fa27",
    "option" : "All requests are forwarded to DynamoDB &amp; results are cached.",
    "isCorrect" : "false"
  }, {
    "id" : "94eca792db6c4200bc486491bd9d15e3",
    "option" : "All requests are forwarded to DynamoDB &amp; results are stored in Item Cache before passing to application.",
    "isCorrect" : "false"
  }, {
    "id" : "cd76d55979c745d69964df9a6eeabd72",
    "option" : "All requests are forwarded to DynamoDB &amp; results are stored in Query Cache before passing to application.",
    "isCorrect" : "false"
  }, {
    "id" : "3361c404859943bf961ccf9d389a0c27",
    "option" : "All requests are forwarded to DynamoDB &amp; results are not cached.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nFor strongly consistent read request from an application, DAX Cluster pass all request to DynamoDB &amp; does not cache for these requests.\nOption A is incorrect as Partly correct as for consistent read request from an application, DAX Cluster pass all requests to DynamoDB &amp; does not cache for these requests.\nOption B is incorrect as Only for GetItem and BatchGetItem eventual consistent read request, Data is stored in Item Cache.\nOption C is incorrect as Only for Query and Scan eventual consistent read request, Data is stored in Query Cache.\nFor more information on DAX for DynamoDB, refer to the following URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.html#DAX.concepts.item-cache\n\nDynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB that provides fast and easy access to data. When an application makes read requests to a DynamoDB table that is associated with a DAX cluster, DAX processes the requests and caches the responses for subsequent requests.\nWith respect to strongly consistent read requests from an application to a DynamoDB table with a DAX cluster, the following is true:\nAnswer: D. All requests are forwarded to DynamoDB, and results are not cached.\nExplanation: Strongly consistent read requests from an application to a DynamoDB table with a DAX cluster are always forwarded to the underlying DynamoDB table for processing. This is because strongly consistent reads require that the data returned is the most up-to-date and accurate version of the data. Therefore, it cannot be cached in the DAX cluster.\nIn contrast, eventually consistent reads can be cached in the DAX cluster because they are allowed to return data that may not be the most up-to-date version of the data. This can result in faster response times for applications that perform eventually consistent reads.\nTo summarize, DAX caches responses for eventually consistent read requests, but strongly consistent read requests are always forwarded to DynamoDB for processing and are not cached in DAX.\n\n"
}, {
  "id" : 21,
  "question" : "Your team lead has finished creating a build project in the console.\nYou have access to run the build but not to access the project.\nYou want to specify a different source location for the build.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "663848bc0adc4a249d598890cc6dd74a",
    "option" : "Issue the update project command and specify the new location of the build.",
    "isCorrect" : "false"
  }, {
    "id" : "5ccc05512d2f48eba19314393b0fce07",
    "option" : "Specify the new location of the build in a new buildspec.yml file and issue the update-project command.",
    "isCorrect" : "false"
  }, {
    "id" : "d8ddaa21162f4244a77e6508604918f4",
    "option" : "Specify the new location of the build in a new buildspec.yml file and use the start-build command.",
    "isCorrect" : "true"
  }, {
    "id" : "39e2899a222540c9a4320939c696f820",
    "option" : "Specify the new location of the build in a new buildspec.yml file and use the update-build command.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOptions A and B are incorrect since the question mentions that you don't have access to the project.\nOption D is incorrect since you have to use the start-build command.\nThe AWS Documentation mentions the following.\nTo override the default build spec file name, location, or both, do one of the following.\nRun the AWS CLI create-project or update-project command, setting the buildspec value to the path to the alternate build spec file relative to the value of the built-in environment variable CODEBUILD_SRC_DIR.\nYou can also do the equivalent with the create project operation in the AWS SDKs.\nFor more information, see Create a Build Project or Change a Build Project's Settings.\nRun the AWS CLI start-build command, setting the buildspecOverride value to the path to the alternate build spec file relative to the value of the built-in environment variable CODEBUILD_SRC_DIR.\nYou can also do the equivalent with the start build operation in the AWS SDKs.\nFor more information on the build specification for AWS CodeBuild, please refer to the below link-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\n\nThe correct answer is C. Specify the new location of the build in a new buildspec.yml file and use the start-build command.\nExplanation:\nAmazon Web Services (AWS) CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It is a powerful service that allows you to build, test, and deploy code quickly and easily.\nWhen you create a build project in AWS CodeBuild, you specify the source code location, build environment, build commands, and other parameters. However, if you want to specify a different source code location after the build project has been created, you need to update the buildspec.yml file and start a new build.\nThe buildspec.yml file is a configuration file that specifies the build commands and settings for your project. It is stored in the root directory of your source code repository. To specify a different source code location, you need to create a new buildspec.yml file that includes the new location.\nOnce you have created the new buildspec.yml file, you can start a new build using the start-build command in the AWS Command Line Interface (CLI) or in the AWS CodeBuild console. The start-build command initiates a new build using the updated buildspec.yml file.\nOption A is incorrect because the update project command is used to update the settings of the build project, not the source code location.\nOption B is partially correct because you need to create a new buildspec.yml file, but you don't need to issue the update-project command. Instead, you need to start a new build using the start-build command.\nOption D is incorrect because there is no update-build command in AWS CodeBuild.\n\n"
}, {
  "id" : 22,
  "question" : "You've just created an AWS Lambda function.\nYou're running the function, but the output of the function is not as expected.\nYou need to check and see what the issue is.\nWhich of the following can help the developer debug the issue with the Lambda function?\n",
  "answers" : [ {
    "id" : "d0eda58aba084abe9fb190d16fa78be7",
    "option" : "Check Cloudwatch logs",
    "isCorrect" : "true"
  }, {
    "id" : "17acf7b876534c9094bff4bbec27c1c3",
    "option" : "Check VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "2cee6309c33d474295163db5058ef1da",
    "option" : "Check AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "31cb6d45fc6a461a8ed214f2e78ef6b2",
    "option" : "Check AWS Inspector.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch.\nTo help you troubleshoot failures in a function, Lambda logs all requests handled by your function and automatically stores logs generated by your code through Amazon CloudWatch Logs.\nOption B is incorrect because VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.Option C is incorrect because AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices.\nIt cannot be used to debug Lambda functions.\nOption D is incorrect because Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.\nIt is not a troubleshooting tool for the Lambda function.\nFor more information on monitoring functions, please refer to the below link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-logs.html\n\nThe correct answer to this question is A. Check CloudWatch logs.\nAWS Lambda is a serverless compute service that allows developers to run code without having to manage servers. When a Lambda function runs, it generates logs that can help developers troubleshoot issues with their code. CloudWatch is a monitoring service provided by AWS that allows users to collect and track metrics, collect and monitor log files, and set alarms.\nBy default, Lambda functions write logs to CloudWatch. Developers can use the CloudWatch console to view logs for their Lambda functions. The logs include information such as the execution time of the function, any error messages generated by the function, and any output generated by the function.\nTo check the logs for a Lambda function, you can do the following:\nOpen the AWS Management Console and navigate to the CloudWatch service. Click on \"Logs\" in the left-hand navigation pane. Find the log group for your Lambda function, which will be named \"/aws/lambda/YourFunctionName\". Click on the log group to view the logs.\nOnce you have the logs open, you can search through them to find any error messages or other issues that might be causing the unexpected output from your Lambda function. This can help you debug the issue and make any necessary changes to your code.\nOptions B, C, and D are not relevant to debugging issues with a Lambda function. VPC Flow Logs are used to capture information about IP traffic going to and from network interfaces in a VPC. AWS Trusted Advisor provides recommendations for optimizing AWS resources and improving security and performance. AWS Inspector is a security assessment service that helps users test and evaluate the security of their applications deployed on AWS. While these services may be useful for other purposes, they are not directly relevant to debugging issues with a Lambda function.\n\n"
}, {
  "id" : 23,
  "question" : "You've been hired as a developer to work on an application.\nThis application is hosted on an EC2 Instance and interacts with an SQS queue.\nIt's been noticed that when the application is pulling messages, a lot of empty responses are being returned.\nWhat change can you make to ensure that the application uses the SQS queue effectively?\n",
  "answers" : [ {
    "id" : "94d7b293f91e4b5b94f9fe2e28ab914c",
    "option" : "Use long polling",
    "isCorrect" : "true"
  }, {
    "id" : "ed82c7aaf01341bcbaf55f1326340515",
    "option" : "Set a custom visibility timeout",
    "isCorrect" : "false"
  }, {
    "id" : "aeb2083095b94937b13fa90c99ea8953",
    "option" : "Use short polling",
    "isCorrect" : "false"
  }, {
    "id" : "465f17c75845471a8d71cbe8ce517649",
    "option" : "Implement exponential backoff.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nOption B is invalid because this is valid only for the processing time for the Messages.\nOption C is invalid because this would not be a cost-effective option.\nOption D is invalid because this is not a practice for SQS queues.\nThe AWS Documentation mentions the following.\nLong polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessagerequest) and false empty responses (when messages are available but aren't included in a response).\nFor more information on long polling in SQS, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html\n\nWhen an application interacts with an Amazon Simple Queue Service (SQS) queue, it can do so using either short polling or long polling. Short polling involves the application repeatedly sending requests to the SQS queue to check for new messages. This approach can result in a lot of empty responses being returned when there are no new messages, which can lead to inefficient use of system resources.\nOn the other hand, long polling involves the application sending a request to the SQS queue and waiting for a response for a specified period of time, known as the long poll timeout. If a new message arrives in the queue during this time, it is returned to the application immediately. If no new message arrives during this time, the SQS queue waits until the long poll timeout expires before returning an empty response. Long polling reduces the number of requests made by the application and can result in more efficient use of system resources.\nTherefore, option A - Use long polling - is the correct answer to this question, as it can help reduce the number of empty responses returned to the application when checking the SQS queue for new messages. Setting a custom visibility timeout (option B) can also be useful, as it determines how long a message remains invisible to other consumers after being retrieved by an application. However, it is not directly related to the issue of empty responses being returned.\nOption C - Use short polling - is not recommended in this scenario, as it can result in the application making many requests to the SQS queue and receiving a lot of empty responses. Finally, option D - Implement exponential backoff - can be useful in scenarios where an application is experiencing errors or timeouts when interacting with an SQS queue. However, it is not directly related to the issue of empty responses being returned.\n\n"
}, {
  "id" : 24,
  "question" : "Your team has currently developed an application using Docker containers.\nAs the development lead, you now need to host this application in AWS.\nYou also need to ensure that the AWS service has orchestration services built-in.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "966928b7a964472db1383d6164dde44d",
    "option" : "Consider building a Kubernetes cluster on EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "55c7b68cc2c24a2eb67bd2d635c817e9",
    "option" : "Consider building a Kubernetes cluster on your on-premise infrastructure.",
    "isCorrect" : "false"
  }, {
    "id" : "d1327205672a4baba1ca438b32af9c72",
    "option" : "Consider using the Elastic Container Service.",
    "isCorrect" : "true"
  }, {
    "id" : "3afeca8efa8b478d8b495033d3b16922",
    "option" : "Consider using the Simple Storage service to store your docker containers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation also mentions the following.\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster.\nYou can host your cluster on a serverless infrastructure that Amazon ECS manages by launching your services or tasks using the Fargate launch type.\nYou can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type for more control.\nOptions A and B are invalid since these would involve additional maintenance activities.\nOption D is incorrect since this is Object-based storage.\nFor more information on the Elastic Container service, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n\nWhen it comes to hosting containerized applications in AWS, there are several services to choose from. However, the requirement here is to use an AWS service that has orchestration services built-in.\nA. Building a Kubernetes cluster on EC2 instances:\nAmazon Elastic Kubernetes Service (EKS) is a fully managed Kubernetes service provided by AWS that enables you to easily run Kubernetes on the AWS cloud. EKS runs Kubernetes control plane instances for you automatically, which are responsible for managing the Kubernetes API and scheduling workloads, and you can run worker nodes in your own Amazon EC2 instances.\nEKS is a good option if you want to manage your own Kubernetes clusters in AWS. You will need to set up and manage the Kubernetes control plane instances, and configure and manage the worker nodes yourself.\nB. Building a Kubernetes cluster on your on-premise infrastructure:\nAmazon EKS-Distro is a Kubernetes distribution that you can use to create and operate Kubernetes clusters on-premises or in your own data center. You can use EKS-Distro to run Kubernetes workloads in your own infrastructure while still leveraging AWS's managed Kubernetes services, like EKS.\nThis option is only suitable if you want to host your Kubernetes cluster on-premises, but still want to benefit from AWS's managed Kubernetes services.\nC. Using the Elastic Container Service (ECS):\nAmazon Elastic Container Service (ECS) is a fully-managed container orchestration service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS uses AWS Fargate or EC2 instances to run your containers, and automatically manages the availability, scalability, and security of your applications.\nECS is a good option if you want to quickly get started with hosting containerized applications in AWS, and do not want to manage the underlying infrastructure. ECS offers features such as automatic scaling, load balancing, and auto-recovery, making it an ideal option for running production workloads.\nD. Using the Simple Storage Service (S3) to store your Docker containers:\nAmazon Simple Storage Service (S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. You can use S3 to store your Docker images and other artifacts that your application needs to run.\nWhile S3 can be used to store Docker containers, it does not provide any orchestration services for running containers. This option is only suitable if you already have a container orchestration platform in place, and just need a place to store your container images.\nIn conclusion, option C (Elastic Container Service) is the best option for hosting containerized applications in AWS with built-in orchestration services, as it provides a fully managed container orchestration service that can run containers using AWS Fargate or EC2 instances, and automatically manages the availability, scalability, and security of your applications.\n\n"
}, {
  "id" : 25,
  "question" : "You are developing an application that is working with a DynamoDB table.\nDuring the development phase, you want to know how much of the Consumed capacity is being used for the queries being fired.\nHow can this be achieved?\n",
  "answers" : [ {
    "id" : "42d2d7c432184296bc1813fc2d39e2dc",
    "option" : "The queries by default sent via the program will return the consumed capacity as part of the result.",
    "isCorrect" : "false"
  }, {
    "id" : "5ad90ad584584724a354d3d0e9d3e06b",
    "option" : "Ensure to set the ReturnConsumedCapacity in the query request to TRU.",
    "isCorrect" : "false"
  }, {
    "id" : "7688e9aa2ae446dda8c24d3304e23312",
    "option" : "Ensure to set the ReturnConsumedCapacity in the query request to TOTAL.",
    "isCorrect" : "true"
  }, {
    "id" : "bf8fbff3aaac4fce99c822b22e95a89c",
    "option" : "Use the Scan operation instead of the query operation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - C.\nThe AWS Documentation mentions the following.\nBy default, a Query operation does not return any data on how much read capacity it consumes.\nHowever, you can specify the ReturnConsumedCapacity parameter in a Query request to obtain this information.\nThe following are the valid settings for ReturnConsumedCapacity.\nNONE-no consumed capacity data is returned.\n(This is the default).\nTOTAL-the response includes the aggregate number of read capacity units consumed.\nINDEXES-the response shows the aggregate number of read capacity units consumed, together with the consumed capacity for each table and index that was accessed.\nBecause of what the AWS Documentation mentions, all other options are invalid.\nFor more information on the Query operation, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html\n\nThe correct answer is B. Ensure to set the ReturnConsumedCapacity in the query request to TRUE.\nWhen working with DynamoDB tables, every operation consumes some amount of capacity, which is measured in Capacity Units (CU). Capacity Units are calculated based on the size of the item being read/written, and the consistency level required. The capacity consumed can be measured in three ways:\nNone: The capacity consumed is not returned as part of the response. Total: The total capacity consumed is returned as part of the response. Indexes: The capacity consumed by the global secondary indexes (if any) is returned as part of the response.\nTo get the consumed capacity for a query, you need to set the ReturnConsumedCapacity parameter to TRUE. This will return the consumed capacity for the query in the response. For example:\njavascript Copy code var params = {     TableName: 'my-table',     KeyConditionExpression: 'partitionKey = :pk',     ExpressionAttributeValues: {         ':pk': 'my-partition-key'     },     ReturnConsumedCapacity: 'TOTAL' };  docClient.query(params, function(err, data) {     if (err) {         console.log('Error: ', err);     } else {         console.log('Consumed Capacity: ', data.ConsumedCapacity);     } });\nIn the above example, we are querying the 'my-table' table for items with partition key 'my-partition-key', and setting the ReturnConsumedCapacity parameter to 'TOTAL'. This will return the total consumed capacity for the query in the 'ConsumedCapacity' field of the response.\nOption A is incorrect because the queries by default do not return consumed capacity as part of the result.\nOption C is incorrect because 'TOTAL' is the correct value to use for the ReturnConsumedCapacity parameter.\nOption D is incorrect because the Scan operation scans the entire table and consumes a lot of capacity. It should only be used when necessary, as it can be very expensive.\n\n"
}, {
  "id" : 26,
  "question" : "Your development team is planning on working with Amazon Step Functions.\nWhich of the following is a recommended practice when working with activity workers and tasks in Step Functions? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "95d78a1e4ee34b35ae9e4871e75b623b",
    "option" : "Ensure to specify a timeout in state machine definitions.",
    "isCorrect" : "true"
  }, {
    "id" : "7012e57092914b61ab16db8bfadb37aa",
    "option" : "We can use only 1 transition per state.",
    "isCorrect" : "false"
  }, {
    "id" : "ab1c6657f6a7442a8adf7207d0866b96",
    "option" : "If you are passing larger payloads between states, consider using the Simple Storage Service.",
    "isCorrect" : "true"
  }, {
    "id" : "171e6d842af74e6fb390098f3a9e8d59",
    "option" : "If you are passing larger payloads between states, consider using EBS volumes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS Documentation mentions the following.\nBy default, the Amazon States Language doesn't set timeouts in state machine definitions.\nWithout an explicit timeout, Step Functions often relies solely on a response from an activity worker to know that a task is complete.\nIf something goes wrong and TimeoutSeconds isn't specified, an execution is stuck waiting for a response that will never come.\nExecutions that pass large payloads of data between states can be terminated.\nIf the data you pass between states might grow to over 32 KB, use Amazon Simple Storage Service (Amazon S3) to store the data, and pass the Amazon Resource Name instead of the raw data.\nAlternatively, adjust your implementation so that you pass smaller payloads in your executions.\nOption B is incorrect since States can have multiple incoming transitions from other states.\nBecause the documentation clearly mentions the best practices, the other options are invalid.\nFor more information on the best practices, please refer to the below URL-\nhttps://docs.aws.amazon.com/step-functions/latest/dg/sfn-best-practices.html\n\nAmazon Step Functions is a serverless workflow service that allows you to coordinate distributed applications and microservices using visual workflows. As part of using Step Functions, there are certain recommended practices to follow when working with activity workers and tasks.\nThe recommended practices for working with activity workers and tasks in Step Functions are:\nEnsure to specify a timeout in state machine definitions: When defining state machines in Step Functions, it is recommended to specify a timeout value for each state. This helps ensure that the workflow progresses in a timely manner, and if a task takes too long to complete, the state machine will time out and move on to the next state. If you are passing larger payloads between states, consider using the Simple Storage Service: When passing large payloads between states in Step Functions, it is recommended to use Amazon Simple Storage Service (S3). S3 provides durable object storage and allows you to store and retrieve large amounts of data reliably and at scale. By storing payloads in S3, you can reduce the size of the state machine definition and simplify its maintenance.\nOption B, \"We can use only 1 transition per state,\" is not a recommended practice when working with activity workers and tasks in Step Functions. In fact, each state can have multiple transitions, which allows you to define more complex workflows with branching and decision points.\nOption D, \"If you are passing larger payloads between states, consider using EBS volumes,\" is not a recommended practice either. EBS volumes are not designed for storing large payloads, and they are not as scalable or durable as S3. Therefore, S3 is the recommended storage service for passing large payloads between states in Step Functions.\n\n"
}, {
  "id" : 27,
  "question" : "Your team has started configuring CodeBuild to run builds in AWS.\nThe source code is stored in a bucket.\nWhen the build is run, you are getting the below error. Error: \"The bucket you are attempting to access must be addressed using the specified endpoint...\" When Running a Build. Which of the following could be the cause of the error?\n",
  "answers" : [ {
    "id" : "3f7eaa583fa241d5a888c84ae553679b",
    "option" : "The bucket is not in the same region as the Code Build project.",
    "isCorrect" : "true"
  }, {
    "id" : "628e124b02e34fd498a24cffdbd32e5c",
    "option" : "Code should ideally be stored on EBS Volumes.",
    "isCorrect" : "false"
  }, {
    "id" : "b44f6af794d94dd1b5b28b0270eece1a",
    "option" : "Versioning is enabled for the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "c28a0e445dd74c7ca5d7d3b881265811",
    "option" : "MFA is enabled on the bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis error is specified in the AWS Documentation.\nBecause the error is clearly mentioned, all other options are invalid.\nFor more information on troubleshooting Code Builds, please refer to the below URL-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html\n\n\nThe error message \"The bucket you are attempting to access must be addressed using the specified endpoint...\" typically indicates that there is a mismatch between the region where the CodeBuild project is running and the region where the S3 bucket is located. The correct answer is therefore option A, \"The bucket is not in the same region as the Code Build project.\"\nWhen you create an S3 bucket, it is created in a specific AWS region. AWS services are designed to be region-specific, so when you try to access an S3 bucket from another region, you may encounter errors like the one above.\nTo fix this error, you should ensure that the S3 bucket and the CodeBuild project are located in the same region. You can either move the bucket to the same region as the CodeBuild project or create a new bucket in the same region and copy the source code to it.\nOption B, \"Code should ideally be stored on EBS Volumes,\" is not related to the error message above. EBS volumes are used to store data for EC2 instances, and they are not used by CodeBuild.\nOption C, \"Versioning is enabled for the bucket,\" and option D, \"MFA is enabled on the bucket,\" are also not related to the error message above. Versioning is a feature of S3 that allows you to keep multiple versions of an object in the same bucket, and MFA (multi-factor authentication) is a security feature that requires an additional factor (such as a code sent to your phone) to access the bucket. These features may be useful for certain use cases, but they are not causing the error message above.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 29,
  "question" : "A company has hired you for its on-going development project.\nThe project entails streaming data onto Amazon Kinesis streams from various log sources.\nYou need to analyze real-time data using standard SQL.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "ffbf2757554c4461b36f00c94ef1bdd8",
    "option" : "Amazon Kinesis Firehose",
    "isCorrect" : "false"
  }, {
    "id" : "50bb283c869c48d687717d55c977621e",
    "option" : "Amazon Kinesis Data Analytics",
    "isCorrect" : "true"
  }, {
    "id" : "94b1d1b8128e4f93ad64d16cb06c0770",
    "option" : "Amazon Athena",
    "isCorrect" : "false"
  }, {
    "id" : "a57ed172ee2c43669b058dbe050332e1",
    "option" : "Amazon EMR.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nOption A is incorrect since it delivers real-time streaming data to destinations such as Amazon Simple Storage Service.\nThis is mentioned in the AWS Documentation.\nWith Amazon Kinesis Data Analytics, you can process and analyze streaming data using standard SQL.\nThe service enables you to quickly author and run powerful SQL code against streaming sources to perform time-series analytics, feed real-time dashboards, and create real-time metrics.\nOption A is incorrect since it delivers real-time streaming data to destinations such as Amazon Simple Storage Service.\nOption C is incorrect since this is used to make it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.\nOption D is incorrect since this is used as a fully managed service for Big Data.\nFor more information on Kinesis Data Analytics, please visit the following URL-\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html\n\nThe correct answer is B. Amazon Kinesis Data Analytics.\nAmazon Kinesis is a managed service that makes it easy to collect, process, and analyze real-time, streaming data. It can be used to stream data from various sources like logs, clickstreams, social media feeds, and more. Amazon Kinesis has three main services: Kinesis Data Streams, Kinesis Firehose, and Kinesis Data Analytics.\nAmazon Kinesis Firehose is a service that can be used to load streaming data into data stores like S3, Redshift, or Elasticsearch. It doesn't provide real-time analytics capabilities.\nAmazon Athena is a serverless interactive query service that can be used to analyze data stored in S3 using SQL. It is not well-suited for real-time data analysis since it is designed to work with data that is already stored in S3.\nAmazon EMR is a managed Hadoop framework that can be used to process large amounts of data using Hadoop, Spark, or other big data frameworks. It is not well-suited for real-time data analysis since it is designed for batch processing of large data sets.\nAmazon Kinesis Data Analytics is a fully managed service that enables real-time processing of streaming data using standard SQL. It can be used to analyze and process data in real-time, and it provides several built-in functions and libraries for performing common streaming data processing tasks like filtering, aggregating, and joining. Kinesis Data Analytics can also be integrated with other AWS services like Lambda, S3, DynamoDB, and Elasticsearch to build end-to-end streaming data pipelines.\nIn summary, Amazon Kinesis Data Analytics is the best service for analyzing real-time data using standard SQL.\n\n"
}, {
  "id" : 30,
  "question" : "You are a developer for a company.\nYou have to develop an application that would transfer the logs from several EC2 Instances to an S3 bucket.\nWhich of the following would you use for this purpose?\n",
  "answers" : [ {
    "id" : "9dc2d94c5bf345d2bbe77baf450e959b",
    "option" : "AWS Database Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "96f2b5c39bae484ea4c9ca7419dc06e2",
    "option" : "AWS Athena",
    "isCorrect" : "false"
  }, {
    "id" : "f6a5ff93ad254b688b179cce89f216f8",
    "option" : "AWS Data Pipeline",
    "isCorrect" : "true"
  }, {
    "id" : "862092b3e4d34696a2cfc9368cb40547",
    "option" : "AWS EMR.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is mentioned in the AWS Documentation.\nAWS Data Pipeline is a web service that you can use to automate data movement and transformation.\nWith AWS Data Pipeline, you can define data-driven workflows so that tasks can depend on the successful completion of previous tasks.\nYou define the parameters of your data transformations, and AWS Data Pipeline enforces the logic that you've set up.\nOption A is incorrect since this is used specifically for migrating databases.\nOption B is incorrect since this is used for performing SQL queries in data stored on S3.\nOption D is incorrect since this is used for Big data applications.\nFor more information on AWS Pipeline, please visit the following URL-\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\n\n\nThe correct answer is C. AWS Data Pipeline.\nAWS Data Pipeline is a web service that enables you to schedule regular data movement and data processing activities in the AWS environment. It can help to transfer data between different AWS services, such as EC2 instances and S3 buckets. It is a perfect solution for handling logs because it supports a variety of data sources and targets, including logs, databases, and AWS services.\nAWS Database Migration Service is a tool that helps to migrate databases to AWS from on-premises or other cloud platforms. It is not suitable for transferring logs from EC2 instances to S3 buckets.\nAWS Athena is an interactive query service that enables you to query data stored in S3 using standard SQL. It is not designed for data transfer but rather for querying data.\nAWS EMR is a managed big data platform that allows you to process and analyze vast amounts of data using Hadoop, Spark, or other frameworks. It is not suitable for transferring logs from EC2 instances to S3 buckets.\nTherefore, AWS Data Pipeline is the most appropriate service for transferring logs from several EC2 instances to an S3 bucket.\n\n"
}, {
  "id" : 31,
  "question" : "You have created REST API using API Gateway.\nWhich of the following mechanism can be used to deny specific IP Addresses from accessing API Gateway? Select 2 correct options.\n",
  "answers" : [ {
    "id" : "8babb02b5f6a47fbb5079092777c393d",
    "option" : "AWS KMS",
    "isCorrect" : "false"
  }, {
    "id" : "1e3992f1409d4084a274a29535ba8c24",
    "option" : "AWS Security Center",
    "isCorrect" : "false"
  }, {
    "id" : "969a0386f3fc46dfa9f94fb039ef0d36",
    "option" : "Resource Policies",
    "isCorrect" : "true"
  }, {
    "id" : "e2ecd4f53a25412e87ffcfd1da56db16",
    "option" : "AWS WAF.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - C and D.\nOption A is incorrect as AWS Key Management Service (KMS) is an encryption and key management service scaled for the cloud.\nOption B is incorrect as AWS Security Center is a central location from which you can receive security updates and where you can report any security concerns.\nOption C is correct as Resource Policies for API Gateway allows you to deny or permit a specific IP address from where API Gateway can be accessed.\nOption D is correct because you can use AWS WAF for your Amazon API Gateway APIs to protect from attacks such as SQL injection and Cross-Site Scripting (XSS)\nAdditionally, you can filter web requests based on IP address, geographic area, request size, and/or string or regular expression patterns using the rules.\nPlease refer the below link.\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/amazon-api-gateway-adds-support-for-aws-waf/\n\nThe correct options for denying specific IP addresses from accessing API Gateway are C. Resource Policies and D. AWS WAF.\nC. Resource Policies: Resource policies can be used to restrict access to API Gateway APIs based on IP addresses or IP address ranges. Resource policies are JSON policy documents that can be attached to the API Gateway methods, resources, and stages. You can use IP-based condition operators such as aws:SourceIp and aws:SourceIpV4 to specify the IP addresses that are allowed or denied access to the API.\nD. AWS WAF: AWS WAF (Web Application Firewall) is a web application firewall service that can be used to protect web applications from common web exploits. You can use AWS WAF to block IP addresses or IP address ranges that are known to be malicious or suspicious. AWS WAF can be integrated with API Gateway using an AWS Lambda function. The Lambda function can inspect incoming requests and decide whether to allow or block the requests based on the configured WAF rules.\nA. AWS KMS: AWS KMS (Key Management Service) is a managed service that can be used to create and manage encryption keys. AWS KMS is not related to denying specific IP addresses from accessing API Gateway.\nB. AWS Security Center: AWS Security Center is a security service that provides a centralized view of security alerts and compliance status across an AWS account. AWS Security Center is not related to denying specific IP addresses from accessing API Gateway.\n\n"
}, {
  "id" : 32,
  "question" : "A Developer is migrating an on-premises web application to the AWS Cloud.\nThe application currently runs on a 32-processor server and stores session state in memory.\nOn Friday afternoons the server runs at 75% CPU utilization, but only about 5% CPU utilization at other times. How should the Developer change to code to better take advantage of running in the cloud?\n",
  "answers" : [ {
    "id" : "5a21f288b0844007b16657e8aafcb47a",
    "option" : "Compress the session state data in memory.",
    "isCorrect" : "false"
  }, {
    "id" : "a9ab6decbaab4d53935892ea649b2122",
    "option" : "Store session state on EC2 instance Store.",
    "isCorrect" : "false"
  }, {
    "id" : "3705058a6d794026a241b5283959c4a1",
    "option" : "Encrypt the session state data in memory.",
    "isCorrect" : "false"
  }, {
    "id" : "61a471b8d29742798646491c9041c20a",
    "option" : "Store session state in an ElastiCache cluster.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nElastiCache is the perfect solution for managing session state.\nThis is also given in the AWS Documentation.\nIn order to address scalability and to provide a shared data storage for sessions that can be accessed from any individual web server, you can abstract the HTTP sessions from the web servers themselves.\nA common solution to this is to leverage an In-Memory Key/Value store such as Redis and Memcached.\nOption A is incorrect since compression is not the ideal solution.\nOption B is incorrect since EC2 Instance Store is too volatile.\nOption C is incorrect since this is ok from a security standpoint but will just make the performance worse for the application.\nFor more information on Session Management, please refer to the below Link-\nhttps://aws.amazon.com/caching/session-management/\n\nThe optimal solution for storing session state data in the AWS Cloud depends on the specific needs of the application. However, in general, the best option is to use a scalable and highly available session state storage service.\nOption A, compressing the session state data in memory, may help reduce memory usage, but it does not address the issue of scalability and availability.\nOption B, storing session state on EC2 instance store, is not a recommended option because the instance store is ephemeral storage and can be lost if the instance fails or terminates. It is not a highly available storage solution.\nOption C, encrypting the session state data in memory, may be a good security practice, but it does not address the scalability and availability requirements of the application.\nOption D, storing session state in an ElastiCache cluster, is the recommended option for session state storage in the AWS Cloud. Amazon ElastiCache is a managed service that provides scalable and highly available in-memory data storage. By using ElastiCache, the application can take advantage of the distributed nature of the service and scale horizontally to handle the increased load on Friday afternoons. ElastiCache also provides features such as automatic failover and data replication across multiple Availability Zones, ensuring high availability of the data.\nTherefore, the correct answer is D - Store session state in an ElastiCache cluster.\n\n"
}, {
  "id" : 33,
  "question" : "An organization's application needs to monitor application-specific events with a standard AWS service.\nThe service should capture the number of logged-in users and trigger events accordingly.\nDuring peak times, monitoring frequency will occur every 10 seconds. What should be done to meet these requirements?\n",
  "answers" : [ {
    "id" : "7aab40407e80452bb07062bbf2ac10cf",
    "option" : "Create an Amazon SNS notification.",
    "isCorrect" : "false"
  }, {
    "id" : "81e9ff0ba4034928885329a3f7311a91",
    "option" : "Create a standard resolution custom Amazon CloudWatch log.",
    "isCorrect" : "false"
  }, {
    "id" : "af81e9b36aed45a8847271f6ca06993a",
    "option" : "Create a high-resolution custom Amazon CloudWatch metric.",
    "isCorrect" : "true"
  }, {
    "id" : "bc354cfd766a43a6b8725e0985904d88",
    "option" : "Create a custom Amazon CloudTrail log.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is clearly mentioned in the AWS Documentation.\nWhen creating an alarm, select a greater than or equal period to the frequency of the metric to be monitored.\nFor example, basic monitoring for Amazon EC2 provides metrics for your instances every 5 minutes.\nWhen setting the alarm on a basic monitoring metric, select a period of at least 300 seconds (5 minutes)\nDetailed monitoring for Amazon EC2 provides metrics for your instances every 1 minute.\nWhen setting the alarm on a detailed monitoring metric, select a period of at least 60 seconds (1 minute).\nIf you set the alarm on a high-resolution metric, you can specify a high-resolution alarm with a period of 10 seconds or 30 seconds, or you can set a regular alarm with a period of any multiple of 60 seconds.\nOption A is incorrect since the question does not mention anything on notifications.\nOption B is incorrect since the standard resolution counters will not help define triggers within a 10 second interval.\nOption D is incorrect since Cloudtrail is used for API Activity logging.\nFor more information on Cloudwatch metrics, please refer to the below Link-\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html\n\nTo monitor application-specific events with a standard AWS service, we have several options available. In this case, we need to capture the number of logged-in users and trigger events accordingly. Also, during peak times, monitoring frequency will occur every 10 seconds.\nOption A: Create an Amazon SNS notification Amazon Simple Notification Service (SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. It can be used to send notifications via email, text, or push notification. However, SNS is not suitable for capturing and storing metrics.\nOption B: Create a standard resolution custom Amazon CloudWatch log Amazon CloudWatch Logs is a fully managed service that makes it easy to collect, monitor, and analyze log files from EC2 instances, AWS CloudTrail, and other AWS services. CloudWatch Logs can capture and store any type of log data, including application logs. However, it is not designed to capture metrics and trigger events.\nOption C: Create a high-resolution custom Amazon CloudWatch metric Amazon CloudWatch Metrics is a fully managed service that provides monitoring and observability for resources and applications on AWS. With CloudWatch Metrics, you can collect and store metrics, set alarms, and automatically react to changes in your AWS resources. CloudWatch Metrics provides a wide range of predefined metrics for AWS services and applications, and you can also create custom metrics based on your specific needs. CloudWatch Metrics can capture metrics at a resolution of up to one second, which is suitable for monitoring application-specific events every 10 seconds.\nOption D: Create a custom Amazon CloudTrail log Amazon CloudTrail is a fully managed service that enables you to monitor, troubleshoot, and audit your AWS resources. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. However, CloudTrail is not designed to capture metrics and trigger events.\nTherefore, the correct option to capture the number of logged-in users and trigger events accordingly, with monitoring frequency during peak times of every 10 seconds, is to create a high-resolution custom Amazon CloudWatch metric (Option C).\n\n"
}, {
  "id" : 34,
  "question" : "Which of the following is the best way to update an S3 notification configuration when a change in the lambda version needs to be referred by an S3 bucket after new objects are created in the bucket?\n",
  "answers" : [ {
    "id" : "82555223246940c99c9c74ad24e0c4f8",
    "option" : "Specify Lambda Alias ARN in the notification configuration.",
    "isCorrect" : "true"
  }, {
    "id" : "e1d5c280573941f98ef821f207816d7a",
    "option" : "Specify Lambda Function ARN in the notification configuration.",
    "isCorrect" : "false"
  }, {
    "id" : "0c54e890c6e14274816fabe95a80ece0",
    "option" : "Specify Lambda Qualified ARN in the notification configuration.",
    "isCorrect" : "false"
  }, {
    "id" : "a163da8e5bb94d51a18d4949ad2d50e4",
    "option" : "Specify Lambda $LATEST ARN in the notification configuration.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nWhen a Lambda Alias ARN is used in the notification configuration &amp; a new version of the Lambda function is created, you need to update Alias ARN pointing to a new function.\nNo changes are required to be done at the Amazon S3 bucket.\nOption B is incorrect.\nWhen a Lambda Function ARN is used in the notification configuration, whenever there is a new Lambda Function created, you will need to update ARN in notification configuration to point to the latest version.\nOption C is incorrect when a Lambda Qualified ARN is used in the notification configuration.\nWhenever there is a new Lambda Function created, you will need to update ARN in the notification configuration to point to the latest version.\nOption D is incorrect as there is nothing as $LATEST ARN.\n$LATEST version has two ARN associated with it, Qualified ARN &amp; Unqualified ARN.\nFor more information on AWS Lambda Aliases, refer to the following URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html\n\nThe correct answer is C. Specify Lambda Qualified ARN in the notification configuration.\nWhen an S3 bucket event occurs, S3 can send a notification to an AWS Lambda function to invoke it. To specify the Lambda function to invoke, you need to provide an Amazon Resource Name (ARN) that identifies the function.\nThere are four different types of Lambda ARNs:\nLambda Function ARN (e.g. arn:aws:lambda:us-east-1:123456789012:function:my-function) Lambda Version ARN (e.g. arn:aws:lambda:us-east-1:123456789012:function:my-function:1) Lambda Alias ARN (e.g. arn:aws:lambda:us-east-1:123456789012:function:my-function:PROD) Lambda $LATEST ARN (e.g. arn:aws:lambda:us-east-1:123456789012:function:my-function:$LATEST)\nThe Lambda Function ARN is the ARN of the specific version of the Lambda function that you want to invoke. If you specify a Lambda Function ARN in the S3 notification configuration and then create a new version of the function, the S3 notification configuration will continue to invoke the old version of the function until you manually update the notification configuration.\nThe Lambda Version ARN is the ARN of a specific version of the Lambda function. If you specify a Lambda Version ARN in the S3 notification configuration and then create a new version of the function, the S3 notification configuration will continue to invoke the same version of the function. You need to manually update the S3 notification configuration to invoke the new version of the function.\nThe Lambda Alias ARN is the ARN of a named alias for a Lambda function. If you specify a Lambda Alias ARN in the S3 notification configuration and then update the alias to point to a new version of the function, the S3 notification configuration will automatically start invoking the new version of the function.\nThe Lambda $LATEST ARN is the ARN of the most recent version of the Lambda function. If you specify a Lambda $LATEST ARN in the S3 notification configuration and then create a new version of the function, the S3 notification configuration will continue to invoke the old version of the function until you manually update the notification configuration.\nTherefore, the best way to update an S3 notification configuration when a change in the lambda version needs to be referred by an S3 bucket after new objects are created in the bucket is to specify the Lambda Qualified ARN in the notification configuration. A Lambda Qualified ARN includes both the function version number and the function alias, so when you update the alias to point to a new version of the function, the S3 notification configuration will automatically start invoking the new version of the function.\n\n"
}, {
  "id" : 35,
  "question" : "A developer working on an AWS CodeBuild project wants to override a build command as part of a build run to test a change.\nThe developer has access to run the builds but does not have access to the code and to edit the CodeBuild project. What process should the Developer use to override the build command?\n",
  "answers" : [ {
    "id" : "5b3e88e7151e4f6eb73fd6752852aa67",
    "option" : "Update the buildspec.yml configuration file that is part of the source code and run a new build.",
    "isCorrect" : "false"
  }, {
    "id" : "a47be345211a40fa8e9da8bc02f44d51",
    "option" : "Update the command in the Build Commands section during the build run in the AWS console.",
    "isCorrect" : "false"
  }, {
    "id" : "0078b06ac76a4c39ae6515f592645d4c",
    "option" : "Run the start build AWS CLI command with buildspecOverride property set to the new buildspec.yml file.",
    "isCorrect" : "true"
  }, {
    "id" : "56c39bcf5cbb4d94bd9423c41f710624",
    "option" : "Update the buildspec property in the StartBuild API to override the build command during build run.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nUse the AWS CLI command to specify different parameters that need to be run for the build.\nSince the developer can run the build, he can run the build by changing the parameters from the command line.\nThe same is also mentioned in the AWS Documentation.\nOption A is incorrect because A buildspec is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build.\nYou can include a buildspec as part of the source code, or you can define a buildspec when you create a build project.\nHowever, the developer does not have access to the code.\nOption B is incorrect because the developer does not have access to the code and to edit the CodeBuild project.\nOption D is incorrect because it should be the buildspecOverride property instead of buildspec property.\nFor more information on running the command via the CLI, please refer to the below Link-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/run-build.html#run-build-cli\nNote:\nAs per the question, the developer doesn't have access to edit the code build project but only have access to run the build.\nSo for overriding the existing build we can make use of the BuildspecOverride attribute.\nAs per AWS,\nbuildspecOverride.\n: Optional string.\nA build spec declaration that overrides for this build the one defined in the build project.\nIf this value is set, it can be either an inline build spec definition or the path to an alternate build spec file relative to the value of the built-in\nCODEBUILD_SRC_DIR.\nenvironment variable.\n\n\nThe correct answer for this scenario is C - Run the start build AWS CLI command with buildspecOverride property set to the new buildspec.yml file.\nHere's an explanation of why the other options are not correct:\nA. Update the buildspec.yml configuration file that is part of the source code and run a new build. This option is not feasible because the developer does not have access to the code. Even if the developer had access to the code, updating the buildspec.yml file would require committing the change to the repository and triggering a new build run, which could take time and may not be desirable if the developer just needs to test a quick change.\nB. Update the command in the Build Commands section during the build run in the AWS console. This option is not feasible because the developer does not have access to edit the CodeBuild project. The Build Commands section can only be edited by users with permissions to modify the CodeBuild project.\nD. Update the buildspec property in the StartBuild API to override the build command during build run. This option is not entirely correct because the buildspec property in the StartBuild API is used to specify the location of the buildspec.yml file, but it does not allow for overriding the build commands. However, the buildspecOverride property in the StartBuild API can be used to specify a new buildspec.yml file to override the build commands.\nTherefore, the best option for the developer is to use the AWS CLI to start a new build run and pass the buildspecOverride property with the location of the new buildspec.yml file containing the updated build commands. This approach will allow the developer to quickly test their changes without having to commit any changes to the source code or have permissions to edit the CodeBuild project.\n\n"
}, {
  "id" : 36,
  "question" : "You are working on a new project involving the creation of Lambda functions for a Serverless Web Application.\nA large team of developers will work on this project performing multiple changes to the Lambda function code.\nAs the best practice, you have been asked to publish a version while creating a Lambda function or updating a function code.\nWhich of the following is TRUE with regards to the published version of a Lambda Function?\n",
  "answers" : [ {
    "id" : "1c9e2616da0247ad9b44ff5cb905be2c",
    "option" : "A published version is a snapshot of function code and has the same version number when a previous version is deleted &amp; recreated.",
    "isCorrect" : "false"
  }, {
    "id" : "fda8158683cd4f9293263de361cd1dde",
    "option" : "A published version is a snapshot of function code and has a unique Amazon Resource Name that can be updated using “UpdateFunctionCode.”",
    "isCorrect" : "false"
  }, {
    "id" : "d0b8a7c0b9a449df98a65a075443036e",
    "option" : "A published version is a snapshot of function code and configuration that can be updated using the “Publish” parameter.",
    "isCorrect" : "false"
  }, {
    "id" : "5ecf128a4605488391d87d408e0f38e2",
    "option" : "A published version is a snapshot of function code and configuration that can`t be changed.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nA Publish version is a snapshot copy of a Lambda Function code &amp; configuration in $LATEST version.\nNo configuration changes can be done to a published version &amp; it has a unique ARN which can't be modified.\nOption A is incorrect as a published version can only have a different version from previous ones, even when the previous version is deleted &amp; recreated.\nOption B is incorrect as after a version is published, ARN can't be modified.\nOption C is incorrect as when a version is published.\nNo configuration changes can be made to this version.\nFor more information on AWS Lambda Versioning, refer to the following URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html\n\nSure! When you create or update a Lambda function, it is a good practice to publish a version of that function. A published version is a snapshot of the function code and its configuration at a specific point in time. This allows you to make changes to the function without affecting any existing applications that rely on a particular version of the function. You can use the version number to refer to a specific version of the function when you create or update your application.\nNow let's go through each answer choice and see which one is true:\nA. A published version is a snapshot of function code and has the same version number when a previous version is deleted & recreated. This statement is incorrect. When you delete and recreate a function, the version number of the newly created function will be different from the deleted version.\nB. A published version is a snapshot of function code and has a unique Amazon Resource Name that can be updated using “UpdateFunctionCode.” This statement is also incorrect. The Amazon Resource Name (ARN) of a published version cannot be updated using the UpdateFunctionCode API. The ARN of a function version is fixed and includes the version number.\nC. A published version is a snapshot of function code and configuration that can be updated using the “Publish” parameter. This statement is partly correct. When you create or update a function, you can specify a Publish parameter to create a new version of the function. The new version will have a unique version number and a fixed ARN that includes the version number. However, once a version is published, it cannot be updated. If you want to make changes to the function code or configuration, you need to create a new version.\nD. A published version is a snapshot of function code and configuration that can't be changed. This statement is mostly correct. Once a version is published, it cannot be modified. If you want to make changes to the function code or configuration, you need to create a new version.\nTherefore, the correct answer is D: A published version is a snapshot of function code and configuration that can't be changed.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 38,
  "question" : "A developer is building an application that needs access to an S3 bucket.\nAn IAM role is created with the required permissions to access the S3 bucket.\nWhich API call should the Developer use in the application so that the code can access to the S3 bucket?\n",
  "answers" : [ {
    "id" : "02a529ab1b9e47119fa0bd541dd14f90",
    "option" : "IAM: AccessRole",
    "isCorrect" : "false"
  }, {
    "id" : "b7a57cb2b32f4443bd872ec7a6705af6",
    "option" : "STS: GetSessionToken",
    "isCorrect" : "false"
  }, {
    "id" : "9727f0bfdc004127a3139c4db0435b65",
    "option" : "IAM:GetRoleAccess",
    "isCorrect" : "false"
  }, {
    "id" : "690219ef4e944dab8d1d9062d68cf102",
    "option" : "STS:AssumeRole.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThis is given in the AWS Documentation.\nA role specifies a set of permissions that you can use to access AWS resources.\nIn that sense, it is similar to an IAM user.\nAn application assumes a role to receive permissions to carry out required tasks and interact with AWS resources.\nThe role can be in your own account or any other AWS account.\nFor more information about roles, their benefits, and how to create and configure them, see IAM Roles, and Creating IAM Roles.\nTo learn about the different methods that you can use to assume a role, see Using IAM Roles.\nImportant.\nThe permissions of your IAM user and any roles that you assume are not cumulative.\nOnly one set of permissions is active at a time.\nWhen you assume a role, you temporarily give up your previous user or role permissions and work with the permissions assigned to the role.\nWhen you exit the role, your user permissions are automatically restored.\nTo assume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use.\nWhen you call AssumeRole, you can optionally pass a JSON policy.\nThis allows you to restrict permissions for that for the role's temporary credentials.\nThis is useful when you need to give the temporary credentials to someone else.\nThey can use the role's temporary credentials in subsequent AWS API calls to access resources in the account that owns the role.\nYou cannot use the passed policy to grant permissions that are in excess of those allowed by the permissions policy of the role that is being assumed.\nTo learn more about how AWS determines the effective permissions of a role, see Policy Evaluation Logic.\nOption A is incorrect because IAM does not have this API.\nOption B is incorrect because STS:GetSessionToken is used if you want to use MFA to protect programmatic calls to specific AWS API operations like Amazon EC2 StopInstances.\nMFA-enabled IAM users would need to call GetSessionToken and submit an MFA code associated with their MFA device.\nOption C is incorrect because IAM does not have this API.\nFor more information on switching roles, please refer to the below Link-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-api.html\n\nThe correct answer is D. STS:AssumeRole.\nExplanation: When a developer creates an IAM role with permissions to access an S3 bucket, they can use the AWS Security Token Service (STS) AssumeRole API to obtain temporary security credentials that can be used to access the S3 bucket.\nThe AssumeRole API allows an IAM user or an AWS service to assume a role temporarily to perform specific actions on AWS resources. When an IAM user or an AWS service assumes a role, AWS returns temporary security credentials that can be used to access resources that are allowed by the role.\nUsing the temporary security credentials obtained through AssumeRole, the developer can then access the S3 bucket using the S3 API.\nA. IAM: AccessRole is not a valid API call.\nB. STS: GetSessionToken returns temporary credentials for an IAM user. It is not used to assume a role.\nC. IAM:GetRoleAccess is not a valid API call.\nD. STS:AssumeRole is the correct API call to use when assuming an IAM role to obtain temporary security credentials.\n\n"
}, {
  "id" : 39,
  "question" : "A developer has recently deployed an AWS Lambda function that computes a Fibonacci sequence using recursive Lambda invocations.\nA pre-defined AWS IAM policy is being used for this function, and only the required dependencies were packaged.\nA few days after deployment, the Lambda function is being throttled. What should the Developer have done to prevent this, according to best practices?\n",
  "answers" : [ {
    "id" : "5c7770a247da472e9f261b878e4bc213",
    "option" : "Use more restrictive IAM policies.",
    "isCorrect" : "false"
  }, {
    "id" : "f77451f8c83b44eab319afb8f765ea31",
    "option" : "Avoid recursive Lambda invocations.",
    "isCorrect" : "true"
  }, {
    "id" : "b62833829a084a37976c5bbf455f7d5e",
    "option" : "Request a concurrency service limit increase.",
    "isCorrect" : "false"
  }, {
    "id" : "3469ebbce6e246328247fdc0b77abef8",
    "option" : "Increase the memory allocation range.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe question's focus is on the best practice methods for Lambda functions.\nSince the question asks us to choose the best option that the developer might have done to prevent this throttling issue, he should have written a code that avoids the recursive call of the function within itself as it is not recommended as a best practice.\nFor the \"Lambda function code\" best practice, it is recommended that we should avoid recursive code in the Lambda function.\n\"Avoid using recursive code in your Lambda function, wherein the function automatically calls itself until some arbitrary criteria are met.\nThis could lead to an unintended volume of function invocations and escalated costs.\nIf you do accidentally do so, set the function concurrent execution limit to '0' (Zero) immediately to throttle all invocations to the function, while you update the code.\"\nOption A is incorrect since using IAM Policies will not help in resolving the issue.\nOption C is incorrect since this is about concurrency on the number of AWS Lambda executions.\nOption D is incorrect since the issue here is with the number of executions and not on the amount of memory used for the executions.\nFor more information, please refer to the below Link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\n\nThe correct answer is C. Request a concurrency service limit increase.\nExplanation:\nAWS Lambda uses a concurrency model to limit the number of simultaneous function executions. Each function instance requires a certain amount of memory, and AWS Lambda allocates CPU power in proportion to the memory allocation. By default, AWS limits the number of concurrent executions across all functions within an AWS account.\nIn this scenario, the Lambda function is being throttled because it has exceeded the concurrency limit. The developer could request a concurrency service limit increase to prevent this from happening in the future.\nOption A, using more restrictive IAM policies, would not address the issue of concurrency throttling. IAM policies control access to AWS resources, but they do not affect the concurrency model.\nOption B, avoiding recursive Lambda invocations, is not always possible or practical. While it is generally a best practice to avoid recursion in Lambda functions, there may be cases where it is necessary or beneficial to use recursion.\nOption D, increasing the memory allocation range, would not directly address the issue of concurrency throttling. While increasing the memory allocation range may provide more CPU power to each function instance, it would not increase the overall concurrency limit.\n\n"
}, {
  "id" : 40,
  "question" : "A company is writing a Lambda function that will run in multiple stages, such a dev, test, and production.\nThe function is dependent upon several external services, and it must call different endpoints for these services based on the function's deployment stage. What Lambda feature will enable the developer to ensure that the code references the correct endpoints when running in each stage?\n",
  "answers" : [ {
    "id" : "1869657c1fb24c1a81da9bee2225c5a2",
    "option" : "Tagging",
    "isCorrect" : "false"
  }, {
    "id" : "b05111d6999b43b684443662715c821d",
    "option" : "Concurrency",
    "isCorrect" : "false"
  }, {
    "id" : "6d15f8295fcb4c3785426b196b75a79d",
    "option" : "Aliases",
    "isCorrect" : "false"
  }, {
    "id" : "32e3f5b27b344866a5d14f714579adf7",
    "option" : "Environment variables.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nYou can create different environment variables in the Lambda function that can be used to point to the different services.\nThe below screenshot from the AWS Documentation shows how this can be done with databases.\nOption A is invalid since this can only be used to add metadata for the function.\nOption B is invalid since this is used for managing the concurrency of execution.\nOption C is invalid since this is used for managing the different versions of your Lambda function.\nFor more information on AWS Lambda environment variables, please refer to the below Link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/env_variables.html\n\n\nThe correct answer is D. Environment variables.\nAWS Lambda is a serverless compute service that allows developers to run code in response to events without provisioning or managing servers. AWS Lambda functions can be deployed in multiple stages, such as development, testing, and production, and each stage can have its own configuration settings.\nIn this scenario, the Lambda function depends on several external services, and it needs to call different endpoints for these services based on the function's deployment stage. To accomplish this, the developer can use environment variables.\nEnvironment variables are key-value pairs that can be used to store configuration settings, such as API endpoint URLs, access keys, and secrets. Environment variables can be set at the function level or at the account level, and they can be accessed from within the Lambda function code.\nBy setting environment variables for each stage of the function, the developer can ensure that the function references the correct endpoints when running in each stage. For example, the developer could set the API endpoint URL for the development stage as an environment variable named DEV_API_ENDPOINT_URL, and set the API endpoint URL for the production stage as an environment variable named PROD_API_ENDPOINT_URL. The Lambda function code could then reference the appropriate environment variable to determine the correct API endpoint to call.\nIn summary, environment variables are a powerful feature of AWS Lambda that can be used to store configuration settings for each stage of a function's deployment. By using environment variables to store endpoint URLs and other settings, developers can ensure that their Lambda function code references the correct endpoints for each stage.\n\n"
}, {
  "id" : 41,
  "question" : "You are using AWS SAM to define a Lambda function and configure CodeDeploy to manage deployment patterns.\nWith the new Lambda function working as per expectation which of the following will shift traffic from the original Lambda function to the new Lambda function in the shortest time frame?\n",
  "answers" : [ {
    "id" : "1f45a226ddc043898e63d0718c9f6330",
    "option" : "Canary10Percent5Minutes",
    "isCorrect" : "true"
  }, {
    "id" : "ab6c00dbd83845e699cf803ed2a5d312",
    "option" : "Linear10PercentEvery10Minutes",
    "isCorrect" : "false"
  }, {
    "id" : "c3b98e11e7924828932f25c6adf05f1b",
    "option" : "Canary10Percent15Minutes",
    "isCorrect" : "false"
  }, {
    "id" : "fd43cefb47dd4973a5679494c873d93d",
    "option" : "Linear10PercentEvery5Minute.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nWith the Canary Deployment Preference type, Traffic is shifted in two intervals.\nWith Canary10Percent5Minutes, 10 percent of traffic is shifted in the first interval while remaining all traffic is shifted after 5 minutes.\nOption B is incorrect as Linear10PercentEvery10Minutes will add 10 percent traffic linearly to a new version every 10 minutes.\nSo, after 100 minutes all traffic will be shifted to the new version.\nOption C is incorrect as Canary10Percent15Minutes will send 10 percent traffic to the new version and 15 minutes later complete deployment by sending all traffic to the new version.\nOption D is incorrect as Linear10PercentEvery5Minute will add 10 percent traffic linearly to the new version every 5 minutes.\nSo, after 50 minutes all traffic will be shifted to the new version.\nFor more information on Deployment Preference Type for AWS SAM templates, refer to the following URL-\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\n\nAWS SAM (Serverless Application Model) is a framework for building serverless applications using AWS Lambda. CodeDeploy is a service provided by AWS to automate code deployments to any instance, including EC2 instances and Lambda functions. In this scenario, AWS SAM is being used to define a Lambda function, and CodeDeploy is being configured to manage deployment patterns.\nWhen a new version of a Lambda function is deployed using CodeDeploy, there are two types of deployment patterns available: canary and linear.\nCanary: This pattern allows you to deploy a new version of the Lambda function to a small subset of users or traffic. This subset is called a canary group. Once the new version has been deployed to the canary group, CodeDeploy monitors the group for errors or other issues. If there are no issues, CodeDeploy gradually increases the traffic to the new version until it is handling all traffic. If there are issues, CodeDeploy rolls back the deployment to the previous version. Linear: This pattern allows you to deploy a new version of the Lambda function to a small percentage of traffic at a time. You can specify the percentage of traffic that should be shifted to the new version in each step, as well as the time interval between each step. This deployment pattern is useful for gradually transitioning traffic to the new version while monitoring for errors or other issues.\nIn this scenario, the question is asking which deployment pattern will shift traffic from the original Lambda function to the new Lambda function in the shortest time frame. Of the options provided, the canary pattern with a 10% traffic shift and 5-minute monitoring interval (Option A) will shift traffic to the new Lambda function in the shortest time frame. This is because it starts with a smaller subset of traffic and has a shorter monitoring interval, allowing for a quicker transition to the new version if there are no issues.\nOption B (linear pattern with 10% traffic shift every 10 minutes) and Option D (linear pattern with 10% traffic shift every 5 minutes) will take longer to shift traffic to the new Lambda function because they start with a larger percentage of traffic and have longer intervals between each shift.\nOption C (canary pattern with 10% traffic shift and 15-minute monitoring interval) will also take longer to shift traffic to the new Lambda function because it has a longer monitoring interval.\nTherefore, the correct answer is A. Canary10Percent5Minutes.\n\n"
}, {
  "id" : 42,
  "question" : "A Developer is migrating an on-premises application to the AWS Cloud.\nThe application currently uses Microsoft SQL, encrypting some of the data using Transparent Data Encryption.\nWhich service should the Developer use to minimize code changes?\n",
  "answers" : [ {
    "id" : "2ac5aec8642f4c138d099f89ab49907b",
    "option" : "Amazon RDS",
    "isCorrect" : "true"
  }, {
    "id" : "2d0eb174e99d4805a5c1ceea5895458e",
    "option" : "Amazon Aurora",
    "isCorrect" : "false"
  }, {
    "id" : "c692e4ff15c54fc4988b6d2998ccfa39",
    "option" : "Amazon Redshift",
    "isCorrect" : "false"
  }, {
    "id" : "7c952ab1db4642ecb8bf76b6a898749e",
    "option" : "Amazon DynamoDB.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is also mentioned in the AWS Documentation.\nAmazon RDS supports using Transparent Data Encryption (TDE) to encrypt stored data on your DB instances running Microsoft SQL Server.\nTDE automatically encrypts data before it is written to storage and automatically decrypts data when the data is read from storage.\nAmazon RDS supports TDE for the following SQL Server versions and editions:\nSQL Server 2017 Enterprise Edition.\nSQL Server 2016 Enterprise Edition.\nSQL Server 2014 Enterprise Edition.\nSQL Server 2012 Enterprise Edition.\nSQL Server 2008 R2 Enterprise Edition.\nTo enable transparent data encryption for a DB instance running SQL Server, specify the TDE option in an Amazon RDS option group associated with that DB instance.\nOption B is incorrect because Microsoft SQL is not compatible with Aurora.\nOption C is incorrect because RedShift is used as a Data warehouse.\nMicrosoft SQL does not support it.\nOption D is incorrect because DynamoDB is a no-SQL database, primarily used with serverless architectures.\nFor more information on Encryption on Microsoft SQL Server AWS, please refer to the below Link-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.SQLServer.Options.TDE.html\n\nThe service that should be used to minimize code changes while migrating an on-premises application that currently uses Microsoft SQL with transparent data encryption to the AWS Cloud is Amazon RDS (Relational Database Service).\nAmazon RDS is a web service that makes it easier to set up, operate, and scale a relational database in the cloud. It is designed to simplify the process of deploying, managing, and scaling a relational database in the cloud. Amazon RDS supports several database engines, including Microsoft SQL Server, Oracle, MySQL, PostgreSQL, and Amazon Aurora.\nWhen migrating an on-premises application to the AWS Cloud, minimizing code changes is important to ensure that the migration process is as seamless as possible. Amazon RDS allows the developer to use the same database engine and encryption technology that is currently being used on-premises, which helps to minimize code changes.\nAdditionally, Amazon RDS supports transparent data encryption for Microsoft SQL Server, which means that the data remains encrypted both at rest and in transit without requiring any code changes. This helps to ensure that data remains secure during the migration process.\nAmazon Aurora, Amazon Redshift, and Amazon DynamoDB are all powerful AWS database services, but they are not designed to support transparent data encryption for Microsoft SQL Server. Therefore, they would require code changes to support the migration of an on-premises application that currently uses Microsoft SQL with transparent data encryption.\n\n"
}, {
  "id" : 43,
  "question" : "A Developer is writing several Lambda functions that each access data in a common RDS DB instance.\nThey must share a connection string that contains the secret database credentials.\nA company policy requires that all secrets be stored encrypted. Which solution will minimize the amount of code the Developer must write?\n",
  "answers" : [ {
    "id" : "36c053338abf4693bb8a158f13cfec76",
    "option" : "Use common DynamoDB table to store settings.",
    "isCorrect" : "false"
  }, {
    "id" : "ec85285ed5a841f6a7d48f2e7eceb75c",
    "option" : "Use AWS Lambda environment variables.",
    "isCorrect" : "false"
  }, {
    "id" : "cc2abc05ef7b4391b72dd7ebb3a59fa7",
    "option" : "Use Systems Manager Parameter Store secure strings.",
    "isCorrect" : "true"
  }, {
    "id" : "3135f7e2080c4d39abf9b6f63782e005",
    "option" : "Use a table in a separate RDS database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following to support this.\nAWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management.\nYou can store data such as passwords, database strings, and license codes as parameter values.\nYou can store values as plain text or encrypted data.\nYou can then reference values by using the unique name that you specified when you created the parameter.\nOptions A and D are incorrect and inefficient since you don't need a separate table.\nAlso, it does not mention in the answer about the encryption of the underlying tables.\nOption B is not correct since you need to share the encrypted connection strings.\nFor more information on the Systems Manager Parameter Store, please refer to the below Links-\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\nhttps://aws.amazon.com/blogs/compute/sharing-secrets-with-aws-lambda-using-aws-systems-manager-parameter-store/\n\nThe correct answer to this question is C. Use Systems Manager Parameter Store secure strings.\nExplanation: The Developer is writing several Lambda functions that access data in a common RDS DB instance. The connection string that contains the secret database credentials needs to be shared among these functions. According to the company policy, all secrets need to be stored encrypted.\nTo minimize the amount of code the Developer must write, they can use AWS Systems Manager Parameter Store to securely store the connection string. Parameter Store provides a centralized location to manage configuration data and secrets, which can be easily retrieved by Lambda functions or any other AWS resource.\nSpecifically, the Developer can store the connection string as a secure string parameter in Parameter Store, which encrypts the parameter value using AWS Key Management Service (KMS). The Developer can then configure the Lambda functions to retrieve the parameter value at runtime using the AWS SDK, without having to hardcode the credentials in the code.\nOption A, using a common DynamoDB table to store settings, would require the Developer to write additional code to retrieve and parse the settings from the DynamoDB table.\nOption B, using AWS Lambda environment variables, would require the Developer to set the environment variable for each function, which can be error-prone and time-consuming.\nOption D, using a table in a separate RDS database, would require the Developer to create and manage a separate RDS instance, which can be costly and complex.\nTherefore, option C is the best solution to minimize the amount of code the Developer must write and to comply with the company policy for storing encrypted secrets.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 45,
  "question" : "An organization deployed their static website on Amazon S3\nNow, the Developer has a requirement to serve dynamic content using a serverless solution.\nWhich combination of services should be used to implement a serverless application for the dynamic content? Select 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "5de7926bcbca4783a219f9e60e1bffd8",
    "option" : "Amazon API Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "cde10289c4b34ffc952229548cdbbca7",
    "option" : "Amazon EC2",
    "isCorrect" : "false"
  }, {
    "id" : "4ac546585c464690b18b524893b30325",
    "option" : "AWS ECS",
    "isCorrect" : "false"
  }, {
    "id" : "530497dfe16b4e338535abec5224b871",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "65aa7af315a14232879ddb01f8ae5572",
    "option" : "Amazon kinesis.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and D.\nOut of the above list, Given the scenario, API Gateway and AWS Lambda are the best two choices to build this serverless application.\nThe AWS Documentation mentions the following.\nAWS Lambda lets you run code without provisioning or managing servers.\nYou pay only for the compute time you consume - there is no charge when your code is not running.\nFor more information on AWS Lambda, please refer to the below Link-\nhttps://aws.amazon.com/lambda/\nAmazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\nFor more information on the API gateway, please refer to the below Link-\nhttps://aws.amazon.com/api-gateway/\nOption B is incorrect because EC2 is not a serverless offering.\nTo use serverless compute on Cloud, you can use Lambda.\nOption C is incorrect because ECS is primarily used for hosting containers and serverless offering for the same solution is Fargate.\nOption E is incorrect because Kinesis Data Streams is a scalable and durable real-time data streaming service that can continuously capture gigabytes of data per second from hundreds of thousands of sources.\nIt is not required in this scenario.\n\nTo serve dynamic content using a serverless solution with a static website hosted on Amazon S3, the following two services should be used:\nAmazon API Gateway: It is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It can be used to create APIs that integrate with AWS Lambda functions, which are used to serve the dynamic content. The API Gateway provides features such as caching, throttling, and request/response transformation, which can help improve the performance and security of the API. AWS Lambda: It is a serverless compute service that allows developers to run code without provisioning or managing servers. Developers can use Lambda to execute code in response to events, such as API requests, and can integrate with other AWS services, such as Amazon API Gateway. Lambda functions can be written in a variety of programming languages and can be scaled automatically based on the number of requests.\nTogether, Amazon API Gateway and AWS Lambda can be used to create a serverless application that serves dynamic content in response to API requests. The API Gateway can act as the front-end for the application, accepting requests from clients and routing them to the appropriate Lambda function. The Lambda function can then execute the necessary logic to generate the dynamic content and return it to the client through the API Gateway.\nThe other options listed in the question are not relevant to this scenario:\nAmazon EC2: It is a web service that provides resizable compute capacity in the cloud. However, it requires the provisioning and management of servers, which is not a requirement for a serverless solution. AWS ECS: It is a container orchestration service that allows developers to run and scale Docker containers. It is not directly relevant to serving dynamic content in a serverless fashion. Amazon Kinesis: It is a streaming data service that can be used to process and analyze real-time data streams. It is not directly relevant to serving dynamic content in a serverless fashion.\n\n"
}, {
  "id" : 46,
  "question" : "An application is publishing a custom CloudWatch metric any time an HTTP 504 error appears in the application error logs.\nThese errors are being received intermittently.\nThere is a CloudWatch Alarm for this metric, and the Developer would like the alarm to trigger ONLY if it breaches two evaluation periods or more. What should be done to meet these requirements?\n",
  "answers" : [ {
    "id" : "55d77f1f89934e648c3c8c2faf2a4288",
    "option" : "Update the CloudWatch Alarm to send a custom notification depending on results.",
    "isCorrect" : "false"
  }, {
    "id" : "85fa251a3ddf4eb9aaaedfda552f818c",
    "option" : "Publish the value zero whenever there are no “HTTP 504” errors.",
    "isCorrect" : "false"
  }, {
    "id" : "91ec8e102b294f8ea66cc48d5a0c3a6e",
    "option" : "Use high – resolution metrics to get data pushed to CloudWatch more frequently.",
    "isCorrect" : "false"
  }, {
    "id" : "55d47ebfc7974938b25f1b21a07a01ad",
    "option" : "The evaluation period and Data Points to Alarm should be set to 2 while creating this alarm.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nOur scenario states that we are receiving HTTP Error 504 intermittently.\nThe scenario requires that the ALARM should trigger ONLY if it breaches 2 evaluation periods.\nNone of the options listed is a good choice.\nWhen you create an alarm, you specify three settings to enable CloudWatch to evaluate when to change the alarm state:\nPeriod is the length of time to evaluate the metric to create each individual data point for a metric.\nIt is expressed in seconds.\nIf you choose one minute as the period, there is one data point every minute.\nEvaluation Period is the number of the most recent data points to evaluate when determining alarm state.\nDatapoints to Alarm is the number of data points within the evaluation period that must be breached to cause the alarm to go to the ALARM state.\nThe breaching data points do not have to be consecutive.\nThey must all be within the last number of data points equal to the Evaluation Period.\nLet us look at an example.\nIn the following figure, the alarm threshold is set to three units.\nThe alarm is configured to go to the ALARM state, and both Evaluation Period and Datapoints to Alarm are 3\nThat is, when all three data points in the most recent three consecutive periods are above the threshold, the alarm goes to the ALARM state.\nIn the figure, this happens in the third through fifth time periods.\nAt period six, the value dips below the threshold, so one of the periods being evaluated is not breaching, and the alarm state changes to OK.\nDuring the ninth time period, the threshold is breached again, but for only one period.\nConsequently, the alarm state remains OK.\nOption A is incorrect since here there is no mention of any special kind of notification.\nOption B is incorrect since you don't need to mention a 0 value.\nPlace a 1 value when the result is received.\nOption C is incorrect since there is no mention of the frequency, so we don't know if we need high resolution for metrics.\nFor more information on the aggregation of data in Cloudwatch, please refer to the below Link-\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-common-scenarios.html#CloudWatch-Agent-aggregating-metrics\n\n\nThe correct answer is D.\nExplanation:\nIn this scenario, the Developer wants to trigger a CloudWatch alarm only if it breaches two evaluation periods or more, so we need to adjust the evaluation period and the number of data points required to breach the alarm.\nA CloudWatch alarm has two important parameters to consider for this scenario: Evaluation Period and Datapoints to Alarm.\nEvaluation Period specifies the time range in which the alarm will evaluate the metric data to decide whether to breach the alarm threshold. The duration of the evaluation period is specified in minutes, and it must be a multiple of 60. For example, if the evaluation period is set to 5 minutes, CloudWatch will evaluate the metric data every 5 minutes.\nDatapoints to Alarm specifies the number of data points that must breach the threshold before the alarm triggers. This parameter prevents false alarms triggered by occasional spikes in the metric data.\nTo meet the requirements of this scenario, we need to set the Evaluation Period and Datapoints to Alarm parameters appropriately. We can't use custom notifications or high-resolution metrics to solve this problem. Similarly, publishing a value of zero for the metric when there are no errors is not a good solution since it doesn't represent the actual state of the system.\nTherefore, the correct answer is D. We should set the Evaluation Period to at least 2 times the time it takes for the metric to be published, and set Datapoints to Alarm to 2. This means that the alarm will trigger only if the HTTP 504 error appears in two or more consecutive evaluation periods. By setting the Datapoints to Alarm to 2, we ensure that the alarm only triggers when the error is recurring rather than just a one-time event.\nIn summary, the correct answer is D.\n\n"
}, {
  "id" : 47,
  "question" : "A Developer has been asked to create an AWS Elastic Beanstalk environment for a production web application that needs to handle thousands of requests.\nCurrently, the dev environment is running on a t1.micro instance. What is the best way for the developer to provision a new production environment with a m4.large instance instead of a t1.micro?\n",
  "answers" : [ {
    "id" : "617171d0fd6c4e9b96367d6215888597",
    "option" : "Use CloudFormation to migrate the Amazon EC2 instance type from t1.micro to m4.large.",
    "isCorrect" : "false"
  }, {
    "id" : "cf5677ffe0ec40858a33aaac0fc9280f",
    "option" : "Create a new configuration file with the instance type as m4.large and reference this file when provisioning the new environment.",
    "isCorrect" : "true"
  }, {
    "id" : "38a2a121522e4e3b876db5fd931550da",
    "option" : "Provision a m4.large instance directly in the dev environment and deploy to the new production environment.",
    "isCorrect" : "false"
  }, {
    "id" : "195dc025dc7f46d3b49ffe90994d0fbe",
    "option" : "Change the instance type value in the configurations file to m4.large by using the update autoscaling group CLI command.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe Elastic Beanstalk console and EB CLI set configuration options when you create an environment.\nYou can also set configuration options in saved configurations and configuration files.\nIf the same option is set in multiple locations, the value used is determined by order of precedence.\nConfiguration option settings can be composed in text format and saved before environment creation, applied during environment creation using any supported client, and added, modified, or removed after environment creation.\nDuring environment creation, configuration options are applied from multiple sources with the following precedence, from highest to lowest:\nSettings applied directly to the environment - Settings specified during a create environment or update environment operation on the Elastic Beanstalk API by any client, including the AWS Management Console, EB CLI, AWS CLI, and SDKs.\nThe AWS Management Console and EB CLI also apply recommended values for some options that apply at this level unless overridden.\nSaved Configurations - Settings for any options that are not applied directly to the environment are loaded from a saved configuration, if specified.\nConfiguration Files (.ebextensions) - Settings for any options that are not applied directly to the environment and not specified in a saved configuration are loaded from configuration files in the .ebextensions folder at the root of the application source bundle.\nConfiguration files are executed in alphabetical order.\nFor example, .ebextensions/01run.config is executed before .ebextensions/02do.config.\nDefault Values - If a configuration option has a default value, it only applies when the option is not set at any of the above levels.\nIf the same configuration option is defined in more than one location, the setting with the highest precedence is applied.\nWhen a setting is applied from a saved configuration or settings applied directly to the environment, the setting is stored as part of the environment's configuration.\nThese settings can be removed with the AWS CLI or with the EB CLI.\n.\nSettings in configuration files are not applied directly to the environment and cannot be removed without modifying the configuration files and deploying a new application version.\nIf a setting applied with one of the other methods is removed, the same setting will be loaded from configuration files in the source bundle.\nOption A is incorrect since the Elastic Beanstalk service already manages the environment, and we don't need Cloudformation for this.\nOption C is incorrect since the changes need to be done for the current configuration.\nFor more information on making this change, please refer to the below Link-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.ec2.html\n\nOption A: Use CloudFormation to migrate the Amazon EC2 instance type from t1.micro to m4.large. Using CloudFormation to migrate the Amazon EC2 instance type from t1.micro to m4.large is not the best way to provision a new production environment with a m4.large instance. CloudFormation is used to automate the process of creating, modifying, and deleting AWS resources, including EC2 instances. However, CloudFormation can take some time to provision resources, which may not be ideal for a production environment that needs to handle thousands of requests.\nOption B: Create a new configuration file with the instance type as m4.large and reference this file when provisioning the new environment. Creating a new configuration file with the instance type as m4.large and referencing this file when provisioning the new environment is a good way to provision a new production environment with a m4.large instance. Elastic Beanstalk allows developers to create multiple configuration files and switch between them easily, which means that developers can create a new configuration file with the desired instance type and use it to provision the new environment.\nOption C: Provision a m4.large instance directly in the dev environment and deploy to the new production environment. Provisioning a m4.large instance directly in the dev environment and deploying to the new production environment is not the best way to provision a new production environment with a m4.large instance. The developer should create a new environment for the production environment, which will use the desired instance type.\nOption D: Change the instance type value in the configurations file to m4.large by using the update autoscaling group CLI command. Changing the instance type value in the configurations file to m4.large by using the update autoscaling group CLI command is not the best way to provision a new production environment with a m4.large instance. The developer should create a new environment for the production environment, which will use the desired instance type.\nIn conclusion, the best way for the developer to provision a new production environment with a m4.large instance instead of a t1.micro is to create a new configuration file with the instance type as m4.large and reference this file when provisioning the new environment.\n\n"
}, {
  "id" : 48,
  "question" : "An organization has an Amazon Aurora RDS instance that handles all of its AWS-based e-commerce activity.\nThe application accessing the database needs to create large sales reports on an hourly basis, running 15 minutes after the hour.\nThis reporting activity is slowing down the e-commerce application. Which combination of actions should be taken to reduce the impact on the main e-commerce application? Select 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "3d7f894e781144118343b4c7b24e9fcd",
    "option" : "Point the reporting application to the read replica.",
    "isCorrect" : "true"
  }, {
    "id" : "3d5e14f17f4c4777bb0c1e5728cd3309",
    "option" : "Migrate the data to a set of highly available Amazon EC2 instances.",
    "isCorrect" : "false"
  }, {
    "id" : "ed137cf6b19b4ffe8ad1dd82e93e608f",
    "option" : "Use SQS Buffering to retrieve data for reports.",
    "isCorrect" : "false"
  }, {
    "id" : "a5d5258a6354434e828093defe3e491d",
    "option" : "Create a read replica of the database.",
    "isCorrect" : "true"
  }, {
    "id" : "4002db2980b94b8093570a6da755192c",
    "option" : "Create an SQS queue to implement SQS Buffering.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and D.\nThe AWS Documentation mentions the following.\nAmazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances.\nThis feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\nYou can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.\nOption B is incorrect, since the AWS RDS service already has features to support the requirement.\nOptions C and E are incorrect since using SQS would be inefficient.\nFor more information on AWS Read Replica's, please refer to the below Link-\nhttps://aws.amazon.com/rds/details/read-replicas/\n\nThe optimal combination of actions to reduce the impact of running hourly sales reports on an Aurora RDS instance that handles e-commerce activity are:\nPoint the reporting application to the read replica:\nThis option allows the e-commerce application to use the master database instance while the reporting application uses a read replica of the database. This ensures that the e-commerce application is not impacted by the heavy reporting queries. Since read replicas are asynchronous copies of the master database, there will be some latency involved in replicating data changes from the master to the replica. However, this should not be an issue for generating hourly reports.\nCreate an SQS queue to implement SQS Buffering:\nThis option involves decoupling the reporting application from the database by introducing an SQS queue to buffer the incoming data. The reporting application can write data to the queue, and the data can be consumed by a separate process that generates the reports. This approach provides scalability and resilience to the reporting application since it is no longer dependent on the database. It also ensures that the e-commerce application is not impacted by the heavy reporting queries.\nOption B: Migrate the data to a set of highly available Amazon EC2 instances is not a suitable solution because it introduces a lot of overhead in terms of managing the database instances and ensuring high availability. This option also does not provide any decoupling between the e-commerce and reporting applications, so the e-commerce application may still be impacted by heavy reporting queries.\nOption C: Use SQS Buffering to retrieve data for reports is not a complete solution because it only focuses on buffering the data to generate reports. It does not address the issue of heavy reporting queries impacting the e-commerce application.\nOption D: Create a read replica of the database is a good option, but it is not sufficient on its own. Read replicas are only copies of the master database, so heavy reporting queries can still impact the e-commerce application if they are executed on the master database. Combining this option with option A or option B could provide a more comprehensive solution.\nTherefore, the best combination of actions to reduce the impact of generating hourly sales reports on an Aurora RDS instance handling e-commerce activity are to point the reporting application to the read replica and to create an SQS queue to implement SQS Buffering.\n\n"
}, {
  "id" : 49,
  "question" : "An organization is using AWS Elastic Beanstalk for a web application.\nThe Developer needs to configure the Elastic Beanstalk environment with deployment methods to create new instances and deploy code to those instances. Which methods will deploy code ONLY to new instances? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "6d11a4ea69f04803833222aea09e9b1b",
    "option" : "All at once deployment",
    "isCorrect" : "false"
  }, {
    "id" : "6c848ae80ac246e787b3cbf50997585d",
    "option" : "Immutable deployment",
    "isCorrect" : "true"
  }, {
    "id" : "ebf1bfe3af1b48fabd70472bd4faee00",
    "option" : "Rolling deployment",
    "isCorrect" : "false"
  }, {
    "id" : "d55ab7c57cf34b539b1ff8beb8764f26",
    "option" : "Linear deployment",
    "isCorrect" : "false"
  }, {
    "id" : "6d362930c6e64dcf8d88c6033079d7ea",
    "option" : "Blue/Green deployment.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and E.\nThe AWS Documentation mentions the following.\nImmutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version.\nImmutable deployments can prevent issues caused by partially completed rolling deployments.\nIf the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched.\nAnd with Blue Green deployments, you can have a separate deployment environment as well.\nOption A is incorrect because All at once deployment deploys the new version to all instances simultaneously.\nAll instances in your environment are out of service for a short time while the deployment occurs.\nOption C is incorrect because Rolling deployment deploys the new version in batches.\nEach batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.\nOption D is incorrect because there is no Linear deployments.\nFor more information on Deployment options, please refer to the below Links-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\n\nOut of the given deployment methods, Immutable deployment and Blue/Green deployment will deploy code ONLY to new instances.\nImmutable Deployment: Immutable deployment is a deployment method in which new instances are created before the code deployment. The deployment process involves creating a new set of instances, deploying code to them, and then directing traffic to the new instances. The old instances are terminated once the new instances are up and running. This method ensures that there is no downtime and eliminates the possibility of rollback. In an immutable deployment, the original instances are not updated; instead, new instances are created with the updated code. Blue/Green Deployment: Blue/Green deployment is another deployment method that involves the creation of a new environment with the updated code. In this method, the new environment is created alongside the existing environment. The new environment is identical to the old environment, except for the updated code. Once the new environment is ready, traffic is directed to it, and the old environment is terminated. This deployment method ensures zero downtime and enables a quick rollback if there are any issues.\nThe other deployment methods given in the options, All at once deployment, Rolling deployment, and Linear deployment, do not deploy code only to new instances.\nAll at Once deployment: In this method, the deployment process updates all instances in the environment at once. This method is the fastest but also carries the highest risk since it can result in downtime if there are any issues with the deployment. Rolling Deployment: In this method, the deployment process updates a percentage of instances at a time, allowing for a gradual update of the environment. This method reduces the risk of downtime but still carries some risk since some instances may be updated while others are not. Linear Deployment: Linear deployment is similar to rolling deployment, but it updates instances in a linear fashion. In this method, a fixed number of instances are updated at a time until all instances are updated. This method also reduces the risk of downtime but still carries some risk since some instances may be updated while others are not.\nIn summary, Immutable deployment and Blue/Green deployment methods are the only two methods that will deploy code only to new instances. The other deployment methods update existing instances as well.\n\n"
}, {
  "id" : 50,
  "question" : "A developer is writing an application that will store data in a DynamoDB table.\nThe ratio of reads operations to write operations will be 1000 to 1, with the same data being accessed frequently. What should the Developer enable on the DynamoDB table to optimize performance and minimize costs?\n",
  "answers" : [ {
    "id" : "be1b56bae89c4ca6ab52348adcad6884",
    "option" : "Amazon DynamoDB auto scaling",
    "isCorrect" : "false"
  }, {
    "id" : "2cfc16f3dba24c5f9629b541665df020",
    "option" : "Amazon DynamoDB cross-region replication",
    "isCorrect" : "false"
  }, {
    "id" : "6b68197324e94a10a25c1afa203d6744",
    "option" : "Amazon DynamoDB Streams",
    "isCorrect" : "false"
  }, {
    "id" : "bc1904acf738488eaa4820fe9d0c47a0",
    "option" : "Amazon DynamoDB Accelerator.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nDAX is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications.\nDAX addresses three core scenarios:\n1\nAs an in-memory cache, DAX reduces the response times of eventually-consistent read workloads by order of magnitude, from single-digit milliseconds to microseconds.\n2\nDAX reduces operational and application complexity by providing a managed service that is API-compatible with Amazon DynamoDB and requires only minimal functional changes to use with an existing application.\n3\nFor read-heavy or bursty workloads, DAX provides increased throughput and potential operational cost savings by reducing the need to over-provision read capacity units.\nThis is especially beneficial for applications that require repeated reads for individual keys.\nOption A is incorrect since this is good when you have unpredictable workloads.\nOption B is incorrect since this is good for disaster recovery scenarios.\nOption C is incorrect since this is good to stream data to other sources.\nFor more information on DynamoDB Accelerator, please refer to the below Link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\n\nThe optimal solution for this scenario is to enable Amazon DynamoDB auto scaling (option A).\nDynamoDB auto scaling is a feature that automatically adjusts the provisioned throughput capacity of a DynamoDB table based on the workload. It can automatically increase or decrease the read and write capacity units in response to changes in the request traffic patterns, ensuring that the application has enough capacity to handle the workload.\nIn this case, since the read to write ratio is 1000:1 and the same data is accessed frequently, the application will require a high read capacity and a low write capacity. Enabling auto scaling will ensure that the table has the required capacity to handle the read traffic, while minimizing the cost of the write traffic.\nAmazon DynamoDB cross-region replication (option B) is not relevant to this scenario, as it is used for replicating data between multiple DynamoDB tables in different regions, for purposes such as disaster recovery or serving global users.\nAmazon DynamoDB Streams (option C) is a feature that captures a time-ordered sequence of item-level modifications in a DynamoDB table. While it can be useful for building additional features such as real-time data processing or event-driven architectures, it is not directly relevant to optimizing the read-to-write ratio or minimizing costs.\nAmazon DynamoDB Accelerator (option D) is a caching service that can improve the performance of DynamoDB queries by providing sub-millisecond response times. While it can be useful for reducing the latency of frequently accessed data, it is not directly relevant to optimizing the read-to-write ratio or minimizing costs.\n\n"
}, {
  "id" : 51,
  "question" : "You are using AWS SAM templates to deploy a serverless application.\nWhich of the following resource will embed nested applications from Amazon S3 buckets?\n",
  "answers" : [ {
    "id" : "01892234085b44c2af99eb64d00f8d11",
    "option" : "AWS::Serverless::API",
    "isCorrect" : "false"
  }, {
    "id" : "eeff59ca9e7340d6bb7e345fb812de52",
    "option" : "AWS::Serverless::Application",
    "isCorrect" : "true"
  }, {
    "id" : "a013ece0b0824fe5ba25a288608417f9",
    "option" : "AWS::Serverless::LayerVersion",
    "isCorrect" : "false"
  }, {
    "id" : "32d46f52b99d4fbbb205283be98770ee",
    "option" : "AWS::Serverless::Function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS::Serverless::Application resource in AWS SAM template is used to embed application from Amazon S3 buckets.\nOption A is incorrect as AWS::Serverless::API is used for creating API Gateway resources &amp; methods that can be invoked through HTTPS endpoints.\nOption C is incorrect as AWS::Serverless::LayerVersion resource type creates Lambda layered function.\nOption D is incorrect as AWS::Serverless::Function resource describes the configuration for creating Lambda function.\nFor more information on using AWS SAM templates, refer to the following URLs-\nhttps://aws.amazon.com/blogs/compute/announcing-nested-applications-for-aws-sam-and-the-aws-serverless-application-repository/\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-template.html\n\nThe correct answer is B. AWS::Serverless::Application.\nAWS Serverless Application Model (AWS SAM) is an open-source framework that helps to build and deploy serverless applications on AWS. It extends AWS CloudFormation to provide a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application.\nAWS::Serverless::Application resource type allows you to specify a nested application that is stored in an Amazon S3 bucket. The nested application can include AWS SAM templates, Lambda functions, APIs, and any other AWS resource that you want to include.\nBy using AWS::Serverless::Application resource, you can package and reuse complex sets of resources that you have created before in separate stacks. The nested application allows you to easily manage your infrastructure as code and reduce code duplication.\nAWS::Serverless::API resource type is used to define an Amazon API Gateway REST API with the specified routes, methods, models, and integration to AWS Lambda functions.\nAWS::Serverless::LayerVersion resource type is used to define a Lambda layer. It contains libraries, custom runtimes, or other dependencies that are reused across multiple Lambda functions.\nAWS::Serverless::Function resource type is used to define an AWS Lambda function. It contains the code that is executed in response to an event.\nIn summary, only the AWS::Serverless::Application resource type allows you to embed nested applications from Amazon S3 buckets.\n\n"
}, {
  "id" : 52,
  "question" : "You are a developer that has recently been hired for your API expertise.\nThe company is currently using API Gateway services for development.\nYou need to control the behavior of an API's front-end interactions.\nWhich of the following could be done to achieve this? (Select TWO.)\n",
  "answers" : [ {
    "id" : "bb6a38cc224b418192f84c4960d3c6c9",
    "option" : "Modify the configuration of the Method request.",
    "isCorrect" : "true"
  }, {
    "id" : "474b17da929b432baf169d7a22a1bc18",
    "option" : "Modify the configuration of the Integration request.",
    "isCorrect" : "false"
  }, {
    "id" : "35a5822d9f5c4ef295cb2f157a48910b",
    "option" : "Modify the configuration of the Method response.",
    "isCorrect" : "true"
  }, {
    "id" : "30fe4f97a59a4f0f95a3f06f6f87b08e",
    "option" : "Modify the configuration of the Integration response.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThis is also mentioned in the AWS Documentation.\nAs an API developer, you control the behaviors of your API's frontend interactions by configuring the method request and a method response.\nYou control the behaviors of your API's backend interactions by setting up the integration request and integration response.\nThese involve data mappings between a method and its corresponding integration.\nOptions B and D are incorrect since these are used to control the behaviors of your API's backend interactions.\nFor more information on creating an API via the gateway, please refer to the below Link-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-from-example-console.html\n\nAPI Gateway is a fully managed service that makes it easier for developers to create, publish, and manage APIs at any scale. It provides a range of features such as authentication, authorization, monitoring, and caching to help developers build robust and secure APIs. In this context, the question is asking about controlling the behavior of an API's front-end interactions.\nTo achieve this goal, you need to modify the configurations of the method and integration requests and responses. These configurations define how the API Gateway processes incoming requests and outgoing responses. Here's a detailed explanation of each option:\nA. Modify the configuration of the Method request: The method request configuration defines how API Gateway handles incoming requests before they reach the backend integration. For example, you can specify the required headers, query parameters, or path parameters that must be present in the request to access the API. You can also configure request validation rules to ensure that the incoming data is in the correct format and meets specific criteria. Modifying the method request configuration is useful when you need to control how the API Gateway interacts with the front-end clients.\nB. Modify the configuration of the Integration request: The integration request configuration defines how API Gateway sends the request to the backend integration. It includes information such as the HTTP method, endpoint URL, headers, and body. You can use this configuration to customize the request sent to the backend, add query parameters or modify the request payload. This is useful when you need to modify the request sent to the backend based on the requirements of the front-end clients.\nC. Modify the configuration of the Method response: The method response configuration defines how API Gateway handles the response received from the backend integration. For example, you can specify the headers, status codes, or response templates to send back to the front-end clients. This configuration is useful when you need to customize the response sent back to the client based on the requirements.\nD. Modify the configuration of the Integration response: The integration response configuration defines how API Gateway handles the response received from the backend integration before sending it to the front-end clients. It includes information such as the headers, status codes, and response templates. You can use this configuration to customize the response sent to the front-end clients based on the requirements. This is useful when you need to modify the response received from the backend based on the requirements of the front-end clients.\nIn summary, options A and B are correct as they involve modifying the configurations of the method and integration requests, respectively. Options C and D are incorrect as they involve modifying the configurations of the method and integration responses, respectively.\n\n"
}, {
  "id" : 53,
  "question" : "You're developing an AWS Lambda function that is interacting with a DynamoDB table.\nThe function was working well, but now it is giving the results with a time delay.\nYou need to debug the code to understand where the bottleneck is which is causing the performance issue.\nWhich of the following is the ideal way to debug the code?\n",
  "answers" : [ {
    "id" : "585326a32dae48bcbfb256f738592c1d",
    "option" : "Use Log statements in the code to detect the delay.",
    "isCorrect" : "false"
  }, {
    "id" : "4fee5c3f517843beae7d592998d25167",
    "option" : "Use Cloudwatch logs to detect where the delay could be.",
    "isCorrect" : "false"
  }, {
    "id" : "f2dcd2d781f44b80a4bfe86175e72c16",
    "option" : "Look at the throttling errors in Cloudwatch metrics.",
    "isCorrect" : "false"
  }, {
    "id" : "2cd1deeee0b241d6972bc25f9421c28d",
    "option" : "Use AWS X-Ray to see where the downstream delay could be.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nWith AWS X-Ray, you can actually see traces in your AWS Lambda function, allowing you to see a detailed level of tracing to your downstream services.\nThe below snapshot from the AWS documentation shows an example of this.\nOption A is incorrect since this is not an efficient way to check for performance errors.\nOption B is incorrect since the logs might not be able to give you that level of tracing to detect the error.\nOption C is incorrect since throttling errors will not give you the cause of the performance issue.\nFor more information on using AWS Lambda with X-Ray, please refer to the below Link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html\n\n\nWhen a Lambda function interacting with DynamoDB table is giving results with time delay, it's important to identify the bottleneck in the code to improve its performance. There are several ways to debug the code, but among the options provided, the ideal way to debug the code is by using AWS X-Ray to see where the downstream delay could be (option D).\nAWS X-Ray is a service that helps developers to analyze and debug distributed applications such as Lambda functions by providing end-to-end visibility into the application. It provides a graphical representation of the application's architecture and highlights the performance issues and errors. It can also identify the cause of the delay in the application's downstream dependencies.\nUsing Log statements (option A) in the code is a common way to debug the code, but it requires that you manually instrument the code and insert logging statements to identify the performance issue. This method can be time-consuming and may not always provide accurate information on the root cause of the issue.\nCloudwatch logs (option B) can be useful in debugging the code. However, it may not be the ideal way to identify performance issues. It provides a centralized repository for storing and analyzing logs, but it requires manual analysis of the logs to identify the cause of the delay.\nCloudwatch metrics (option C) provide a way to monitor the performance of the Lambda function and identify throttling errors. However, it may not provide enough information to identify the cause of the delay in the function's performance.\nTherefore, using AWS X-Ray to see where the downstream delay could be (option D) is the ideal way to debug the code. AWS X-Ray provides a visual representation of the application's architecture, identifies performance issues and errors, and provides detailed information about the downstream dependencies. This information helps to identify the root cause of the delay and improve the performance of the Lambda function.\n\n"
}, {
  "id" : 54,
  "question" : "A company is planning on using AWS CodePipeline for their underlying CI/CD process.\nThe code will be picked up from an S3 bucket.\nThe company policy mandates that all data should be encrypted at rest and that the keys are managed by the customer.\nWhich of the following measures would you take to ensure that the CI/CD process conforms to this policy? Choose 2 possible actions from the options given below.\n",
  "answers" : [ {
    "id" : "23ca3c5974af44c198dd5610cac5247a",
    "option" : "Ensure that server-side encryption is enabled on the S3 bucket and data is encrypted at-rest on the CodeBuild environment using customer-managed CMK.",
    "isCorrect" : "true"
  }, {
    "id" : "0a49a13b896c47f8b1b7e5503bbc3215",
    "option" : "Ensure that server-side encryption is enabled on the CodePipeline stage.",
    "isCorrect" : "false"
  }, {
    "id" : "04f78bb857b044fb987fbe3023437590",
    "option" : "Configure the code pickup stage in CodePipeline to use AWS KMS.",
    "isCorrect" : "false"
  }, {
    "id" : "02097e79ab634edf8cddb18d681d3b8a",
    "option" : "Configure AWS KMS with customer managed keys and use it for S3 bucket encryption.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThis is also mentioned in the AWS Documentation.\nThere are two ways to configure server-side encryption for Amazon S3 artifacts.\nAWS CodePipeline creates an Amazon S3 artifact bucket and default AWS-managed SSE-KMS encryption keys when creating a pipeline using the Create Pipeline wizard.\nThe master key is encrypted along with object data and managed by AWS.\nYou can create and manage your own customer-managed SSE-KMS keys.\nOptions B and C are incorrect since this needs to be configured at the S3 bucket level.\nFor more information on Encryption in S3 with CodePipeline, please refer to the below Link-\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/S3-artifact-encryption.html\nPlease refer to the following link on page 252.\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/codebuild-user.pdf\n\nTo ensure that the CI/CD process conforms to the company policy of encrypting all data at rest and managing the keys, the following measures should be taken:\nEnsure that server-side encryption is enabled on the S3 bucket and data is encrypted at-rest on the CodeBuild environment using customer-managed CMK. Enabling server-side encryption on the S3 bucket ensures that all data uploaded to the bucket is automatically encrypted at rest. To ensure that the encryption keys are managed by the customer, a customer-managed CMK (Customer Master Key) should be used to encrypt the data in the CodeBuild environment. Configure AWS KMS with customer managed keys and use it for S3 bucket encryption. AWS KMS (Key Management Service) is a fully managed service that makes it easy to create and control the encryption keys used to encrypt data. By configuring AWS KMS with customer-managed keys, the customer can ensure that they have full control over the keys used to encrypt their data. Using AWS KMS to encrypt the S3 bucket data ensures that all data uploaded to the bucket is automatically encrypted at rest and that the encryption keys are managed by the customer.\nTherefore, the two possible actions to ensure that the CI/CD process conforms to the company policy are A and D.\n\n"
}, {
  "id" : 55,
  "question" : "An application hosted in AWS has been configured to use a DynamoDB table.\nSeveral items are written to the DynamoDB table.\nThese items are only accessed in a particular time frame, after which they can be deleted.\nWhich of the following is an ideal way to manage the deletion of the stale items?\n",
  "answers" : [ {
    "id" : "6467c7a95cb3476292d1828bd6882a11",
    "option" : "Perform a scan on the table for the stale items and issue the Delete operation.",
    "isCorrect" : "false"
  }, {
    "id" : "d6819ea55cac42f29730d48f3e1417a7",
    "option" : "Create an additional column to store the date. Perform a query for the stale objectsand the perform the Delete operation.",
    "isCorrect" : "false"
  }, {
    "id" : "db5a50f368884d97b6910134859c8866",
    "option" : "Enable versioning for the items in DynamoDB and delete the last accessed version.",
    "isCorrect" : "false"
  }, {
    "id" : "24ddff2a6a7b48d3ae07c35107c03652",
    "option" : "Enable TTL for the items in DynamoD.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nTime To Live (TTL) for DynamoDB allows you to define when items in a table expire so that they can be automatically deleted from the database.\nTTL is provided at no extra cost as a way to reduce storage usage and reduce the cost of storing irrelevant data without using provisioned throughput.\nWith TTL enabled on a table, you can set a timestamp for deletion on a per-item basis, allowing you to limit storage usage to only those records that are relevant.\nOptions A and B are incorrect since these would not be cost-effective and have a performance issue on the underlying DynamoDB table.\nOption C is incorrect since versioning is not possible in DynamoDB.For more information on Time to Live for items in DynamoDB, please refer to the below Link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n\nThe ideal way to manage the deletion of stale items in a DynamoDB table is to enable Time To Live (TTL) for the items. Therefore, option D is the correct answer.\nEnabling TTL allows DynamoDB to automatically delete items that have exceeded their expiration time. To use TTL, a column must be designated as the TTL attribute in the table. This attribute should contain the timestamp indicating when the item should expire. Once TTL is enabled, DynamoDB automatically deletes the expired items.\nOption A is not ideal because scanning the entire table can be expensive and inefficient, especially if the table has a large number of items. Scanning for stale items is also prone to false positives and may result in deleting valid data.\nOption B involves creating an additional column to store the date and then querying for stale items based on this column. While this method can work, it requires additional development effort and may not be as efficient as using TTL.\nOption C involves enabling versioning for the items in the table. This method allows you to retain previous versions of an item, but it does not automatically delete stale items. You would still need to query for stale items and delete them manually, which is not ideal.\n\n"
}, {
  "id" : 56,
  "question" : "You are using AWS Envelope Encryption to encrypt all of your sensitive data.\nWhich of the following is true with regards to the AWS Envelope Encryption service?\n",
  "answers" : [ {
    "id" : "335d881292c64ffb84ebcc3a064bcb79",
    "option" : "First, the data is encrypted using an encrypted Data Key. The encrypted Data Key is then further encrypted using an encrypted Master Key.",
    "isCorrect" : "false"
  }, {
    "id" : "2a3ce5e0a4bc462f91e7849f375c79ce",
    "option" : "First, the data is encrypted using a plaintext Data Key. The Data Key is then further encrypted using an encrypted Master Key.",
    "isCorrect" : "false"
  }, {
    "id" : "c8fd7f0170d44c3c9c75250557ce5617",
    "option" : "First, the data is encrypted using an encrypted Data Key. The encrypted Data Key is then further encrypted using a plaintext Master Key.",
    "isCorrect" : "false"
  }, {
    "id" : "83af761ff5db4565b638a19652f2a143",
    "option" : "First, the data is encrypted using a plaintext Data Key. The Data Key is then further encrypted using a plaintext Master Key.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nWith Envelope Encryption, unencrypted data is encrypted using a plaintext Data key.\nThis Data key is further encrypted using a plaintext Master key.\nThis plaintext Master key is securely stored in AWS KMS&amp; known as Customer Master Keys.\nOption A is incorrect as the Data Key used for encryption of data is plaintext along with the Master key used to encrypt Data Keys.\nOption B is incorrect as the Master key used to encrypt Data Keys is in plaintext format.\nOption C is incorrect as the Data Key used for encryption of data is in plaintext format.\nFor more information on AWS KMS Envelope Encryption, refer to the following URL-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\n\nSure, I'd be happy to provide you with a detailed explanation of AWS Envelope Encryption.\nAWS Envelope Encryption is a security mechanism that allows you to encrypt sensitive data using a combination of symmetric and asymmetric encryption techniques. It works by generating a unique Data Key for each piece of data that needs to be encrypted. The Data Key is then used to encrypt the data, and the encrypted Data Key is stored alongside the data. This is known as \"envelope encryption\" because the data is wrapped in an envelope of encryption keys.\nThe Data Key is itself encrypted using a Master Key. This Master Key can be stored and managed securely using AWS Key Management Service (KMS), which is a fully managed service that makes it easy to create and control encryption keys.\nNow, let's take a look at the options provided in the exam question:\nA. First, the data is encrypted using an encrypted Data Key. The encrypted Data Key is then further encrypted using an encrypted Master Key.\nThis option is correct. Envelope Encryption works by first encrypting the Data Key using an encrypted Master Key. The encrypted Data Key is then used to encrypt the sensitive data. This provides an additional layer of security, as an attacker would need both the Master Key and the Data Key to decrypt the sensitive data.\nB. First, the data is encrypted using a plaintext Data Key. The Data Key is then further encrypted using an encrypted Master Key.\nThis option is incorrect. The Data Key should never be stored or transmitted in plaintext, as it would compromise the security of the entire system. Therefore, this option is not a valid option for AWS Envelope Encryption.\nC. First, the data is encrypted using an encrypted Data Key. The encrypted Data Key is then further encrypted using a plaintext Master Key.\nThis option is incorrect. The Master Key should always be encrypted, whether it is stored or transmitted. Therefore, this option is not a valid option for AWS Envelope Encryption.\nD. First, the data is encrypted using a plaintext Data Key. The Data Key is then further encrypted using a plaintext Master Key.\nThis option is incorrect. Both the Data Key and the Master Key should be encrypted, whether they are stored or transmitted. Therefore, this option is not a valid option for AWS Envelope Encryption.\nTo summarize, the correct answer is A, which states that the data is first encrypted using an encrypted Data Key, which is then further encrypted using an encrypted Master Key.\n\n"
}, {
  "id" : 57,
  "question" : "Your team has been instructed to deploy a Microservices and an ETL based application onto AWS.\nThere is a requirement to manage the containerization of the application using Docker.\nWhich of the following would the ideal way to implement this with the least amount of administrative effort?\n",
  "answers" : [ {
    "id" : "b8ce3b40582642e7a3c987cab7c939e2",
    "option" : "Use AWS OpsWorks",
    "isCorrect" : "false"
  }, {
    "id" : "732e8e11b8fd475dabcf6a32756775c7",
    "option" : "Use the Elastic Container Service.",
    "isCorrect" : "true"
  }, {
    "id" : "dfdde35fffdf4127b1b821395518c5fe",
    "option" : "Deploy Kubernetes on EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "686e6ff1c0d540b0bfd34f003743fb9d",
    "option" : "Use the CloudFormation service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe Elastic Container Service is a fully managed container orchestration service available in AWS.\nThe AWS Documentation mentions the following.\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers.\nIt allows you to run and scale containerized applications on AWS easily.\nAmazon ECS eliminates the need for you to install and operate your own container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines.\nOption A is incorrect because AWS OpsWorks helps to configure and operate applications in a cloud enterprise by using Puppet or Chef, which is not required in the question.\nOption C is incorrect since even though Kubernetes is a fully managed solution, hosting it on EC2 Instances will incur more administrative headache.\nOption D is incorrect because AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources.\nBut it does not manage the Docker containers like ECS.\nFor more information on Amazon ECS, please refer to the below Link-\nhttps://aws.amazon.com/ecs/\nThe following diagram illustrates an architectural implementation of microservices on AWS.\nThe API of a microservice is the central entry point for all client requests.\nThe application logic hides behind a set of programmatic interfaces, typically a RESTful web services API.\nThis API accepts and processes calls from clients and might implement functionality such as traffic management, request filtering, routing, caching, and authentication and authorization.\nMany AWS customers use the Elastic Load Balancing (ELB) Application Load Balancer together with Amazon EC2 Container Service (Amazon ECS) and Auto Scaling to implement a microservices application.\nThe Application Load Balancer routes traffic based on advanced application-level information that includes the content of the request.\nFor more information, please refer to:\nhttps://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/simple-microservices-architecture-on-aws.html\n\n\nThe ideal way to deploy a containerized microservices and ETL based application onto AWS with the least amount of administrative effort would be to use the Elastic Container Service (ECS) provided by AWS.\nHere's why:\nA. Use AWS OpsWorks: OpsWorks is a configuration management service that automates the deployment of applications, but it is not specifically designed for container orchestration. While it does support Docker, it is not the most ideal solution for containerization because it would require additional effort to manage the containerization of the application using Docker.\nB. Use the Elastic Container Service: ECS is a fully-managed container orchestration service provided by AWS. With ECS, you can easily deploy, manage, and scale containerized applications using Docker containers. ECS abstracts away much of the underlying infrastructure, making it easier to deploy and manage containerized applications. ECS is also tightly integrated with other AWS services, such as Elastic Load Balancing and CloudWatch, making it easy to build scalable and highly available applications. Additionally, ECS supports both Fargate and EC2 launch types, providing flexibility in how you want to manage your infrastructure.\nC. Deploy Kubernetes on EC2 Instances: Deploying Kubernetes on EC2 instances is another option, but it would require more administrative effort compared to using ECS. Kubernetes is a powerful and flexible container orchestration platform, but it also requires a steep learning curve and can be complex to set up and manage. Additionally, managing the underlying infrastructure, such as EC2 instances, can be time-consuming and require additional effort.\nD. Use the CloudFormation service: CloudFormation is an Infrastructure as Code (IaC) service that allows you to define and manage your AWS infrastructure using templates. While CloudFormation can be used to deploy containerized applications, it is not specifically designed for container orchestration. You would still need to manage the containerization of the application using Docker, which would require additional effort.\nIn summary, the ideal way to deploy a containerized microservices and ETL based application onto AWS with the least amount of administrative effort would be to use the Elastic Container Service (ECS). ECS is a fully-managed container orchestration service provided by AWS that abstracts away much of the underlying infrastructure, making it easier to deploy and manage containerized applications.\n\n"
}, {
  "id" : 58,
  "question" : "You are developing a banking application that will interact with a DynamoDB table.\nThe table is going to take in a lot of read and write operations.\nWhich of the following would be the ideal partition key for the DynamoDB table to ensure ideal performance?\n",
  "answers" : [ {
    "id" : "9880abc836d84cb3bd81ca835abae410",
    "option" : "CustomerID",
    "isCorrect" : "true"
  }, {
    "id" : "f05fc4622f6c419cb0f9cf17dd01cce4",
    "option" : "CustomerName",
    "isCorrect" : "false"
  }, {
    "id" : "836708f383c24256922de6e02beb7cf9",
    "option" : "Location",
    "isCorrect" : "false"
  }, {
    "id" : "a4e53cec57394937af65f2f0a4b77f16",
    "option" : "Age.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation gives the ideal way to construct partition Keys.\nRecommendations for partition keys.\nUse high-cardinality attributes.\nThese attributes have distinct values for each item, like e-mailid, employee_no, customerid, sessionid, orderid, and so on.\nUse composite attributes.\nTry to combine more than one attribute to form a unique key, if that meets your access pattern.\nFor example, consider an orders table with customerid+productid+countrycode as the partition key and order_date as the sort key.\nOption B is incorrect because CustomerName may be the same.\nOption C is incorrect because some customers can have the same location.\nOption D is incorrect because Age can be the same for customers.\nFor more information on choosing the right partition Key, please refer to the below Link-\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\n\nWhen designing a DynamoDB table, the partition key is a crucial factor that affects the table's performance. It determines how data is distributed across partitions, and therefore impacts the scalability and speed of read and write operations.\nIn this scenario, we want to choose a partition key that can evenly distribute the data across partitions, avoid hot partitions (where a single partition receives a disproportionate amount of traffic), and minimize the number of partitions that need to be accessed for each query.\nOption A: CustomerID CustomerID could be a good partition key if it is a unique identifier for each customer and is evenly distributed across the table. If so, it would avoid hot partitions and provide good performance. However, if there are customers with the same ID, this option could lead to hot partitions and poor performance.\nOption B: CustomerName Using CustomerName as a partition key is not recommended because it is not guaranteed to be unique and evenly distributed. Names can be duplicated, and some letters may be used more frequently than others, which could lead to uneven distribution of data and hot partitions.\nOption C: Location If the application is designed to query data based on a specific location, Location could be a good choice as the partition key. Queries that involve filtering or sorting by location would only need to access a small subset of the partitions, resulting in faster performance. However, if the application needs to retrieve data across multiple locations, this option could lead to hot partitions and poor performance.\nOption D: Age Using Age as a partition key is not recommended because it is unlikely to be evenly distributed across the table. There may be age ranges that are overrepresented or underrepresented, leading to uneven distribution of data and hot partitions.\nIn conclusion, Option A (CustomerID) could be the ideal partition key for the DynamoDB table, provided that CustomerID is a unique identifier that is evenly distributed across the table.\n\n"
}, {
  "id" : 59,
  "question" : "You are developing an application that will be comprised of the following architecture - 1\nA set of EC2 instances to process the messages. 2\nThese (EC2 instances) will be spun up by an Autoscaling group. 3\nSQS Queues to maintain the processing messages. 4\nThere will be 2 pricing tiers. How will you ensure that the premium customers' messages are given more preference?\n",
  "answers" : [ {
    "id" : "45b3edb05a3049efa64a76571d8473c6",
    "option" : "Create 2 Autoscaling Groups, one for normal and one for premium customers.",
    "isCorrect" : "false"
  }, {
    "id" : "38db05e73427445482fc254b7c546e6f",
    "option" : "Create 2 sets of EC2 Instances, one for normal and one for premium customers.",
    "isCorrect" : "false"
  }, {
    "id" : "16454af4900f44cb83111240be9f4438",
    "option" : "Create 2 SQS queues, one for normal and one for premium customers.",
    "isCorrect" : "true"
  }, {
    "id" : "e9f3ab30291d4d9ca69b1b876d2e108d",
    "option" : "Create 2 Elastic Load Balancers, one for normal and one for premium customers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe ideal option would be to create 2 SQS queues.\nMessages can then be processed by the application from the high priority queue first.\nOption A is incorrect because 2 Auto Scaling groups will still launch the same set of EC2 instances that are defined in the Launch Configuration/Template.\nOption B is incorrect because launching 2 sets of EC2 instances involves lots of manual work.\nOption D is incorrect because having 2 ELB will still require a different set of EC2 instances.\nFor more information on SQS, please refer to the below Link-\nhttps://aws.amazon.com/sqs/\n\nThe correct answer is C. Create 2 SQS queues, one for normal and one for premium customers.\nExplanation:\nTo ensure that premium customers' messages are given more preference, we need to create separate SQS queues for normal and premium customers. This approach is known as queue-based load leveling.\nWhen a message is sent to a queue, it is stored there until it is consumed by a consumer. In this architecture, EC2 instances act as consumers that process messages from the queue.\nTo ensure that premium customers' messages are given more preference, we can set up a priority queue. Priority queues ensure that messages with higher priority are processed first. In this case, premium customers' messages would have a higher priority than normal customers' messages.\nWe can also set up different polling rates for the two queues. For example, we can set a lower polling rate for the normal queue and a higher polling rate for the premium queue. This ensures that messages in the premium queue are processed more quickly.\nCreating separate Autoscaling groups or EC2 instances would not be an efficient approach as it would increase the complexity and cost of the architecture. Creating separate Elastic Load Balancers would also not be an effective approach as they are used for distributing traffic and not for managing queues.\n\n"
}, {
  "id" : 60,
  "question" : "A software engineer is developing an application in AWS using AWS SDK.\nYou need a mechanism implemented in the AWS SDK to deal with \"logging\" and for \"re-trying\" the errors.\nWhich of the following mechanisms is suitable?\n",
  "answers" : [ {
    "id" : "dbaf9c36bb09410db5cf06a4e3c5b0a7",
    "option" : "Multiple SQS queues",
    "isCorrect" : "false"
  }, {
    "id" : "1c2e26dd6e2b4b6a8c88c1fd272f2ee0",
    "option" : "Exponential backoff algorithm",
    "isCorrect" : "true"
  }, {
    "id" : "bfcf96c22be545a182fde4f1f23d0879",
    "option" : "For-loop operations",
    "isCorrect" : "false"
  }, {
    "id" : "471a295f856741b7be2ea9604aa810c9",
    "option" : "Amazon SNS notifications.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nEach AWS SDK implements automatic retry logic.\nIf you're not using an AWS SDK, you should retry original requests that receive server (5xx) or throttling errors.\nHowever, client errors (4xx) indicate that you need to revise the request to correct the problem before trying again.\nIn addition to simple retries, each AWS SDK implements an exponential backoff algorithm for better flow control.\nThe idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses.\nYou should implement a maximum delay interval, as well as a maximum number of retries.\nOption A is incorrect because it is not a mechanism implemented in AWS SDK.\nOption C is incorrect because it does not help to deal with the errors.\nOption D is incorrect because it does not deal with the errors rather it is used notify.\nFor more information on API retries, please refer to the below Link-\nhttps://docs.aws.amazon.com/general/latest/gr/api-retries.html\n\nOption B, the exponential backoff algorithm, is the most suitable mechanism to deal with logging and retrying errors in the AWS SDK.\nThe exponential backoff algorithm is a mechanism that enables an application to retry an API call that has failed, with an exponentially increasing amount of time between each retry attempt. This approach helps to prevent overloading of the system and improve the chances of success by allowing time for the system to recover from any issues.\nFor instance, if an AWS SDK API call fails, the application will wait for a short period, then attempt to retry the call. If the second attempt fails, the application will wait for a longer period, then retry again. This process will continue until the call succeeds, or the maximum number of retries is reached.\nOption A, multiple SQS queues, is not suitable for this situation. SQS queues are used to decouple and scale microservices, but they are not designed to handle retries.\nOption C, for-loop operations, are not suitable either as they do not incorporate any mechanism for error handling or retrying.\nOption D, Amazon SNS notifications, is not the best fit for this scenario since SNS is used for push notification and pub/sub messaging rather than retrying failed requests.\nTherefore, the best option for logging and retrying errors in the AWS SDK is to use the exponential backoff algorithm.\n\n"
}, {
  "id" : 61,
  "question" : "You have created the following stages in CodePipeline. What happens if there is a failure detected in the \"Build\" stage?\n\n",
  "answers" : [ {
    "id" : "36cac7baa27a466cafaf0aea1e12d483",
    "option" : "A rollback will happen at the \"Source\" stage.",
    "isCorrect" : "false"
  }, {
    "id" : "dda421ea5a36485f9bd769a98e46773d",
    "option" : "The \"Build\" step will be attempted again.",
    "isCorrect" : "false"
  }, {
    "id" : "f67963757df54147975e2b85e6cd8b0c",
    "option" : "The \"Build\" step will be skipped and the \"Staging\" step will start.",
    "isCorrect" : "false"
  }, {
    "id" : "14cab978e3bb4cebac8825a6f8a0b604",
    "option" : "The entire process will halt.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nIn AWS CodePipeline, an action is a task performed on an artifact in a stage.\nIf an action or a set of parallel actions is not completed successfully, the pipeline stops running.\nOptions A, B and C are incorrect since the default action will be that the entire pipeline will be stopped if the build does not succeed.\nFor more information on Actions retry, please refer to the below Link-\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-retry.html\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 63,
  "question" : "You are using AWS DynamoDB as a database to save sales data for a global appliance company.\nWhich of the following keys can be used to encrypt the DynamoDB data at rest? (Select TWO)\n",
  "answers" : [ {
    "id" : "6fb86625a5bf4b87a1728fecb894742d",
    "option" : "A customer managed CMK in KMS",
    "isCorrect" : "true"
  }, {
    "id" : "54af3ab0f3eb45f9a03a19c9e27b2394",
    "option" : "AWS managed CMK (aws/rds)",
    "isCorrect" : "false"
  }, {
    "id" : "782cd31ee72f4a62b74c596fafcddaef",
    "option" : "AWS managed CMK (aws/dynamoDB)",
    "isCorrect" : "true"
  }, {
    "id" : "60eeb506d19a4599b7e5beb04275e201",
    "option" : "Server side default encryption key (aws/s3)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A, C.\nFor DynamoDB, you can select an option to encrypt some or all of your tables under a customer-managed CMK or the AWS managed CMK in your account.\nFor more information on AWS KMS, refer to the following URL-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html\n\nDynamoDB supports encryption at rest, which means that your data is encrypted while it is stored in the DynamoDB. By encrypting your data at rest, you can help secure your data from unauthorized access and protect it from security threats.\nDynamoDB uses the AWS Key Management Service (KMS) to encrypt your data at rest. When you create a DynamoDB table, you can choose to use either the default encryption key or a customer-managed CMK (Customer Master Key) in KMS to encrypt your data.\nThe following keys can be used to encrypt the DynamoDB data at rest:\nA. A customer-managed CMK in KMS: You can use a customer-managed CMK in KMS to encrypt your DynamoDB data at rest. With this option, you have full control over the encryption key, and you can rotate the key as needed.\nB. AWS managed CMK (aws/rds): This key is not applicable for encrypting DynamoDB data. It is used for encrypting Amazon RDS (Relational Database Service) data at rest.\nC. AWS managed CMK (aws/dynamoDB): This key is the default encryption key for DynamoDB. When you create a DynamoDB table, it is automatically encrypted using this key.\nD. Server-side default encryption key (aws/s3): This key is used for encrypting data at rest in Amazon S3 (Simple Storage Service) and is not applicable for encrypting DynamoDB data.\nTherefore, the correct answer is A and C. You can use a customer-managed CMK in KMS or the AWS managed CMK (aws/dynamoDB) to encrypt your DynamoDB data at rest.\n\n"
}, {
  "id" : 64,
  "question" : "Your company is going to develop an application in .NET Core with DynamoDB.\nThere is a requirement that all data needs to be encrypted at rest.\nIf the DynamoDB table has already been created, what else is needed to achieve this?\n",
  "answers" : [ {
    "id" : "36b13653fe2a413e8c05b30aa11c7091",
    "option" : "No additional configurations are required since server-side encryption is enabled on all DynamoDB table data.",
    "isCorrect" : "true"
  }, {
    "id" : "8a9d78a4c2254894a1870f9eeb1b3161",
    "option" : "Enable encryption on the existing table.",
    "isCorrect" : "false"
  }, {
    "id" : "1aebb2a2916b47eca99bf494ad6d136e",
    "option" : "You cannot enable encryption at rest. Consider using the AWS RDS service instead.",
    "isCorrect" : "false"
  }, {
    "id" : "432e41ebcb44409f8103d3deaee685ff",
    "option" : "You cannot enable encryption at rest. Consider using the S3 service instead.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nOption B is incorrect since Encryption can only be configured during table creation time.\nOptions C and D are incorrect since Encryption is possible in DynamoDB.The AWS Documentation mentions the following.\nAmazon DynamoDB offers fully managed encryption at rest.\nDynamoDB encryption at rest provides enhanced security by encrypting your data at rest using an AWS Key Management Service (AWS KMS) managed encryption key for DynamoDB.\nThis functionality eliminates the operational burden and complexity involved in protecting sensitive data.\nFor more information on DynamoDB Encryption at rest, please refer to the below Link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html\n\nThe correct answer is B. Enable encryption on the existing table.\nAmazon DynamoDB provides encryption at rest by encrypting all table data using an AES-256 algorithm. By default, server-side encryption is enabled on all DynamoDB table data. However, this does not mean that encryption is automatically enabled for all existing tables. You must enable encryption for each table individually.\nTo enable encryption at rest for an existing DynamoDB table, you can perform the following steps:\nOpen the Amazon DynamoDB console. Choose the table that you want to encrypt. Choose the \"Manage DynamoDB Encryption\" option from the \"Actions\" dropdown menu. Choose the KMS key that you want to use to encrypt the table. Choose the \"Encrypt\" button.\nAfter you have enabled encryption for a table, all data that is written to the table will be encrypted at rest. Note that you cannot disable encryption after it has been enabled for a table.\nOption A is incorrect because while server-side encryption is enabled on all DynamoDB table data, it does not mean that encryption is automatically enabled for all existing tables.\nOption C is incorrect because you can enable encryption at rest for DynamoDB tables.\nOption D is incorrect because S3 is a different service from DynamoDB, and it does not provide encryption for table data at rest.\n\n"
}, {
  "id" : 65,
  "question" : "You have been instructed to manage the deployments of an application onto Elastic Beanstalk.\nSince this is just a development environment, you have been told to ensure that the least amount of time is taken for each deployment.\nWhich of the following deployment mechanism would you consider based on this requirement?\n",
  "answers" : [ {
    "id" : "a7d91aa6501f4ef5bdf043c1c9fcd1b4",
    "option" : "All at once",
    "isCorrect" : "true"
  }, {
    "id" : "f24482da1c834fc9a83c5deaec18dc87",
    "option" : "Rolling",
    "isCorrect" : "false"
  }, {
    "id" : "6abe98ed9a564361a44abf4d725f1106",
    "option" : "Immutable",
    "isCorrect" : "false"
  }, {
    "id" : "d01bf283a23d4959bb14f2f613b95fb0",
    "option" : "Rolling with additional batch.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nBelow is the screenshot of the deployment options.\nThe ‘All at once' is the least deployment option.\nBased on the above screenshot, all the other options become invalid.\nFor more information on the deployment options, please refer to the below Link-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\n\n\nWhen deploying an application onto Elastic Beanstalk, there are several deployment mechanisms available that can be used based on the specific requirements of the application and deployment process.\nThe four deployment mechanisms available are:\nA. All at once: This deployment mechanism deploys the new version of the application to all instances in the environment at the same time. It is the fastest deployment option since all instances are updated simultaneously. However, this option can result in downtime as all instances are unavailable during the deployment process.\nB. Rolling: This deployment mechanism updates a percentage of the instances in the environment at a time. After the initial batch is updated, traffic is redirected to the updated instances, and the process repeats until all instances have been updated. This option minimizes downtime as traffic is redirected to the updated instances, but it can take longer than the \"all at once\" option since the update process is spread out over a longer period of time.\nC. Immutable: This deployment mechanism deploys a new version of the application to a new set of instances and then swaps the new instances with the old instances. This option results in zero downtime as the new instances are running before the old instances are terminated. However, this option can take longer than the other options since new instances must be launched and configured before the swap can occur.\nD. Rolling with additional batch: This deployment mechanism is similar to the \"rolling\" option, but it deploys an additional batch of instances before the initial batch is updated. This option reduces downtime by ensuring that there are always enough instances available to handle traffic. However, this option can take longer than the \"rolling\" option since there are more instances to update.\nBased on the requirement to minimize the time taken for each deployment in a development environment, the \"All at once\" deployment mechanism would be the best option since it is the fastest deployment option available. However, it is important to note that this option can result in downtime during the deployment process, so it should be used with caution in production environments.\n\n"
}, {
  "id" : 66,
  "question" : "You're developing an application onto AWS which is based on the Microservices.\nThese Microservices will be created based on AWS Lambda functions.\nBecause of the complexity of the flow of these different components, you need some way to manage the workflow of execution of these various Lambda functions.\nHow could you manage this effectively now and for the future addition of Lambda functions to the application?\n",
  "answers" : [ {
    "id" : "38a604a0d08548bb9eae28c46bf96a8d",
    "option" : "Consider creating a master Lambda function that would coordinate the execution of the other Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "48e86033bce34d1596268ae2442a6fc5",
    "option" : "Consider creating a separate application hosted on an EC2 Instance which would coordinate the execution of the other Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "a45ecdfde6184dff94ff61d105491cf2",
    "option" : "Consider using Step Functions to coordinate the execution of the other Lambda functions.",
    "isCorrect" : "true"
  }, {
    "id" : "cbed12e156f440e2ae7d4f8182f43e5a",
    "option" : "Consider using SQS queues to coordinate the execution of the other Lambda functions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe best way to manage this is to use Step Functions.\nThe AWS Documentation mentions the following about Step Functions.\nAWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows.\nYou build applications from individual components that perform a discrete function, or task, allowing you to scale and change applications quickly.\nStep Functions provide a reliable way to coordinate components and step through the functions of your application.\nOptions A and B are invalid.\nEven though feasible, it would just bring too much of effort and maintenance into the entire system.\nOption D is invalid because this is good in managing the messaging between distributed components of an application.\nFor more information on Step Functions, please refer to the below Link-\nhttps://docs.aws.amazon.com/step-functions/latest/dg/welcome.html\n\nWhen developing an application based on microservices architecture using AWS Lambda functions, it is important to manage the workflow of execution of these various Lambda functions. This management should be effective now and for future addition of Lambda functions to the application.\nThere are different ways to manage the workflow of execution of Lambda functions in AWS. The options provided in the question are:\nA. Consider creating a master Lambda function that would coordinate the execution of the other Lambda functions. This option involves creating a master Lambda function that will manage the workflow of execution of the other Lambda functions. The master Lambda function will be responsible for invoking the other Lambda functions based on the workflow design. This option is feasible for smaller applications with simple workflows.\nB. Consider creating a separate application hosted on an EC2 Instance which would coordinate the execution of the other Lambda functions. This option involves creating a separate application hosted on an EC2 instance that will manage the workflow of execution of the Lambda functions. The separate application will be responsible for invoking the Lambda functions based on the workflow design. This option is feasible for applications with complex workflows and requires additional management of the EC2 instance.\nC. Consider using Step Functions to coordinate the execution of the other Lambda functions. This option involves using AWS Step Functions to manage the workflow of execution of the Lambda functions. AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. AWS Step Functions can manage the Lambda functions and other AWS services using AWS SDKs, which makes it easy to implement, test, and deploy workflows.\nD. Consider using SQS queues to coordinate the execution of the other Lambda functions. This option involves using Amazon Simple Queue Service (SQS) to manage the workflow of execution of the Lambda functions. SQS is a fully managed message queuing service that enables decoupling of components of a cloud application. The Lambda functions can be triggered by SQS messages, and the order of execution can be managed using message priority.\nIn summary, the best option to manage the workflow of execution of Lambda functions depends on the complexity of the application and the workflow. For simple workflows, option A or B can be considered. For complex workflows, options C or D can be considered. However, using AWS Step Functions (option C) is a recommended option because it provides a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows.\n\n"
}, {
  "id" : 67,
  "question" : "You are developing an application that will be used to receive data from multiple devices.\nYou need to perform some preprocessing on the data before it can be analyzed by the Analytics tool.\nAll the received data are compressed records that need to be decompressed to be analyzed further.\nWhich of the following can be used to carry out this intermediate activity?\n",
  "answers" : [ {
    "id" : "8a752d785acc4f51a691455247533f76",
    "option" : "Use Step Functions to pre-process the data.",
    "isCorrect" : "false"
  }, {
    "id" : "f7b65ac189b540a4abc485a42722593c",
    "option" : "Use Kinesis with AWS Lambda functions to pre-process the data.",
    "isCorrect" : "true"
  }, {
    "id" : "2845314f203547179302ba118338c690",
    "option" : "Use the AWS CloudFront service to pre-process the data.",
    "isCorrect" : "false"
  }, {
    "id" : "783e45bf581941699351eaab20627b78",
    "option" : "Use ELB to pre-process the data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nMany customers use Amazon Kinesis to ingest, analyze, and persist their streaming data.One of the easiest ways to gain real-time insights into your streaming data is to use Kinesis Analytics.It enables you to query the data in your stream or build entire streaming applications using SQL.\nCustomers use Kinesis Analytics for things like filtering, aggregation, and anomaly detection.\nA data producer is compressing JSON records before sending them to a Kinesis stream or a Kinesis Firehose delivery stream.\nYou want to use Kinesis Analytics to analyze these compressed records.Before you can use SQL to perform the analysis, you must first decompress each input record so that it's represented as decompressed JSON.This enables it to map to the schema you've created in the Kinesis Analytics application.\nOption A is incorrect since this service is used to coordinate different parts of a distributed application.\nOption C is incorrect since this service is used to cache static and dynamic content of a website hosted in AWS Cloud.\nOption D is incorrect since CloudFront can not be used to pre-process the data.\nFor more information on preprocessing data in Kinesis, please refer to the below Link-\nhttps://aws.amazon.com/blogs/big-data/preprocessing-data-in-amazon-kinesis-analytics-with-aws-lambda/\n\nThe correct answer is B. Use Kinesis with AWS Lambda functions to pre-process the data.\nExplanation:\nWhen dealing with large amounts of data coming in from multiple sources, it is often necessary to preprocess that data before it can be analyzed. In this scenario, the data is compressed and needs to be decompressed before it can be analyzed. AWS provides a variety of services that can be used for this kind of preprocessing, but the most appropriate service in this case is Kinesis with AWS Lambda functions.\nKinesis is a service provided by AWS that is designed for real-time processing of streaming data. Kinesis can be used to receive and process large volumes of data in real-time from multiple sources. AWS Lambda, on the other hand, is a compute service that allows you to run code in response to events, such as the arrival of new data in Kinesis. Lambda can be used to perform data preprocessing tasks such as decompressing data.\nUsing Kinesis with Lambda functions provides several benefits for this use case:\nScalability: Kinesis is designed to handle large volumes of data, and Lambda can be used to process that data in parallel, making it possible to handle even more data. Flexibility: Since Lambda functions can be written in many programming languages, you can choose the language that best suits your needs. Cost-effective: With Kinesis and Lambda, you only pay for what you use. This means that you can process large volumes of data without incurring high costs.\nA is incorrect because Step Functions is a service that is used to coordinate the components of distributed applications and workflows using visual workflows. It is not designed for data preprocessing tasks.\nC is incorrect because AWS CloudFront is a content delivery network that is used to deliver content to users from edge locations. It is not designed for data preprocessing tasks.\nD is incorrect because Elastic Load Balancing (ELB) is a service that distributes incoming network traffic across multiple targets, such as EC2 instances. It is not designed for data preprocessing tasks.\n\n"
}, {
  "id" : 68,
  "question" : "Your team has completed development of an application.\nNow, this needs to be deployed to an application on an EC2 Instance.\nThe Application data will be stored on a separate volume which needs to be encrypted at rest.\nHow can you ensure this requirement is met?\n",
  "answers" : [ {
    "id" : "cfa747284d424d5cb3e037b255455a95",
    "option" : "Ensure that Encryption is enabled during volume creation time.",
    "isCorrect" : "true"
  }, {
    "id" : "142269c2d0704936b565b3ad37c7b9ea",
    "option" : "Ensure to use Throughput Optimized HDD to allow for Encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "bac3287f572f4cbab439c800a56c5270",
    "option" : "Create a Customer master key in the KMS service.",
    "isCorrect" : "false"
  }, {
    "id" : "cf2358ef62f1472fb109034a6d28ecda",
    "option" : "Create an EBS Encryption Key.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon EBS encryption uses AWS Key Management Service (AWS KMS) customer master keys (CMKs) when creating encrypted volumes and any snapshots created from them.\nA unique AWS-managed CMK is created for you automatically in each region where you store AWS assets.\nThis key is used for Amazon EBS encryption.\nOption B is incorrect since Encryption is possible on all EBS volume types.\nOption C is incorrect because encryption of an EBS volume can be done without generating a CMK in the console.\nOption D is incorrect since you need to create the Encryption Key in the KMS service.\nFor more information on EBS Encryption, please refer to the below Link-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\n\nTo ensure that the data stored on a separate volume is encrypted at rest when deploying an application on an EC2 instance, the following steps can be taken:\nA. Ensure that Encryption is enabled during volume creation time: When creating the volume for the EC2 instance, select the option to enable encryption. This will ensure that all data written to the volume is encrypted at rest. However, this alone may not be sufficient to ensure the security of the data, as the encryption key may not be protected adequately.\nB. Ensure to use Throughput Optimized HDD to allow for Encryption: This option is incorrect. The type of EBS volume used has no impact on the ability to encrypt the data at rest. All EBS volume types support encryption, and the encryption process is performed at the software level, rather than the hardware level.\nC. Create a Customer master key in the KMS service: Create a customer master key (CMK) in the AWS Key Management Service (KMS) service. This key can be used to encrypt the volume, ensuring that only authorized users can access the data. The CMK is a cryptographic key that is used to encrypt and decrypt data. Once the key is created, it can be used to encrypt the volume.\nD. Create an EBS Encryption Key: This option is incorrect. An EBS encryption key is used to encrypt the data stored on an EBS volume. However, it is not sufficient to ensure the encryption of the data at rest on a separate volume attached to an EC2 instance. A CMK should be used instead.\nIn summary, to ensure that the data stored on a separate volume attached to an EC2 instance is encrypted at rest, the recommended approach is to create a CMK in the KMS service and use it to encrypt the volume during creation.\n\n"
}, {
  "id" : 69,
  "question" : "You've been given the requirement to customize the content which is distributed to users via a Cloudfront Distribution.\nThe content origin is an S3 bucket.\nHow could you achieve this?\n",
  "answers" : [ {
    "id" : "8cfee5c1565a4da88017bf7dc5a7fa69",
    "option" : "Add an event to the S3 bucket. Make the event invoke a Lambda function that would customize the content.",
    "isCorrect" : "false"
  }, {
    "id" : "b828947a5c4846aeb51f463f21c8bb6c",
    "option" : "Add a Step Function. Add a step with a Lambda function just before the content gets delivered to the users.",
    "isCorrect" : "false"
  }, {
    "id" : "9586af72575b44209be9e2d7d0fa882a",
    "option" : "Consider using Lambda@Edge.",
    "isCorrect" : "true"
  }, {
    "id" : "0279f51f9b824e58956299c8d4ebb0e2",
    "option" : "Consider using a separate application on an EC2 Instance for this purpose.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nLambda@Edge is an extension of AWS Lambda, a compute service that lets you execute functions that customize the content that CloudFront delivers.\nYou can author functions in one region and execute them in AWS locations globally closer to the viewer, without provisioning or managing servers.\nLambda@Edge scales automatically, from a few requests per day to thousands per second.\nProcessing requests at AWS locations closer to the viewer than on origin servers significantly reduces latency and improves the user experience.\nOption A is incorrect because although adding an event to the S3 bucket and then invoking the event by the Lambda function to customize the content may do the needful, a lot of manual intervention is required.\nWith Lambda @ Edge, it can be done more easily.\nOption B is incorrect because Step Function is normally used to sequence AWS Lambda functions and multiple AWS services.\nThis scenario is not suitable.\nOption D is incorrect because adding EC2 is not cost-efficient.\nFor more information on Lambda@Edge, please refer to the below Link-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-at-the-edge.html\n\nThe correct answer for this question is option C, \"Consider using Lambda@Edge.\"\nLambda@Edge is a serverless computing service provided by AWS that allows developers to run custom code in response to events generated by Amazon CloudFront. With Lambda@Edge, you can write and deploy code to modify and customize content delivered through CloudFront.\nTo customize the content distributed to users via a CloudFront Distribution, you can create a Lambda@Edge function that modifies the content at the edge location before it is delivered to the end-user. You can attach this Lambda function to your CloudFront Distribution and configure it to run whenever a viewer requests content from your distribution.\nThis solution is much simpler and more efficient than the other options listed.\nOption A, \"Add an event to the S3 bucket. Make the event invoke a Lambda function that would customize the content,\" requires additional setup and configuration. You would need to create an event notification in S3 to trigger the Lambda function, and you would need to ensure that the Lambda function has access to modify the content in the S3 bucket.\nOption B, \"Add a Step Function. Add a step with a Lambda function just before the content gets delivered to the users,\" is also possible, but it is more complex and requires more configuration. You would need to create a Step Function that includes a Lambda function that modifies the content, and you would need to configure CloudFront to use the Step Function to modify the content.\nOption D, \"Consider using a separate application on an EC2 Instance for this purpose,\" is also possible, but it requires you to manage the EC2 instance, which can be time-consuming and costly. Additionally, using an EC2 instance for this purpose is overkill when a serverless solution like Lambda@Edge is available.\n\n"
}, {
  "id" : 70,
  "question" : "Your team has been instructed to develop a completely new solution for AWS.\nCurrently, you have a limitation on the tools available to manage the complete lifecycle of the project.\nWhich of the following service from AWS could help you handle all aspects of development and deployment?\n",
  "answers" : [ {
    "id" : "511e9e5579a04b9ba7fc0bcc242a9d9d",
    "option" : "AWS CodePipeline",
    "isCorrect" : "false"
  }, {
    "id" : "072bb56a4d6a47589684e7c7256c5157",
    "option" : "AWS CodeBuild",
    "isCorrect" : "false"
  }, {
    "id" : "5a43155e5f7e44529f0525cbc2f5550c",
    "option" : "AWS CodeCommit",
    "isCorrect" : "false"
  }, {
    "id" : "d8e8d6071f6c4d38a1ba9653d65ae758",
    "option" : "AWS CodeStar.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAWS CodeStar is a cloud-based service for creating, managing, and working with software development projects on AWS.\nYou can quickly develop, build, and deploy applications on AWS with an AWS CodeStar project.\nAn AWS CodeStar project creates and integrates AWS services for your project development toolchain.\nDepending on your choice of AWS CodeStar project template, that toolchain might include source control, build, deployment, virtual servers or serverless resources, and more.\nAWS CodeStar also manages the permissions required for project users (called team members)\nBy adding users as team members to an AWS CodeStar project, project owners can quickly grant each team member role-appropriate access to a project and its resources.\nOption A is incorrect since this service is used for managing CI/CD pipelines.\nOption B is incorrect since this service is used for managing code builds.\nOption C is incorrect since this service is used for managing source code versioning repositories.\nFor more information on AWS CodeStar, please refer to the below Link-\nhttps://docs.aws.amazon.com/codestar/latest/userguide/welcome.html\n\nAWS offers various services that can help you manage the complete lifecycle of your project, including development, testing, deployment, and monitoring. To choose the right service, you need to consider your specific requirements and constraints.\nIn this scenario, the team is instructed to develop a completely new solution for AWS, and they have a limitation on the tools available to manage the complete lifecycle of the project. Therefore, they need a service that can handle all aspects of development and deployment.\nOut of the given options, AWS CodeStar is the service that can help you manage the complete lifecycle of your project. AWS CodeStar is a fully managed service that enables you to quickly develop, build, and deploy applications on AWS. It provides project templates for various programming languages and frameworks, which can help you get started quickly. AWS CodeStar integrates with other AWS services, such as AWS CodeCommit, AWS CodeBuild, and AWS CodePipeline, to provide a complete DevOps solution.\nAWS CodeStar offers the following features:\nProject templates: AWS CodeStar provides project templates for various programming languages and frameworks, such as Java, Python, Node.js, Ruby, and PHP. You can choose a template that best fits your project requirements. Integrated tools: AWS CodeStar integrates with other AWS services, such as AWS CodeCommit, AWS CodeBuild, and AWS CodePipeline, to provide a complete DevOps solution. Continuous integration and delivery (CI/CD): AWS CodeStar provides automated build, test, and deployment pipelines using AWS CodePipeline and AWS CodeBuild. You can configure the pipelines to automatically deploy your application to AWS services, such as Amazon EC2, AWS Lambda, and Amazon ECS. Team collaboration: AWS CodeStar provides a collaboration environment for your team members. You can use AWS CodeStar to invite team members, assign roles and permissions, and manage access to your AWS resources.\nTherefore, AWS CodeStar is the best option for the team in this scenario as it can handle all aspects of development and deployment, including project templates, integrated tools, CI/CD, and team collaboration.\n\n"
}, {
  "id" : 71,
  "question" : "Your team has just finished developing a new version of an existing application.\nThis is a web-based application hosted on AWS.\nCurrently, Route 53 is being used to point the company's DNS name to the web site.\nYour Management has instructed you to deliver the new application to a portion of the users for testing.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "b59956c255be4902847308586d619115",
    "option" : "Port the application onto Elastic beanstalk and use the Swap URL feature.",
    "isCorrect" : "false"
  }, {
    "id" : "53aa777fd96741c78f27eb739de88751",
    "option" : "Use Route 53 weighted Routing policies.",
    "isCorrect" : "true"
  }, {
    "id" : "1f68ad5a2a4e4f3b845b7b6764ab5734",
    "option" : "Port the application onto Opswork by creating a new stack.",
    "isCorrect" : "false"
  }, {
    "id" : "0aab9dde1e7f44609ad016726a36bd76",
    "option" : "Use Route 53 failover Routing policies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following to support this.\nWeighted Routing.\nWeighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource.\nThis can be useful for various purposes, including load balancing and testing new versions of software.\nTo configure weighted routing, you create records that have the same name and type for each of your resources.\nYou assign each record a relative weight that corresponds with how much traffic you want to send to each resource.\nAmazon Route 53 sends traffic to a resource based on the weight that you assign to the record as a proportion of the total weight for all records in the group:\nFormula for how much traffic is routed to a given resource:\nweight for a specified record/sum of the weights for all records.\nFor example, if you want to send a tiny portion of your traffic to one resource and the rest to another resource, you might specify weights of 1 and 255\nThe resource with a weight of 1 gets 1/256th of the traffic (1/1+255), and the other resource gets 255/256ths (255/1+255)\nYou can gradually change the balance by changing the weights.\nIf you want to stop sending traffic to a resource, you can change the weight for that record to 0.\nOptions A and C is incorrect since this would cause a full flown deployment of the new app and is just a maintenance overhead to port the application to a new service environment.\nOption D is incorrect since this should only be used for failover conditions.\nFor more information on the weighted routing policy, please refer to the below Link-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\n\nThe answer is B. Use Route 53 weighted Routing policies.\nExplanation: To achieve this, we can use Route 53 weighted routing policies to direct a portion of traffic to the new application while leaving the rest of the traffic directed to the existing application. This allows us to test the new application in a controlled manner.\nWeighted routing policies allow you to specify the percentage of traffic that should be sent to each of your application endpoints. For example, you can configure Route 53 to send 80% of the traffic to the existing application and 20% to the new application.\nTo set this up, you would create two records in your Route 53 hosted zone. One record would point to the existing application, and the other would point to the new application. You would then create a weighted routing policy that would distribute the traffic between the two records based on the weights you specify.\nOnce you have verified that the new application is working correctly, you can then update the weighted routing policy to direct all traffic to the new application, effectively making it live.\nThe other options are not suitable for achieving this requirement: A. Port the application onto Elastic Beanstalk and use the Swap URL feature: This option allows you to deploy a new version of an application to Elastic Beanstalk and swap the URLs so that the new version becomes the live version. However, this would affect all users, and we need to target a portion of the users for testing.\nC. Port the application onto Opswork by creating a new stack: OpsWorks is a configuration management service that allows you to automate deployments and manage infrastructure using Chef. This option would require significant effort to set up a new stack and deploy the application, and it is not necessary for achieving the requirement.\nD. Use Route 53 failover Routing policies: Failover routing policies are used to direct traffic to a secondary endpoint when the primary endpoint becomes unavailable. This is not suitable for directing traffic to a new application for testing purposes.\n\n"
}, {
  "id" : 72,
  "question" : "Your AWS Lambda function writes to an S3 bucket.\nWhich of the following is the best practice to pass operational parameters, such as the bucket name, to your Lambda function?\n",
  "answers" : [ {
    "id" : "9392a229ed87416eb936f5b0fc977ef5",
    "option" : "Configure Amazon S3 bucket name with AWS Lambda Environment Variables.",
    "isCorrect" : "true"
  }, {
    "id" : "2ac391c395134ab1995da7a6b9fe6091",
    "option" : "Hard Code Amazon S3 bucket name to Lambda Function.",
    "isCorrect" : "false"
  }, {
    "id" : "1cf958387f3e47c8a480e351b2a0d2e8",
    "option" : "Configure Amazon S3 bucket name with AWS Lambda Function Variables.",
    "isCorrect" : "false"
  }, {
    "id" : "cc9fc5c77408417d95dcbaf25ff6d8b5",
    "option" : "Configure an Alias with Amazon S3 bucket name &amp; pass it to Lambda Function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nFor Lambda functions, Environment variables can be enabled to dynamically pass settings to function code and libraries without making changes to your code.\nEnvironment variables are key-value pairs that are created as a part of function configuration.\nInstead of hard coding the S3 bucket name in the Lambda function, it can pass as an environmental variable in the above requirement.\nOption B is incorrect as Hard coding the S3 Bucket name in a Lambda function is not a best practice.\nOption C is incorrect as Function Variables enable one or more Lambda functions to be published while environmental variables allow to pass settings to function codes dynamically.\nIn the above case, the S3 bucket name needs to be passed to function codes.\nOption D is incorrect as this is not a correct way of passing the S3 bucket name to a Lambda function.\nFor more information on AWS Lambda Environment Variables, refer to the following URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\n\nWhen writing to an S3 bucket from an AWS Lambda function, it is important to pass operational parameters, such as the bucket name, to your function in a secure and efficient manner. Here are the best practices for passing the bucket name to your Lambda function:\nA. Configure Amazon S3 bucket name with AWS Lambda Environment Variables: This is the recommended approach for passing the bucket name to your Lambda function. You can define environment variables for your function that specify the bucket name. Environment variables are easy to manage and can be updated without modifying your code. Additionally, they are encrypted at rest and in transit, making them a secure way to pass sensitive information to your function.\nB. Hard Code Amazon S3 bucket name to Lambda Function: This approach is not recommended as it makes your code less flexible and harder to maintain. Hardcoding the bucket name also makes it more difficult to test your code and could lead to errors if the bucket name needs to be changed.\nC. Configure Amazon S3 bucket name with AWS Lambda Function Variables: This approach is similar to using environment variables, but function variables are not encrypted by default. While this approach is possible, it is not recommended as it is less secure than using environment variables.\nD. Configure an Alias with Amazon S3 bucket name & pass it to Lambda Function: This approach is not recommended as aliases are intended for versioning and deployment of your Lambda function, not for passing operational parameters.\nIn summary, the best practice for passing operational parameters, such as the S3 bucket name, to your AWS Lambda function is to use environment variables.\n\n"
}, {
  "id" : 73,
  "question" : "You have a number of Lambda functions that need to be deployed using AWS CodeDeploy.\nThe lambda functions have gone through multiple code revisions, and versioning in Lambda is being used to maintain the revisions.\nWhich of the following must be done to ensure that the right version of the function is deployed in AWS CodeDeploy?\n",
  "answers" : [ {
    "id" : "1c0904133d844aa99a7793386e480662",
    "option" : "Specify the version to be deployed in the AppSpec file.",
    "isCorrect" : "true"
  }, {
    "id" : "d4e49213ce4b47dda3e028efb24a2733",
    "option" : "Specify the version to be deployed in the BuildSpec file.",
    "isCorrect" : "false"
  }, {
    "id" : "aacbbf00a9d84085bd8b26b5b5c7bda8",
    "option" : "Create a Lambda function environment variable called ‘VER’ and mention the version that needs to be deployed.",
    "isCorrect" : "false"
  }, {
    "id" : "bf12ba165b0f4e698c316e25a8d98ed7",
    "option" : "Create an ALIAS for the Lambda function. Mark this as the recent version. Use this ALIAS in CodeDeploy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nIf your application uses the AWS Lambda compute platform, the AppSpec file can be formatted with either YAML or JSON.\nIt can also be typed directly into an editor in the console.\nThe AppSpec file is used to specify.\nThe AWS Lambda function version to deploy.\nThe functions to be used as validation tests.\nOption B is incorrect because you cannot specify the version in the BuildSpec file, as it is a collection of build commands and related settings, in YAML format, that CodeBuild uses to run a build.\nOption C and D are incorrect because both options do not mention how to use the variable or alias of the Lambda function.\nThe CodeDeploy AppSpec file should be used to specify the Lambda function version.\nOption A is more accurate.\nFor more information on the application specification files, please refer to the below Link-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/application-specification-files.html\n\nWhen deploying Lambda functions using AWS CodeDeploy, the correct version of the function needs to be specified to ensure that the desired code is deployed. Versioning in Lambda can be used to maintain the revisions of the code, and there are different ways to specify the version to be deployed in CodeDeploy.\nLet's review the options provided in the question:\nA. Specify the version to be deployed in the AppSpec file.\nAWS CodeDeploy uses an AppSpec file to define the deployment instructions for an application. The AppSpec file specifies the source files to be deployed, the destination locations for the files, and any hooks or scripts to be run during the deployment process. It is possible to specify the version of the Lambda function to be deployed in the AppSpec file, using the \"LambdaRevision\" property. This is a valid option for ensuring that the right version of the function is deployed in CodeDeploy.\nB. Specify the version to be deployed in the BuildSpec file.\nAWS CodeBuild uses a BuildSpec file to define the build and packaging instructions for a project. The BuildSpec file specifies the build environment, the build commands, and the artifacts to be generated by the build process. It is not possible to specify the version of the Lambda function to be deployed in the BuildSpec file, as this file is used for the build process, not for the deployment process. This option is not valid for ensuring that the right version of the function is deployed in CodeDeploy.\nC. Create a Lambda function environment variable called ‘VER' and mention the version that needs to be deployed.\nLambda function environment variables can be used to pass configuration information to the function code. However, setting an environment variable called \"VER\" with the desired version of the function is not a valid way to ensure that the right version of the function is deployed in CodeDeploy. Environment variables are not used by CodeDeploy to determine which version of the function to deploy.\nD. Create an ALIAS for the Lambda function. Mark this as the recent version. Use this ALIAS in CodeDeploy.\nAn alias is a pointer to a specific version of a Lambda function. By creating an alias and associating it with the desired version of the function, it is possible to ensure that the right version of the function is deployed in CodeDeploy. The alias can be used in the AppSpec file to specify the function to be deployed, and any subsequent updates to the alias will automatically route traffic to the new version. This is a valid option for ensuring that the right version of the function is deployed in CodeDeploy.\nIn summary, the correct answers for ensuring that the right version of the function is deployed in AWS CodeDeploy are:\nSpecify the version to be deployed in the AppSpec file. Create an ALIAS for the Lambda function. Mark this as the recent version. Use this ALIAS in CodeDeploy.\n\n"
}, {
  "id" : 74,
  "question" : "You've been hired to develop a gaming application for a large company.\nThe application will be developed using AWS resources.\nYou need to ensure that the right services are used during the development and subsequent deployment of the application.\nWhich of the following would you consider incorporating to ensure leaderboards can be maintained accurately in the application?\n",
  "answers" : [ {
    "id" : "c1fa4ef162fc4711a13d6d6ab12d65c4",
    "option" : "AWS ElasticBeanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "2b8d6f06134f4ff6b493e6304d12a8e8",
    "option" : "AWS ElastiCache – Redis",
    "isCorrect" : "true"
  }, {
    "id" : "1c4c60782cfb41a1b45c1348fe23a997",
    "option" : "AWS ElastiCache – Memcached",
    "isCorrect" : "false"
  }, {
    "id" : "8becd0aec4974a94a60297099a2ee09a",
    "option" : "AWS Opswork.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following as one of the key advantages of using AWS Redis ElastiCache.\nGaming Leaderboards (Redis Sorted Sets)\nRedis sorted sets move the computational complexity associated with leaderboards from your application to your Redis cluster.\nLeaderboards, such as the Top 10 scores for a game, are computationally complex, especially with a large number of concurrent players and continually changing scores.\nRedis sorted sets guarantee both uniqueness and element ordering.\nUsing Redis sorted sets, each time a new element is added to the sorted set, it's reranked in real-time.\nIt's then added to the set in its appropriate numeric position.\nIn the following diagram, you can see how an ElastiCache for Redis gaming leaderboard works.\nOption A and D are incorrect because both Elastic BeanStalk and OpsWorks are orchestration services offered by Amazon Web Services for deploying applications.\nThey do not provide the service required in the question.\nOption C is incorrect because Redis provides more features than Memcached, such as backup and restore.\nOption B is better.\nFor the differences between Memcached and Redis, please check https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/SelectEngine.html.\nFor more information on AWS ElastiCache Redis, please refer to the below Link-\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-use-cases.html#elasticache-for-redis-use-cases\n\n\nTo maintain leaderboards accurately in a gaming application developed using AWS resources, the most suitable option would be to use a caching service to store and retrieve data quickly.\nOption B and C, AWS ElastiCache - Redis and AWS ElastiCache - Memcached respectively, are both caching services provided by AWS, with Redis being the more popular option. These caching services are ideal for applications that require low-latency data access, which is important for gaming applications that require real-time updates to leaderboards.\nAWS ElasticBeanstalk (Option A) is a platform that helps developers deploy and manage web applications in various programming languages such as Java, Python, PHP, Ruby, etc. It automates the deployment process, making it easy to deploy, monitor, and scale web applications. However, it is not specifically designed to manage leaderboards in a gaming application.\nAWS Opswork (Option D) is an application management service that helps developers automate operations such as deployment, configuration, and management of applications. It can be used to deploy and manage applications using Chef, a configuration management tool. While it can be used to deploy a gaming application, it does not have any specific features for maintaining leaderboards.\nTherefore, the most suitable option for maintaining leaderboards accurately in a gaming application developed using AWS resources is to use AWS ElastiCache - Redis (Option B).\n\n"
}, {
  "id" : 75,
  "question" : "You're developing an application that will be hosted on an EC2 Instance.\nThis will be part of an Autoscaling Group.\nThe application needs to get the private IP of the instance to send it across to a controller-based application.\nWhich of the following can be done to achieve this?\n",
  "answers" : [ {
    "id" : "ac82b7f1a7104f3caff6e91838abee77",
    "option" : "Query the Instance Meta Data",
    "isCorrect" : "true"
  }, {
    "id" : "b7064d87a72e42da9b327be1db2e08aa",
    "option" : "Query the Instance User Data",
    "isCorrect" : "false"
  }, {
    "id" : "5d988ea017ac453cb66a16e8129c5443",
    "option" : "Have an Admin get the IP address from the console",
    "isCorrect" : "false"
  }, {
    "id" : "522921dfaa2142a2b3e1a3bf3412332e",
    "option" : "Make the application run IFConfig.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe application can use the application metadata to get the private IP address.\nThe below snapshot from the AWS Documentation shows the information that you can get from the Instance metadata.\nOption B is invalid because this cannot be used to get the IP address of the instance.\nOption C is invalid because this is not an automated approach.\nOption D is invalid because we don't know the type of instance the application is running on.\nFor more information on AWS Instance Metadata, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\n\n\nTo get the private IP of an EC2 instance that is part of an Autoscaling Group, there are multiple ways to achieve this:\nOption A: Query the Instance Metadata Each EC2 instance has metadata associated with it that can be queried by making a request to the metadata service at a well-known IP address (169.254.169.254). This service provides information about the instance, including its private IP address. To retrieve the private IP address of an EC2 instance, you can make an HTTP request to the following URL within the instance: http://169.254.169.254/latest/meta-data/local-ipv4 This will return the private IP address of the instance.\nOption B: Query the Instance User Data When launching an EC2 instance, you can provide user data in the form of a script or text that is executed when the instance is first launched. This user data can be used to pass configuration information, launch scripts, and more. To retrieve the private IP address of an EC2 instance using user data, you can include a script that fetches the IP address and then passes it to the application. This script can be run as part of the user data, which is executed when the instance is first launched.\nOption C: Have an Admin get the IP address from the console This option is not recommended since it requires manual intervention and is prone to errors. In addition, this method will not work well in a dynamic Autoscaling Group environment where instances are frequently launched and terminated.\nOption D: Make the application run IFConfig This option is not recommended since it requires the application to have permissions to run system-level commands like IFConfig, which is not recommended from a security perspective.\nIn summary, the recommended options to retrieve the private IP address of an EC2 instance that is part of an Autoscaling Group are either to query the instance metadata or pass it as user data during instance launch.\n\n"
}, {
  "id" : 76,
  "question" : "You are using S3 buckets to store images.\nThese S3 buckets invoke a lambda function on upload.\nThe Lambda function creates thumbnails of the images and stores them in another S3 bucket.\nAn AWS CloudFormation template is used to create the Lambda function with the resource \"AWS::Lambda::Function\"\nWhich of the following attributes is the method name that Lambda calls to execute the function? Sample CloudFormation template:\n\n",
  "answers" : [ {
    "id" : "9b99c2b84e6149cb980f0ffe6121b929",
    "option" : "FunctionName",
    "isCorrect" : "false"
  }, {
    "id" : "d63e8dbddcfc4645ab933a6b9824db56",
    "option" : "Layers",
    "isCorrect" : "false"
  }, {
    "id" : "e04a186e5d0f48b5a3d76f6cec7a076a",
    "option" : "Environment",
    "isCorrect" : "false"
  }, {
    "id" : "1347b1d8bba24e2e82a1aa5600229d80",
    "option" : "Handler.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThe handler is the name of the method within a code that Lambda calls to execute the function.\nOption A is incorrect as the version number changes when the functions are \"published\", so FunctionName is incorrect.\nOption B is incorrect as it's a list of function layers added to the Lambda function execution environment.\nOption C is incorrect as these are variables that are accessible during Lambda function execution.\nFor more information on declaring Lambda Function in AWS CloudFormation Template, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-lambda-function.html\n\n"
}, {
  "id" : 77,
  "question" : "You've been asked to develop an application on the AWS Cloud.\nThe application will be used to store confidential documents in an S3 bucket.\nYou need to ensure that the bucket is defined in such a way that it does not accept objects that are not encrypted.\n",
  "answers" : [ {
    "id" : "3bf2a73730604d77be36b6bd1b6a2570",
    "option" : "Ensure a condition is set in the bucket policy.",
    "isCorrect" : "true"
  }, {
    "id" : "106bed2d698d43e7b1f05ed6b9c4c2cf",
    "option" : "Ensure that a condition is set in an IAM policy.",
    "isCorrect" : "false"
  }, {
    "id" : "557cf3d053d244d6bbfbbc9cc8d77e73",
    "option" : "Enable MFA for the underlying bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "5d54389c64714a2f9e6f0ed44391efa4",
    "option" : "Enable CORS for the underlying bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation gives an example on the same.\nOption B is incorrect since the condition needs to be put in the Bucket policy.\nOption C is incorrect since this is only used for MFA Delete for accidental deletion of objects.\nOption D is incorrect since CORS is only used for cross-domain access.\nFor more information on using KMS Encryption for S3, please refer to the below link-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html\n\n\nTo ensure that the S3 bucket only accepts encrypted objects, you can enable default encryption for the bucket, and configure the bucket policy to require encrypted objects.\nHere are the steps to achieve this:\nEnable default encryption for the bucket: Go to the S3 console. Select the bucket that you want to enable encryption for. Click on the \"Properties\" tab. Click on the \"Default encryption\" section. Select \"AES-256\" or \"AWS-KMS\" as the default encryption type. Configure the bucket policy to require encrypted objects: Go to the S3 console. Select the bucket that you want to configure the policy for. Click on the \"Permissions\" tab. Click on the \"Bucket policy\" section. Add a statement to the policy that requires encryption for all objects uploaded to the bucket. Here is an example of what the policy might look like: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Sid\":\"RequireEncryption\", \"Effect\":\"Deny\", \"Principal\": \"\", \"Action\":\"s3:PutObject\", \"Resource\":\"arn:aws:s3:::your-bucket-name/\", \"Condition\":{ \"StringNotEquals\":{ \"s3:x-amz-server-side-encryption\":\"AES256\" } } } ] } This policy statement denies any PutObject requests to the bucket that don't have the x-amz-server-side-encryption header set to AES256.\nOption A is correct. You can ensure that the bucket only accepts encrypted objects by setting a condition in the bucket policy, as described above.\nOption B is incorrect. IAM policies are used to grant or deny permissions to users or groups, and cannot be used to enforce encryption requirements on S3 objects.\nOption C is incorrect. MFA (Multi-Factor Authentication) is a security feature that requires users to provide two or more forms of authentication to access AWS resources. It is not relevant to enforcing encryption requirements on S3 objects.\nOption D is incorrect. CORS (Cross-Origin Resource Sharing) is a mechanism that allows web pages to make requests to a different domain than the one that served the web page. It is not relevant to enforcing encryption requirements on S3 objects.\n\n"
}, {
  "id" : 78,
  "question" : "Your application has the requirement to store data in a backend data store.\nIndexing should be possible on the data, but the data does not conform to any schema.\nWhich of the following would be the ideal data store to choose for this application?\n",
  "answers" : [ {
    "id" : "db3a85c6db8f43c3876c8a39ce4b1d18",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "4ad359fe88fc47e7987f46c7f10cd979",
    "option" : "AWS DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "a7963753d9ff4762a5cc5bc8dab3bbc5",
    "option" : "AWS Redshift",
    "isCorrect" : "false"
  }, {
    "id" : "64c3f595eaae4a38abd03f11e926b8bc",
    "option" : "AWS S3",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe below AWS Documentation mentions the differences between AWS DynamoDB and other traditional database systems.\nOne of the major differences is the schemaless nature of the database.\nOption A is invalid since this is normally used for databases that perform to a particular schema.\nOption C is invalid since this is normally used for columnar based databases.\nOption D is invalid since this is normally used for object-level storage.\nFor more information on the differences, please refer to the below link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html\n\n\nThe ideal data store to choose for an application that requires indexing on data without any fixed schema would be AWS DynamoDB, which is a NoSQL database service provided by AWS.\nDynamoDB is designed to handle massive amounts of data and provides fast and predictable performance with seamless scalability. It is a fully managed service, which means that AWS takes care of the underlying infrastructure, backups, and software updates.\nDynamoDB is a NoSQL database, which means that it doesn't require a fixed schema, and you can store data in a flexible way. This is ideal when you have data that doesn't fit into a fixed schema, as it allows you to add new attributes to your data without having to modify your existing schema.\nFurthermore, DynamoDB provides indexing capabilities, which allow you to quickly and efficiently retrieve data based on specific criteria. You can create indexes on one or more attributes, which can be used to filter and sort your data.\nIn contrast, AWS RDS is a managed relational database service, which requires a fixed schema and is ideal for applications that require a structured data model. AWS Redshift is a data warehousing service, which is optimized for running complex queries and analytics on structured data. AWS S3, on the other hand, is an object storage service that is designed for storing and retrieving unstructured data such as files and multimedia content, and does not provide indexing capabilities.\nTherefore, based on the requirements of the application in question, AWS DynamoDB would be the ideal data store to choose.\n\n"
}, {
  "id" : 79,
  "question" : "An application is being developed that is going to write data to a DynamoDB table.\nYou have to set up the read and write throughput for the table.\nData is going to be read at the rate of 300 items every 30 seconds.\nEach item is of size 6KB.\nThe reads can be eventual consistent reads.\nWhat would be the Read Capacity Unit that needs to be set on the table?\n",
  "answers" : [ {
    "id" : "42fc808d63844511b464c7cccbc24094",
    "option" : "10",
    "isCorrect" : "true"
  }, {
    "id" : "08bdc98be4874494a0809b4c289c2d98",
    "option" : "20",
    "isCorrect" : "false"
  }, {
    "id" : "78b821e924634a6a9edfe604d52f807a",
    "option" : "6",
    "isCorrect" : "false"
  }, {
    "id" : "2d0b2dc816c84beca7f871aa75ade747",
    "option" : "30",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nSince there are 300 items read every 30 seconds, that means there are (300/30) = 10 items read every second.\nSince each item is 6KB in size, that means 2 reads will be required for each item.\nThe question specified \"eventual consistent reads\" which means we need to divide by 2 since 1 RCU has two eventually consistent reads.\nSo we have a total of 2*(10 / 2) = 2*5= 10 reads for the number of items per second.\nFor more information on Read and Write capacity, please refer to the below link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html\n\nTo calculate the read capacity units required for DynamoDB, we need to consider the following factors:\nItem size: In this case, each item is 6KB in size. Read consistency: The question states that eventual consistent reads are acceptable. Eventual consistent reads are less demanding on throughput than strongly consistent reads. Read frequency: The question states that 300 items will be read every 30 seconds.\nWith these factors in mind, we can use the following formula to calculate the required read capacity units:\nRead Capacity Units = (Item size / 4 KB) x (Reads per second) x (Read Consistency)\nHere, the item size is 6 KB, which is greater than the 4 KB base item size that DynamoDB uses for calculating throughput. So we need to round it up to the nearest multiple of 4 KB, which is 8 KB.\nUsing this value, we can calculate the read capacity units as follows:\nRead Capacity Units = (8 KB / 4 KB) x (300 / 30) x (0.5) = 4 x 10 x 0.5 = 20\nTherefore, the correct answer is B. 20. We need to set the read capacity units to 20 for the DynamoDB table to handle the given read workload.\n\n"
}, {
  "id" : 80,
  "question" : "You are in charge of deploying an application hosted on an EC2 Instance and sitting behind an Elastic Load Balancer.\nYou have been requested to monitor the incoming client connections to the Elastic Load Balancer.\nWhich of the below options can suffice this requirement?\n",
  "answers" : [ {
    "id" : "7234efbd8c304a71a3fce9b839240b12",
    "option" : "Use AWS CloudTrail with your load balancer.",
    "isCorrect" : "false"
  }, {
    "id" : "666df21c9de64210ad9b802972fbc875",
    "option" : "Enable access logs on the load balancer.",
    "isCorrect" : "true"
  }, {
    "id" : "d1eeffdd0b80423fbe42d36196733414",
    "option" : "Use a CloudWatch Logs Agent by installing on EC2.",
    "isCorrect" : "false"
  }, {
    "id" : "44c692bfe2cd465ea67a9eccb6572a69",
    "option" : "Create a custom metric CloudWatch filter on your load balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nElastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer.\nEach log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses.\nYou can use these access logs to analyze traffic patterns and troubleshoot issues.\nOption A is INCORRECT because CloudTrail captures all API calls for Elastic Load Balancing as events.\nThis is not the recommended approach to monitoring incoming connections to the ELB.Option B is CORRECT.\nEach log contains information such as the time the request was received, the client's IP address, latencies, request paths, and server responses.\nYou can use these access logs to analyze traffic patterns and to troubleshoot issues.\nOption C is invalid since the Logs agents are installed on EC2 Instances and not on the ELB.Option D is invalid since the metrics will not provide detailed information on the incoming connections.\nFor more information on Application Load balancer Logs, please refer to the below link-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html\nPlease refer to page 99 on the below link-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/elb-ag.pdf\n\nThe option that can suffice the requirement of monitoring incoming client connections to the Elastic Load Balancer (ELB) is to enable access logs on the load balancer. Therefore, the correct answer is B.\nExplanation: The Elastic Load Balancer (ELB) is a highly scalable AWS service that distributes incoming traffic across multiple targets such as Amazon EC2 instances, containers, and IP addresses. The ELB can be used to achieve fault tolerance, high availability, and elasticity for your applications.\nTo monitor incoming client connections to the ELB, you can enable access logs on the load balancer. Access logs capture detailed information about each request that is received by the load balancer, including the time of the request, the client's IP address, the requested URL, the response code, and more.\nBy analyzing the access logs, you can gain insights into your application's performance, troubleshoot issues, and optimize your infrastructure. You can also use third-party tools such as Elasticsearch or Kibana to visualize and analyze the access logs.\nThe other options listed in the question are not suitable for monitoring incoming client connections to the ELB:\nA. AWS CloudTrail is a service that records API calls made in your AWS account and delivers log files to an Amazon S3 bucket. CloudTrail is not directly related to monitoring client connections to an ELB.\nC. CloudWatch Logs Agent is used to collect logs from Amazon EC2 instances and on-premises servers. It is not directly related to monitoring client connections to an ELB.\nD. Creating a custom metric CloudWatch filter on your load balancer allows you to aggregate metrics based on specific criteria, but it is not directly related to monitoring client connections to an ELB.\n\n"
}, {
  "id" : 81,
  "question" : "You are in charge of developing an application that will make use of AWS services.\nThere is a key requirement from an architecture point of view that the entire system should be decoupled to ensure less dependency.\nWhich of the following are the suitable services to decouple different components of an application?\n",
  "answers" : [ {
    "id" : "98a5258d550a4e6d9e19198f41cde074",
    "option" : "AWS CodePipeline",
    "isCorrect" : "false"
  }, {
    "id" : "fa67ba2addde411a8ad9615312a612d6",
    "option" : "AWS Simple Queue Service",
    "isCorrect" : "true"
  }, {
    "id" : "35deb9266caf4cea9e573ef22593c01d",
    "option" : "AWS Simple Notification Service",
    "isCorrect" : "true"
  }, {
    "id" : "56c6ad7f93184aefadb73f23a30c9acc",
    "option" : "AWS CodeBuild",
    "isCorrect" : "false"
  }, {
    "id" : "1714901069ab476d9a922d9ba261d58d",
    "option" : "AWS CodeStar.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe AWS Documentation mentions the following.\nAmazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components.\nAmazon SQS offers common constructs such as dead-letter queues and cost allocation tags.\nAmazon SNS enables you to modernize your applications and decouple them into smaller, independent components that are easier to develop, deploy and maintain.\nAWS SNS topics can be subscribed to by multiple decoupled services.\nSNS is mostly used in conjunction with SQS to provide decoupled services.\nOption A is incorrect since this service is used to build CI/CD pipelines for integration and deployment.\nOption D is incorrect since this service is used as the build and test stage of the CI/CD deployments.\nOption E is incorrect since AWS CodeStar is a cloud-based service for creating, managing, and working with software development projects on AWS.\nFor more information on the Simple Queue Service, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html\nhttps://aws.amazon.com/sns\n\nTo achieve a decoupled architecture, it's important to choose the right AWS services that can help to reduce dependency and increase flexibility. The suitable AWS services for decoupling the different components of an application are:\nB. AWS Simple Queue Service (SQS) AWS Simple Queue Service (SQS) is a fully managed message queuing service that enables decoupling and scaling of microservices, distributed systems, and serverless applications. It allows different components of an application to communicate asynchronously, and it ensures reliability, durability, and fault tolerance. SQS is a perfect choice for decoupling as it decouples the producer and consumer of messages, and the producer doesn't need to know the state or status of the consumer.\nC. AWS Simple Notification Service (SNS) AWS Simple Notification Service (SNS) is a fully managed messaging service that enables decoupling and coordination of microservices, distributed systems, and serverless applications. It allows different components of an application to communicate asynchronously and ensures reliable and scalable message delivery. SNS is a perfect choice for decoupling as it decouples the sender and receiver of messages and allows sending messages to multiple subscribers in different formats (e.g., email, SMS, HTTP endpoint).\nA. AWS CodePipeline, D. AWS CodeBuild, and E. AWS CodeStar are not suitable for decoupling different components of an application, as they are tools for building, testing, and deploying code, and they do not provide messaging capabilities for decoupling.\nIn summary, AWS Simple Queue Service (SQS) and AWS Simple Notification Service (SNS) are the suitable AWS services for decoupling different components of an application.\n\n"
}, {
  "id" : 82,
  "question" : "A static web site has been hosted on a bucket and is now being accessed by users.\nOne of the web pages javascript section has been changed to access data hosted in another S3 bucket.\nNow that same web page is no longer loading in the browser.\nWhich of the following can help alleviate the error?\n",
  "answers" : [ {
    "id" : "60efd414346d4d5688f59e64eba221be",
    "option" : "Enable versioning for the underlying S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "b3caac2278b0467093c228f553d2af1b",
    "option" : "Enable Replication so that the objects get replicated to the other bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "883d8d4bc9db4cebbf3b5236afdc5620",
    "option" : "Enable CORS on the bucket containing the data.",
    "isCorrect" : "true"
  }, {
    "id" : "4d0c9d62bff84459aaabc58f9fd5064f",
    "option" : "Change the Bucket policy for the bucket to allow access from the other bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is given as use case scenarios in the AWS Documentation.\nCross-Origin Resource Sharing: Use-case Scenarios.\nThe following are example scenarios for using CORS.\nScenario 1: Suppose you are hosting a website in an Amazon S3 bucket named website described in Hosting a Static Website on Amazon S3\nYour users load the website endpoint http://website.s3-website-us-east-1.amazonaws.com.\nNow you want to use JavaScript on the webpages stored in this bucket to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket website.s3.amazonaws.com.\nA browser would normally block JavaScript from allowing those requests, but with CORS, you can configure your bucket to enable cross-origin requests from website.s3-website-us-east-1.amazonaws.com explicitly.\nScenario 2: Suppose that you want to host a web font from your S3 bucket.\nAgain, browsers require a CORS check (also called a preflight check) for loading web fonts.\nYou would configure the bucket that hosts the web font to allow any origin to make these requests.\nOption A is incorrect because Enabling versioning does not solve the problems of accessing form the different buckets.\nYou need to enable CORS on the underlying bucket.\nOption B is incorrect because Enabling replication will cost you more as you are maintaining two copies of data.\nOption D is incorrect because changing the bucket policy allows access from another bucket, but this will open the whole bucket, not an ideal solution.\nFor more information on Cross-Origin Resource Sharing, please refer to the below link-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\n\nThe error that is being observed suggests that the web page's JavaScript code is trying to access data hosted in another S3 bucket but is unable to do so. There could be several reasons for this error, but the most common reason is a Cross-Origin Resource Sharing (CORS) issue.\nCORS is a security feature implemented by web browsers that restricts web pages from accessing resources located on a different domain than the one the web page is hosted on. This security feature is designed to prevent malicious scripts from accessing data from other websites without permission.\nIn this scenario, the web page is trying to access data hosted in another S3 bucket. However, if the CORS configuration on that S3 bucket is not set up correctly, the web page's JavaScript code will not be able to access the data, resulting in an error.\nTherefore, the correct answer to this question is option C - Enable CORS on the bucket containing the data.\nEnabling CORS allows the web page's JavaScript code to access data from other domains, including the S3 bucket where the data is hosted. This can be achieved by configuring the CORS rules on the S3 bucket hosting the data to allow access from the domain where the web page is hosted.\nOptions A, B, and D are incorrect because they do not address the root cause of the error, which is the CORS issue. Enabling versioning or replication or changing the bucket policy may be required for other reasons, but they will not help alleviate the error in this scenario.\n\n"
}, {
  "id" : 83,
  "question" : "Your mobile application includes a photo-sharing service that is expecting tens of thousands of users at launch.\nYou will leverage Amazon Simple Storage Service (S3) to store the user Images, and you must decide how to authenticate and authorize your users for access to these images.\nYou also need to manage the storage of these images.\nWhich two of the following approaches should you use? Choose two answers from the options below.\n",
  "answers" : [ {
    "id" : "d5776abf37b241a8ae13baf1baa005cd",
    "option" : "Create an Amazon S3 bucket per user, and use your application to generate the S3 URL for the appropriate content.",
    "isCorrect" : "false"
  }, {
    "id" : "59f3a09e9169488bb01187d4e3a6ee32",
    "option" : "Use AWS Identity and Access Management (IAM) user accounts as your application-level user database, and offload the burden of authentication from your application code.",
    "isCorrect" : "false"
  }, {
    "id" : "e00601fde4524baba207ae11fc09b573",
    "option" : "Authenticate your users at the application level, and use AWS Security Token Service (STS)to grant token-based authorization to S3 objects.",
    "isCorrect" : "true"
  }, {
    "id" : "264c0f0da61f4d4f8fef692e7f311cfe",
    "option" : "Authenticate your users at the application level, and send an SMS token message to the user. Create an Amazon S3 bucket with the same name as the SMS message token, and move the user’s objects to that bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "4f715202f6a84014aea59d6571e55a72",
    "option" : "Use a key-based naming scheme comprised from the user IDs for all user objects in a single Amazon S3 bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and E.\nThe AWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users).\nThe token can then be used to grant access to the objects in S3.\nYou can then provide access to the objects based on the key values generated via the user id.\nOption A is possible but then becomes a maintenance overhead because of the number of buckets.\nOption B is invalid because IAM users is not a good security practice.\nOption D is invalid because SMS tokens are not efficient for this requirement.\nFor more information on the Security Token Service, please refer to the below link-\nhttps://docs.aws.amazon.com/STS/latest/APIReference/Welcome.html\n\nOption A: Create an Amazon S3 bucket per user, and use your application to generate the S3 URL for the appropriate content.\nCreating an S3 bucket for each user may result in high costs and maintenance overhead, as you will need to manage a large number of buckets. Additionally, the bucket naming conventions may be difficult to maintain over time. This approach may work for small-scale applications, but for tens of thousands of users, it is not scalable.\nOption B: Use AWS Identity and Access Management (IAM) user accounts as your application-level user database, and offload the burden of authentication from your application code.\nThis is a good option as it allows you to use the built-in IAM authentication and authorization mechanisms provided by AWS. IAM is a centralized service for managing users, groups, and permissions. IAM allows you to control access to AWS services and resources, including S3. By using IAM, you can offload the burden of authentication and authorization from your application code and rely on AWS to manage this for you.\nOption C: Authenticate your users at the application level, and use AWS Security Token Service (STS) to grant token-based authorization to S3 objects.\nThis option allows you to authenticate your users at the application level, but instead of managing authorization within your application, you can use the AWS Security Token Service (STS) to issue temporary security credentials to your users. These credentials are valid for a limited time and can be used to access S3 objects. This approach can help improve security by reducing the risk of long-term credential theft.\nOption D: Authenticate your users at the application level, and send an SMS token message to the user. Create an Amazon S3 bucket with the same name as the SMS message token, and move the user's objects to that bucket.\nThis approach is not recommended as it may be difficult to manage and maintain over time. Additionally, it may be hard to scale this approach as it requires creating a new bucket for each user.\nOption E: Use a key-based naming scheme comprised from the user IDs for all user objects in a single Amazon S3 bucket.\nThis approach is a good option as it allows you to store all user objects in a single S3 bucket, reducing the overhead of managing multiple buckets. By using a key-based naming scheme that includes the user ID, you can easily identify and manage user objects. Additionally, S3 provides fine-grained access control that can be used to manage authorization for individual objects.\n\n"
}, {
  "id" : 84,
  "question" : "Your current log analysis application takes more than four hours to generate a report of the top 10 users of your web application.\nYou have been asked to implement a system that can report this information in real-time, ensure that the report is always up to date, and handle increases in the number of requests to your web application.\nChoose the option that is cost-effective and can fulfill the requirements.\n",
  "answers" : [ {
    "id" : "bb79cbffc59148df96abc74efdc6a0b8",
    "option" : "Publish your data to CloudWatch Logs, and configure your application to Autoscale to handle the load on demand.",
    "isCorrect" : "false"
  }, {
    "id" : "7ac3faf59d244394b646f07b5280d457",
    "option" : "Publish your log data to an Amazon S3 bucket.Use AWS CloudFormation to create an Auto Scaling group to scale your post-processing application which is configured to pull down your log files stored an Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "87a05660a83a476a91a6ea91ee5de92f",
    "option" : "Post your log data to an Amazon Kinesis data stream, and subscribe your log-processing application so that is configured to process your logging data.",
    "isCorrect" : "true"
  }, {
    "id" : "1db41baecc234c47a08866fe64387dae",
    "option" : "Create a multi-AZ Amazon RDS MySQL cluster, post the logging data to MySQL, and run a map reduce job to retrieve the required information on user counts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nWhen you see Amazon Kinesis as an option, this becomes the ideal option to process data in real-time.\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to process streaming data cost-effectively at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, data lakes, and data warehouses, or build your own real-time applications using this data.\nFor more information on AWS Kinesis, please refer to the below link-\nhttps://aws.amazon.com/kinesis/\nOption A is incorrect because logs in CloudWatch Logs are not in real-time.\nOption B is incorrect because you have to pay for S3 buckets and EC2 instances cost, which is not a cost-effective solution.\nAnd the logs may not be in real-time.\nOption D is incorrect because it is not cost-efficient to store the logs in RDS MySQL.\n\nOption A: Publishing log data to CloudWatch Logs can provide real-time log data analysis. Auto Scaling can be used to manage load on demand, which can improve the application's scalability. However, using CloudWatch Logs to generate the report may not be the most cost-effective option. This is because CloudWatch Logs can be expensive when the number of log events increases, which can be a concern if the web application has high traffic.\nOption B: Publishing log data to an Amazon S3 bucket can provide an easy and cost-effective way to store and retrieve log data. CloudFormation can be used to automate the creation of an Auto Scaling group, which can scale the post-processing application up or down based on demand. This option can be more cost-effective than using CloudWatch Logs because storing data in an S3 bucket is cheaper than using CloudWatch Logs.\nOption C: Posting log data to an Amazon Kinesis data stream can provide a scalable and reliable way to process real-time streaming data. Kinesis allows for processing of large amounts of data in real-time, and can be used to subscribe the log-processing application, which can be configured to process the logging data. This option can be more suitable for high traffic web applications that generate large amounts of log data.\nOption D: Creating a multi-AZ Amazon RDS MySQL cluster can provide a reliable and scalable way to store log data. Running a map reduce job to retrieve the required information on user counts can provide near real-time reports. However, this option may not be as cost-effective as other options, especially if the application generates a large amount of log data.\nBased on the requirements provided, Option C (using Amazon Kinesis data stream) appears to be the most suitable option for handling real-time log analysis of a high-traffic web application. However, the choice ultimately depends on the specific requirements of the application, including factors such as cost, scalability, and reliability.\n\n"
}, {
  "id" : 85,
  "question" : "You've been instructed to develop a mobile application that will make use of AWS services.\nYou need to decide on a data store to store the user sessions.\nWhich of the following would be an ideal data store for session management?\n",
  "answers" : [ {
    "id" : "53571369b8244d04b931dee8d31449c0",
    "option" : "AWS Simple Storage Service",
    "isCorrect" : "false"
  }, {
    "id" : "5eee4c2c1ae04010a88f953ffec97b80",
    "option" : "AWS DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "5568ba4bbe094158a642b09cf781c60e",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "dd290ffe7774477cb8285795ae20a543",
    "option" : "AWS Redshift.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nDynamoDB is an alternative solution that can be used for the storage of session management.\nThe latency of access to data is less.\nHence, this can be used as a data store for session management.\nOption A is incorrect since this service is used for object-level storage.\nOption C is incorrect since this service is used for storing relational data.\nOption D is incorrect since this service is used as a data warehousing solution.\nFor more information on an example on this, please refer to the below link-\nhttps://aws.amazon.com/blogs/aws/scalable-session-handling-in-php-using-amazon-dynamodb/\n\nOut of the options provided, AWS DynamoDB would be an ideal data store for session management.\nAWS Simple Storage Service (S3) is an object storage service that is used to store and retrieve any amount of data. While it is a cost-effective and reliable storage option for objects such as files, it is not suitable for session management. S3 is designed for storing and retrieving large files or objects such as images, videos, and backups.\nAWS Relational Database Service (RDS) is a managed relational database service that allows users to run a fully managed database in the cloud. RDS supports popular relational database engines such as MySQL, Oracle, and PostgreSQL. Although RDS is suitable for storing data that requires a relational model, it is not the best option for session management. Session data is usually small and does not require complex database queries, so using a relational database could be an overkill.\nAWS Redshift is a data warehouse service that is designed for analytics workloads. It is optimized for querying large datasets and performing complex joins between tables. Redshift is not suitable for session management as it is designed for analytics and reporting purposes, and not for storing and retrieving session data.\nAWS DynamoDB is a fully managed NoSQL database service that is designed for fast and flexible document and key-value data storage. DynamoDB is a good option for session management as it can handle high volume, low latency requests and supports fast read and write operations. DynamoDB can also scale easily to accommodate changes in traffic volume, making it suitable for applications that experience fluctuating traffic patterns.\nIn summary, AWS DynamoDB is the ideal data store for session management because it is designed for fast and flexible data storage, can handle high volume, low latency requests, and can scale easily to accommodate changes in traffic volume.\n\n"
}, {
  "id" : 86,
  "question" : "Your application currently interacts with a DynamoDB table.\nRecords are inserted into the table via the application.\nThere is now a requirement to ensure that another record is inserted into a secondary table whenever items are updated in the DynamoDB primary table.\nWhich of the below feature should be used when developing such a solution?\n",
  "answers" : [ {
    "id" : "c377534293054751b51ee41b8785e64b",
    "option" : "AWS DynamoDB Encryption",
    "isCorrect" : "false"
  }, {
    "id" : "9972b90775954cab9408cb1a7c2ac0aa",
    "option" : "AWS DynamoDB Streams",
    "isCorrect" : "true"
  }, {
    "id" : "628adaa1784345cb9904dfad0ab36ce4",
    "option" : "AWS DynamoDB Accelerator",
    "isCorrect" : "false"
  }, {
    "id" : "4d6f8fa59def48728c1b03207b440bdf",
    "option" : "AWSTable Accelerator.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is also mentioned as a use case in the AWS Documentation.\nDynamoDB Streams Use Cases and Design Patterns.\nThis post describes some common use cases you might encounter, along with their design options and solutions, when migrating data from relational data stores to Amazon DynamoDB.We will consider how to manage the following scenarios.\nHow do you set up a relationship across multiple tables in which, based on the value of an item from one table, you update the item in a second table?\nHow do you trigger an event based on a particular transaction?\nHow do you audit or archive transactions?\nHow do you replicate data across multiple tables (similar to that of materialized views/streams/replication in relational data stores)?\nRelational databases provide native support for transactions, triggers, auditing, and replication.\nTypically, a transaction in a database refers to performing create, read, update, and delete (CRUD) operations against multiple tables in a block.\nA transaction can have only two states-success or failure.\nIn other words, there is no partial completion.\nAs a NoSQL database, DynamoDB is not designed to support transactions.\nAlthough client-side libraries are available to mimic the transaction capabilities, they are not scalable and cost-effective.\nFor example, the Java Transaction Library for DynamoDB creates 7N+4 additional writes for every write operation.\nThis is partly because the library holds metadata to manage the transactions to ensure that it's consistent and can be rolled back before commit.\nYou can use DynamoDB Streams to address all these use cases.\nDynamoDB Streams is a powerful service that you can combine with other AWS services to solve many similar problems.\nWhen enabled, DynamoDB Streams captures a time-ordered sequence of item-level modifications in a DynamoDB table and durably stores the information for up to 24 hours.\nApplications can access a series of stream records, which contain an item change, from a DynamoDB stream in near real-time.\nAWS maintains separate endpoints for DynamoDB and DynamoDB Streams.\nTo work with database tables and indexes, your application must access a DynamoDB endpoint.\nTo read and process DynamoDB Streams records, your application must access a DynamoDB Streams endpoint in the same Region.\nOption A is incorrect because DynamoDB Encryption helps you with the security, not adding the data to secondary tables.\nOption C is incorrect because DynamoDB Accelerator is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement.\nIt does not solve the problem.\nOption D is incorrect because there is no service named Table Accelerator.\nFor more information on use cases and design patterns for DynamoDB streams, please refer to the below link-\nhttps://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/\n\nThe correct answer is B. AWS DynamoDB Streams.\nDynamoDB Streams is a feature of DynamoDB that captures a time-ordered sequence of item-level modifications made to a DynamoDB table. With DynamoDB Streams, you can process the data modifications in near-real time, and use the processed data to build applications that can react to changes in a DynamoDB table. In other words, DynamoDB Streams allows you to build applications that can react to changes in a DynamoDB table in real time.\nIn this scenario, you can use DynamoDB Streams to capture the item-level modifications made to the primary table, and use the processed data to insert a new record into the secondary table whenever an item is updated in the primary table. This can be achieved by creating a DynamoDB Streams-enabled table, and configuring the stream to trigger an AWS Lambda function whenever an item is updated in the primary table. The AWS Lambda function can then insert a new record into the secondary table based on the updated item in the primary table.\nA. AWS DynamoDB Encryption is not relevant to this scenario. It is a feature that allows you to encrypt your data at rest in DynamoDB, and does not provide a solution for inserting records into a secondary table when items are updated in the primary table.\nC. AWS DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB that can improve the performance of DynamoDB applications. While DAX can improve the performance of DynamoDB queries, it does not provide a solution for inserting records into a secondary table when items are updated in the primary table.\nD. AWS Table Accelerator is not a feature of AWS. It is not relevant to this scenario.\n\n"
}, {
  "id" : 87,
  "question" : "An application has been making use of AWS DynamoDB for its back-end data store.\nThe size of the table has now grown to 20 GB, and the scans on the table are causing throttling errors.\nWhich of the following should now be implemented to avoid such errors?\n",
  "answers" : [ {
    "id" : "a3573e458b6446cf8bbb1659a4dea618",
    "option" : "Large Page size",
    "isCorrect" : "false"
  }, {
    "id" : "84127f2410a440a8a600fadc8ca6fcb1",
    "option" : "Reduced page size",
    "isCorrect" : "true"
  }, {
    "id" : "fa7440e4c5294c18bc7689ce7a0290df",
    "option" : "Parallel Scans",
    "isCorrect" : "false"
  }, {
    "id" : "22c24077bbc341278dd45b4c5f0aede3",
    "option" : "Sequential scans.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nWhen you scan your table in Amazon DynamoDB, you should follow the DynamoDB best practices for avoiding sudden bursts of read activity.\nYou can use the following technique to minimize the impact of a scan on a table's provisioned throughput.\nReduce page size.\nBecause a Scan operation reads an entire page (by default, 1 MB), you can reduce the scan operation's impact by setting a smaller page size.\nThe Scan operation provides a Limit parameter that you can use to set the page size for your request.\nEach Query or Scan request with a smaller page size uses fewer read operations and creates a \"pause\" between each request.\nFor example, suppose that, each item is 4 KB, and you set the page size to 40 items.\nA Query request would then consume only 20 eventually consistent read operations or 40 strongly consistent read operations.\nA larger number of smaller Query or Scan operations would allow your other critical requests to succeed without throttling.\nOption A is incorrect because the page size should be reduced rather than enlarged.\nOption C is incorrect because a parallel scan with many workers can easily consume all of the provisioned read capacity.\nOption D is incorrect because the sequential scan processes data sequentially.\nIt does not help to avoid the throttling errors.\nFor more information, please check below AWS Docs-\nhttps://aws.amazon.com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html\n\nIn this scenario, the table in DynamoDB has grown to a size of 20 GB, which has led to throttling errors during scans on the table. To avoid such errors, the following solutions can be implemented:\nA. Large Page size - This option is not recommended as a larger page size would mean more data being fetched in each query, which can lead to higher latency and increase the likelihood of throttling errors. Therefore, this option is incorrect.\nB. Reduced page size - Reducing the page size would mean that lesser data is being fetched during each query, which can help reduce latency and prevent throttling errors. Therefore, this option could be a possible solution.\nC. Parallel Scans - Parallel scans can be used to split the workload across multiple workers, thus reducing the load on each worker and preventing throttling errors. Therefore, this option could also be a possible solution.\nD. Sequential scans - Sequential scans would not be an effective solution in this scenario as the size of the table has grown to a point where sequential scans can lead to throttling errors. Therefore, this option is incorrect.\nIn conclusion, reducing the page size and using parallel scans are both possible solutions to avoid throttling errors when scanning a 20 GB DynamoDB table.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 89,
  "question" : "Your company is planning to create new development environments in AWS.\nThey want to use their existing Chef recipes that they use for their on-premise configuration for servers in AWS.\nWhich of the following service would be ideal to use in this regard?\n",
  "answers" : [ {
    "id" : "1753fd3dd1374d72945d4b82e1626bb1",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "28c14ee974df4ac9ad3b3b2130341949",
    "option" : "AWS OpsWorks",
    "isCorrect" : "true"
  }, {
    "id" : "f1c9c9e0f1d14af7aa3ae27587883673",
    "option" : "AWS Cloudformation",
    "isCorrect" : "false"
  }, {
    "id" : "98aebb2cfd4b4311bd71ea49e5a0644c",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\nChef and Puppet are automation platforms that allow you to use code to automate the configurations of your servers.\nOpsWorks lets you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.\nAll other options are invalid since they cannot be used to work with Chef recipes for configuration management.\nFor more information on AWS Opswork, please refer to the below link-\nhttps://aws.amazon.com/opsworks/\n\nThe ideal service to use for this scenario would be AWS OpsWorks (option B).\nAWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. It allows users to create and manage AWS resources, and deploy and manage applications across multiple instances.\nOpsWorks is designed to support custom configurations, including custom Chef recipes, and provides a range of automation features for tasks such as deployment, monitoring, and scaling. It allows users to define custom stacks, which are collections of AWS resources that are managed together as a single unit.\nWith OpsWorks, users can define custom Chef recipes that can be used to automate the configuration of AWS resources, including EC2 instances, databases, and load balancers. By using the same Chef recipes that are used for on-premise configurations, developers can ensure consistency across their infrastructure and reduce the risk of errors or inconsistencies.\nAWS Elastic Beanstalk (option A) is a service that allows users to deploy and manage web applications, but it is not designed for configuration management. AWS CloudFormation (option C) is a service that allows users to define and manage infrastructure as code, but it does not provide built-in support for Chef recipes. AWS SQS (option D) is a message queue service and is not related to configuration management.\n\n"
}, {
  "id" : 90,
  "question" : "Your company has developed a web application and hosted it on an Amazon S3 bucket configured for static content.\nThe users can log in to this app using their Google/Facebook login accounts.\nThe application uses the AWS SDK for JavaScript in the browser to access data stored in an Amazon DynamoDB table.\nHow can you ensure that API keys for access to your data in DynamoDB are secure?\n",
  "answers" : [ {
    "id" : "d63cce151a5047c79dbe54d7c5d0b177",
    "option" : "Create an Amazon S3 role in IAM with access to the specific DynamoDB tables, and assign it to the bucket hosting the website.",
    "isCorrect" : "false"
  }, {
    "id" : "bcf35dab062d4b4f8e467cd47b9a39cf",
    "option" : "Configure S3 bucket tags with your AWS access keys to the bucket hosting the website, so that the application can query them for access.",
    "isCorrect" : "false"
  }, {
    "id" : "293c67c20b854fd2abad034d6444d3d3",
    "option" : "Configure a web identity federation role within IAM to enable access to the correct DynamoDB resources and retrieve temporary credentials.",
    "isCorrect" : "true"
  }, {
    "id" : "4c04aa10bfe94428b0bfae587118081d",
    "option" : "Store AWS keys in global variables within your application and configure the application to use these credentials when making requests.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nWith web identity federation, you don't need to create custom sign-in code or manage your own user identities.\nInstead, users of your app can sign in using a well-known identity provider (IdP) -such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP, receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account.\nUsing an IdP helps you keep your AWS account secure because you don't have to embed and distribute long-term security credentials with your application.\nOption A is incorrect since Roles cannot be assigned to S3 buckets.\nOptions B and D are incorrect since the AWS Access keys should not be used.\nFor more information on Web Identity Federation, please refer to the below link AWS-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\n\nThe correct answer is C. Configure a web identity federation role within IAM to enable access to the correct DynamoDB resources and retrieve temporary credentials.\nExplanation: When an application running in a browser makes AWS service requests, it requires credentials to access those services. Storing AWS access keys within the application is not a secure practice since these keys can be easily compromised. AWS recommends using temporary security credentials obtained through web identity federation for this use case. Web identity federation enables users to sign in to the application using their existing Google/Facebook login accounts, which the application can use to obtain temporary security credentials. These credentials grant the application access to the AWS services and resources specified in the AWS Identity and Access Management (IAM) policy associated with the role.\nThe steps to implement this solution are:\nCreate an IAM role: First, you need to create an IAM role that specifies the permissions that the application requires to access the DynamoDB table. You can use the IAM console or the AWS CLI to create this role. Configure web identity federation: Next, you need to configure web identity federation for the IAM role. This involves creating an identity provider (in this case, Google/Facebook), an application in the provider's console, and configuring the IAM role to trust the provider and allow access to the necessary resources. Update the application: Finally, you need to update the application code to obtain temporary security credentials from the web identity federation provider and use these credentials to access the DynamoDB table.\nOption A is incorrect because creating an S3 role will not provide the necessary permissions to access the DynamoDB table.\nOption B is incorrect because configuring S3 bucket tags with AWS access keys is not a secure way to provide access to AWS services.\nOption D is incorrect because storing AWS access keys in global variables within the application is not a secure practice since these keys can be easily compromised.\n\n"
}, {
  "id" : 91,
  "question" : "You are planning to deploy a built application onto an EC2 Instance.\nThere will be several tests conducted on this Instance.\nYou want to have the ability to capture the logs from the web server to help diagnose any issues if they occur.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "bcf24e30af6f43909e9002f879b54a92",
    "option" : "Enable Cloudtrail for the region.",
    "isCorrect" : "false"
  }, {
    "id" : "3cbc0e23a76247ca841a91a205ddc4cd",
    "option" : "Install the Cloudwatch agent on the Instance.",
    "isCorrect" : "true"
  }, {
    "id" : "76f3236094214e75ada1e05899fb970e",
    "option" : "Use the VPC Flow logs to get the logs from the Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "73b1558a376548d0bc5ded8c890df0b0",
    "option" : "Create a dashboard for the key Cloudwatch metrics.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nYou can install the Cloudwatch agent on the machine and then configure it to send the web server's logs to a central location in Cloudwatch.\nOption A is invalid since this is used for API monitoring activity.\nOption C is invalid since it is used to get the network traffic coming to an Instance hosted in a VPC.Option D is invalid since this will not give the detailed level of logs that is required.\nFor more information on the Cloudwatch agent, please refer to the below link-\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html\n\nTo capture the logs from the web server on an EC2 instance, you can use the Cloudwatch service provided by AWS. Cloudwatch is a monitoring and observability service provided by AWS, which allows you to monitor and collect metrics, logs, and events from various AWS resources.\nOut of the given options, the most suitable approach for capturing logs from the web server on an EC2 instance is to install the Cloudwatch agent on the instance (Option B). The Cloudwatch agent is a small software component that you can install on your EC2 instances to collect system-level metrics and log files from the instance. The agent can collect data from multiple sources on the instance, including log files, JSON metrics files, and the output of scripts or commands. The agent then sends this data to Cloudwatch for storage and analysis.\nTo install the Cloudwatch agent on an EC2 instance, you need to follow the steps mentioned below:\nCreate an IAM role for the EC2 instance that has the required permissions to send logs to Cloudwatch. Launch the EC2 instance and associate the IAM role with the instance. Download and install the Cloudwatch agent on the instance. You can use the AWS Systems Manager Run Command feature to install the agent on multiple instances at once. Configure the agent to collect and send the logs to Cloudwatch. You can configure the agent using the agent configuration file or the command line. Verify that the logs are being sent to Cloudwatch by checking the Cloudwatch logs console.\nBy using the Cloudwatch agent, you can capture logs from the web server on the EC2 instance and send them to Cloudwatch. This will help you diagnose any issues that may occur during testing. You can also create Cloudwatch alarms and notifications based on the log data to get alerts when certain events occur.\n\n"
}, {
  "id" : 92,
  "question" : "Your application currently makes use of AWS Cognito for managing user identities.\nYou want to analyze the information that is stored in AWS Cognito for your application.\nWhich of the following features of AWS Cognito should you use for this purpose?\n",
  "answers" : [ {
    "id" : "b1781e81220045e489f732a1c8e6a508",
    "option" : "Cognito Data",
    "isCorrect" : "false"
  }, {
    "id" : "fb6d625a41b64dc7a48e43bf8032d85d",
    "option" : "Cognito Events",
    "isCorrect" : "false"
  }, {
    "id" : "2273539a3cfa4fafa9674851360af381",
    "option" : "Cognito Streams",
    "isCorrect" : "true"
  }, {
    "id" : "339dc5ad17b54a19b817e32df6b5f230",
    "option" : "Cognito Callbacks.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAmazon Cognito Streams gives developers control and insight into their data stored in Amazon Cognito.\nDevelopers can now configure a Kinesis stream to receive events as data is updated and synchronized.\nAmazon Cognito can push each dataset change to a Kinesis stream you own in real-time.\nAll other options are invalid since you should use Cognito Streams.\nFor more information on Cognito Streams, please refer to the below link-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-streams.html\n\nThe correct answer is C. Cognito Streams.\nAWS Cognito is a fully managed service that provides user authentication, authorization, and user management functionalities to mobile and web applications. It enables developers to easily add user sign-up, sign-in, and access control capabilities to their applications.\nAWS Cognito Streams is a feature that allows developers to stream events that are triggered by changes in user data stored in Cognito User Pools to other AWS services in real-time. This feature can be used to analyze and process the data that is stored in Cognito for the purpose of business intelligence, fraud detection, user behavior analysis, and other analytics use cases.\nWhen Cognito Streams is enabled, Cognito sends user events such as sign-up, sign-in, and account update events to an Amazon Kinesis Data Stream in real-time. Developers can then use AWS Lambda or other AWS services to process and analyze the data that is stored in the Kinesis stream.\nTo enable Cognito Streams, you need to create a Kinesis Data Stream in your AWS account and configure it to receive user events from your Cognito User Pool. You also need to configure your User Pool to stream events to the Kinesis Data Stream.\nIn conclusion, if you want to analyze the information that is stored in AWS Cognito for your application, you should use Cognito Streams to stream user events to other AWS services for processing and analysis.\n\n"
}, {
  "id" : 93,
  "question" : "You've developed a set of scripts using AWS Lambda.\nThese scripts need to access EC2 Instances in a VPC.\nWhich of the following needs to be done to ensure that the AWS Lambda function can access the resources in the VPC? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ec97bf4f88834e8eb85bcebacedaf077",
    "option" : "Ensure that the subnet ID’s are configured in the Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "507c83bea12045c9af3129ba4370b963",
    "option" : "Ensure that the NACL ID’s are configured in the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "d18a0945fc124d7384f3cacfea1aacfd",
    "option" : "Ensure that the Security Group ID’s are configured in the Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "d5c5271eac5e415f89a976c78a4560c6",
    "option" : "Ensure that the VPC Flow Log ID’s are configured in the Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nOptions B and D are incorrect since you have to mention the Subnet and Security IDs for the Lambda function to access the resources in the VPC.The AWS Documentation mentions the following.\nYou can enable AWS Lambda to access resources in a Virtual Private Cloud (VPC)\nYour Lambda functions can now access Amazon RDS databases, Amazon Redshift data warehouses, Amazon ElasticCache nodes, and other endpoints that are accessible only from within a particular VPC (e.g., web service running on EC2)\nYou must provide additional VPC-specific configuration information such as VPC subnet IDs and security group IDs in order to enable your Lambda functions to access resources in an Amazon VPC.\nFor more information on configuring a lambda function to access resources in a VPC, please refer to the below link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/vpc.html\nhttps://aws.amazon.com/about-aws/whats-new/2016/02/access-resources-within-a-vpc-using-aws-lambda/\n\nTo enable a Lambda function to access resources within a VPC, you need to take the following two steps:\nConfigure the Lambda function to run inside the VPC: You can achieve this by specifying the subnet IDs that your Lambda function can use. Configure the security group to allow inbound and outbound traffic for the Lambda function: In this step, you need to configure a security group for the Lambda function that allows inbound and outbound traffic to the VPC resources.\nAnswer A and C are correct:\nA. Ensure that the subnet ID's are configured in the Lambda function: When you create a Lambda function, you can select one or more subnets in the VPC where you want your Lambda function to reside. Ensure that the subnet ID's are correctly configured so that your Lambda function can access resources within the VPC.\nC. Ensure that the Security Group ID's are configured in the Lambda function: You can attach a security group to your Lambda function, which controls the inbound and outbound traffic to and from the function. Ensure that the Security Group ID's are correctly configured so that your Lambda function can access resources within the VPC.\nOption B and D are incorrect:\nB. Ensure that the NACL ID's are configured in the Lambda function: Network Access Control Lists (NACLs) are stateless firewalls that control traffic to and from subnets in a VPC. However, NACLs are not associated with Lambda functions. So, this option is incorrect.\nD. Ensure that the VPC Flow Log ID's are configured in the Lambda function: VPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. However, VPC Flow Logs are not associated with Lambda functions. So, this option is also incorrect.\nIn summary, to allow a Lambda function to access resources within a VPC, you need to configure the function to run inside the VPC by specifying the subnet IDs, and configure a security group for the function that allows inbound and outbound traffic to the VPC resources.\n\n"
}, {
  "id" : 94,
  "question" : "You've currently been tasked to migrate an existing on-premises web application into Elastic Beanstalk.\nYou can't find any relevant AMIs in the beanstalk service that would be suitable to host your application.\nWhat would you consider as an easy step among the following to host the application?\n",
  "answers" : [ {
    "id" : "da53d130310a4f99b6a480f2b590c552",
    "option" : "Migrate your application to Docker containers and then migrate to the Elastic Beanstalk environment.",
    "isCorrect" : "false"
  }, {
    "id" : "bd68048182b14793863088190cdce11c",
    "option" : "Consider using CloudFormation to deploy your environment to Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "ae5aa4ac49134f26a049c77ff2b57d46",
    "option" : "Consider using Packer to create a custom platform",
    "isCorrect" : "true"
  }, {
    "id" : "c11b82e046b24f8898a364352d669c7c",
    "option" : "Consider deploying your application using the Elastic Container Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following to support this.\nCustom Platforms.\nElastic Beanstalk supports custom platforms.\nA custom platform is more advanced customization than a Custom Image in several ways.\nA custom platform lets you develop an entirely new platform from scratch, customizing the operating system, additional software, and scripts that Elastic Beanstalk runs on platform instances.\nThis flexibility allows you to build a platform for an application that uses a language or other infrastructure software.\nTo create a custom platform, you build an Amazon Machine Image (AMI) from one of the supported operating systems-Ubuntu, RHEL, or Amazon Linux and add further customizations.\nYou create your own Elastic Beanstalk platform using Packer, an open-source tool for creating machine images for many platforms, including AMIs for Amazon EC2\nAn Elastic Beanstalk platform comprises an AMI configured to run a software set that supports an application, and metadata can include custom configuration options and default configuration option settings.\nOptions A and D are incorrect because it could require a lot of effort to migrate the application to start using Docker containers.\nOption B is incorrect because using Cloudformation alone cannot be used alone for this requirement.\nFor more information on Custom Platforms, please refer to the below link-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/custom-platforms.html\n\nSure, I'd be happy to explain the answer options in detail to help you better understand which one is the best choice.\nOption A: Migrate your application to Docker containers and then migrate to the Elastic Beanstalk environment.\nThis option involves containerizing your application using Docker and then deploying it to Elastic Beanstalk. Elastic Beanstalk supports Docker containers, so this approach can be a good fit if you're familiar with Docker and want more control over the environment. By using Docker, you can ensure that your application will run consistently across different environments, which can be especially useful if you need to deploy your application to multiple environments (e.g., dev, test, prod).\nOption B: Consider using CloudFormation to deploy your environment to Elastic Beanstalk.\nCloudFormation is a service that allows you to define your infrastructure as code. With CloudFormation, you can define the resources you need (e.g., EC2 instances, load balancers, security groups) and how they're configured, and then deploy them in a consistent and repeatable way. If you choose this option, you would use CloudFormation to create the Elastic Beanstalk environment and configure it to run your application.\nOption C: Consider using Packer to create a custom platform.\nPacker is a tool that allows you to create machine images for different platforms (e.g., AWS, Azure, VMware) from a single configuration file. By using Packer, you can create a custom AMI that includes all the software and configuration required to run your application. If you choose this option, you would create a custom AMI using Packer, and then use the custom AMI to launch instances in your Elastic Beanstalk environment.\nOption D: Consider deploying your application using the Elastic Container Service.\nElastic Container Service (ECS) is a fully managed container orchestration service that makes it easy to run, stop, and manage Docker containers on a cluster of EC2 instances. With ECS, you can easily deploy and scale containerized applications. If you choose this option, you would containerize your application using Docker, and then deploy it to ECS. This option gives you more control over the underlying infrastructure than Elastic Beanstalk, but also requires more management overhead.\nIn conclusion, all of the options listed can work for hosting an application on Elastic Beanstalk, but each one has its own pros and cons. Option A is a good choice if you're familiar with Docker and want more control over the environment. Option B is a good choice if you're comfortable with writing infrastructure as code using CloudFormation. Option C is a good choice if you want to create a custom AMI that includes all the software and configuration required to run your application. Option D is a good choice if you're already familiar with container orchestration using ECS.\n\n"
}, {
  "id" : 95,
  "question" : "Company B is writing 10 items to the Dynamo DB table every second.\nEach item is 15.5Kb in size.\nWhat would be the required provisioned write throughput for best performance? Choose the correct answer from the options below.\n",
  "answers" : [ {
    "id" : "80a677ce381c46e983d78a811089d258",
    "option" : "10",
    "isCorrect" : "false"
  }, {
    "id" : "5de5b6e6296c494890ac82209aceb4c0",
    "option" : "160",
    "isCorrect" : "true"
  }, {
    "id" : "fe0b8f3776c240468308278103e521c4",
    "option" : "155",
    "isCorrect" : "false"
  }, {
    "id" : "a5b22a8dc8954dbdac11c1b2c9a1d524",
    "option" : "16",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAs per the documentation, when working with write capacity, the rule is to divide the item size by 1KB.Hence, 15.5 divided by 1 is 15.5\nWhen we round-off to the nearest 1KB value, it's 16.Since we are writing 10 items per second, we need to multiply 10*16 = 160.\nFor more information on Read and Write capacity, please refer to the below link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html\n\nTo calculate the required provisioned write throughput for best performance, we need to use the following formula:\nRequired Write Throughput = (Item Size * Write Frequency)/1KB\nWhere:\nItem Size is the size of each item (15.5KB in this case) Write Frequency is the number of items written per second (10 in this case) 1KB is the size of a single write operation in DynamoDB.\nSo, substituting the values:\nRequired Write Throughput = (15.5 * 10)/1 = 155\nTherefore, the correct answer is option C: 155.\n\n"
}, {
  "id" : 96,
  "question" : "You're a developer at a company that needs to deploy an application using Elastic Beanstalk.\nThere is a requirement to place a healthcheck.config file for the environment.\nIn which of the following location should this config file be placed to ensure that it is part of the elastic beanstalk environment?\n",
  "answers" : [ {
    "id" : "901d354c0fe845b69804342ac04a9188",
    "option" : "In the application root folder",
    "isCorrect" : "false"
  }, {
    "id" : "b28550fd91cb44dc9542eb96d96715bd",
    "option" : "In the config folder",
    "isCorrect" : "false"
  }, {
    "id" : "c1c8c7d283e947dd9b5d9416e58130b5",
    "option" : "In the packages folder",
    "isCorrect" : "false"
  }, {
    "id" : "0e30d474265340abbc18645b9323b967",
    "option" : "In the .ebextensions folder.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nElastic Beanstalk supports two methods of saving configuration option settings.\nConfiguration files in YAML or JSON format can be included in your application's source code in a directory named .ebextensions and deployed as part of your application source bundle.\nYou create and manage configuration files locally.\nAll other options are incorrect because the AWS documentation specifically mentions that you need to place custom configuration files in the .ebextensions folder.\nFor more information on the environment configuration method, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-methods-before.html\n\nThe correct answer is D. In the .ebextensions folder.\nWhen using Elastic Beanstalk, the .ebextensions directory allows the user to configure a number of options for their application environment. This directory should be located in the root folder of your application.\nThe .ebextensions directory allows you to specify various configuration files, including healthcheck.config. The healthcheck.config file can be used to specify a URL that Elastic Beanstalk should use to determine whether the environment is healthy or not. This URL can be used to perform a simple HTTP GET request to a specific endpoint on your application.\nBy placing the healthcheck.config file in the .ebextensions directory, Elastic Beanstalk will recognize it as part of the environment configuration and will apply the settings specified in the file.\nTherefore, option D (In the .ebextensions folder) is the correct answer.\n\n"
}, {
  "id" : 97,
  "question" : "A Lambda function with proper IAM roles has been developed with the default settings and uses Node.js.\nThe function makes calls to a DynamoDB table.\nIt is estimated that the Lambda function would run for 5 minutes.\nWhen the lambda function is executed, it does not add the required rows to the DynamoDB table and the function execution timeouts.\nWhat needs to be changed to ensure that the Lambda function works as desired?\n",
  "answers" : [ {
    "id" : "75f5eeee36204ccfa444b8fb8b6d6df4",
    "option" : "Ensure that the underlying programming language is changed to python.",
    "isCorrect" : "false"
  }, {
    "id" : "307ffb95354046eba62f5b7d5dfc44d3",
    "option" : "Enlarge the timeout for the function.",
    "isCorrect" : "true"
  }, {
    "id" : "78fd512828cd43e9b5a91ab0bb2fe4e2",
    "option" : "Change the memory assigned to the function to 1 G.",
    "isCorrect" : "false"
  }, {
    "id" : "c8c6c348a7764a2e99a1365c1950e9fb",
    "option" : "Assign an IAM user to the Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nIf the lambda function was created with the default settings, it would have the default timeout of 3 seconds, as shown below.\nSince the function executes in a timespan of 300 seconds, this value needs to be changed.\nOption A is incorrect since the programming language is not an issue.\nOption C is incorrect since it does not mention the amount of memory required in the question.\nOption D is incorrect since IAM roles should be assigned to the Lambda function.\nFor more information on configuring Lambda functions, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/resource-model.html\n\n\nThe correct answer is B. Enlarge the timeout for the function.\nExplanation:\nWhen a Lambda function is executed, it is allocated a certain amount of time to complete its execution, which is known as the timeout setting. The default timeout for a Lambda function is 3 seconds, but it can be increased up to a maximum of 15 minutes. In this scenario, the Lambda function is estimated to run for 5 minutes, but it is timing out before completing its execution. Therefore, the most appropriate solution is to increase the timeout setting for the Lambda function.\nChanging the underlying programming language to Python or changing the memory assigned to the function to 1 G would not necessarily resolve the issue. These changes may improve the performance of the function, but they would not directly address the timeout issue.\nAssigning an IAM user to the Lambda function is not relevant to this scenario. IAM roles are used to grant permissions to AWS services and resources, such as DynamoDB, that the Lambda function needs to access. If the function was able to make calls to the DynamoDB table, it is assumed that the necessary IAM roles have already been assigned.\nIn summary, increasing the timeout setting for the Lambda function would likely resolve the issue of the function timing out before completing its execution.\n\n"
}, {
  "id" : 98,
  "question" : "You need to set up a RESTful API service in AWS that would be serviced via the following URL: https://democompany.com/customers Which of the following combination of services can be used to develop and host the RESTful API service? (Select TWO)\n",
  "answers" : [ {
    "id" : "758eeb7f9bc94fa5bf0bbd3c1f143815",
    "option" : "AWS Lambda and AWS API Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "ef2118db52a34de886f3b48ef9d8b33d",
    "option" : "AWS CloudFront and Application Load Balancer",
    "isCorrect" : "false"
  }, {
    "id" : "db8760f531074d65ac0b5407fa9c44ad",
    "option" : "AWS EC2 and Application Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "f6e717309dff46f593ec460b54f2cbca",
    "option" : "AWS SQS and CloudFront.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nAWS Lambda can be used to host the code, and the API gateway can be used to access the API's which point to AWS Lambda.\nAlternatively, you can create your own API service, host it on an EC2 Instance, and then use the AWS Application Load balancer to do path-based routing.\nOption B is incorrect since AWS CloudFront is not the correct service to host an application or create RESTful API.\nELB is a load-balancing solution.\nOption D is incorrect since AWS SQS is a queuing service.\nFor more information on an example with RESTful API's, please refer to the below URL-\nhttps://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/module-4/\n\nThe correct options for the given scenario are A. AWS Lambda and AWS API Gateway, and C. AWS EC2 and Application Load Balancer.\nExplanation:\nRESTful API service allows clients to access and manipulate data over the HTTP protocol. AWS offers several services for developing and hosting RESTful APIs.\nA. AWS Lambda and AWS API Gateway: AWS Lambda is a serverless compute service that allows developers to run code without provisioning or managing servers. AWS API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. These services can be used together to develop and host RESTful APIs. Lambda functions can be used to create the API endpoints, and API Gateway can be used to manage the API, handle authentication and authorization, and provide throttling, caching, and other features.\nC. AWS EC2 and Application Load Balancer: Amazon Elastic Compute Cloud (EC2) is a web service that provides resizable compute capacity in the cloud. An Application Load Balancer (ALB) is a fully managed service that routes traffic to EC2 instances. These services can be used together to develop and host RESTful APIs. EC2 instances can be used to create the API endpoints, and ALB can be used to load balance the traffic, handle SSL termination, and provide other features.\nB. AWS CloudFront and Application Load Balancer: AWS CloudFront is a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally. It can be used to accelerate the delivery of RESTful APIs. However, it cannot be used alone to develop and host RESTful APIs. An Application Load Balancer (ALB) can be used with CloudFront to load balance the traffic, handle SSL termination, and provide other features.\nD. AWS SQS and CloudFront: AWS Simple Queue Service (SQS) is a fully managed message queuing service that enables decoupling and scaling of microservices, distributed systems, and serverless applications. It cannot be used alone to develop and host RESTful APIs. AWS CloudFront can be used with SQS to deliver messages to clients, but it is not a suitable option for developing and hosting RESTful APIs.\nTherefore, the correct options for developing and hosting RESTful APIs are A. AWS Lambda and AWS API Gateway, and C. AWS EC2 and Application Load Balancer.\n\n"
}, {
  "id" : 99,
  "question" : "You are working on a POC for a new gaming application in us-east-1 region which will be using Amazon Cognito Events to execute AWS Lambda function.\nAWS Lambda function will issue a winning badge for a player post reaching every new level.\nYou are getting error as “LambdaThrottledException” for certain cases when you are performing application testing with large number of players.\nWhich of the following action needs to be implemented to resolve this error message?\n",
  "answers" : [ {
    "id" : "fb3e47d804664a3a869eb115a39cb22b",
    "option" : "Make sure Lambda Function responds in 5 sec.",
    "isCorrect" : "false"
  }, {
    "id" : "01072cabc8cf49daa49d7fe92caf3120",
    "option" : "Make sure Amazon Cognito provides all records in a dataset as input to the function.",
    "isCorrect" : "false"
  }, {
    "id" : "cf8b97d8304c4c3ca05d97b671587f80",
    "option" : "Modify the Lambda function to enable retry when writing the function for the Sync Trigger event.",
    "isCorrect" : "true"
  }, {
    "id" : "4f9c9253251c47199ec23a9edcc5c73b",
    "option" : "Make sure you are updating “datasetRecords” field &amp; not any other fields.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nTo resolve “LambdaThrottledException” error while using Amazon Cognito Events, you need to perform retry on sync operations while writing Lambda function.\nOption A is incorrect as If a Lambda Function does not respond in 5 sec, “LambdaSocketTimeoutException” error will be generated &amp; not “LambdaThrottledException”\nOptionB is incorrect as this will not generate error “LambdaThrottledException”.\nOptionD is incorrect as If you are updating other fields, this will result in failure in updating records &amp; not generate error “LambdaThrottledException”.\nFor more information on Amazon Cognito Events, refer to the following URL,\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-events.html\n\nThe error message \"LambdaThrottledException\" indicates that AWS Lambda is unable to process all of the requests that it is receiving, because it has reached its concurrency limit. In other words, Lambda is currently handling as many requests as it can, and it cannot handle any more until some of the current requests have completed.\nTo resolve this error message, you need to take steps to reduce the number of requests being sent to Lambda, or increase the capacity of Lambda to handle more requests.\nOption A, \"Make sure Lambda Function responds in 5 sec\", is not a solution to the problem, as it does not address the underlying issue of Lambda being throttled due to high concurrency. While it is important to optimize the performance of your Lambda function, reducing the response time to 5 seconds may not be possible or practical for all use cases.\nOption B, \"Make sure Amazon Cognito provides all records in a dataset as input to the function\", is not relevant to the problem, as it pertains to the input data provided to the Lambda function rather than the Lambda concurrency issue.\nOption C, \"Modify the Lambda function to enable retry when writing the function for the Sync Trigger event\", is a potential solution to the problem. By enabling retry for the Lambda function, you can ensure that any failed requests are retried automatically, reducing the likelihood of requests being dropped due to throttling. This approach may require some additional configuration and testing to ensure that retries are handled correctly and do not result in duplicate or conflicting updates.\nOption D, \"Make sure you are updating “datasetRecords” field & not any other fields\", is also not relevant to the problem, as it pertains to the data being updated by the Lambda function rather than the Lambda concurrency issue.\nTherefore, the correct answer is C: Modify the Lambda function to enable retry when writing the function for the Sync Trigger event.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 101,
  "question" : "Which of the following is the right sequence of hooks that get called in AWS CodeDeploy?\n",
  "answers" : [ {
    "id" : "eeae237f00cb42268ffc120d34fc179f",
    "option" : "Application Stop-&gt;BeforeInstall-&gt;After Install-&gt;Application Start",
    "isCorrect" : "true"
  }, {
    "id" : "f5b42302f30244319bb5f426bd386651",
    "option" : "BeforeInstall-&gt;After Install-&gt; Application Stop-&gt; Application Start",
    "isCorrect" : "false"
  }, {
    "id" : "412f8ce53d004003bb9cf22565ebba1b",
    "option" : "BeforeInstall-&gt;After Install-&gt;Validate Service-&gt; Application Start",
    "isCorrect" : "false"
  }, {
    "id" : "b67d0a4df7f54300a3d915c6cf4cc3fd",
    "option" : "BeforeInstall-&gt;Application Stop-&gt; Validate Service-&gt; Application Start.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is also mentioned in the AWS Documentation.\nOptions B and D are incorrect because the application needs to be stopped before doing the pre-install tasks such as decrypting files and creating a backup.\nOption C is incorrect because ValidateService should be the last event.\nIt is used to verify if the deployment is completed successfully.\nFor more information on the hooks order, please refer to the below URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-run-order\n\n\nAWS CodeDeploy is a fully managed deployment service that automates software deployments to various computing services such as Amazon EC2 instances, AWS Lambda functions, and on-premises servers. It uses a sequence of lifecycle events, also known as hooks, to perform deployments.\nThe right sequence of hooks that get called in AWS CodeDeploy is:\nB. BeforeInstall->After Install->Application Stop->Application Start\nHere is a detailed explanation of each hook in the sequence:\nBeforeInstall: This hook is the first in the sequence and is used to prepare the system for the installation of new revisions of an application. It can be used to backup or delete old files, create new directories, and perform other necessary setup tasks before deployment. AfterInstall: This hook is called after the application is installed on the target instances. It can be used to configure the new version of the application, set up permissions, and perform other post-installation tasks. ApplicationStop: This hook is called before the application is stopped on the target instances. It can be used to save any in-flight transactions or data, stop any services or processes that may interfere with the deployment, and perform other necessary tasks before the application is stopped. ApplicationStart: This hook is called after the new version of the application is started on the target instances. It can be used to perform final configurations, such as updating DNS records, load balancer settings, and other tasks.\nTherefore, option B is the right sequence of hooks that get called in AWS CodeDeploy: BeforeInstall->After Install->Application Stop->Application Start.\n\n"
}, {
  "id" : 102,
  "question" : "As a developer, you have created a Lambda function used to work with a bucket in Amazon S3\nThe Lambda function is not working as expected.\nYou need to debug the issue and understand what's the underlying issue.\nHow can you accomplish this in an easily understandable way?\n",
  "answers" : [ {
    "id" : "9bc3d5107ac84feea4ad5d3a3ff184ca",
    "option" : "Use AWS CloudWatch metrics.",
    "isCorrect" : "false"
  }, {
    "id" : "7805ba11e9954e75991756b6725a4820",
    "option" : "Use AWS CloudWatch logs.",
    "isCorrect" : "true"
  }, {
    "id" : "7410779115164f7dbf3f7f35192faa16",
    "option" : "Raise a ticket with the AWS Support team.",
    "isCorrect" : "false"
  }, {
    "id" : "c58f2301eae24e08961a4b0e2c9dd995",
    "option" : "Use AWS CloudTrail logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is also mentioned in the AWS Documentation.\nYou can insert logging statements into your code to help you validate that your code is working as expected.\nLambda automatically integrates with Amazon CloudWatch Logs and pushes all logs from your code to a CloudWatch Logs group associated with a Lambda function (/aws/lambda/)\nOption A is incorrect since the metrics will only give the rate at which the function is executing.\nBut it cannot help debug the actual error.\nOption C is incorrect since there is no need to raise a support ticket with the AWS support team.\nOption D is incorrect since this is only used for API monitoring.\nFor more information on monitoring functions, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html\n\nSure, I can provide a detailed explanation of the answer options:\nA. Use AWS CloudWatch metrics: AWS CloudWatch Metrics are a way to monitor and collect data in real-time about the resources and services used in AWS. Metrics help you understand the performance and behavior of your AWS resources, including Lambda functions. You can use metrics to identify issues and to debug problems. In the context of this question, you could use CloudWatch metrics to monitor the Lambda function's invocation rate, error rate, and duration, among other things. This information can help you understand if the function is being triggered correctly, how often it's running, and if there are any issues with its execution. However, metrics alone might not provide enough detail to debug the issue.\nB. Use AWS CloudWatch logs: AWS CloudWatch logs are a way to collect, monitor, and analyze log data from AWS resources, including Lambda functions. CloudWatch logs can be used to troubleshoot issues and to debug problems in Lambda functions. The logs contain detailed information about the function's execution, including any errors or exceptions that occurred. In the context of this question, you could use CloudWatch logs to understand what's happening inside the Lambda function. By analyzing the logs, you could identify any errors or exceptions that occurred, and potentially identify the root cause of the issue. CloudWatch logs are a powerful tool for debugging issues with Lambda functions.\nC. Raise a ticket with the AWS Support team: Raising a ticket with the AWS Support team can be a good option if you're unable to identify the root cause of the issue on your own. AWS Support can help you troubleshoot and debug problems with your Lambda function, as well as provide guidance on how to optimize its performance. However, raising a ticket with AWS Support can take some time, and it might not be the most efficient way to debug the issue.\nD. Use AWS CloudTrail logs: AWS CloudTrail logs provide a record of API calls made in your AWS account, including those made by Lambda functions. CloudTrail logs can be used to identify the root cause of issues with Lambda functions, such as unauthorized access or misconfigured permissions. In the context of this question, you could use CloudTrail logs to understand what's happening at the API level when the Lambda function is being invoked. By analyzing the logs, you could identify any issues with permissions or access that might be causing the problem.\nIn summary, the most effective and easily understandable way to debug the issue with the Lambda function is to use AWS CloudWatch logs. The logs provide detailed information about the function's execution and can help you identify the root cause of the issue. However, it's also worth considering other options, such as CloudWatch metrics or CloudTrail logs, depending on the specific issue at hand.\n\n"
}, {
  "id" : 103,
  "question" : "You are developing a function that will be hosted in AWS Lambda.\nThe function will be developed in .Net.\nSeveral external libraries are needed for the code to run.\nWhich of the following is the best practice when it comes to working with external dependencies for AWS Lambda?\n",
  "answers" : [ {
    "id" : "f4e3c6d7c2eb433f80c0c1de6d5d5790",
    "option" : "Make sure that the dependencies are put in the root folder.",
    "isCorrect" : "false"
  }, {
    "id" : "147e0a9038df48808b56cc3a95597321",
    "option" : "Selectively only include the libraries that are required.",
    "isCorrect" : "false"
  }, {
    "id" : "4020c67e110c44eba827642f03e84600",
    "option" : "Make sure the libraries are installed in the beginning of the function.",
    "isCorrect" : "false"
  }, {
    "id" : "b62afbe8df96417189a7d46fc924eb68",
    "option" : "Place the entire SDK dependencies in Amazon S3.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nWhen developing a function that will be hosted in AWS Lambda, it is important to consider the use of external dependencies. These dependencies are typically libraries or other software components that are required for the function to run. In the case of .NET, these dependencies may come in the form of DLLs (Dynamic Link Libraries).\nHere are the four options presented in the question, along with explanations of each:\nA. Make sure that the dependencies are put in the root folder. This option suggests that all external dependencies should be placed in the root folder of the function. While this may work, it is not the best practice because it can make the deployment package larger than it needs to be, which can increase deployment time and consume unnecessary storage resources.\nB. Selectively only include the libraries that are required. This option suggests that only the necessary libraries should be included in the deployment package. This is a best practice because it reduces the size of the deployment package and minimizes the deployment time.\nC. Make sure the libraries are installed in the beginning of the function. This option suggests that the libraries should be installed at the beginning of the function. However, this is not possible with AWS Lambda because the function is executed in a container that is managed by AWS. Therefore, the dependencies need to be included in the deployment package.\nD. Place the entire SDK dependencies in Amazon S3. This option suggests that the dependencies should be placed in Amazon S3. However, this is not the best practice because it adds unnecessary complexity to the deployment process. It is better to include the dependencies in the deployment package, which makes the deployment process simpler and more efficient.\nIn conclusion, the best practice when it comes to working with external dependencies for AWS Lambda is to selectively include only the libraries that are required in the deployment package. This will help to minimize the size of the package and reduce deployment time.\n\n"
}, {
  "id" : 104,
  "question" : "Your team has a Code Commit repository in your account.\nYou need to give developers in another account access to your Code Commit repository.\nWhich of the following is the most effective way to grant access?\n",
  "answers" : [ {
    "id" : "321f7a675abb46caa6a1abeea8a84847",
    "option" : "Create IAM users for each developer and provide access to the repository",
    "isCorrect" : "false"
  }, {
    "id" : "ea74d889523a443bb2b9440f9df60d9e",
    "option" : "Create an IAM Group , add the IAM users and then provide access to the repository",
    "isCorrect" : "false"
  }, {
    "id" : "d8bcba306efd44f28ac6ba9f2a350445",
    "option" : "Create a cross account role , give the role the privileges. Provide the role ARN to the developers.",
    "isCorrect" : "false"
  }, {
    "id" : "43189d00fa9a49d88078adcc74c2db15",
    "option" : "Enable public access for the repository.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nThe most effective way to grant developers in another AWS account access to your Code Commit repository is to create a cross-account role, give the role the necessary privileges, and provide the role ARN to the developers.\nOption A, creating IAM users for each developer and providing access to the repository, is not an effective solution because it requires manual management of users and their permissions. It also adds unnecessary complexity as the number of developers increases.\nOption B, creating an IAM group and adding the IAM users, is better than Option A as it provides a better way to manage permissions for multiple users. However, it still requires manual management of IAM users, and the number of users increases over time.\nOption D, enabling public access for the repository, is not a recommended option as it is not secure and may result in unauthorized access or data leaks.\nOption C, creating a cross-account role, is the recommended and most effective option. The cross-account role allows users in another AWS account to assume the role and access the repository with the necessary permissions. This approach provides a secure and centralized way to manage access to the Code Commit repository, and it eliminates the need for managing individual IAM users or groups.\nTo implement this solution, follow these steps:\nCreate a new IAM role in the AWS account where the Code Commit repository exists. Define the necessary permissions for the role, including access to the Code Commit repository. Add a trust policy to the role, allowing users in the other AWS account to assume the role. Share the role ARN with the developers in the other AWS account. In the other AWS account, create an IAM user or group, and grant permission to assume the cross-account role. Developers can then assume the cross-account role and access the Code Commit repository with the necessary permissions.\n\n"
}, {
  "id" : 105,
  "question" : "You have a lambda function that is processed asynchronously.\nYou need a way to check and analyse issues if the function fails, for further processing at a later time.\nHow could you accomplish this?\n",
  "answers" : [ {
    "id" : "634bd3388bfe4b06b90f5eca5af567ce",
    "option" : "Use AWS CloudWatch metrics.",
    "isCorrect" : "false"
  }, {
    "id" : "7484c168f67d4876adcf580214db053f",
    "option" : "Assign a dead letter queue.",
    "isCorrect" : "true"
  }, {
    "id" : "e958cec6109f4b859e68ee75b22e1c1c",
    "option" : "Configure CloudWatch Events.",
    "isCorrect" : "false"
  }, {
    "id" : "b910f8de627d4f9b97c75758e5a93ceb",
    "option" : "Use AWS CloudTrail logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is also mentioned in the AWS Documentation.\nAny Lambda function invoked asynchronously is retried twice before the event is discarded.\nIf the retries fail and you're unsure why use Dead Letter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue to analyze the failure.\nOption A is incorrect since the metrics will only give the rate at which the function is executing.\nBut it cannot help debug the actual error.\nOption C is incorrect because CloudWatch Events cannot help to handle failed or discarded events.\nOption D is incorrect since this is only used for API monitoring.\nFor more information on dead letter queues with AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/dlq.html\n\nWhen a Lambda function is processed asynchronously, AWS Lambda service will not wait for the function's response before sending a response back to the calling service or application. Therefore, if the Lambda function fails, the calling service or application will not be able to know that the function has failed. To address this issue, you need to implement a mechanism to monitor and handle failed executions of the Lambda function.\nOne way to accomplish this is by assigning a Dead Letter Queue (DLQ). A DLQ is a queue where AWS Lambda sends events that cannot be processed. When you configure a DLQ for a Lambda function, if the function fails, the event that triggered the function will be sent to the DLQ instead of being lost. You can then use tools like Amazon Simple Notification Service (SNS) or Amazon Simple Queue Service (SQS) to receive notifications or messages from the DLQ and take further actions, such as retrying the function, analysing the failure, or processing the event at a later time.\nAWS CloudWatch is a monitoring service that provides data and insights into your AWS resources and applications. You can use CloudWatch to monitor and analyze metrics, logs, and events related to your Lambda function. For example, you can set up CloudWatch Alarms to receive notifications when certain metrics, such as the number of errors or the duration of the function, exceed a threshold. However, CloudWatch alone cannot handle failed executions of a Lambda function.\nCloudWatch Events is a service that enables you to respond to events that occur in your AWS environment. You can use CloudWatch Events to trigger automated workflows, such as sending notifications, running Lambda functions, or invoking APIs, based on specific events or patterns. For example, you can set up a CloudWatch Event rule to trigger a Lambda function whenever a failed execution of your Lambda function is detected. However, CloudWatch Events alone cannot handle failed executions of a Lambda function.\nAWS CloudTrail is a service that logs and monitors API activity in your AWS account. You can use CloudTrail to track changes to your AWS resources, such as Lambda functions, and investigate security incidents, troubleshoot issues, or meet compliance requirements. However, CloudTrail logs do not provide real-time visibility into the execution of a Lambda function, and they do not provide a mechanism to handle failed executions.\nTherefore, the correct answer for this question is B: Assign a dead letter queue. By assigning a DLQ to your Lambda function, you can ensure that failed executions are not lost and can be processed at a later time.\n\n"
}, {
  "id" : 106,
  "question" : "You are planning to use AWS Kinesis streams for an application being developed for a company.\nThe company policy mandates that all data is encrypted at rest.\nHow can you accomplish this in the easiest way possible for Kinesis streams?\n",
  "answers" : [ {
    "id" : "42a03808ba3d40d7b9d63851f2fed464",
    "option" : "Use the SDK for Kinesis to encrypt the data before being stored at rest.",
    "isCorrect" : "false"
  }, {
    "id" : "01445ebd31d34610a0f3b329e3bf6772",
    "option" : "Enable server-side encryption for Kinesis streams.",
    "isCorrect" : "true"
  }, {
    "id" : "43375393da4f4e09bbc2f57c4571fcb9",
    "option" : "Enable client-side encryption for Kinesis streams.",
    "isCorrect" : "false"
  }, {
    "id" : "390aab0b08f14e06a4d878a4a1e753fa",
    "option" : "Use the AWS CLI to encrypt the data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe easiest way is to use the in-built server-side encryption that is available with Kinesis streams.\nThe AWS Documentation mentions the following.\nServer-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify.\nData is encrypted before it's written to the Kinesis stream storage layer and decrypted after it's retrieved from storage.\nAs a result, your data is encrypted at rest within the Kinesis Data Streams service.\nThis allows you to meet strict regulatory requirements and enhance the security of your data.\nOptions A and C are invalid since this would involve too much effort for encrypting and decrypting to the streams.\nOption D is invalid since this is the same as encrypting the data before it reaches the stream.\nFor more information on server-side encryption with streams, please refer to the below URL-\nhttps://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html\n\nThe easiest way to ensure that data in Kinesis streams is encrypted at rest is to enable server-side encryption for the Kinesis stream. Server-side encryption is a feature provided by AWS that automatically encrypts data at rest. When server-side encryption is enabled for a Kinesis stream, the data is encrypted before being written to disk, and decrypted when it is read back.\nTo enable server-side encryption for Kinesis streams, you need to use the AWS Management Console or the AWS CLI. When creating a Kinesis stream, you can choose to enable server-side encryption and select the AWS KMS (Key Management Service) key to use for encryption.\nOnce server-side encryption is enabled for a Kinesis stream, all data written to the stream will be automatically encrypted using the selected KMS key. You do not need to modify your application code or use the SDK to encrypt the data before it is stored at rest.\nClient-side encryption is another option for encrypting data in Kinesis streams, but it requires more effort as it involves encrypting the data before it is sent to the Kinesis stream and decrypting it after it is read back. This approach can be useful in scenarios where you need to maintain full control over the encryption process, but it is not necessary if you just need to ensure that data is encrypted at rest.\nIn summary, enabling server-side encryption for Kinesis streams is the easiest and most effective way to ensure that data is encrypted at rest. This can be done using the AWS Management Console or the AWS CLI, and does not require any changes to your application code.\n\n"
}, {
  "id" : 107,
  "question" : "You are developing an application that is going to make use of Amazon Kinesis.\nDue to the high throughput, you decide to have multiple shards for the streams.\nWhich of the following is TRUE when it comes to processing data across multiple shards?\n",
  "answers" : [ {
    "id" : "267d9092b6284ea2b036d00466a86f0d",
    "option" : "You cannot guarantee the order of data across multiple shards. It`s possible only within a shard.",
    "isCorrect" : "true"
  }, {
    "id" : "155ea019d18c4862bc1b81a3432af866",
    "option" : "Order of data is possible across all shards in a stream.",
    "isCorrect" : "false"
  }, {
    "id" : "7707e77ddd674d929943b4cd0c8f45c9",
    "option" : "Order of data is not possible at all in Kinesis streams.",
    "isCorrect" : "false"
  }, {
    "id" : "9b9894aa35594c87a6afe77ec4607c63",
    "option" : "You need to use Kinesis firehose to guarantee the order of data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nKinesis Data Streams lets you order records and read and replay records in the same order to many Kinesis Data Streams applications.\nTo enable write ordering, Kinesis Data Streams expects you to call the PutRecord API to write serially to a shard while using the sequenceNumberForOrdering parameter.\nSetting this parameter guarantees strictly increasing of sequence numbers for puts from the same client and to the same partition key.\nOption A is correct as it cannot guarantee the ordering of records across multiple shards.\nOption B, C and D are incorrect becauseKinesis Data Streams can order records on a single shard.\nEach data record has a sequence number that is unique within its shard.\nKinesis Data Streams assigns the sequence number after you write to the stream with putRecords or client.putRecord.\nFor more information, please refer to:\nhttps://aws.amazon.com/blogs/database/how-to-perform-ordered-data-replication-between-applications-by-using-amazon-dynamodb-streams/\nhttps://docs.aws.amazon.com/streams/latest/dev/key-concepts.html\n\nWhen using Amazon Kinesis, data is ingested into a stream, which is composed of multiple shards. Each shard is an independent sequence of data records, and Kinesis assigns a unique sequence number to each record that is inserted into the stream.\nWhen processing data across multiple shards in Kinesis, it is important to keep in mind that there is no inherent ordering of data across shards. Each shard operates independently, and data may be processed in a different order than it was ingested.\nTherefore, the correct answer is A: You cannot guarantee the order of data across multiple shards. It's possible only within a shard.\nHowever, if you need to maintain order across shards, you can use the sequence number assigned to each record by Kinesis. By using the sequence numbers, you can determine the order in which the records were ingested into the stream and reorder them accordingly during processing. Alternatively, you can use a timestamp as part of the record data to maintain ordering across multiple shards.\nIn summary, when processing data across multiple shards in Kinesis, you cannot rely on inherent ordering, but you can use sequence numbers or timestamps to reorder the data as needed. It is not necessary to use Kinesis Firehose to guarantee the order of data.\n\n"
}, {
  "id" : 108,
  "question" : "A company is planning to develop an application that is going to make use of a DynamoDB table.\nThe structure of the table is given below. Attribute Name Type Description Product ID Number Unique ID of product Review ID Number Unique ID of an optional customer review Product Name String Name of the product Product Description String Description of the product Which of the following should be chosen as the partition key to ensure the MOST effective distribution of keys?\n",
  "answers" : [ {
    "id" : "ae9665929beb4e38a1eebda6f9fc68f3",
    "option" : "Product ID",
    "isCorrect" : "true"
  }, {
    "id" : "79cdea6dab194843aaa9040a1a4e6dc0",
    "option" : "Review ID",
    "isCorrect" : "false"
  }, {
    "id" : "43a3715fb4794ac9be2f305f7d61b1f6",
    "option" : "Product Name",
    "isCorrect" : "false"
  }, {
    "id" : "8def5b08f79a4342a00b7c4be0f0d63f",
    "option" : "Production Description.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe most effective one will be the Product ID since you have a unique ID for each product.\nOption A is correct.\nOption B is incorrect because reviews are optional (as stated in the question)\nSo we can't guarantee where reviews will be present.\nOptions C and D are incorrect since it would not be best to keep these as the partition keys.\nFor more information on DynamoDB and partition keys, please refer to the below URLs-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html\nhttps://aws.amazon.com/blogs/database/choosing-the-right-dynamodb-partition-key/\n\nIn DynamoDB, a partition key is used to distribute data across partitions and is used to determine the physical storage of data in the backend. It is important to choose the right partition key to ensure the most effective distribution of data and avoid hot partitions or uneven load distribution.\nLooking at the given structure of the DynamoDB table, there are four attributes available to choose as the partition key.\nProduct ID and Review ID are both number types, but Product ID is unique for each product while Review ID is unique for each review. Therefore, using Product ID as the partition key would result in a more even distribution of keys as each product has its unique ID, whereas each review is tied to a specific product and might not be as evenly distributed.\nProduct Name and Product Description are both string types and are less likely to provide an even distribution of keys. Moreover, it is not recommended to use string types as partition keys as it can result in hot partitions and uneven load distribution.\nTherefore, in this scenario, the most effective partition key to ensure even distribution of keys would be Product ID. So, option A. Product ID should be chosen as the partition key for this DynamoDB table.\n\n"
}, {
  "id" : 109,
  "question" : "Your company is planning to use the Simple Storage Service to store objects that will be accessed by users.\nThere is speculation that there would be roughly 6000 GET requests per second.\nWhich of the following could be used to ensure optimal performance? (Select TWO)\n",
  "answers" : [ {
    "id" : "20a611f2a64e42ddaa2fdc2cb48d1ceb",
    "option" : "Use a CloudFront distribution in front of the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "1066b54d79994e85b3bb92ca7d0b814c",
    "option" : "Enable S3 Transfer Acceleration.",
    "isCorrect" : "true"
  }, {
    "id" : "c3b5d8bd68594a70b417c04363329d80",
    "option" : "Enable versioning for the objects.",
    "isCorrect" : "false"
  }, {
    "id" : "9284778befd34964b1c544b8a8a3e3bd",
    "option" : "Enable Cross Region Replication for the bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nStoring your static content with S3 provides a lot of advantages.\nBut to help optimize your application's performance and security while effectively managing cost, we recommend that you also set up Amazon CloudFront to work with your S3 bucket to serve and protect the content.\nCloudFront is a content delivery network (CDN) service that delivers static and dynamic web content, video streams, and APIs around the world, securely and at scale.\nS3 Transfer Acceleration improves transfer performance by routing uploads and downloads through Amazon CloudFront's globally distributed Edge Locations and over AWS backbone networks, and by using network protocol optimizations.\nOption C is incorrect as it can only be used to prevent the accidental deletion of objects.\nOption D is incorrect as S3 Cross-region replication can be used to improve latency (across regions) as well as disaster recovery.\nThe question does not give any region-specific details or any need for bucket migration or high availability of bucket and therefore it is an inappropriate option.\nReference:\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/\nhttps://aws.amazon.com/s3/transfer-acceleration/\n\nTo ensure optimal performance when accessing objects from Amazon S3, the following options can be considered:\nA. Use a CloudFront distribution in front of the S3 bucket: Using CloudFront, a content delivery network (CDN) service, in front of the S3 bucket can help to improve performance by caching frequently accessed objects in edge locations around the world. This will reduce the latency and improve the overall user experience.\nB. Enable S3 Transfer Acceleration: S3 Transfer Acceleration uses Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet to S3 buckets. This service can help to improve the speed of uploading and downloading objects from S3, especially when the objects are large or being accessed from across regions.\nC. Enable versioning for the objects: Enabling versioning for objects in an S3 bucket allows you to keep multiple versions of the same object. This can be useful for backup and recovery purposes. Versioning can also help to ensure that data is not lost or corrupted accidentally.\nD. Enable Cross Region Replication for the bucket: Cross Region Replication allows you to replicate objects from one S3 bucket to another in a different region. This can be useful for data protection, compliance, and disaster recovery. By replicating data to a different region, you can ensure that the data is available even if the original region experiences an outage.\nIn this scenario, option A and B can be used to ensure optimal performance. Using CloudFront will help to improve the latency and overall user experience by caching frequently accessed objects, and enabling S3 Transfer Acceleration can improve the speed of uploading and downloading objects from S3. However, options C and D are not directly related to improving performance in this scenario.\nTherefore, the correct answers are A and B.\n\n"
}, {
  "id" : 110,
  "question" : "Your company currently stores its objects in S3.The current request rate is around 11000 GET requests per second.\nThere is now a mandate for objects to be encrypted at rest.\nSo you enable encryption using KMS.\nThere are now performance issues being encountered.\nWhat could be the main reason behind this?\n",
  "answers" : [ {
    "id" : "4a7f39926df84965b5aa20975cb44ddd",
    "option" : "Amazon S3 will now throttle the requests since they are now being encrypted using KMS.",
    "isCorrect" : "false"
  }, {
    "id" : "216c767c5f434043ada7cfba94da9660",
    "option" : "You need to also enable versioning to ensure optimal performance.",
    "isCorrect" : "false"
  }, {
    "id" : "91fd8f124f3b48d1be8f993eb16153ac",
    "option" : "You are now exceeding the throttle limits for KMS API calls.",
    "isCorrect" : "true"
  }, {
    "id" : "115c52d4659a4c5bafeb62bb2c6987dc",
    "option" : "You need to also enable CORS to ensure optimal performance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is also mentioned in the AWS Documentation.\nYou can make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf.\nThe limit applies to both kinds of requests.\nOption A is incorrect because S3 will not throttle requests just because encryption is enabled.\nFor example, you might store data in Amazon S3 using server-side encryption with AWS KMS (SSE-KMS)\nEach time you upload or download an S3 object that's encrypted with SSE-KMS, Amazon S3 makes a\nGenerateDataKey.\n(for uploads) or\nDecrypt.\n(for downloads) request to AWS KMS on your behalf.\nThese requests count toward your limit, so AWS KMS throttles the requests if you exceed a combined total of 5500 (or 10,000) uploads or downloads per second of S3 objects encrypted with SSE-KMS.\nOptions B and D are incorrect because these will not help increase performance.\nFor more information on KMS limits improvement, please refer to the below URL-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/limits.html\n\nThe most likely reason for the performance issues after enabling encryption using KMS in S3 is option C: You are now exceeding the throttle limits for KMS API calls.\nKMS is a service in AWS that provides encryption key management, and it's used by many other services in AWS to encrypt data at rest. When you enable encryption on an S3 bucket, S3 uses KMS to encrypt the data. When a user requests an object from the bucket, S3 contacts KMS to decrypt the object before returning it to the user.\nKMS has a throttle limit on the number of API requests it can handle per second. By default, this limit is set to 10 requests per second. When you exceed this limit, KMS throttles the requests and returns a 429 Too Many Requests error. This can cause a significant slowdown in the performance of your S3 bucket, especially if you're receiving a large number of requests.\nOption A, Amazon S3 will now throttle the requests since they are now being encrypted using KMS, is not entirely correct. While it's true that enabling encryption using KMS can cause some additional overhead, S3 does not throttle requests specifically because they are being encrypted using KMS. S3 will throttle requests if they exceed the S3 request rate limit for the bucket, but this is a separate issue from KMS throttling.\nOption B, You need to also enable versioning to ensure optimal performance, is not correct. Enabling versioning on an S3 bucket does not have any direct impact on the performance of the bucket, whether encryption is enabled or not.\nOption D, You need to also enable CORS to ensure optimal performance, is also not correct. CORS (Cross-Origin Resource Sharing) is a mechanism that allows web pages to request resources from a different domain, and it's used to enable web applications that are hosted on one domain to access resources hosted on another domain. It's not directly related to the performance of S3, and enabling CORS on an S3 bucket will not affect the performance of the bucket in any way.\n\n"
}, {
  "id" : 111,
  "question" : "Your company is planning to use the Simple Storage Service to host objects that users will access.\nYou need to store the photos based on the date and you also need the folder structure to be concise.\nThere is a speculation that there would be roughly 6000 GET requests per second.\nWhich of the following is the right way to use object keys for optimal performance?\n",
  "answers" : [ {
    "id" : "952b23a2d4284f95bd7833a4a40dd536",
    "option" : "exampleawsbucket/photo1/2019-14-03-15-00-00/photo1.jpg",
    "isCorrect" : "false"
  }, {
    "id" : "d048ef49a2114b07b2459873fcc48ba6",
    "option" : "exampleawsbucket/sample/2019-14-03-15-00-00photo1.jpg",
    "isCorrect" : "false"
  }, {
    "id" : "48caf0de88b5427da9feb849d293cc89",
    "option" : "exampleawsbucket/2019-14-03-15-00-00/photo1.jpg",
    "isCorrect" : "true"
  }, {
    "id" : "d5e84d51931147d083d69e140a44d0c6",
    "option" : "exampleawsbucket/sample/photo1.jpg.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAccording to the latest AWS documentation, randomizing prefix naming is not required to improve S3 performance.\n\"For example, previously Amazon S3 performance guidelines recommended randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals.\nYou no longer have to randomize prefix naming for performance and can use sequential date-based naming for your prefixes.\"\nOption A is incorrect because there is no need to use a subfolder of photo1.\nOption B is incorrect because there is no need to use a subfolder of sample.\nIt needs subfolders based on the date.\nOption D is incorrect because it does not store the photos based on the date.\nFor more details, please check below AWS Docs: https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html.\n\nThe correct answer is C: exampleawsbucket/2019-14-03-15-00-00/photo1.jpg.\nExplanation: When storing objects in Amazon S3, it is recommended to use a flat structure for the object key names. This helps in better performance, and efficient management of the objects in the bucket. Using a folder structure or hierarchical structure with many levels can cause performance degradation, especially when you have a large number of objects in the bucket.\nOption A: exampleawsbucket/photo1/2019-14-03-15-00-00/photo1.jpg is not the best approach as it uses a hierarchical structure, with multiple levels of folders. This can lead to performance issues when you have a large number of objects in the bucket.\nOption B: exampleawsbucket/sample/2019-14-03-15-00-00photo1.jpg is not recommended as it does not have a clear separation between the date and the name of the photo. This can make it difficult to manage and query the objects in the bucket.\nOption C: exampleawsbucket/2019-14-03-15-00-00/photo1.jpg is the best option as it uses a flat structure, with the date and photo name clearly separated. This makes it easier to manage and query the objects in the bucket.\nOption D: exampleawsbucket/sample/photo1.jpg is not recommended as it does not provide any information about the date of the photo. This can make it difficult to organize and query the objects based on the date.\nRegarding the expected 6000 GET requests per second, S3 can handle much higher traffic than that without any issues. So, there is no need to worry about performance in this scenario.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 113,
  "question" : "You've developed a Lambda function and now want to debug it, after the execution completes.\nYou add the necessary print statements in the code to assist in the debugging process.\nYou go to Cloudwatch logs but there are no logs for the Lambda function.\nWhich of the following could be the underlying issue for this?\n",
  "answers" : [ {
    "id" : "b06d0f2397b348a9a38728525bc12087",
    "option" : "You’ve not enabled versioning for the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "3a777d6d89614406a16bedc501a9cac6",
    "option" : "The IAM role needed for the lambda function to write the logs to Cloudwatch logs does not have the necessary permissions.",
    "isCorrect" : "true"
  }, {
    "id" : "03613e0a72764fa391bd125313d4b1d2",
    "option" : "There is not enough memory assigned to the function.",
    "isCorrect" : "false"
  }, {
    "id" : "5de9ada0827f41b58bc9d890b8db326b",
    "option" : "There is not enough time assigned to the function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nNote.\n\"If your Lambda function code is executing, but you don't see any log data being generated after several minutes, this could mean your execution role for the Lambda function did not grant permissions to write log data to CloudWatch Logs.\nFor information about how to make sure that you have set up the execution role correctly to grant these permissions, see Manage Permissions: Using an IAM Role (Execution Role)\".\nOption A is incorrect since versioning will not help in this case.\nOptions C and D are incorrect since these were the cases if the function would not complete execution, but the question says debugging starts after the execution is completed.\nFor more information on monitoring Lambda functions, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html\n\nWhen you create a Lambda function, you can choose to have it write its logs to CloudWatch Logs. CloudWatch Logs is a managed service that makes it easy to centralize and monitor logs from all your AWS resources, including Lambda functions. You can use CloudWatch Logs to troubleshoot and debug your Lambda function code by reviewing the logs generated during the execution of the function.\nIf you have added print statements in your Lambda function code to assist in the debugging process, but you don't see any logs in CloudWatch Logs after the execution completes, it could be due to one of the following reasons:\nA. You've not enabled versioning for the Lambda function: When you enable versioning for a Lambda function, AWS creates a new immutable version of the function each time you update it. This ensures that the function code and configuration remains unchanged and can be invoked repeatedly. If versioning is not enabled, the function code and configuration may change unexpectedly, leading to issues with logging. However, the lack of versioning is unlikely to be the primary cause of the absence of logs in CloudWatch Logs.\nB. The IAM role needed for the lambda function to write the logs to Cloudwatch logs does not have the necessary permissions: When a Lambda function writes logs to CloudWatch Logs, it needs an AWS Identity and Access Management (IAM) role with the necessary permissions. If the role associated with the Lambda function does not have the necessary permissions to write logs to CloudWatch Logs, no logs will be generated. Therefore, this is a possible cause of the issue.\nC. There is not enough memory assigned to the function: When a Lambda function is executed, it is assigned a certain amount of memory, which determines the amount of CPU and network resources that are allocated to it. If the function runs out of memory during execution, it may terminate prematurely and fail to generate logs. However, the lack of memory is unlikely to be the primary cause of the absence of logs in CloudWatch Logs.\nD. There is not enough time assigned to the function: When a Lambda function is executed, it is also assigned a maximum execution time, which determines how long the function can run before it is terminated. If the function exceeds its maximum execution time, it may terminate prematurely and fail to generate logs. However, the lack of time is unlikely to be the primary cause of the absence of logs in CloudWatch Logs.\nTherefore, the most likely cause of the issue is that the IAM role associated with the Lambda function does not have the necessary permissions to write logs to CloudWatch Logs. You can resolve this by adding the necessary permissions to the role or by creating a new role with the necessary permissions and associating it with the Lambda function.\n\n"
}, {
  "id" : 114,
  "question" : "Your company has asked you to maintain an application using Elastic Beanstalk.\nThey have mentioned that when updates are made to the application, the infrastructure maintains its full capacity.\nWhich of the following deployment methods should you use for this requirement? Please select 2 correct options.\n",
  "answers" : [ {
    "id" : "758abc20f39c409a91c9b26242eb18ec",
    "option" : "All at once",
    "isCorrect" : "false"
  }, {
    "id" : "03dd4d1cd3e6459a88b0e9976dbe84e0",
    "option" : "Rolling",
    "isCorrect" : "false"
  }, {
    "id" : "0b5c90192b8942f88eae59ed8ef856d9",
    "option" : "Immutable",
    "isCorrect" : "true"
  }, {
    "id" : "cc25b394fb7a49a2a9f36771de197c7d",
    "option" : "Rolling with additional batch.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nSince the only requirement is that the infrastructure should maintain its full capacity, So answers should be both C &amp;\nD.You can now use an immutable deployment policy when updating your application or environment configuration on Elastic Beanstalk.\nThis policy is well suited for updates in production environments where you want to minimize downtime and reduce the risk from failed deployments.\nIt ensures that the impact of a failed deployment is limited to a single instance and allows your application to serve traffic at full capacity throughout the update.\nYou can now also use a rolling with additional batch policy when updating your application.\nThis policy ensures that the impact of a failed deployment is limited to a single batch of instances and allows your application to serve traffic at full capacity throughout the update.\nOption A is incorrect because All at once is used to deploy the new version to all instances simultaneously.\nAll instances in your environment are out of service for a short time while the deployment occurs.\nOption B is incorrect because Rolling is used to deploy the new version in batches.\nEach batch is taken out of service during the deployment phase, reducing your environment's capacity by the number of instances in a batch.\nPlease refer to the following links for more information.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html#environments-cfg-rollingdeployments-method\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environmentmgmt-updates-imm\n\nThe correct deployment methods for maintaining full capacity when updates are made to an Elastic Beanstalk application are Rolling and Immutable.\nRolling Deployment: Rolling deployment is a deployment method in which a new version of the application is deployed to a subset of instances in the environment, and then gradually rolled out to the remaining instances. This method ensures that the environment maintains full capacity during the deployment.\nFor example, if an environment has 10 instances, the rolling deployment will first deploy the new version to 2 instances, then 2 more, and so on until all 10 instances are running the new version. This way, the environment always has at least 8 instances running the old version while 2 instances are being updated.\nImmutable Deployment: Immutable deployment is a deployment method in which a new version of the application is deployed to new instances in the environment, and then the old instances are terminated. This method also ensures that the environment maintains full capacity during the deployment.\nFor example, if an environment has 10 instances, the immutable deployment will launch 10 new instances with the new version, and then terminate the 10 old instances running the old version. This way, the environment always has 10 instances running the new version.\nOption A, All at Once deployment method deploys the new version to all instances at once, which can result in downtime and reduced capacity during the deployment.\nOption D, Rolling with additional batch, is a variation of the rolling deployment method where an additional batch of instances is launched before the old instances are terminated. This can be useful in certain scenarios but is not necessary for the requirement mentioned in the question.\nTherefore, the correct options for this requirement are Rolling and Immutable deployment methods.\n\n"
}, {
  "id" : 115,
  "question" : "Your company has asked you to maintain an application using Elastic Beanstalk.\nAt times, you normally hit the application version limit when deploying new versions of the application.\nWhich of the following is the most effective way to manage this issue?\n",
  "answers" : [ {
    "id" : "e22e1a3cd7654317975916bc797ffeff",
    "option" : "Create multiple environments and deploy the different versions to different environments.",
    "isCorrect" : "false"
  }, {
    "id" : "d45ef9dd6cf541fa902dcd13c3a23fe4",
    "option" : "Create an Application Version Lifecycle policy.",
    "isCorrect" : "true"
  }, {
    "id" : "86861046b05e4c3d97135879b0ad1adc",
    "option" : "Create multiple applications and deploy the different versions to different applications.",
    "isCorrect" : "false"
  }, {
    "id" : "fe99e8f7421c401dadd05f57ba28d22e",
    "option" : "Delete the application versions manually.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nEach time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application version.\nIf you don't delete versions that you no longer use, you will eventually reach the application version limit and be unable to create new versions of that application.\nYou can avoid hitting the limit by applying an application version lifecycle policy to your applications.\nA lifecycle policy tells Elastic Beanstalk to delete application versions that are old or delete application versions when the total number of versions for an application exceeds a specified number.\nOptions A and C are invalid because they are not the right approaches when managing the deployment of application versions.\nOption D, even though possible, is not the most effective way.\nFor more information on the application lifecycle, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-lifecycle.html\n\nThe correct answer is A. Create multiple environments and deploy the different versions to different environments.\nExplanation: Elastic Beanstalk is a service that simplifies the deployment and management of applications on AWS. Elastic Beanstalk allows developers to upload their code and run the application in the cloud. Each time you upload your code, it creates a new version of your application.\nHowever, Elastic Beanstalk has a limit on the number of application versions that can be deployed at the same time. If you reach this limit, you will not be able to deploy new versions until you delete some of the old versions. This can be a problem if you need to keep older versions of your application for testing or rollback purposes.\nTo manage this issue, you can create multiple environments and deploy different versions of your application to different environments. Each environment is a separate instance of your application, with its own URL, configuration, and resources. You can have multiple environments running simultaneously, each with its own version of the application.\nFor example, you could have a production environment, a staging environment, and a development environment. Each environment could have a different version of your application, and you could deploy new versions to each environment independently. This allows you to test new versions of your application in a controlled environment before deploying them to production.\nCreating an Application Version Lifecycle policy (option B) is not the most effective way to manage this issue, as it only automates the deletion of old versions. This may help to prevent hitting the version limit, but it does not provide a solution for keeping multiple versions of your application for testing or rollback purposes.\nCreating multiple applications (option C) is not necessary, as Elastic Beanstalk allows you to create multiple environments within a single application. Creating multiple applications would add unnecessary complexity to your deployment process.\nDeleting application versions manually (option D) is not a sustainable solution, as it is time-consuming and prone to human error. It is also not a good practice to delete application versions without proper testing and approval.\n\n"
}, {
  "id" : 116,
  "question" : "How many read request units of the DynamoDB table are required for an item up to 8 KB using one strongly consistent read request?\n",
  "answers" : [ {
    "id" : "95c9b15bfee048de89e14d38098b63f8",
    "option" : "1",
    "isCorrect" : "false"
  }, {
    "id" : "9902e0d909114a7dae628fc116db8aee",
    "option" : "2",
    "isCorrect" : "true"
  }, {
    "id" : "10bf69ec3687445fa5a1b97f962c25f7",
    "option" : "4",
    "isCorrect" : "false"
  }, {
    "id" : "79d7c6862d644f14bdc720f2e282800b",
    "option" : "8",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nOne read request unit represents one strongly consistent read request, or two eventually consistent read requests for an item up to 4 KB in size.\nAs in the question, the item is up to 8KB, the DynamoDB table needs two read request units.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nIn Amazon DynamoDB, read operations are measured in terms of \"Read Capacity Units\" (RCUs). Each RCU represents the capacity to read one item up to 4 KB in size per second. DynamoDB provides two types of read operations: strongly consistent read and eventually consistent read.\nA strongly consistent read returns the most recent version of the data, reflecting all writes that received a successful response prior to the read. A strongly consistent read requires more resources than an eventually consistent read.\nFor an item up to 8 KB using one strongly consistent read request, we need to calculate the number of RCUs required.\nSince the item is up to 8 KB in size, it will require 2 RCUs for each read operation. This is because each RCU can read up to 4 KB, and the item is 8 KB.\nTherefore, the number of read request units required for one strongly consistent read request of an item up to 8 KB is 2.\nHence, the correct answer is (A) 1.\n\n"
}, {
  "id" : 117,
  "question" : "Your team is developing a solution that will make use of DynamoDB tables.\nDue to the nature of the application, the data is needed across a couple of regions across the world.\nWhich of the following would help reduce the latency of requests to DynamoDB from different regions?\n",
  "answers" : [ {
    "id" : "8ba6820f97cb4bd4b4db7804ec974ccb",
    "option" : "Enable Multi-AZ for the DynamoDB table.",
    "isCorrect" : "false"
  }, {
    "id" : "0be6218cfb204303bfaf75ab365fc7ce",
    "option" : "Enable global tables for DynamoD.",
    "isCorrect" : "true"
  }, {
    "id" : "04dcfd08f1934766861ded4311be0caf",
    "option" : "Enable Indexes for the table.",
    "isCorrect" : "false"
  }, {
    "id" : "9d1f1e5c677e400ea9c0688c8148da7c",
    "option" : "Increase the read and write throughput for the table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database without building and maintaining your own replication solution.\nWhen you create a global table, you specify the AWS regions where you want the table to be available.\nDynamoDB performs all of the necessary tasks to create identical tables in these regions and propagate ongoing data changes to all of them.\nOption A is incorrect because there is no such option of Multi-AZ.\nAnd this is enabled by default as data is replicated across at least 2 AZs.\nOption C is incorrect because there is no such feature.\nOption D is incorrect because increasing read and write capacities does not help to reduce latency in different regions.\nFor more information on Global tables, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\n\nTo reduce the latency of requests to DynamoDB from different regions, the best option would be to enable global tables for DynamoDB.\nGlobal tables in DynamoDB enables replication of table data across multiple AWS regions, which means that data can be read and written to the table in any of the regions where the table is replicated. This helps to reduce the latency of requests and improves the overall performance of the application.\nEnabling Multi-AZ for the DynamoDB table would only provide high availability within a single region and would not help to reduce the latency for requests from different regions.\nEnabling indexes for the table could improve the performance of specific queries but would not necessarily improve the overall latency of requests from different regions.\nIncreasing the read and write throughput for the table would only improve the capacity of the table to handle requests, but would not necessarily reduce the latency of requests from different regions.\nTherefore, option B, enabling global tables for DynamoDB, is the most appropriate choice to reduce latency for requests to DynamoDB from different regions.\n\n"
}, {
  "id" : 118,
  "question" : "Your team is developing a solution that will make use of DynamoDB tables.\nCurrently, the application is designed to perform scans on the entire table.\nWhich of the following can be done to re-design and improve the application's performance when it interacts with the DynamoDB table? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "1184601ef4a340d6824a5accddb1e9bb",
    "option" : "Consider using parallel scans.",
    "isCorrect" : "true"
  }, {
    "id" : "73939b7bdc864832997289b03cf17f33",
    "option" : "Consider using large tables.",
    "isCorrect" : "false"
  }, {
    "id" : "c4da801ea7564dd78dedfef469eead4c",
    "option" : "Consider using string partition keys.",
    "isCorrect" : "false"
  }, {
    "id" : "5c150992d1bf45b29d59215ebb88a739",
    "option" : "Consider using queries.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThe AWS Documentation mentions the following.\nMany applications can benefit from using parallel Scan operations rather than sequential scans.\nFor example, an application that processes a large table of historical data can perform a parallel scan much faster than a sequential one.\nMultiple worker threads in a background \"sweeper\" process could scan a table at a low priority without affecting production traffic.\nIn each of these examples, a parallel Scan is used in such a way that it does not starve other applications of provisioned throughput resources.\nIf possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results.\nAlso, as a table or index grows, the Scan operation slows.\nThe Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation.\nFor faster response times, design your tables and indexes so that your applications can use Query instead of Scan.\nOption B is incorrect since having larger tables would make the issue worse.\nOption C is incorrect since this would not help in the issue.\nFor more information on scans and queries, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html\n\nThe correct answers are A and D.\nA) Consider using parallel scans: Scanning an entire DynamoDB table can be time-consuming and expensive, especially for larger tables. Parallel scans can help to reduce the time it takes to scan the entire table by dividing the work into smaller parts and scanning multiple segments of the table at the same time. This can significantly improve the performance of your application when interacting with DynamoDB tables.\nD) Consider using queries: Querying is a more efficient way to retrieve data from DynamoDB compared to scanning an entire table. Queries retrieve only the items that match a specified partition key value or a combination of partition key value and sort key value. In contrast, scanning retrieves all items in a table, which can be wasteful if you only need a subset of the data. Therefore, using queries instead of scans can help to improve the performance of your application when interacting with DynamoDB tables.\nB) Consider using large tables: Using large tables does not improve performance. Instead, it can make it harder to manage and maintain your data.\nC) Consider using string partition keys: Using string partition keys does not necessarily improve performance. Instead, it can affect the data distribution across partitions and may lead to uneven data distribution and hot partitions. Therefore, choosing the right partition key for your data is more important than using a specific data type such as strings.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 120,
  "question" : "A company is planning on using Amazon Kinesis firehose to stream data into an S3 bucket.\nThey need the data to be transformed first before it can be sent to the S3 bucket.\nWhich of the following would be used for the transformation process?\n",
  "answers" : [ {
    "id" : "819d6ba4d0ed468eafe98d456abd438f",
    "option" : "AWS SQS",
    "isCorrect" : "false"
  }, {
    "id" : "7429c6cef7d440ac9dffa4580827c297",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "c5a94879f53f4ea6b2f335d0bd97bead",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "5539f17bfd5c46cbaff451fc676c8953",
    "option" : "AWS API Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nKinesis Data Firehose can invoke your Lambda function to transform incoming source data and deliver the transformed data to destinations.\nYou can enable the Kinesis Data Firehose data transformation when you create your delivery stream.\nOption A is incorrect because SQS is a queue service and can not transform the data.\nOption C is incorrect because although with EC2 instances, you can achieve the desired result, you need to manage and provision the server.\nLambda is a better option.\nOption D is incorrect because an API gateway is used to make API calls and not transform it.\nFor more information on Kinesis Firehose, please refer to the below URL-\nhttps://docs.aws.amazon.com/firehose/latest/dev/data-transformation.html\n\nThe correct answer is B. AWS Lambda.\nAWS Kinesis Firehose is a fully managed service that can be used to capture, transform, and load streaming data in real-time into Amazon S3 or Amazon Redshift. Firehose can automatically convert the incoming data to a desired format before loading it into S3 or Redshift. This conversion is known as data transformation.\nAWS Lambda is a serverless compute service that enables you to run code without provisioning or managing servers. You can use Lambda to run your custom code to transform the incoming data stream into a desired format before it is loaded into S3 using Kinesis Firehose.\nLambda can be triggered by Kinesis Firehose when new data is received. The Lambda function can then transform the data using custom code, such as JSON parsing or data normalization, before sending it to S3. Lambda can scale automatically to handle any amount of incoming data and can be used to apply custom transformations on data streams in real-time.\nSQS is a message queuing service that allows you to decouple and scale microservices, distributed systems, and serverless applications. It is not designed for data transformation and is not a good fit for this scenario.\nEC2 is a virtual server that can be used to run applications in the cloud. However, it requires manual provisioning and management of servers, which goes against the serverless approach of Kinesis Firehose and Lambda.\nAPI Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It is not designed for data transformation and is not a good fit for this scenario.\nIn conclusion, AWS Lambda is the best choice for transforming the data before it is loaded into S3 using Kinesis Firehose.\n\n"
}, {
  "id" : 121,
  "question" : "Your company is hosting a static website in S3\nThe code has recently been changed wherein Javascript calls are being made to the web pages in the same bucket via the Fully Qualified Domain Name.\nBut the browser is blocking the requests.\nWhat should be done to alleviate the issue?\n",
  "answers" : [ {
    "id" : "e61387b06f074ee7a6dfd0b88de48b08",
    "option" : "Enable CORS on the bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "5637dae3ec3e47c9b3129f3e6642a8f3",
    "option" : "Enable versioning on the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "e8c80f889ad2467193d995415b0928d6",
    "option" : "Enable CRR on the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "83ace935d82a4d289d98a0315421c3ec",
    "option" : "Enable encryption the bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nOption B is incorrect because this is used to prevent accidental deletion of objects in S3.\nOption C is incorrect because this is used for Cross-region replication of objects.\nOption D is incorrect because this is used to encrypt objects at rest.\nThe AWS Documentation mentions the following.\nCross-Origin Resource Sharing: Use-case Scenarios.\nThe following are example scenarios for using CORS:\nScenario 1: Suppose you are hosting a website in an Amazon S3 bucket named website described in Hosting a Static Website on Amazon S3\nYour users load the website endpoint http://website.s3-website-us-east-1.amazonaws.com.\nNow you want to use JavaScript on the webpages stored in this bucket to make authenticated GET and PUT requests against the same bucket by using the Amazon S3 API endpoint for the bucket website.s3.amazonaws.com.\nA browser would normally block JavaScript from allowing those requests, but with CORS you can configure your bucket to enable cross-origin requests from website.s3-website-us-east-1.amazonaws.com explicitly.\nScenario 2: Suppose that you want to host a web font from your S3 bucket.\nAgain, browsers require a CORS check (also called a preflight check) for loading web fonts.\nYou would configure the bucket that hosts the web font to allow any origin to make these requests.\nFor more information on Cross-Origin access, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\n\nThe issue of the browser blocking requests made to the same bucket via the Fully Qualified Domain Name in the static website hosted on S3 can be resolved by enabling Cross-Origin Resource Sharing (CORS) on the bucket. Therefore, the correct answer is option A.\nExplanation:\nWhen a browser requests a resource from a different domain or origin, CORS comes into play. CORS allows the server to include additional headers in its response to the browser, indicating which origins are allowed to access the resources on the server. By default, web browsers block requests to different origins for security reasons.\nIn this scenario, the web pages and the Javascript calls are both in the same bucket, but the browser is still blocking the requests. This is because the browser considers the Fully Qualified Domain Name as a different origin, and therefore blocks the requests.\nEnabling CORS on the bucket allows the server to specify which origins are allowed to access the resources. Once CORS is enabled, the server will include the necessary CORS headers in its response to the browser, allowing the browser to access the resources in the same bucket via the Fully Qualified Domain Name.\nOption B, enabling versioning on the bucket, is not related to the issue described in the scenario. Versioning on a bucket allows you to keep multiple versions of an object in the bucket, but it will not resolve the issue of the browser blocking requests.\nOption C, enabling Cross-Region Replication (CRR) on the bucket, is also not related to the issue described in the scenario. CRR is used to replicate objects between S3 buckets in different regions for disaster recovery and other purposes.\nOption D, enabling encryption on the bucket, will not resolve the issue of the browser blocking requests. Encryption on a bucket provides an additional layer of security by encrypting the objects stored in the bucket, but it will not affect the ability of the browser to access the resources in the bucket.\n\n"
}, {
  "id" : 122,
  "question" : "A company is storing sensitive data in their S3 bucket.\nThe company policy states that all objects in the S3 bucket need to be encrypted at rest.\nWhich of the following helps ensure this policy is met?\n",
  "answers" : [ {
    "id" : "46939061e3a54ef69d79227b13f20f1d",
    "option" : "Deny permission to upload an object if the header does not include x-amz-server-side-encryption.",
    "isCorrect" : "true"
  }, {
    "id" : "42a2d6c137524a4eb40973ae1327c54d",
    "option" : "Deny permission to upload an object if the header includes x-amz-server-side-encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "e1d95fc0364847b0ac2814bccf8e0164",
    "option" : "Deny permission to upload an object if the header does not include x-allow-encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "a061b366ea9e4ee2b4a678346e238d59",
    "option" : "Deny permission to upload an object if the header includes x-allow-encryption.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - A.\nThis is also given in the AWS Documentation.\nOption B is incorrect as it denies the upload if the header includes x-amz-server-side-encryption.\nOption C and D are incorrect because there is no such header as x-allow-encryption.\nFor more information on Server-Side Encryption, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\n\n\nThe correct answer is A. Deny permission to upload an object if the header does not include x-amz-server-side-encryption.\nExplanation: Amazon S3 provides server-side encryption (SSE) to encrypt data at rest. SSE encrypts the data as it is being uploaded to S3, and decrypts the data when it is being downloaded from S3. Amazon S3 SSE provides three options for managing keys: SSE-S3, SSE-KMS, and SSE-C.\nTo enforce a policy that requires all objects in the S3 bucket to be encrypted at rest, you can use a Bucket Policy or an IAM Policy that denies permission to upload objects that are not encrypted at rest. The correct way to do this is by using the \"x-amz-server-side-encryption\" header.\nIf you want to enforce that all objects in a bucket are encrypted at rest using SSE-S3, you can use the following Bucket Policy:\njson Copy code {    \"Version\":\"2012-10-17\",    \"Statement\":[       {          \"Sid\":\"RequireServerSideEncryption\",          \"Effect\":\"Deny\",          \"Principal\":\"*\",          \"Action\":\"s3:PutObject\",          \"Resource\":\"arn:aws:s3:::examplebucket/*\",          \"Condition\":{             \"StringNotEquals\":{                \"s3:x-amz-server-side-encryption\":\"AES256\"             }          }       }    ] }\nThis policy denies permission to upload an object if the header does not include x-amz-server-side-encryption with the value AES256, which means the object must be encrypted at rest using SSE-S3.\nOption B is incorrect because it denies permission to upload an object if the header includes x-amz-server-side-encryption. This would prevent users from uploading encrypted objects, which is not what the policy requires.\nOption C and D are incorrect because they use a custom header \"x-allow-encryption\", which is not a valid S3 header for SSE.\n\n"
}, {
  "id" : 123,
  "question" : "As a developer, you have enabled server logging on an S3 bucket.\nYou have a simple static web page with CSS pages uploaded to the bucket which is 1 MB in total size.\nAfter a duration of 2 weeks, you come back and see that the size of the bucket has increased to 50MB.\nWhich of the following could be a reason for this?\n",
  "answers" : [ {
    "id" : "7a393e8db84c419fb7db0e3e2c540c93",
    "option" : "You have enabled CRR on the bucket as well. That is why the space is being consumed.",
    "isCorrect" : "false"
  }, {
    "id" : "2b23d127ee8a4b4d94995ad8114aa9fd",
    "option" : "You have enabled Encryption on the bucket as well. That is why the space is being consumed.",
    "isCorrect" : "false"
  }, {
    "id" : "fc7c5d6d6cf54bbfa4607045a9c5f01e",
    "option" : "Server access logs are configured to be delivered to the same bucket as the source bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "433d13cc6a6c44dd8894dc03f21402fe",
    "option" : "Monitoring has been enabled for the bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAn S3 bucket with server access logging enabled can accumulate many server log objects over time.\nYour application might need these access logs for a specific period after creation, and after that, you might want to delete them.\nYou can use the Amazon S3 lifecycle configuration to set rules so that Amazon S3 automatically queues these objects for deletion at the end of their life.\nThe correct answer is.\nC.\nThis is normal behavior since the logs are being delivered to the same bucket.\nOption A is incorrect because CRR (Cross-region replication) does not consume space in the S3 bucket.\nOption B is incorrect because encryption does not consume space.\nOption D is incorrect because monitoring and logging are two different things.\nMonitoring deals with metrics and does not add data to the S3 bucket.\nFor more information on deleting logs files, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/deleting-log-files-lifecycle.html\n\nOption C - \"Server access logs are configured to be delivered to the same bucket as the source bucket\" could be a reason for the increase in the bucket size.\nWhen server logging is enabled on an S3 bucket, it starts recording access logs for all requests made to the bucket. These logs contain information such as the requester's IP address, the time of the request, and the object requested.\nBy default, these logs are stored in a separate bucket created by S3, but you can configure S3 to deliver the logs to a specific bucket. If you have configured S3 to deliver the logs to the same bucket as the source bucket, this can result in an increase in the bucket size.\nIn this scenario, since the developer has enabled server logging on the bucket, S3 is likely recording access logs for all requests made to the bucket. If the developer has configured S3 to deliver these logs to the same bucket as the source bucket, the logs will be stored in the bucket and increase its size over time.\nOption A - \"You have enabled CRR on the bucket as well\" does not make sense because Cross-Region Replication (CRR) is used to replicate objects across different S3 buckets in different regions, and it does not impact the bucket size.\nOption B - \"You have enabled Encryption on the bucket as well\" could increase the size of the bucket, but it would not cause such a significant increase in just two weeks. Encryption adds some additional metadata to the object, which can slightly increase the object's size, but it is not significant enough to increase the bucket size by 50MB in just two weeks.\nOption D - \"Monitoring has been enabled for the bucket\" is also unlikely to be the reason for the increase in bucket size. Monitoring enables you to monitor the metrics of the bucket, such as the number of requests, data transferred, etc. It does not store any data in the bucket, so it cannot increase the bucket size.\n\n"
}, {
  "id" : 124,
  "question" : "Your application is developed to pick up metrics from several servers and push them off to Cloudwatch.\nAt times, the application gets client 429 errors.\nWhich of the following can be done from the programming side to resolve such errors?\n",
  "answers" : [ {
    "id" : "ab16cb6900f14d7db2145aacf79f4313",
    "option" : "Use the AWS CLI instead of the SDK to push the metrics.",
    "isCorrect" : "false"
  }, {
    "id" : "191b7031618449c1af17b0ba573918d9",
    "option" : "Ensure that all metrics have a timestamp before sending them across.",
    "isCorrect" : "false"
  }, {
    "id" : "032eb3ec5c3247168414ee4b6d30f647",
    "option" : "Use exponential backoff in your requests.",
    "isCorrect" : "true"
  }, {
    "id" : "c282d41c051a47fda0a59a0a021e39dc",
    "option" : "Enable encryption for the requests.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe main reason for such errors is that throttling occurs when many requests are sent via API calls.\nThe best way to mitigate this is to stagger the rate at which you make the API calls.\nThis is also given in the AWS Documentation.\nIn addition to simple retries, each AWS SDK implements an exponential backoff algorithm for better flow control.\nThe idea behind exponential backoff is to use progressively longer waits between retries for consecutive error responses.\nYou should implement a maximum delay interval, as well as a maximum number of retries.\nThe maximum delay interval and maximum number of retries are not necessarily fixed values and should be set based on the operation being performed and other local factors, such as network latency.\nOption A is invalid because this accounts for the same thing.\nIt's basically the number of requests that is the issue.\nOption B is invalid because any way you have to add the timestamps when sending the requests.\nOption D is invalid because this would not help in the issue.\nFor more information on API retries, please refer to the below URL-\nhttps://docs.aws.amazon.com/general/latest/gr/api-retries.html\n\nWhen an AWS service returns HTTP 429 \"Too Many Requests\" status code, it indicates that the client has sent too many requests in a given amount of time. This error usually occurs when the client-side code is sending requests too quickly, and the server is unable to process them all.\nTo resolve this error from the programming side, you can take the following steps:\nC. Use exponential backoff in your requests. One of the most effective ways to handle 429 errors is to use exponential backoff in your requests. Exponential backoff is a strategy where the client retries the request with an increasing delay between each retry attempt. The idea is to reduce the number of requests sent to the server, and allow the server to catch up with the workload. With each successive retry, the delay between each request attempt doubles.\nA. Use the AWS CLI instead of the SDK to push the metrics. Using the AWS CLI instead of the SDK can help resolve 429 errors. This is because the AWS CLI has built-in rate limiting to prevent clients from overloading the server with too many requests. However, this approach may not be suitable for all scenarios, such as when the application requires real-time data ingestion.\nB. Ensure that all metrics have a timestamp before sending them across. It's important to ensure that all metrics have a timestamp before sending them across to Cloudwatch. Cloudwatch requires a timestamp for each data point to plot the data on a graph correctly. If a timestamp is not provided, the data point is ignored.\nD. Enable encryption for the requests. Enabling encryption for requests can help prevent malicious actors from intercepting and modifying requests. However, it does not directly address 429 errors.\nIn conclusion, the best option to resolve 429 errors is to use exponential backoff in your requests. This will allow the server to catch up with the workload and prevent the client from overwhelming the server with too many requests.\n\n"
}, {
  "id" : 125,
  "question" : "A company currently allows access to their API's to customers via the API gateway.\nCurrently, all clients have a 6-month period to move from using the older API's to newer versions of the API's.\nThe code for the API is hosted in AWS Lambda.\nWhich of the following is the ideal strategy to employ in such a situation?\n",
  "answers" : [ {
    "id" : "8019aedcec524993ac72a2a9a6cc31ec",
    "option" : "Create another AWS Lambda version and give the link to that version to the customers.",
    "isCorrect" : "false"
  }, {
    "id" : "441f04308ae34bbfba19f4fea0e5d4f8",
    "option" : "Create another AWS Lambda ALIAS and give the link to that version to the customers.",
    "isCorrect" : "false"
  }, {
    "id" : "ea245808ade7454a8867ccd864726b43",
    "option" : "Create another stage in the API gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "239234ae89f14777888aafec5140b9b6",
    "option" : "Create a deployment package that would automatically change the link to the new Lambda version.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe best way is to create a separate stage in the API gateway as maybe ‘v2', and then customers could use both API versions.\nThey can still slowly change their usage onto the new version in this duration.\nBelow is the concept of the API stage in the AWS Documentation.\nAPI stage.\n\"A logical reference to a lifecycle state of your API (for example, 'dev', 'prod', 'beta', 'v2')\nAPI stages are identified by API ID and stage name\".\nOptions A and B are incorrect since access needs to be provided via the gateway.\nOption D is incorrect since you need to keep both versions running side by side.\nFor more information on the API gateway, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html\n\nIn this scenario, the company wants to ensure that their customers move to newer versions of the API's within a specified timeframe. The API is hosted on AWS Lambda, and the API gateway is used to manage the access to the API.\nThe ideal strategy to employ in such a situation is to create another stage in the API gateway (Option C).\nCreating another stage in the API gateway allows the company to deploy and test new versions of the API without affecting the current production version. This new stage is typically used for testing purposes, and it allows the company to make changes to the API without affecting the current production environment.\nOnce the new version is tested and approved, the company can then promote the new version to production. This allows the customers to move from the older version of the API to the newer version of the API in a controlled manner, without disrupting their access to the API.\nOption A, creating another Lambda version, is not the ideal strategy in this scenario. Creating a new version of the Lambda function would mean that the company would have to manage multiple versions of the code, which can quickly become unwieldy.\nOption B, creating another Lambda alias, is also not the ideal strategy in this scenario. While aliases are useful for managing different versions of the Lambda function, they are not designed for managing different versions of the API.\nOption D, creating a deployment package that would automatically change the link to the new Lambda version, is not the ideal strategy in this scenario. While this option may work, it is not the most efficient way to manage the API's versions. Creating a new stage in the API gateway is a more efficient and controlled way to manage the API's versions.\n\n"
}, {
  "id" : 126,
  "question" : "You are using Amazon Cognito identity pools to assign authenticated SAML users temporary access for downloading data fromAmazon S3 buckets.\nFor this, you have created multiple rules for each role that gets assigned to users.\nWhich of the following criteria is matched for evaluating these rules?\n",
  "answers" : [ {
    "id" : "3488a04218b9470cadc79c32512ce542",
    "option" : "Rules are evaluated in sequential order &amp; rule with lower value is preferred.",
    "isCorrect" : "false"
  }, {
    "id" : "243f86f2e7e64a019ea7aaaae5173f6a",
    "option" : "Rules are evaluated in sequential order &amp; IAM role for first matching rule is used ,unless a standard attribute is specified to override the order.",
    "isCorrect" : "false"
  }, {
    "id" : "9b8b69fbd63e45d7b9142c98bd9facdc",
    "option" : "Rules are evaluated in sequential order &amp; rule with higher value is preferred.",
    "isCorrect" : "false"
  }, {
    "id" : "b30b33348ff1407aa74b38e08fbffb9e",
    "option" : "Rules are evaluated in sequential order &amp; IAM role for first matching rule is used ,unless a ‘CustomRoleArn” is specified to override the order.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nWhen multiple rules are assigned, rules are evaluated in a sequential order &amp; the IAM role for the first matching rule is used unless a‘CustomRoleArn”attribute is added to modify this sequence.\nOption B is incorrect as a standard attribute does not alter rule evaluation.\nOption A &amp; C are incorrect as for each rule there is no preference value.\nFor more information on Role-Based Access Control for AWS Cognito, refer to the following URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/role-based-access-control.html\n\nAmazon Cognito identity pools provide temporary AWS credentials to users who authenticate with SAML (Security Assertion Markup Language) identity providers. These credentials are used to access AWS resources, such as S3 buckets, for a limited time.\nTo assign roles to authenticated SAML users, you can create rules within your Cognito identity pool. These rules evaluate user attributes provided by the SAML identity provider to determine which role to assign to the user. Each rule has a set of conditions that must be met for it to apply.\nNow, let's look at the criteria that are matched for evaluating these rules:\nA. Rules are evaluated in sequential order & rule with lower value is preferred.\nThis statement means that Cognito evaluates the rules in the order they are defined, from top to bottom. The rule with the lowest order value is evaluated first. If the conditions of this rule are met, the corresponding role is assigned to the user, and the evaluation stops. If the conditions of the first rule are not met, the evaluation continues with the next rule. This continues until a rule is matched or all rules are evaluated.\nB. Rules are evaluated in sequential order & IAM role for first matching rule is used, unless a standard attribute is specified to override the order.\nThis statement is also correct, but it adds an important detail. Cognito evaluates the rules in the order they are defined, from top to bottom. The first rule that matches the user's attributes is used to assign an IAM role. However, if a standard attribute is specified to override the order, then the specified attribute is used instead.\nC. Rules are evaluated in sequential order & rule with higher value is preferred.\nThis statement is incorrect. It contradicts statement A, which correctly states that the rule with the lowest value is preferred.\nD. Rules are evaluated in sequential order & IAM role for first matching rule is used, unless a ‘CustomRoleArn” is specified to override the order.\nThis statement is incorrect. It is similar to statement B, but it specifies a non-existent attribute. There is no \"CustomRoleArn\" attribute that can be used to override the order of rule evaluation.\nIn summary, the correct answer is B: Rules are evaluated in sequential order & IAM role for first matching rule is used, unless a standard attribute is specified to override the order.\n\n"
}, {
  "id" : 127,
  "question" : "A company has a Cloudformation template that is used to create a huge list of resources.\nIt creates a VPC, subnets, EC2 Instances, Autoscaling Groups, Load Balancers etc.\nWhich of the following should be considered when designing such Cloudformation templates?\n",
  "answers" : [ {
    "id" : "c153ec500b404c398849f6d4fde77362",
    "option" : "Ensure to create one entire stack from the template.",
    "isCorrect" : "false"
  }, {
    "id" : "e7522d237bfe454389960308cdfdcd96",
    "option" : "Look towards breaking the templates into smaller manageable templates.",
    "isCorrect" : "true"
  }, {
    "id" : "4a7606c38809491686fa097b118209f8",
    "option" : "Package the templates together and use the cloudformation deploy command.",
    "isCorrect" : "false"
  }, {
    "id" : "989e08ce7a954156bd80b7bb07e20086",
    "option" : "Package the templates together and use the cloudformation package command.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis recommendation is also given in the AWS Documentation.\nAs your infrastructure grows, common patterns can emerge in which you declare the same components in each of your templates.\nYou can separate out these common components and create dedicated templates for them.\nThat way, you can mix and match different templates but use nested stacks to create a single, unified stack.\nNested stacks are stacks that create other stacks.\nTo create nested stacks, use the AWS::CloudFormation::Stack resource in your template to reference other templates.\nOption A is incorrect since this is not the recommended design practice.\nOptions C and D are incorrect because these are used for packaging and deployment and not for the design stages.\nFor more information on best practices for Cloudformation, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html\n\nWhen designing a CloudFormation template that creates a large number of resources, there are a few things to consider:\nA. Ensure to create one entire stack from the template:\nWhile it is possible to create one large CloudFormation stack with all of the resources defined in a single template, this approach can make the stack difficult to manage, troubleshoot, and update. In addition, a large stack can make it harder to understand the overall architecture of the infrastructure. For example, if the stack creation fails, it can be difficult to determine which resource caused the failure. Therefore, this option is not the best approach.\nB. Look towards breaking the templates into smaller manageable templates:\nOne approach to managing a large number of resources is to break the CloudFormation template into smaller, more manageable templates. This approach can help with troubleshooting, understanding the overall architecture of the infrastructure, and updating the resources more efficiently. By breaking the templates into smaller templates, you can also reuse common resources across multiple stacks, which can save time and reduce complexity.\nC. Package the templates together and use the cloudformation deploy command:\nThe cloudformation deploy command packages the CloudFormation templates and any referenced artifacts into an Amazon S3 bucket, creates a new CloudFormation stack, and updates the stack if it already exists. This approach can be useful for deploying multiple CloudFormation stacks at once, such as when you have multiple environments (e.g., dev, staging, prod) that require the same resources. However, this option is not directly related to designing the CloudFormation templates themselves.\nD. Package the templates together and use the cloudformation package command:\nThe cloudformation package command packages the CloudFormation templates and any referenced artifacts into an Amazon S3 bucket. This command is useful when the templates are too large to be deployed directly to CloudFormation, or when the templates reference local files (e.g., shell scripts). This option is also not directly related to designing the CloudFormation templates themselves.\nIn conclusion, the best option for designing a CloudFormation template that creates a large number of resources is to break the template into smaller, more manageable templates.\n\n"
}, {
  "id" : 128,
  "question" : "You have the following YAML file given to you which is required to deploy a Lambda function using serverless deployment. AWSTemplateFormatVersion: '2010-09-09' Transform: AWS::Serverless-2016-10-31 Resources: TestFunction: Type: AWS::Serverless::Function Properties: Handler: index.handler Runtime: nodejs6.10 Environment: Variables: S3_BUCKET: demobucket Which of the following is required to ensure the deployment can take place? Please select 2 correct answers.\n",
  "answers" : [ {
    "id" : "a144057f28a149d18e40e7e6e6d39a08",
    "option" : "Use the cloudformation package command to package the deployment.",
    "isCorrect" : "true"
  }, {
    "id" : "43a9d9adb79648e8b10705ac60b15ef1",
    "option" : "Use the cloudformation package command to deploy the template.",
    "isCorrect" : "false"
  }, {
    "id" : "b2c9b030a3464c54a7e64c5a7416e3be",
    "option" : "Place the function code at the root level of the working directory along with the YAML file.",
    "isCorrect" : "true"
  }, {
    "id" : "adf8a075b8924b918dae1610ed777a0c",
    "option" : "Place the function code in the .eb extensions folder.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe above snippet is used to create a serverless application that is deployed using the serverless deployment language.\nYou need to ensure that the Lambda function is present as part of the deployment package.\nOption B is incorrect since these are not CloudFormation specific templates.\nOption D is incorrect since this is normally used for Elastic Beanstalk deployments.\nFor more information on serverless deployment, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/serverless-deploy-wt.html\n\nTo ensure the deployment can take place for the given YAML file, you need to perform the following steps:\nUse the AWS CLI or AWS SDKs to package the deployment using the CloudFormation package command. This command will create a new CloudFormation template that includes the AWS Lambda function code, and all required dependencies. Use the CloudFormation deploy command to deploy the packaged CloudFormation template to AWS. This command will create a new CloudFormation stack that includes the Lambda function and its associated resources.\nSo, option A is correct as it suggests using the CloudFormation package command to package the deployment.\nOption B is also correct as it suggests using the CloudFormation package command to deploy the template.\nOption C is incorrect because the function code needs to be in a separate directory from the YAML file.\nOption D is also incorrect as the .eb extensions folder is not relevant to AWS Lambda deployment using serverless deployment.\n\n"
}, {
  "id" : 129,
  "question" : "You are developing a common lambda function that will be used for several development environments such as Dev, QA, Staging, etc.\nThe lambda function needs to interact with multiple development environments.\nWhat is the best way to develop the Lambda function and dynamically pass settings to it?\n",
  "answers" : [ {
    "id" : "dea681f98c474acf831b8aa0ee668d40",
    "option" : "Create a Lambda function for each environment so that each function can point to its respective environment.",
    "isCorrect" : "false"
  }, {
    "id" : "51df9fc67c8b484ea21a1068a4449e1d",
    "option" : "Create one Lambda function and use environment variables for each environment to interact.",
    "isCorrect" : "true"
  }, {
    "id" : "c69e87c6f66840ce99f03216c8c4f9d7",
    "option" : "Create one Lambda function and create several versions for each environment.",
    "isCorrect" : "false"
  }, {
    "id" : "74f5871bb375448a8a3f89b700cd9013",
    "option" : "Create one Lambda function and create several ALIAS for each environment.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is mentioned in the AWS Documentation.\nEnvironment variables for Lambda functions enable you to dynamically pass settings to your function code and libraries without making changes to your code.\nEnvironment variables are key-value pairs that you create and modify as part of your function configuration, using either the AWS Lambda Console, the AWS Lambda CLI or the AWS Lambda SDK.\nAWS Lambda then makes these key-value pairs available to your Lambda function code using standard APIs supported by the language, like process.env for Node.js functions.\nOption A is incorrect because there is no need to create a Lambda function for each environment and the question needs a common Lambda function to interact with different environments.\nOption C is incorrect because different versions of Lambda function are not required.\nOption D is incorrect because ALIAS is not the correct method to dynamically pass settings to a Lambda function.\nFor more information on Lambda environment variables, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/env_variables.html\n\nWhen developing a common Lambda function that needs to interact with multiple development environments, it is important to consider how to dynamically pass settings to it. Each development environment might have different configuration settings such as API keys, database endpoints, and credentials.\nOption A suggests creating a Lambda function for each environment, which can quickly become difficult to manage as the number of environments increases. In addition, it would require duplicating code across each function and maintaining each one separately.\nOption B is a better approach as it allows for a single Lambda function to be used across multiple environments. Environment variables can be set for each environment, and the Lambda function can read those variables at runtime to determine the correct configuration settings to use. This approach allows for better code management and reduces the likelihood of errors due to inconsistent configurations across environments.\nOption C suggests creating several versions of the Lambda function for each environment. While this approach could work, it has some drawbacks. First, it would require creating and maintaining multiple versions of the same code, which could lead to duplication and confusion. Second, versioning is intended to support backward compatibility, and it does not address the need for different configuration settings for each environment.\nOption D is the recommended approach for managing Lambda functions across multiple environments. Creating several aliases for each environment allows the same Lambda function code to be used across multiple environments with different configurations. Aliases allow you to publish multiple versions of the same code and assign a different configuration to each alias. This approach ensures that the same code is used across all environments, while allowing for different configurations to be applied as needed.\nIn summary, the best approach for developing a common Lambda function that interacts with multiple development environments is to create one Lambda function and use environment variables or aliases for each environment to dynamically pass configuration settings. Option B and D are valid approaches, but option D using aliases provides better flexibility and management of Lambda functions across multiple environments.\n\n"
}, {
  "id" : 130,
  "question" : "As a developer, you have been told to create an API gateway stage that will directly interact with DynamoDB tables.\nWhich of the following feature of the API Gateway must be used to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "33415944e95f4a4c99c5aac7b6820eb6",
    "option" : "Ensure to create an Integration request.",
    "isCorrect" : "true"
  }, {
    "id" : "eea02bd0245544b393182c3441d1898f",
    "option" : "Ensure to enable CORS.",
    "isCorrect" : "false"
  }, {
    "id" : "9a6de1a8d52d4de08480f9562d1bda36",
    "option" : "Ensure to enable DAX.",
    "isCorrect" : "false"
  }, {
    "id" : "d9d512d9bfa345a9bb0b76afb1163385",
    "option" : "Enable Binary payloads.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is also mentioned in the AWS Documentation.\nFor example, with DynamoDB as the backend, the API developer sets up the integration request to forward the incoming method request to the chosen backend.\nThe setup includes specifications of an appropriate DynamoDB action, required IAM role and policies, and required input data transformation.\nThe backend returns the result to API Gateway as an integration response.\nTo route the integration response to an appropriate method response (of a given HTTP status code) to the client, you can configure the integration response to map required response parameters from integration to method.\nOption B is incorrect since this is only required for cross-domain requests.\nOption C is incorrect since this is only required for low latency to DynamoDB tables.\nOption D is incorrect since this is only required if the request is not text-based.\nFor more information on the developer experience for the API gateway, please refer to the below URLs-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html#api-gateway-overview-developer-experience\nhttps://aws.amazon.com/blogs/compute/using-amazon-api-gateway-as-a-proxy-for-dynamodb/\n\nThe correct answer is A. Ensure to create an Integration request.\nAPI Gateway is a fully managed service provided by AWS that makes it easy to create, deploy, and manage APIs. It enables developers to create RESTful APIs and WebSocket APIs that can be integrated with other AWS services or non-AWS services.\nTo interact with DynamoDB tables, we need to create an integration request in the API Gateway. Integration request is a feature that allows us to integrate our API with a backend service such as AWS Lambda, Amazon EC2, or DynamoDB.\nTo create an integration request, we need to follow the steps below:\nGo to the API Gateway console. Click on the \"Create Resource\" button to create a new resource. Click on the \"Create Method\" button and select the HTTP method (GET, POST, PUT, DELETE, etc.) that you want to use. Select \"Use Lambda Proxy Integration\" or \"HTTP Proxy Integration\". Enter the endpoint URL for DynamoDB and configure the integration request settings.\nOnce the integration request is set up, we can deploy the API Gateway stage and start using it to interact with DynamoDB tables.\nOption B, Enable CORS, is used to allow cross-origin resource sharing, which is needed when an API is accessed from a domain different from the one that served the API. It is not related to interacting with DynamoDB tables.\nOption C, Enable DAX, is used to enable DynamoDB Accelerator (DAX), a fully managed, in-memory cache for DynamoDB. It is used to improve the performance of DynamoDB by reducing the response time and increasing throughput. However, it is not related to interacting with DynamoDB tables through API Gateway.\nOption D, Enable Binary Payloads, is used to enable binary data in the request and response payloads of an API Gateway API. It is not related to interacting with DynamoDB tables.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 132,
  "question" : "You just developed code in AWS Lambda that makes use of recursive functions.\nAfter several invocations, you are beginning to see throttling errors in the metrics.\nWhich of the following should be done to resolve this issue?\n",
  "answers" : [ {
    "id" : "d75dcf5948e04a479c02a0c03f6a00e9",
    "option" : "Place the recursive function in a separate package.",
    "isCorrect" : "false"
  }, {
    "id" : "303b28b6e96847aaa6df527a874221c9",
    "option" : "Use versioning for the recursive function.",
    "isCorrect" : "false"
  }, {
    "id" : "e9cd6fe8f959468491ee39b5e8d07cff",
    "option" : "Avoid using recursive code altogether.",
    "isCorrect" : "true"
  }, {
    "id" : "70a1b2f518794dc79e6a4f980a4f02be",
    "option" : "Use the API gateway to call the recursive code.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is also clearly mentioned in the AWS Documentation.\nAvoid using recursive code in your Lambda function, wherein the function automatically calls itself until some arbitrary criteria are met.\nThis could lead to an unintended volume of function invocations and escalated costs.\nIf you accidentally do so, set the function concurrent execution limit to 0 immediately to throttle all invocations to the function while you update the code.\nOption A is incorrect because placing the recursive function in a separate package still causes the throttling errors.\nOption B is incorrect because using versioning for a recursive function means storing the code in other versions.\nThe recursive function still causes issues.\nOption D is incorrect because adding an API gateway does not remove the recursive function and the issue still exists.\nFor more information on the best practices for AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\n\nWhen you are using AWS Lambda to execute your code, you can run into throttling errors if the function is invoked too frequently. This can happen when you use recursive functions that are invoked multiple times, leading to a large number of concurrent requests. In this scenario, you have to take necessary steps to avoid throttling errors.\nThe best approach to resolve the issue is to use a combination of options B and C.\nB. Use versioning for the recursive function: Versioning allows you to create and manage multiple versions of a Lambda function. With versioning, you can deploy a new version of your function without affecting the existing version, which can help you avoid conflicts or breaking changes. This allows you to manage the recursion limit in each version separately. By setting a recursion limit on the function version, you can limit the number of times the function can invoke itself before it stops. This helps to prevent the function from exceeding the maximum number of concurrent requests and triggering a throttling error.\nC. Avoid using recursive code altogether: If you find that your recursive function is causing a high number of concurrent requests, you can consider avoiding recursive code altogether. You can use loops instead of recursion, which can be more efficient in some cases. Loops can allow you to perform the same operation multiple times without creating a new stack frame, as happens with recursion.\nA. Place the recursive function in a separate package: Placing the recursive function in a separate package may not help in avoiding throttling errors. The issue is not with the function being in the same package as other functions, but with the function itself being invoked too frequently.\nD. Use the API gateway to call the recursive code: The API Gateway is used to manage and control access to web services. It is not a solution to avoid the throttling issue. The issue arises due to the high number of requests generated by the recursive function, and using the API Gateway to call the function does not solve this problem.\nTherefore, option B and C are the best approaches to resolve the throttling error in the case of recursive functions in AWS Lambda.\n\n"
}, {
  "id" : 133,
  "question" : "An application is making a request to AWS STS for temporary access credentials.\nBelow is the response being received. Which of the following is TRUE with regards to the above response?\n\n",
  "answers" : [ {
    "id" : "67135ca8faa842e48ad75e9a48dbaa70",
    "option" : "TheSecretAccessKey can be used like Access keys to make request to resources.",
    "isCorrect" : "false"
  }, {
    "id" : "8ef156923b9c4547a1a5353a0907a06d",
    "option" : "The application will assume the role of arn:aws:sts::123456789012:assumed-role/demo/John",
    "isCorrect" : "true"
  }, {
    "id" : "625236609e1d4ffd83640985fb042d65",
    "option" : "The session token will be valid for the lifetime of the application.",
    "isCorrect" : "false"
  }, {
    "id" : "02f53043cb6344bfaf698270b3c23757",
    "option" : "The Request ID can be used to make requests to access other AWS resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nSome of the aspects that get incorporated in the call to STS are.\nThe Amazon Resource Name (ARN) of the role that the app should assume.\nThe duration, which specifies the duration of the temporary security credentials.\nA role session name, which is a string value that you can use to identify the session.\nThis value can be captured and logged by CloudTrail to help you distinguish between your role users during an audit.\nOptions A and D are incorrect because you need the session token to make requests to access other AWS resources.\nOption C is incorrect because these tokens are short-lived.\nFor more information on temporary access credentials, please refer to the below URL-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html\n"
}, {
  "id" : 134,
  "question" : "You have an application that is hosted on an EC2 Instance.\nThis application is part of a custom domain www.demo.com.\nThe application has been changed to make calls to the API gateway.\nBut the browser is not rendering the responses and Javascript errors are being seen in the developer console.\nWhat must be done to ensure that this issue can be resolved?\n",
  "answers" : [ {
    "id" : "d0cdeae08959462c9b0dde2a1033ea96",
    "option" : "Make the application call a Lambda function instead.",
    "isCorrect" : "false"
  }, {
    "id" : "c528286d79914571a3bb24970d68ae27",
    "option" : "There is an issue with the stage defined on the API gateway, hence define a new stage.",
    "isCorrect" : "false"
  }, {
    "id" : "1f5824a2012348cea521ba3082d3b9bd",
    "option" : "Make use of Cognito user pools.",
    "isCorrect" : "false"
  }, {
    "id" : "8bf9ff20af81406db1e26e05a93af22d",
    "option" : "Enable CORS for the API gateway.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThis is given in the AWS Documentation.\n###########\nWhen your API's resources receive requests from a domain other than the API's own domain, you must enable cross-origin resource sharing (CORS) for selected methods on the resource.\nThis amounts to having your API respond to the OPTIONS preflight request with at least the following CORS-required response headers:\nAccess-Control-Allow-Methods.\nAccess-Control-Allow-Headers.\nAccess-Control-Allow-Origin.\n#############\nOption A is invalid because you should not make the architecture change, since this is not the underlying issue.\nOption B is invalid because this is the problem with CORS and not the stage itself.\nOption C is invalid because using Cognito user pools would add one more unnecessary layer of authentication which is not part of the question requirement.\nFor more information on CORS for API gateway, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\n\nThe issue here is that the application hosted on the EC2 instance is making calls to the API Gateway, but the browser is not rendering the responses and JavaScript errors are being seen in the developer console. This indicates a cross-origin resource sharing (CORS) issue, as the browser is blocking the API Gateway responses due to the security policy in place.\nTo resolve this issue, we need to enable CORS for the API Gateway. This can be done by configuring the API Gateway to include the necessary CORS headers in its responses. The headers can be set up in the API Gateway's integration response configuration.\nEnabling CORS allows the application hosted on the EC2 instance to access the API Gateway, which is hosted on a different domain. This is necessary to allow the browser to render the responses correctly and prevent JavaScript errors from occurring.\nOption A, making the application call a Lambda function instead, would not resolve the CORS issue. This option is not relevant to the problem at hand.\nOption B, defining a new stage, would also not resolve the CORS issue. This option is related to API Gateway configuration and versioning, but it is not relevant to the problem at hand.\nOption C, making use of Cognito user pools, is not relevant to the problem at hand. Cognito user pools are used for user authentication and authorization, but they do not address the CORS issue.\nTherefore, the correct answer is option D, enabling CORS for the API Gateway.\n\n"
}, {
  "id" : 135,
  "question" : "You've define a DynamoDB table with a read capacity of 5 and a write capacity of 5\nWhich of the following statements are TRUE? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "4d2b4ec49f834e3294f1498b0ce074f3",
    "option" : "Strong consistent reads of a maximum of 20 KB per second",
    "isCorrect" : "true"
  }, {
    "id" : "f9ed1e20b6274f478fb69d82459153bc",
    "option" : "Eventual consistent reads of a maximum of 20 KB per second",
    "isCorrect" : "false"
  }, {
    "id" : "794038dbdec14e788803bf0a5929a585",
    "option" : "Strong consistent reads of a maximum of 40 KB per second",
    "isCorrect" : "false"
  }, {
    "id" : "2db4997864b446bd88372f68c4883f8f",
    "option" : "Eventualconsistent reads of a maximum of 40 KB per second",
    "isCorrect" : "true"
  }, {
    "id" : "a97f427019b94c38a0695dc372c4c24f",
    "option" : "Maximum writes of 5KB per second.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, D and E.\nThis is also given in the AWS Documentation.\nFor example, suppose that you create a table with 5 read capacity units and 5 write capacity units.\nWith these settings, your application could:\nPerform strongly consistent reads of up to 20 KB per second (4 KB × 5 read capacity units).\nPerform eventually consistent reads of up to 40 KB per second (twice as much read throughput).\nWrite up to 5 KB per second (1 KB × 5 write capacity units).\nBased on the documentation, all other options are incorrect.\nFor more information on provisioned throughput, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html\n\nThe answer options provided are:\nA. Strong consistent reads of a maximum of 20 KB per second B. Eventual consistent reads of a maximum of 20 KB per second C. Strong consistent reads of a maximum of 40 KB per second D. Eventual consistent reads of a maximum of 40 KB per second E. Maximum writes of 5KB per second.\nFirst, let's understand the concept of read and write capacity in DynamoDB:\nDynamoDB is a NoSQL database that provides fast and predictable performance with seamless scalability. To achieve this, DynamoDB uses the concept of provisioned throughput, which allows you to specify the read and write capacity for your table, in terms of read capacity units (RCUs) and write capacity units (WCUs), respectively.\nOne RCU represents one strongly consistent read of up to 4 KB per second, or two eventually consistent reads of up to 4 KB per second. Similarly, one WCU represents one write of up to 1 KB per second.\nNow, let's analyze the answer options:\nA. Strong consistent reads of a maximum of 20 KB per second: Since the table has a read capacity of 5, and each RCU can provide up to 4 KB per second, the maximum strong consistent reads per second would be 5 x 4 KB = 20 KB per second. This option is correct.\nB. Eventual consistent reads of a maximum of 20 KB per second: As explained earlier, one RCU can provide two eventually consistent reads of up to 4 KB per second. Therefore, the maximum eventual consistent reads per second would be 2 x 5 x 4 KB = 40 KB per second. This option is incorrect.\nC. Strong consistent reads of a maximum of 40 KB per second: This option is incorrect since the read capacity of the table is only 5, and each RCU can provide up to 4 KB per second. Therefore, the maximum strong consistent reads per second would be 5 x 4 KB = 20 KB per second.\nD. Eventual consistent reads of a maximum of 40 KB per second: This option is incorrect since the maximum eventual consistent reads per second would be 2 x 5 x 4 KB = 40 KB per second, as explained in option B. However, the option mentions eventual consistent reads, which cannot be guaranteed to return the latest data.\nE. Maximum writes of 5KB per second: Since the table has a write capacity of 5, and each WCU can provide up to 1 KB per second, the maximum writes per second would be 5 x 1 KB = 5 KB per second. This option is correct.\nTherefore, the correct options are A, E, and any one of B or D.\n\n"
}, {
  "id" : 136,
  "question" : "An application has a database in an AWS RDS Instance.\nWhen the traffic is high, the application's response time grows as there are many read queries to the RDS database.\nWhich of the following can be used to decrease the response time for the application?\n",
  "answers" : [ {
    "id" : "a886a9797c4b46b49a6b819fb98e3043",
    "option" : "Place a CloudFront distribution in front of the database.",
    "isCorrect" : "false"
  }, {
    "id" : "bfc20007156a461590cf6cd147e82c6e",
    "option" : "Enable Read Replicas for the database.",
    "isCorrect" : "true"
  }, {
    "id" : "7da04187f7e34a7a8c1648f73c6acc8e",
    "option" : "Change the database from RDS to DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "ef8a9171ea444d17947055db319263cf",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nYou can reduce the load on your source DB instance by routing read queries from your applications to the read replica.\nRead replicas allow you to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads.\nOption A is incorrect because the RDS database cannot be the origin of a CloudFront distribution.\nOption C is incorrect because changing the database type is not the ideal approach.\nOption D is incorrect because Multi-AZ is used for fault-tolerant scenarios for the database.\nFor more information on Read Replicas, please refer to the below URL-\nhttps://aws.amazon.com/rds/details/read-replicas/\n\nTo decrease the response time of an application that has a database hosted in an AWS RDS Instance, the following options can be considered:\nA. Place a CloudFront distribution in front of the database: This option is not suitable for improving the performance of an RDS database. CloudFront is a content delivery network (CDN) service that caches and delivers content from a global network of edge locations. It is mainly used for serving static content such as images, videos, and other files to users with low latency. CloudFront does not interact with a database directly and is not intended to improve database performance.\nB. Enable Read Replicas for the database: Enabling Read Replicas can significantly improve the read performance of an RDS database. Read Replicas are copies of the primary database that can be used to offload read traffic from the primary database. Read Replicas are asynchronous, which means that data changes made to the primary database may not be immediately reflected in the Read Replica. However, this is an acceptable trade-off for many applications that can tolerate eventual consistency. By directing read queries to the Read Replicas, the response time of the application can be reduced.\nC. Change the database from RDS to DynamoDB: DynamoDB is a NoSQL database service provided by AWS. It is designed to provide low-latency access to data at any scale. Unlike RDS, DynamoDB is a fully managed NoSQL database that can automatically scale up or down based on the application's needs. However, DynamoDB is not a relational database and may require significant changes to the application's data model and query patterns. Therefore, switching from RDS to DynamoDB is a significant undertaking and may not be suitable for all applications.\nD. Enable Multi-AZ for the database: Enabling Multi-AZ can improve the availability and durability of an RDS database but does not directly improve its performance. Multi-AZ creates a synchronous standby replica of the primary database in a different availability zone (AZ) within the same region. In the event of a planned or unplanned outage of the primary database, Amazon RDS automatically switches to the standby replica, reducing downtime and preserving data durability.\nIn conclusion, the best option to decrease the response time of an application that has an RDS database is to enable Read Replicas for the database.\n\n"
}, {
  "id" : 137,
  "question" : "An application is currently in production that makes calls to an AWS RDS Instance.\nThe database has recently been facing performance problems.\nIt has been noticed that some static read queries are putting a strain on the database.\nWhich of the following can be used to resolve the issue?\n",
  "answers" : [ {
    "id" : "ad59f57bc608400085f2ced9f29a40f7",
    "option" : "Place a CloudFront distribution in front of the database.",
    "isCorrect" : "false"
  }, {
    "id" : "b97c0ef74a0249fcaf5fd4cbbdec2f52",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  }, {
    "id" : "6613363a7bde48a4a47267057cf1bf05",
    "option" : "Place an SQS queue in front of the database.",
    "isCorrect" : "false"
  }, {
    "id" : "1afbc3a842bf4312acd4e603deb711a8",
    "option" : "Place an ElastiCache in front of the database.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nProposed solution: an in-memory cache based on Amazon ElastiCache.\nBecause the issue involves latency to the backend database, we propose an in-memory cache based on Amazon ElastiCache to reduce network latency and offload the database pressure.\nThis solution dramatically reduces the data retrieval latency.\nIt also scales request volume considerably because Amazon ElastiCache can deliver extremely high request rates, measured at over 20 million per second.\nThe following diagram shows the proposed architecture.\nOption A is incorrect since normally CloudFront distribution is placed in front of the front tier of the application.\nOption B is incorrect since this is used for fault-tolerant scenarios for the database.\nOption C is incorrect since this is used for queuing messages.\nFor more information on reducing latency's for hybrid architectures, please refer to the below URL-\nhttps://aws.amazon.com/blogs/database/latency-reduction-of-hybrid-architectures-with-amazon-elasticache/\n\n\nOut of the given options, the most suitable one to resolve the issue of the database facing performance problems due to static read queries is D. Place an ElastiCache in front of the database.\nExplanation: An ElastiCache is a web service that makes it easy to deploy and run an in-memory cache in the cloud. By placing an ElastiCache in front of the database, the static read queries can be served from the cache instead of the database, thereby reducing the strain on the database and improving performance.\nOption A - Place a CloudFront distribution in front of the database: CloudFront is a content delivery network that is used to deliver content to end-users with low latency and high transfer speeds. However, it is not designed to alleviate the load on a database.\nOption B - Enable Multi-AZ for the database: Multi-AZ (Availability Zone) is a feature of RDS that provides high availability and automatic failover to another AZ in case of a database instance failure. While this can help with database availability, it does not address the issue of performance problems due to static read queries.\nOption C - Place an SQS queue in front of the database: Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables decoupling and scaling microservices, distributed systems, and serverless applications. However, it is not designed to alleviate the load on a database.\nTherefore, the most appropriate option to resolve the issue is D. Place an ElastiCache in front of the database.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 139,
  "question" : "You have docker containers that are going to be deployed in the AWS Elastic Container Service.\nYou need to ensure that the underlying EC2 instances hosting the containers cannot access each other (since containers may be used by different customers)\nHow can you accomplish this?\n",
  "answers" : [ {
    "id" : "137e5d76b16b4ffab14d886579e64959",
    "option" : "Place IAM Roles for the underlying EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "8250843b0f444057ba5d6c1f885a797c",
    "option" : "Place the access keys in the Docker containers.",
    "isCorrect" : "false"
  }, {
    "id" : "dfcd595ca1184861aa20d4ebfa6ef8b6",
    "option" : "Place the access keys in the EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "17a667afb3464735b922b8072a1a157d",
    "option" : "Configure the Security Groups of the instances to allow only required traffic.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nQ: How does Amazon ECS isolate containers belonging to different customers?\nAmazon ECS schedules containers for execution on customer-controlled Amazon EC2 instances or with AWS Fargate and builds on the same isolation controls and compliance available for EC2 customers.\nYour compute instances are located in a Virtual Private Cloud (VPC) with an IP range that you specify.\nYou decide which instances are exposed to the Internet and which remain private.\nYour EC2 instances use an IAM role to access the ECS service.\nYour ECS tasks use an IAM role to access services and resources.\nSecurity Groups and networks ACLs allow you to control inbound and outbound network access to and from your instances.\nYou can connect your existing IT infrastructure to resources in your VPC using industry-standard encrypted IPsec VPN connections.\nYou can provision your EC2 resources as Dedicated Instances.\nDedicated Instances are Amazon EC2 Instances that run on hardware dedicated to a single customer for additional isolation.\nOption A is incorrect since the Roles need to be assigned on the task level.\nOptions B and C are incorrect since access keys are not the ideal security practice.\nFor more information on Task IAM Roles in ECS, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html\nFor more information on ECS, please check below AWS Docs.\nhttps://aws.amazon.com/ecs/faqs/\n\nThe correct answer for the given question is D. Configure the Security Groups of the instances to allow only required traffic.\nExplanation: Amazon Elastic Container Service (Amazon ECS) is a highly scalable and fully managed container orchestration service that allows users to easily run, stop, and manage Docker containers on a cluster. When deploying Docker containers in Amazon ECS, the underlying EC2 instances need to be configured properly to ensure that they cannot access each other, as containers may be used by different customers.\nThe most appropriate way to accomplish this is to configure the Security Groups of the instances. Security Groups act as a virtual firewall for the instances and control inbound and outbound traffic. By configuring Security Groups, you can specify which ports and protocols are allowed to access the instances.\nTo ensure that the underlying EC2 instances hosting the containers cannot access each other, you should create a Security Group for each instance, and then configure the Security Group rules to allow only the required traffic. You can specify the IP address range or Security Group ID of the allowed traffic.\nOn the other hand, placing IAM Roles for the underlying EC2 Instances (Answer A) or placing access keys in the Docker containers (Answer B) or placing access keys in the EC2 Instances (Answer C) are not appropriate solutions to this problem. IAM Roles are used to grant permissions to AWS services, and access keys are used to provide programmatic access to AWS resources. They do not provide any control over network traffic between instances, which is what is required in this scenario.\n\n"
}, {
  "id" : 140,
  "question" : "An application needs to use an authentication in AWS.\nUsers need to have MFA enabled when trying to log into the application.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "d17b19a761844ebdb2b704d19c7223d1",
    "option" : "Create an IAM user with public access.",
    "isCorrect" : "false"
  }, {
    "id" : "a7aceb307167400aa93e487ecd1c380c",
    "option" : "Create an IAM group with public access.",
    "isCorrect" : "false"
  }, {
    "id" : "2ebddf2fd9cd49e79d138cbf7a03ad80",
    "option" : "Use AWS Cognito with MF.",
    "isCorrect" : "true"
  }, {
    "id" : "5fb239851a4742f0951751304196388a",
    "option" : "Use AWS STS with SAML.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is mentioned in the AWS Documentation.\nAdding Multi-Factor Authentication (MFA) to a User Pool.\nMulti-factor authentication (MFA) increases security for your app by adding another authentication method and not relying solely on user name and password.\nYou can choose to use SMS text messages or time-based one-time (TOTP) passwords as second factors in signing in to your users.\nWith adaptive authentication, you can configure your user pool to require second-factor authentication in response to an increased risk level.\nTo add adaptive authentication to your user pool, see Adding Advanced Security to a User Pool.\nOptions A and B are incorrect since it's not the right approach to using IAM users or groups to access mobile-based applications.\nOption D is incorrect since SAML is used for federated access.\nFor more information on Cognito with MFA, please refer to the below URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa.html\n\nThe correct answer is C. Use AWS Cognito with MFA.\nExplanation: AWS Cognito is a managed service that provides user sign-up, sign-in, and access control. It supports different authentication mechanisms, including social identity providers, such as Facebook, Google, and Amazon, as well as enterprise identity providers, such as Microsoft Active Directory, and it also offers built-in MFA (Multi-Factor Authentication) support.\nTo use AWS Cognito with MFA, you can create a user pool in AWS Cognito and configure the MFA settings for the user pool. Then, you can create user accounts in the user pool and require MFA for user authentication. Users can enable MFA in their user accounts by setting up MFA devices, such as a smartphone app or a hardware token.\nOption A, creating an IAM user with public access, is not recommended since granting public access to IAM users can lead to security vulnerabilities.\nOption B, creating an IAM group with public access, is also not recommended for the same reasons as option A.\nOption D, using AWS STS with SAML, is not directly related to MFA authentication. AWS STS (Security Token Service) is a service that enables you to request temporary security credentials that can be used to access AWS resources. SAML (Security Assertion Markup Language) is an XML-based protocol that enables authentication and authorization between two parties, such as an identity provider (IdP) and a service provider (SP). While SAML can be used for authentication, it is not specifically designed for MFA authentication.\nIn summary, AWS Cognito is the best option for providing MFA authentication for an application running on AWS.\n\n"
}, {
  "id" : 141,
  "question" : "When calling an API operation on an EC2 Instance, the following error message was returned. A client error (UnauthorizedOperation) occurred when calling the RunInstances operation: You are not authorized to perform this operation.\nEncoded authorization failure message: oGsbAaIV7wlfj8zUqebHUANHzFbmkzILlxyj__y9xwhIHk99U_cUq1FIeZnskWDjQ1wSHStVfdCEyZILGoccGpC iCIhORceWF9rRwFTnEcRJ3N9iTrPAE1WHveC5Z54ALPaWlEjHlLg8wCaB8d8lCKmxQuylCm0r1Bf2fHJRU jAYopMVmga8olFmKAl9yn_Z5rI120Q9p5ZIMX28zYM4dTu1cJQUQjosgrEejfiIMYDda8l7Ooko9H6VmGJX S62KfkRa5l7yE6hhh2bIwA6tpyCJy2LWFRTe4bafqAyoqkarhPA4mGiZyWn4gSqbO8oSIvWYPwea KGkampa0arcFR4gBD7Ph097WYBkzX9hVjGppLMy4jpXRvjeA5o7TembBR-Jvowq6mNim0 Which of the following can be used to get a human-readable error message?\n",
  "answers" : [ {
    "id" : "b1886a61184a4fb2a87f16043736a7f2",
    "option" : "Use the command aws sts decode-authorization-message.",
    "isCorrect" : "true"
  }, {
    "id" : "d7a6e7afdefb47108fae798c90f2e0c0",
    "option" : "Use the command aws get authorization-message.",
    "isCorrect" : "false"
  }, {
    "id" : "b912c78a02324ebd9a512f1069c6a7a6",
    "option" : "Use the IAM Policy simulator, enter the error message to get the human readable format.",
    "isCorrect" : "false"
  }, {
    "id" : "7274345454af4a88be3deb20f8679c7a",
    "option" : "Use the command aws set authorization-message.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is mentioned in the AWS Documentation.\nDecodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request.\nFor example, if a user is not authorized to perform an action that he or she has requested, the request returns a Client.UnauthorizedOperation response (an HTTP 403 response)\nSome AWS actions additionally return an encoded message that can provide details about this authorization failure.\nBecause of the right command used in the documentation, all other options are incorrect.\nFor more information on the command, please refer to the below URL-\nhttps://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html\n\nThe error message indicates that the user or role attempting to call the RunInstances operation on the EC2 instance is not authorized to do so. The encoded message included in the error is not human-readable and must be decoded to get a more understandable error message.\nOption A is the correct answer as it suggests using the aws sts decode-authorization-message command to decode the encoded error message. This command decodes the authorization failure message and returns a human-readable error message that provides information about why the request was denied. This option will provide the most direct and accurate method of obtaining a human-readable error message.\nOption B, aws get authorization-message, is not a valid AWS CLI command and will not return any results.\nOption C, using the IAM Policy simulator, may provide a way to test different policy scenarios to see if they would allow or deny the requested action. However, it does not provide a direct method to obtain a human-readable error message for a specific API operation.\nOption D, aws set authorization-message, is not a valid AWS CLI command and is not relevant to decoding or obtaining a human-readable error message.\nIn summary, the correct answer is A, as it provides the most direct method of obtaining a human-readable error message for the UnauthorizedOperation error.\n\n"
}, {
  "id" : 142,
  "question" : "You have defined some custom policies in AWS.\nYou need to test out the permissions assigned to those policies.\nWhich of the following can be used for this purpose via the CLI? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "b766bf806fc34da2abf0420c6ad9586a",
    "option" : "Get the context keys first.",
    "isCorrect" : "true"
  }, {
    "id" : "97e61286ff904290be649e4a6d977c47",
    "option" : "Use the aws iam simulate-custom-policy command.",
    "isCorrect" : "true"
  }, {
    "id" : "eecd0e06fa9449f28349710196e1ffaa",
    "option" : "Get the AWS IAM Access keys first.",
    "isCorrect" : "false"
  }, {
    "id" : "07618a522dac4f37a37b1cc242452dc0",
    "option" : "Use the aws iam get-custom-policy command.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - A and B.\nThis is mentioned in the AWS Documentation.\nPolicy simulator commands typically require calling API operations to do two things.\nEvaluate the policies and return the list of context keys that they reference.\nYou need to know what context keys are referenced so that you can supply values for them in the next step.\nSimulate the policies, providing a list of actions, resources, and context keys that are used during the simulation.\nBecause of the right command used in the documentation, all other options are incorrect.\nFor more information on policy simulation, please refer to the below URL-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing-policies.html\n\nThe correct answers are A and B.\nTo test the permissions assigned to custom policies in AWS using the CLI, you can follow these steps:\nGet the context keys first: Context keys are variables that define the context of a request, such as the time of day, the IP address, or the user agent. To test permissions, you need to specify the context in which the request is being made. You can use the aws iam get-context-keys-for-custom-policy command to retrieve the context keys required for testing. Use the aws iam simulate-custom-policy command: This command allows you to simulate a request to AWS services to test the permissions assigned to a custom policy. You can specify the context keys retrieved in step 1, as well as the actions and resources to be tested. The command will return a report that shows whether the policy allows or denies access to the specified actions and resources.\nFor example, suppose you have created a custom policy that allows access to an S3 bucket. You can use the following commands to test the policy:\ncsharp Copy code aws iam get-context-keys-for-custom-policy --policy-input-list file://policy.json\nHere, policy.json is the JSON file containing the policy you want to test.\nc Copy code aws iam simulate-custom-policy --policy-input-list file://policy.json \\ --action-names s3:GetObject \\ --resource-arns arn:aws:s3:::example-bucket/* \\ --context-entry-keys ContextKey1,ContextKey2 \\ --context-entry-values value1,value2\nHere, action-names specifies the actions to test, resource-arns specifies the resources to test, and context-entry-keys and context-entry-values specify the context in which the request is being made.\nIt's important to note that testing policies using the CLI is a best practice for ensuring that policies are working as expected before assigning them to users or groups. It can help you catch any errors or unintended consequences of the policy before they cause problems.\n\n"
}, {
  "id" : 143,
  "question" : "You have been instructed to use the CodePipeline service for the CI/CD automation in your company.\nDue to security reasons, the resources that would be part of the deployment are placed in another account.\nWhich of the following steps need to be carried out to accomplish this deployment? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "6843c2ede4674847b636dda78d66c758",
    "option" : "Define an AWS key in KMS. Create and attach policies that will enable the cross-account access.",
    "isCorrect" : "true"
  }, {
    "id" : "0070032a85274f12905ba259a9d010cc",
    "option" : "Create a reference Code Pipeline instance in the other account.",
    "isCorrect" : "false"
  }, {
    "id" : "e5a6dfd482b64bf58a40799d311f1809",
    "option" : "Add a cross account role.",
    "isCorrect" : "true"
  }, {
    "id" : "5b95d1fec67b41998358a407bf20e88c",
    "option" : "Embed the access keys in the code pipeline process.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nOption B is invalid since this would go against the security policy.\nOption D is invalid since this is not a recommended security practice.\nThis is mentioned in the AWS Documentation.\nYou might want to create a pipeline that uses resources created or managed by another AWS account.\nFor example, you might want to use one account for your pipeline and another for your AWS CodeDeploy resources.\nTo do so, you must create an AWS Key Management Service (AWS KMS) key to use, add the key to the pipeline, and set up account policies and roles to enable cross-account access.\nFor more information on pipelines used to access resources in another account, please refer to the below URL-\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create-cross-account.html\n\nIn this scenario, you are required to set up a CodePipeline service for CI/CD automation, and the resources that are part of the deployment are located in another account for security reasons. To achieve this, you need to perform the following two steps:\nDefine an AWS key in KMS. Create and attach policies that will enable the cross-account access You need to define an AWS key in KMS (Key Management Service) to enable secure access to the resources in the other account. KMS is a managed service that helps create and control the encryption keys used to secure your data. With KMS, you can create and manage keys and define policies that control access to those keys. Once the key is defined, you need to create policies that enable cross-account access to the resources. These policies will define who can access the resources and what actions they can perform on them. Add a cross-account role You need to create a cross-account role that will enable access to the resources in the other account. A cross-account role is an IAM (Identity and Access Management) role that you can create in one AWS account and use to access resources in another AWS account. This role will allow CodePipeline to access the necessary resources in the other account during the deployment process.\nOption B (Create a reference CodePipeline instance in the other account) is incorrect because you don't need to create a reference CodePipeline instance in the other account. Option D (Embed the access keys in the CodePipeline process) is also incorrect because embedding access keys in the CodePipeline process is not a secure way to manage access to resources in another account.\n\n"
}, {
  "id" : 144,
  "question" : "You have deployed an application on an EC2 Instance.\nThis application makes calls to a DynamoDB service.\nThere are numerous performance issues present in the application.\nYou decide to use the XRay service to debug the performance issues.\nYou are not able to see the trails in the XRay service.\nWhich of the following could be the underlying issue? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "d41ba4d933104af09561a09239c87608",
    "option" : "The X-Ray daemon is not installed on the EC2 Instance.",
    "isCorrect" : "true"
  }, {
    "id" : "1876d3d19a8b4c1d873446a97ef2c109",
    "option" : "The right AMI is not chosen for the EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "862a199f360a46cfa250f0a86c0035ea",
    "option" : "Ensure that the IAM Role attached to the Instance has permission to upload data onto X-Ray.",
    "isCorrect" : "true"
  }, {
    "id" : "ae2926044e1d4265a83f455d84d0739e",
    "option" : "Ensure that the IAM Role attached to the Instance has permission to upload data onto Cloudwatch.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nYou need to have the daemon service running on the EC2 Instance.\nAnd a role needs to be attached to the EC2 Instance.\nRunning the X-Ray Daemon on Amazon EC2\nYou can run the X-Ray daemon on the following operating systems on Amazon EC2.\nAmazon Linux.\nUbuntu.\nWindows Server (2012 R2 and newer)\nUse an instance profile to grant the daemon permission to upload trace data to X-Ray.\nFor more information, see Giving the Daemon Permission to Send Data to X-Ray.\nOption B is incorrect since the agent can be installed on different types of instances.\nOption D is incorrect since the traces need to be sent to the X-Ray service.\nFor more information on the X-Ray daemon service, please refer to the below URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ec2.html\n\nThe X-Ray service provides tracing of requests in distributed systems, making it easier to identify issues and pinpoint their root cause. When using X-Ray to debug performance issues in an application that makes calls to a DynamoDB service from an EC2 instance, the trails may not be visible due to one or more of the following reasons:\nA. The X-Ray daemon is not installed on the EC2 Instance. The X-Ray daemon is a software component that runs on the EC2 instance and intercepts the outgoing requests from the application to the DynamoDB service. It is responsible for generating trace data and forwarding it to the X-Ray service. If the X-Ray daemon is not installed on the EC2 instance, there will be no trace data to display in the X-Ray console. To resolve this issue, install the X-Ray daemon on the EC2 instance.\nB. The right AMI is not chosen for the EC2 Instance. The AMI (Amazon Machine Image) is the pre-configured virtual machine image used to create an EC2 instance. If the AMI used to create the EC2 instance does not have the X-Ray daemon pre-installed, it will not be possible to generate and upload trace data to the X-Ray service. To resolve this issue, choose an AMI that has the X-Ray daemon pre-installed or manually install the X-Ray daemon on the EC2 instance.\nC. Ensure that the IAM Role attached to the Instance has permission to upload data onto X-Ray. The IAM Role attached to the EC2 instance must have the necessary permissions to upload trace data to the X-Ray service. If the IAM Role does not have the required permissions, the X-Ray daemon will not be able to upload the trace data to the X-Ray service. To resolve this issue, modify the IAM Role to include the necessary permissions for the X-Ray service.\nD. Ensure that the IAM Role attached to the Instance has permission to upload data onto Cloudwatch. The IAM Role attached to the EC2 instance must have the necessary permissions to upload trace data to CloudWatch if you are using it as the storage backend for X-Ray. If the IAM Role does not have the required permissions, the X-Ray daemon will not be able to upload the trace data to CloudWatch. To resolve this issue, modify the IAM Role to include the necessary permissions for CloudWatch.\nIn summary, to resolve the issue of trails not appearing in the X-Ray service when debugging performance issues in an application that makes calls to a DynamoDB service from an EC2 instance, it is necessary to ensure that the X-Ray daemon is installed on the EC2 instance, the right AMI is chosen for the EC2 instance, and the IAM Role attached to the EC2 instance has the necessary permissions to upload data to the X-Ray service and/or CloudWatch.\n\n"
}, {
  "id" : 145,
  "question" : "Your company is hosting a set of resources on the AWS Cloud.\nThere is now a security requirement that states that all API requests to the STS service be monitored.\nHow can you accomplish this requirement?\n",
  "answers" : [ {
    "id" : "59ca95d980e5410b9d4a8738bdd6dfc4",
    "option" : "Monitor the Cloudwatch logs service.",
    "isCorrect" : "false"
  }, {
    "id" : "33fe20ebf7f245d5a14feea84d92b7dc",
    "option" : "View logs in CloudTrail.",
    "isCorrect" : "true"
  }, {
    "id" : "cff6b32d4c29482bbaf604ac443fd7a3",
    "option" : "Use the STS logging service.",
    "isCorrect" : "false"
  }, {
    "id" : "c05fe7e648e14611ba2fc2076dfce16a",
    "option" : "Use Cloudwatch metrics.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nCloudTrail logs all authenticated API requests (made with credentials) to IAM and AWS STS APIs, except DecodeAuthorizationMessage.\nCloudTrail also logs nonauthenticated requests to the AWS STS actions, AssumeRoleWithSAML and AssumeRoleWithWebIdentity, and logs information provided by the identity provider.\nYou can use this information to map calls made by a federated user with an assumed role back to the originating external federated caller.\nOption A is incorrect since the log service will not have the trail of the API calls.\nOption C is incorrect since STS does not have a logging service.\nOption D is incorrect since Cloudwatch metrics will not have the trail of the API calls.\nFor more information on cloudtrail integrations, please refer to the below URL-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/cloudtrail-integration.html\n\nTo accomplish the requirement of monitoring all API requests to the STS service in AWS, the best option would be to view logs in CloudTrail.\nCloudTrail is a service that records all API activity and generates detailed event logs, including API requests made to STS (Security Token Service). By default, CloudTrail is enabled in AWS accounts, and you can access it in the CloudTrail console or by using the AWS CLI.\nYou can use CloudTrail to monitor all API requests made to STS, including requests for temporary security credentials, which allow access to AWS resources for a specified time period. With CloudTrail, you can identify the source IP address of the request, the IAM user or role making the request, the time of the request, and other details.\nAdditionally, you can create CloudWatch alarms to notify you when specific API events occur, such as an API call to STS that uses a specific IAM user or role. You can also use CloudWatch metrics to visualize and analyze API usage patterns and identify anomalies.\nWhile it is possible to monitor the CloudWatch logs service or use the STS logging service to monitor API requests to STS, these options may not provide the same level of detail and visibility as CloudTrail. CloudTrail is specifically designed for recording and monitoring API activity, making it the best option for meeting the security requirement of monitoring all API requests to STS.\n\n"
}, {
  "id" : 146,
  "question" : "Your team has developed an application that makes use of AWS resources.\nIn order to provide frequent releases to the customer, you are required to automate the CI/CD process.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "2a15f47d18264e698f0b7f6344d06299",
    "option" : "Create a pipeline using AWS Code deploy. Configure a stage for Unit testing as well in the pipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "6c3b94df5024415ab0252af1c8100bf6",
    "option" : "Use AWS CodeCommit to host your code repository. Use the build tool in AWS CodeCommit to build your pipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "ecc769e2af294b65967bcd08adf1049c",
    "option" : "Create a Pipeline in the AWS CodeBuild Service.",
    "isCorrect" : "false"
  }, {
    "id" : "f2a1ffe1c17a47589187b9f9977d53e3",
    "option" : "Create a pipeline using AWS CodePipeline along with AWS Code Star service.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nAutomated continuous delivery pipeline.\nAWS CodeStar accelerates software release with the help of AWS CodePipeline, a continuous integration and continuous delivery (CI/CD) service.\nEach project comes pre-configured with an automated pipeline that continuously builds, tests, and deploys your code with each commit.\nOption A is incorrect because pipelines are created using the \"AWS Code pipeline\" and \"AWS Code deploy\" automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers.\nOption B is incorrect since AWS CodeCommit does not have the facility to carry out the build.\nOptions C is incorrect since the CodePipeline service is used for building build pipelines.\nFor more information, please refer to the below URL-\nhttps://aws.amazon.com/codestar/features/\nCreating a pipeline in the Code pipeline is not enough to automate the release process you need to integrate with code commit,\nCode build and then deploy your code through Cloudformation/Elastic Beanstalk etc.\nWhereas in Code star Each project comes pre-configured with an automated pipeline that continuously builds, tests, and deploys your code with each commit.\n\n\nIn order to automate the CI/CD process, there are multiple services offered by AWS that can be utilized. The four options provided in the question are as follows:\nA. Create a pipeline using AWS CodeDeploy. Configure a stage for Unit testing as well in the pipeline. AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services like EC2 instances, Lambda functions, and on-premises servers. However, it doesn't provide a complete CI/CD pipeline, but can be a part of the pipeline. In this case, we can create a pipeline using AWS CodeDeploy and configure a stage for Unit testing in the pipeline.\nB. Use AWS CodeCommit to host your code repository. Use the build tool in AWS CodeCommit to build your pipeline. AWS CodeCommit is a fully-managed source control service that hosts secure Git-based repositories. It can be used to store source code and other artifacts, and it integrates with other AWS services like AWS CodeBuild and AWS CodePipeline to enable continuous integration and continuous delivery. In this case, we can use AWS CodeCommit to host our code repository and use the build tool in AWS CodeCommit to build our pipeline.\nC. Create a Pipeline in the AWS CodeBuild Service. AWS CodeBuild is a fully-managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. It can be used to build a CI/CD pipeline that integrates with other AWS services like AWS CodeCommit and AWS CodePipeline. In this case, we can create a pipeline in the AWS CodeBuild service.\nD. Create a pipeline using AWS CodePipeline along with AWS CodeStar service. AWS CodePipeline is a fully-managed continuous delivery service that automates the release process for applications. It can be used to create a CI/CD pipeline that integrates with other AWS services like AWS CodeCommit and AWS CodeBuild. AWS CodeStar is a service that simplifies the process of developing and deploying applications on AWS by providing templates for common development environments and pre-configured pipelines for deploying applications to AWS services. In this case, we can create a pipeline using AWS CodePipeline along with AWS CodeStar service.\nTo summarize, all four options can be used for automating the CI/CD process, but the choice depends on the specific needs of the application and the team. Option A and B focus on using AWS CodeDeploy and AWS CodeCommit respectively, while option C and D focus on using AWS CodeBuild and AWS CodePipeline respectively. It is also possible to combine these services to create a customized pipeline that fits the specific requirements of the application.\n\n"
}, {
  "id" : 147,
  "question" : "You are using the AWS CodeDeploy service to deploy an application onto AWS.\nThe application uses secure parameters which are stored in the AWS Systems Manager Parameter Store.\nWhich of the following must be done so that the deployment can be automated via CodeDeploy? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ba56660b27434e8099575279d407a178",
    "option" : "Use the aws ssm get-parameters with the --with-decryption option.",
    "isCorrect" : "true"
  }, {
    "id" : "4bd6dd7346a541d29a6e568d5fa281c2",
    "option" : "Use the AWS SSM get-parameters with the --with-no-decryption option.",
    "isCorrect" : "false"
  }, {
    "id" : "965536b1bbfb410fa17b4ad5f1351006",
    "option" : "Give permissions to the AWS Code Deploy service via AWS Access Keys.",
    "isCorrect" : "false"
  }, {
    "id" : "a0fdedf253c84dd3b76cf5b2bd1f1593",
    "option" : "Give permissions to the AWS Code Deploy service via an IAM Role.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nYou need to specify the --with-decryption option.\nThis allows the CodeDeploy service to decrypt the password so that it can be used in the application.\nAlso, use IAM Roles to ensure the CodeDeploy service can access the KMS service.\nOption B is incorrect since you need to specify the --with-decryption option.\nOption C is incorrect since this is not a secure way to access AWS services.\nFor more information on an example on this, please refer to the below URL-\nhttps://aws.amazon.com/blogs/mt/use-parameter-store-to-securely-access-secrets-and-config-data-in-aws-codedeploy/\n\nTo automate deployment using AWS CodeDeploy with secure parameters, the following two steps must be taken:\nA. Use the aws ssm get-parameters command with the --with-decryption option: When deploying an application using AWS CodeDeploy, secure parameters are stored in the AWS Systems Manager Parameter Store. These secure parameters may contain sensitive information such as database passwords or API keys, which should not be disclosed to unauthorized users. To retrieve these parameters during deployment, the aws ssm get-parameters command should be used with the --with-decryption option. This option allows the retrieved parameters to be decrypted, making them usable in the deployment process.\nB. Give permissions to the AWS CodeDeploy service via an IAM Role: To allow the AWS CodeDeploy service to access the AWS Systems Manager Parameter Store, permissions must be granted via an IAM Role. The IAM Role should be created with the necessary permissions to access the Parameter Store and retrieve the secure parameters needed for deployment. The role should also have permissions to access any other AWS resources needed during the deployment process, such as EC2 instances or S3 buckets.\nTherefore, the correct answers to this question are A and D. Option A specifies how to retrieve the secure parameters during deployment, and option D describes how to give permissions to the AWS CodeDeploy service to access the Parameter Store.\n\n"
}, {
  "id" : 148,
  "question" : "An application is currently accessing a DynamoDB table.\nCurrently, the table queries are performing well.\nChanges have been made to the application, and now the performance of the application is starting to degrade.\nAfter looking at the changes, you see that the queries use an attribute that is not the partition key.\nWhich of the following would be the adequate change to make to resolve the issue?\n",
  "answers" : [ {
    "id" : "3526fbc76af240d6b04f21fe35f82780",
    "option" : "Add a Global Secondary Index to the DynamoDB table.",
    "isCorrect" : "true"
  }, {
    "id" : "41cb31371fc94926a5a2dc9c76d46af8",
    "option" : "Change all the queries to ensure they use the partition key.",
    "isCorrect" : "false"
  }, {
    "id" : "befc66a467044d5ab758ca94e8fe7e36",
    "option" : "Enable global tables for DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "50c13433c27e4f0db735075573d07eb0",
    "option" : "Change the read capacity on the table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon DynamoDB provides fast access to items in a table by specifying primary key values.\nHowever, many applications might benefit from having one or more secondary (or alternate) keys available to allow efficient access to data with attributes other than the primary key.\nYou can create one or more secondary indexes on a table and issue Query or Scan requests against these indexes to address this.\nA secondary index is a data structure that contains a subset of attributes from a table, along with an alternate key to support Query operations.\nYou can retrieve data from the index using a Query, in much the same way as you use Query with a table.\nA table can have multiple secondary indexes, which gives your applications access to many different query patterns.\nOption B, although possible, is not the ideal approach to change the application code.\nOption C is used for disaster recovery scenarios.\nOption D is not right because we don't know if this would solve the issue in the long run.\nFor more information on Secondary Indexes, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SecondaryIndexes.html\n\nThe issue is that the application queries are using an attribute that is not the partition key, which is causing the performance of the application to degrade. To resolve this issue, one of the following changes should be made:\nA. Add a Global Secondary Index (GSI) to the DynamoDB table.\nA GSI is a replica of the original table with an alternate key schema. It allows you to query the table using non-key attributes. Adding a GSI with the non-key attribute used in the queries as the partition key will allow the queries to perform efficiently without requiring a full table scan. Therefore, adding a GSI is an adequate change to resolve the issue.\nB. Change all the queries to ensure they use the partition key.\nChanging all the queries to use the partition key would ensure efficient query performance. However, this might not be feasible or desirable, especially if the non-key attribute is necessary for the queries. Therefore, this may not be an adequate change to resolve the issue.\nC. Enable global tables for DynamoDB.\nGlobal tables are used to replicate a DynamoDB table across multiple AWS regions, allowing for low-latency access to the data from anywhere in the world. Enabling global tables will not address the issue at hand, which is the inefficient queries due to the non-use of the partition key. Therefore, this is not an adequate change to resolve the issue.\nD. Change the read capacity on the table.\nChanging the read capacity on the table would not address the issue at hand, which is the inefficient queries due to the non-use of the partition key. Therefore, this is not an adequate change to resolve the issue.\nIn conclusion, adding a Global Secondary Index to the DynamoDB table is the adequate change to make to resolve the issue.\n\n"
}, {
  "id" : 149,
  "question" : "Your team developed and deployed a java application on an EC2 Instance.\nTo test the application, you were given access credentials through keys using environment variables.\nThis gave you the permissions to write to an S3 bucket.\nOnce the testing was confirmed, an IAM role was assigned to the instance.\nThis role only has permissions to read, but you notice that the application still has access to write to the S3 bucket.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "38a980d4db8c4e958bffa7012dbfe5f8",
    "option" : "You need to restart the instance for the role settings to take effect.",
    "isCorrect" : "false"
  }, {
    "id" : "94d611b9c8e94bcfbe90a26bf351b86f",
    "option" : "The environment variables which were set for SDK access are taking priority.",
    "isCorrect" : "true"
  }, {
    "id" : "678a187a74464b95bba87be5be0dd642",
    "option" : "The CLI is corrupted, hence the credentials are not being revoked.",
    "isCorrect" : "false"
  }, {
    "id" : "2d9f6ae02bbb4f56a77ff52ac19c799d",
    "option" : "The EBS volume needs to be reattached again for the instance profile to take effect.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nBelow is an excerpt from the documentation on how the credentials are evaluated when it comes to access.\nSo when using the CLI, if the environment variables were set with the Access Keys, they would take preference over the IAM Role.\nTo make requests to Amazon Web Services, you must supply AWS credentials to the AWS SDK for Java.\nYou can do this in the following ways:\n- Use the default credential provider chain (recommended).\n- Use a specific credential provider or provider chain (or create your own).\n- Supply the credentials yourself.\nThese can be root account credentials, IAM credentials, or temporary credentials retrieved from AWS STS.\nUsing the Default Credential Provider Chain.\nWhen you initialize a new service client without supplying any arguments, the AWS SDK for Java attempts to find AWS credentials using the default credential provider chain implemented by the DefaultAWSCredentialsProviderChain class.\nThe default credential provider chain looks for credentials in this order.\nEnvironment variables-\nAWS_ACCESS_KEY_ID.\nand\nAWS_SECRET_ACCESS_KEY.\nThe AWS SDK for Java uses the EnvironmentVariableCredentialsProvider class to load these credentials.\nJava system properties-\naws.accessKeyId.\nand\naws.secretKey.\nThe AWS SDK for Java uses the SystemPropertiesCredentialsProvider to load these credentials.\nWeb Identity Token credentials from the environment or container.\nThe default credential profiles file- typically located at\n~/.aws/credentials.\n(location can vary per platform), and shared by many of the AWS SDKs and by the AWS CLI.\nThe AWS SDK for Java uses the ProfileCredentialsProvider to load these credentials.\nAmazon ECS container credentials- loaded from the Amazon ECS if the environment variable\nAWS_CONTAINER_CREDENTIALS_RELATIVE_URI.\nis set.\nThe AWS SDK for Java uses the ContainerCredentialsProvider to load these credentials.\nYou can specify the IP address for this value.\nInstance profile credentials- used on EC2 instances, and delivered through the Amazon EC2 metadata service.\nThe AWS SDK for Java uses the InstanceProfileCredentialsProvider to load these credentials.\nYou can specify the IP address for this value.\nOptions A and D are incorrect since the IAM Role is instantly applied to the EC2 Instance.\nOption C is incorrect because even if the CLI is corrupted.\nThis would still not be the cause of the underlying issue.\nFor more information on an example of how credentials are evaluated, please refer to the below URL-\nhttps://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html\n\nThe most likely reason why the application still has access to write to the S3 bucket, even after assigning an IAM role with read-only permissions to the EC2 instance, is that the environment variables set for SDK access are taking priority over the instance profile.\nWhen a Java application runs on an EC2 instance, it can use the AWS SDK to interact with AWS services such as S3. To access the SDK, the application needs to provide credentials, which can be done using environment variables or by using an IAM role attached to the instance.\nIn this case, during the testing phase, the team provided access credentials through environment variables to the application, which allowed it to write to the S3 bucket. However, after the testing was confirmed, an IAM role was assigned to the instance, which only has permissions to read from the S3 bucket.\nEven though the IAM role was assigned to the instance, the application still has access to write to the S3 bucket because the environment variables for SDK access are taking priority. The application is still using the access credentials provided through the environment variables, which have write permissions to the S3 bucket.\nTo solve this problem, the application needs to be updated to use the instance profile credentials instead of the environment variables. This can be done by configuring the AWS SDK to use the instance profile credentials provider. Once this is done, the application will use the IAM role assigned to the instance and will only have read-only permissions to the S3 bucket.\nRestarting the instance (option A) or reattaching the EBS volume (option D) will not solve the problem, as they do not affect the SDK credentials used by the application. The CLI being corrupted (option C) is also unlikely to be the cause of the issue, as it does not affect the SDK credentials used by the application.\n\n"
}, {
  "id" : 150,
  "question" : "You are planning on deploying an application to the worker role in Elastic Beanstalk.\nMoreover, this worker application is going to run the periodic tasks.\nWhich of the following is a must-have as part of the deployment?\n",
  "answers" : [ {
    "id" : "86a20827810e4150ad751a640bbb0dc4",
    "option" : "An appspec.yaml file",
    "isCorrect" : "false"
  }, {
    "id" : "62c33c1b10904b05b0c69d79cbbf18b7",
    "option" : "A cron.yaml file",
    "isCorrect" : "true"
  }, {
    "id" : "4575e2e4498a47979dbaa299e56ede0d",
    "option" : "A cron.config file",
    "isCorrect" : "false"
  }, {
    "id" : "d29b3b3113d64872ab98345061103c30",
    "option" : "An appspec.json file.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - B.\nThis is also given in the AWS Documentation.\nCreate an Application Source Bundle.\nWhen you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle.\nYour source bundle must meet the following requirements.\nConsist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)\nNot exceed 512 MB.\nNot include a parent folder or top-level directory (subdirectories are fine)\nIf you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file.\nFor more information, see Periodic Tasks.\nBecause of the exact requirement given in the AWS Documentation, all other options are invalid.\nFor more information on creating an application source bundle for Elastic beanstalk, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html\n\nThe correct answer is B. A cron.yaml file.\nWhen deploying an application to the worker role in Elastic Beanstalk, a cron.yaml file is a must-have as part of the deployment. This file is used to define periodic tasks that need to be run on the worker instances.\nA cron job is a scheduled task that runs automatically at a specified interval. The cron.yaml file allows you to define these scheduled tasks, including the frequency and the command to be executed.\nFor example, suppose you have a task that needs to be run every hour. In that case, you can create a cron.yaml file that defines this task and the frequency at which it should be run. The file may look like this:\nyaml Copy code version: 1 cron:  - name: hourly-task    url: /task    schedule: \"0 * * * *\"\nThis file specifies that the task named \"hourly-task\" should be run every hour. The task is executed by calling the URL \"/task\" on the worker instance.\nIt is worth noting that the appspec.yaml file is used for the deployment of web applications, not worker applications. The appspec.json file is not a valid file type for Elastic Beanstalk deployments, and the cron.config file does not exist. Therefore, options A, C, and D are incorrect.\n\n"
}, {
  "id" : 151,
  "question" : "You are developing a Java-based application that needs to make use of the AWS KMS service for encryption.\nWhich of the following must be done for the encryption and decryption process? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "02955a7c626c4c1b869265f0e9b4fbd1",
    "option" : "Use the Customer master key to encrypt the data.",
    "isCorrect" : "false"
  }, {
    "id" : "9cb255d1968e4052a0631df913353881",
    "option" : "Use the Customer master key to generate a data key for the encryption process.",
    "isCorrect" : "true"
  }, {
    "id" : "a9433e081b6a4f548a4ca6e2904a1732",
    "option" : "Use the Customer master key to decrypt the data.",
    "isCorrect" : "false"
  }, {
    "id" : "45ef10ace48e42f6adb51cf114a238b5",
    "option" : "Use the generated data key to decrypt the data.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe AWS Documentation mentions the following.\nThe AWS Encryption SDK is a client-side encryption library that makes it easier for you to implement cryptography best practices in your application.\nIt includes secure default behaviour for developers who are not encryption experts, while being flexible enough to work for the most experienced users.\nOptions A and C are incorrect because you should never use the Customer master keys directly to encrypt the decryption process.\nIn the AWS Encryption SDK, by default, you generate a new data key for each encryption operation.\nFor more information on the Encryption SDK, please refer to the below URL-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/programming-top.html\nNote:\nAWS Docs Says.\n\"When you encrypt your data, your data is protected, but you have to protect your encryption key.\nOne strategy is to encrypt it.\nEnvelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key.\nYou can even encrypt the data encryption key under another encryption key and encrypt that encryption key with another encryption key.\nBut, eventually, one key must remain in plaintext so you can decrypt the keys and your data.\nThis top-level plaintext key encryption key is known as the master key.\"\nFor more information on the enveloping, please refer to the below URL-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping\n\nAWS Key Management Service (KMS) is a managed service that allows you to create and control the encryption keys used to encrypt your data. You can use the AWS KMS service to encrypt and decrypt data at rest and in-transit in various AWS services and in your own applications.\nWhen using the AWS KMS service for encryption and decryption, you must perform the following two steps:\nGenerate a Data Encryption Key (DEK): This key is used to encrypt the data that you want to protect. Protect the DEK: The DEK is then encrypted with a Master Key, which is used to protect the DEK. This is the key that you use to encrypt and decrypt the data.\nTherefore, the correct answers to the question are:\nB. Use the Customer master key to generate a data key for the encryption process.\nD. Use the generated data key to decrypt the data.\nThe Customer master key is used to generate a unique Data Encryption Key (DEK) that will be used to encrypt the data. The generated data key is then used to encrypt the data. To decrypt the data, you will use the same data key that was used to encrypt it.\nOption A is incorrect because you use the Customer master key to generate a data key, not to encrypt the data directly.\nOption C is incorrect because you use the Customer master key to encrypt the data key, not to decrypt the data.\nIn summary, AWS KMS uses a Customer master key to encrypt and decrypt data, but it generates a Data Encryption Key (DEK) that is used to encrypt and decrypt the actual data.\n\n"
}, {
  "id" : 152,
  "question" : "You have developed a Web-based application that makes calls to backend API.\nWhich of the following headers are required by Web browsers to be set up in each API method which has CORS enabled? (Select TWO)\n",
  "answers" : [ {
    "id" : "a405f31a2dc745d0ab7be6f01f07bf71",
    "option" : "Access-Control-Allow-Headers",
    "isCorrect" : "true"
  }, {
    "id" : "6509f66624534100b6ff8f80655beb01",
    "option" : "Access-Control-Allow-CORS",
    "isCorrect" : "false"
  }, {
    "id" : "4bef7e532ce8495eac31e71dd5df6eb6",
    "option" : "Access-Control-Expose-Headers",
    "isCorrect" : "false"
  }, {
    "id" : "fd983f8c443144b09f1d33cf03556361",
    "option" : "Access-Control-Expose-Origin",
    "isCorrect" : "false"
  }, {
    "id" : "5e7dbcb9dced46bda841305c28f92a77",
    "option" : "Access-Control-Allow-Origin.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - A, E.\nTo support CORS, API resource needs to implement an OPTIONS method to respond to the OPTIONS preflight request with the following headers.\nAccess-Control-Allow-Headers.\nAccess-Control-Allow-Origin.\nAccess-Control-Allow-Methods.\nOption B, C &amp; Dare incorrect as both these headers are not required to be included as a part of the OPTIONS method.\nFor more information on enabling CORS on resource using API Gateway, refer to the following URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/enable-cors-for-resource-using-swagger-importer-tool.html\n\nThe answer is A. Access-Control-Allow-Headers and E. Access-Control-Allow-Origin.\nExplanation:\nWhen a web application running on a domain makes a request to a resource on a different domain, it is known as a Cross-Origin Resource Sharing (CORS) request. In such a scenario, a browser first sends a preflight request (OPTIONS) to the server, which in turn responds with headers that allow or deny access to the requested resource.\nThe following are the headers that need to be set up in each API method with CORS enabled:\nAccess-Control-Allow-Origin: This header is used to indicate which domain(s) are allowed to make requests to the resource. It can have the value of '*' to allow all domains or a specific domain name. Access-Control-Allow-Headers: This header is used to indicate which HTTP headers can be used during the actual request. If this header is not present, the browser will not send any custom headers in the request. Access-Control-Expose-Headers: This header is used to indicate which response headers can be accessed by the client-side JavaScript code. By default, only a few response headers are exposed to JavaScript.\nOption B (Access-Control-Allow-CORS) and D (Access-Control-Expose-Origin) are not valid CORS headers.\nTherefore, the correct answer is A. Access-Control-Allow-Headers and E. Access-Control-Allow-Origin.\n\n"
}, {
  "id" : 153,
  "question" : "You have a legacy application that works via XML messages.\nYou need to place the application behind the API gateway for customers to make API calls.\nWhich of the following would you need to configure?\n",
  "answers" : [ {
    "id" : "944ea44200024a1fbfd8d9197041c4d0",
    "option" : "Enable Payload compression.",
    "isCorrect" : "false"
  }, {
    "id" : "60fb3d0be3be47fc8a36ef22fc5bf8c4",
    "option" : "You will need to work with the Request and Response Data mapping.",
    "isCorrect" : "true"
  }, {
    "id" : "c0c5059b5bf7474698712e0aecc3ed4f",
    "option" : "Enable CORS.",
    "isCorrect" : "false"
  }, {
    "id" : "94ec0d3a72804e43b6d15b46c693a30a",
    "option" : "Enable multiple stages.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is also mentioned in the AWS Documentation.\nFor example, suppose that an API has an application/json template defined for a request payload and has an application/xml template defined for the response payload.\nIf the client sets the \"Content-Type : application/json\", and \"Accept : application/xml\" headers in the request, both the request and response payloads will be processed with the corresponding mapping templates.\nIf the Accept:application/xml header is absent, the application/xml mapping template will be used to map the response payload.\nTo return the response payload unmapped, you must set up an empty template for application/json.\nSince the documentation clearly mentions what should be the way to handle such requests, all other options are incorrect.\nFor more information on request-response data mappings, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/request-response-data-mappings.html\n\nTo place the legacy application behind the API gateway for customers to make API calls using XML messages, you would need to work with Request and Response Data Mapping. Therefore, option B is the correct answer.\nExplanation: API Gateway is a fully managed service provided by AWS that enables you to create, publish, monitor, and secure APIs at any scale. It supports multiple types of API endpoints like RESTful, WebSocket, HTTP, and Lambda function invocations.\nWhen it comes to integrating a legacy application that works via XML messages, you need to configure the Request and Response Data Mapping in API Gateway to transform the incoming and outgoing XML payloads into a format that can be processed by the application.\nThe Request and Response Data Mapping can be done in several ways, such as:\nUsing the mapping templates in API Gateway: API Gateway provides mapping templates that allow you to transform the incoming and outgoing payloads. You can create a mapping template that maps the incoming XML request to the format required by the application and similarly map the response received from the application to the XML format required by the client. Using AWS Lambda functions: You can use AWS Lambda functions to perform the data mapping. You can create a Lambda function that receives the incoming XML payload, transforms it into the required format, and sends it to the legacy application. Similarly, the Lambda function can receive the response from the application, transform it into the required XML format, and send it back to the client.\nRegarding the other options:\nOption A: Enabling Payload Compression is useful when you have large payloads to transmit, and you want to reduce the network bandwidth usage. However, it is not relevant to the given scenario. Option C: Enabling CORS is necessary when you want to allow web pages hosted in a different domain to access the API resources. However, it is not relevant to the given scenario. Option D: Enabling Multiple Stages is useful when you want to deploy multiple versions of your API in different stages, such as development, testing, and production. However, it is not relevant to the given scenario.\n\n"
}, {
  "id" : 154,
  "question" : "You are planning to use the Serverless Application model which will be used to deploy a serverless application consisting of a Node.js function.\nWhich of the following steps need to be carried out? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "d1d3d85e29d04ec3a48d4a5a1c50b46a",
    "option" : "Use the Lambda package command.",
    "isCorrect" : "false"
  }, {
    "id" : "f6cdf9de3f0a44f4b612ffd837210e22",
    "option" : "Use the SAM package command.",
    "isCorrect" : "true"
  }, {
    "id" : "513bfb541a4147e488b615f8b17f726e",
    "option" : "Use the Lambda deploy command.",
    "isCorrect" : "false"
  }, {
    "id" : "a2acafe8f9424f448ea94d65b753211e",
    "option" : "Use the SAM deploy command.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe AWS Documentation gives an example of this.\nHere you need to use the same commands and not the cloud formation commands.\nFor more information on serverless deployment, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/serverless-deploy-wt.html\n\n\nThe correct answers are B. Use the SAM package command and D. Use the SAM deploy command.\nThe Serverless Application Model (SAM) is an open-source framework that is used to build serverless applications on AWS. It is used to simplify the deployment of AWS Lambda functions, API Gateway, and DynamoDB tables.\nWhen you use SAM to deploy your application, the following steps need to be carried out:\nWrite your serverless application code: In this case, you need to write your Node.js function. Write your SAM template: This is a CloudFormation template that defines your serverless application. It describes the AWS resources required by your application, such as Lambda functions, API Gateway, and DynamoDB tables. Package your application: This step involves creating a deployment package that includes your Node.js function code and any dependent libraries. This can be done using the SAM package command, which packages your application and uploads it to an S3 bucket. Deploy your application: This step involves deploying your application to AWS. This can be done using the SAM deploy command, which creates the necessary resources in your AWS account and deploys your application.\nTherefore, the correct steps to be carried out are to use the SAM package command to package your application and then use the SAM deploy command to deploy it.\nOption A (Use the Lambda package command) is incorrect because the Lambda package command is used to package only the Lambda function code, and does not include the CloudFormation template required for deployment.\nOption C (Use the Lambda deploy command) is also incorrect because the Lambda deploy command is used to update the code of an existing Lambda function, and not to deploy a new serverless application.\n\n"
}, {
  "id" : 155,
  "question" : "Your application currently points to several Lambda functions in AWS.\nA change is being made to one of the Lambda functions.\nYou need to ensure that application traffic is shifted slowly from one Lambda function to the other.\nWhich of the following steps would you carry out? Select 2 Options.\n",
  "answers" : [ {
    "id" : "63376a13721b418798d65e056c80ff4c",
    "option" : "Create an ALIAS with the –routing-config parameter.",
    "isCorrect" : "true"
  }, {
    "id" : "485dad89715d4fb7bcd687313dc1265b",
    "option" : "Update the ALIAS with the –routing-config parameter.",
    "isCorrect" : "true"
  }, {
    "id" : "082116a97a8d4b9c9247a6ea6ec2195b",
    "option" : "Create a version with the –routing-config parameter.",
    "isCorrect" : "false"
  }, {
    "id" : "4293858bd7454d4099f57305d64c83e5",
    "option" : "Update the version with the –routing-config parameter.",
    "isCorrect" : "false"
  }, {
    "id" : "817bce07693d40de9ffdb21d45495ade",
    "option" : "Update the function with the - config parameter.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - A and B.\nThis is mentioned in the AWS Documentation.\nBy default, an alias points to a single Lambda function version.\nWhen the alias is updated to point to a different function version, incoming request traffic instantly points to the updated version.\nThis exposes that alias to any potential instabilities introduced by the new version.\nTo minimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions of the Lambda function and dictate what percentage of incoming traffic is sent to each version.\nOptions C and D are incorrect since you need to use ALIAS for this purpose.\nOption E is incorrect because A &amp; B are the correct ways to achieve the requirement.\nFor more information on shifting traffic using ALIAS, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html\n\nTo ensure a slow traffic shift from one Lambda function to another, you can use the AWS Lambda's built-in traffic shifting feature, which allows you to gradually shift traffic from one function version to another, or from one alias to another, using weighted percentages.\nTo implement the traffic shifting, you can perform the following steps:\nCreate a new version of the Lambda function that you want to shift traffic to. You can create a new version from the AWS Lambda console or by using the AWS CLI with the create-function command and the --publish parameter.\nExample CLI command to create a new version of a Lambda function:\ncss Copy code aws lambda create-function --function-name my-function \\ --code S3Bucket=my-bucket,S3Key=my-code.zip --runtime nodejs14.x \\ --handler index.handler --role my-execution-role --publish\nCreate an alias for the new version. An alias is a pointer to a specific version of a function. You can create an alias using the AWS Lambda console or by using the AWS CLI with the create-alias command.\nExample CLI command to create an alias for a Lambda function:\nr Copy code aws lambda create-alias --function-name my-function \\ --name prod --function-version 2\nUpdate the existing alias to gradually shift traffic to the new version. You can update the alias using the AWS Lambda console or by using the AWS CLI with the update-alias command and the --routing-config parameter.\nExample CLI command to update an alias with a routing configuration:\ncss Copy code aws lambda update-alias --function-name my-function \\ --name prod --routing-config '{\"AdditionalVersionWeights\":{\"3\":0.5}}'\nThe --routing-config parameter specifies the percentage of traffic that should be shifted to the new version (in this case, 50%). You can specify additional version weights to gradually shift traffic to multiple versions.\nOverall, to gradually shift traffic from one Lambda function to another, you need to create a new version of the function, create an alias for the new version, and update the existing alias with a routing configuration to gradually shift traffic to the new version.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 158,
  "question" : "AWS CodeDeploy deployment fails to start &amp; generates the following error code, ”HEALTH_CONSTRAINTS_INVALID.” Which of the following can be used to eliminate this error?\n",
  "answers" : [ {
    "id" : "37acf27ecff14531a9e4aa8a2b34ae16",
    "option" : "Make sure that the minimum number of healthy instances is equal to the total number of instances in the deployment group.",
    "isCorrect" : "false"
  }, {
    "id" : "2836ea5686b6456a866e08683153fe64",
    "option" : "Increase the number of healthy instances required during deployment.",
    "isCorrect" : "false"
  }, {
    "id" : "a2edf297e0694f218a376c19a95a4846",
    "option" : "Reduce number of \"healthy instances required\" to less than the total number of instances.",
    "isCorrect" : "true"
  }, {
    "id" : "c40f76d0eb93414a88360e3d62c8b15a",
    "option" : "Make sure that the minimum number of healthy instances is greater than the number of instances in the deployment group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS CodeDeploy generates ”HEALTH_CONSTRAINTS_INVALID” error, when a minimum number of healthy instances defined in the deployment group are not available during deployment.\nTo mitigate this error, make sure the required number of healthy instances are available during deployments.\nOption A is incorrect as During Deployment process, CodeDeploy tracks the health status of the instances in a deployment group.\nIt uses the deployment's specified minimum number of healthy instances to determine whether to continue the deployment.\nFor this, a minimum number of healthy instances should be less than &amp; not equal to the total number of instances in the deployment group.\nOption B is incorrect as to continue with deployment, you should increase the total number of instances in a deployment group compared to a minimum number of healthy instances.\nOption D is incorrect.\nThe number of healthy instances should be greater than &amp; not equal or less than a number of healthy instances specified in a minimum number of healthy instances.\nFor more information on AWS CodeDeploy Error Codes, refer to the following URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/error-codes.html\n\nThe error message \"HEALTH_CONSTRAINTS_INVALID\" in AWS CodeDeploy deployment occurs when the minimum number of healthy instances required for the deployment group is not set correctly. The minimum number of healthy instances is the number of instances that must be in a good health state during the deployment process.\nOption A suggests that the minimum number of healthy instances should be set equal to the total number of instances in the deployment group. However, this setting may not be practical for all deployment scenarios, especially when deploying to a large number of instances. Therefore, this option is not the correct solution.\nOption B suggests increasing the number of healthy instances required during deployment. This option may be useful in some cases, but it does not eliminate the error in all scenarios.\nOption C suggests reducing the number of healthy instances required to less than the total number of instances. This option may work in some cases, but it is not a recommended solution because it increases the risk of deployment failures.\nOption D suggests that the minimum number of healthy instances should be greater than the number of instances in the deployment group. This option is incorrect because it is not possible to have more healthy instances than the total number of instances in the deployment group.\nTherefore, the correct solution is option A, which is to set the minimum number of healthy instances equal to the total number of instances in the deployment group. This setting ensures that all instances in the deployment group are monitored and maintained during the deployment process.\n\n"
}, {
  "id" : 159,
  "question" : "You are developing an application that would be used to upload images from users.\nYou need to effectively store the images and also the name of the user who uploaded the image.\nHow would you accomplish this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "954bf984c98349129df395217332c0ca",
    "option" : "Store the images in DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "608e4cea990246828faa12aaa3a33a2f",
    "option" : "Store the images in S3.",
    "isCorrect" : "true"
  }, {
    "id" : "7eb9a3e494314edabb96cde0870c1865",
    "option" : "Store the name of the user in S3.",
    "isCorrect" : "false"
  }, {
    "id" : "7afe627da0ad4aa29ad355c31a40f20d",
    "option" : "Store the name of the user in DynamoD.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThis is also mentioned in the AWS Documentation.\nAs mentioned above, you can also take advantage of Amazon Simple Storage Service (Amazon S3) to store large attribute values that cannot fit in a DynamoDB item.\nYou can store them as objects in Amazon S3 and then store the object identifier in your DynamoDB item.\nOptions A and C are incorrect since it should be the other way around in terms of storage.\nFor more information on this use case, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html\n\nThe best approach for storing images and user information depends on the specific requirements of the application, but two possible solutions are:\nStore the images in S3 and the name of the user in DynamoDB.\nAmazon S3 (Simple Storage Service) is an object storage service that provides highly scalable and durable storage for any type of data, including images. S3 is designed for storing and retrieving large files, and provides a simple web services interface that can be used to upload and download objects from anywhere in the world.\nTo store user information such as the name of the user who uploaded the image, a NoSQL database like Amazon DynamoDB can be used. DynamoDB is a fully managed NoSQL database that provides fast and predictable performance with seamless scalability. It can be used to store and retrieve any amount of data, and supports key-value and document data models.\nBy storing the images in S3 and the user information in DynamoDB, the application can benefit from the scalability and performance of both services. S3 can handle the storage and retrieval of large files, while DynamoDB can provide fast and efficient access to user information.\nStore both the images and the name of the user in S3.\nAlternatively, both the images and the user information can be stored in S3 using object metadata to store the name of the user who uploaded the image. Object metadata is a set of name-value pairs that are associated with an S3 object. Metadata can be used to store any additional information about the object, such as the name of the user who uploaded it.\nBy storing both the images and user information in S3, the application can benefit from the simplicity and scalability of S3. However, retrieving user information may require additional processing of the object metadata, which may not be as efficient as using a dedicated database like DynamoDB.\nOverall, the best approach depends on the specific requirements of the application, such as the amount and type of data being stored, the expected traffic, and the need for efficient access to user information.\n\n"
}, {
  "id" : 160,
  "question" : "Your application must write to an SQS queue.\nYour corporate security policies require that AWS credentials are always encrypted and are rotated at least once a week.\nHow can you securely provide credentials that allow your application to write to the queue?\n",
  "answers" : [ {
    "id" : "de760199df4a4b72b0c94fe28590881d",
    "option" : "Have the application fetch an access key from an Amazon S3 bucket at run time.",
    "isCorrect" : "false"
  }, {
    "id" : "4ea07d8c63494510bf70968a762e48e7",
    "option" : "Launch the application`s Amazon EC2 instance with an IAM role.",
    "isCorrect" : "true"
  }, {
    "id" : "c08a218df8384245863233bfd702023b",
    "option" : "Embed the Access keys in the application.",
    "isCorrect" : "false"
  }, {
    "id" : "3c3ddcd6fe7c40c4932bb96f70363329",
    "option" : "Create environment variables in the EC2 Instance with the Access Keys.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is clearly mentioned in the AWS Documentation.\nIAM Roles for Amazon EC2\nApplications must sign their API requests with AWS credentials.\nTherefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on EC2 instances.\nFor example, you can securely distribute your AWS credentials to the instances.\nYou enable the applications on those instances to use your credentials to sign requests while protecting your credentials from other users.\nHowever, it's challenging to securely distribute credentials to each instance, especially those that AWS creates on your behalf, such as Spot Instances or instances in Auto Scaling groups.\nYou must also be able to update the credentials on each instance when you rotate your AWS credentials.\nWe designed IAM roles so that your applications can securely make API requests from your instances without requiring you to manage the security credentials that the applications use.\nInstead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles as follows:\nCreate an IAM role.\nDefine which accounts or AWS services can assume the role.\nDefine which API actions and resources the application can use after assuming the role.\nSpecify the role when you launch your instance, or attach the role to a running or stopped instance.\nHave the application retrieve a set of temporary credentials and use them.\nOption A is incorrect because storing the keys in the S3 bucket is not a good practice, and it may cause security issues.\nOption C is incorrect because storing the keys in the application could violate the security.\nIf the source code for the application is hosted on GitHub, everyone can access the keys.\nOption D is incorrect because it is unsuitable for storing keys as environment variables.\nIt may cause security issues.\nFor more information on IAM Roles, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\n\nOption A, having the application fetch an access key from an Amazon S3 bucket at run time, is not recommended as it requires the application to have access to the AWS credentials, which goes against the corporate security policy of encrypting and rotating credentials.\nOption C, embedding the access keys in the application, is also not recommended as this would mean the credentials are visible to anyone who has access to the application code or binary. This would violate the corporate security policy of keeping credentials encrypted.\nOption D, creating environment variables in the EC2 instance with the access keys, is not secure as it would allow anyone with access to the EC2 instance to view the credentials. This would violate the corporate security policy of keeping credentials encrypted.\nOption B, launching the application's Amazon EC2 instance with an IAM role, is the recommended way to securely provide credentials that allow the application to write to the SQS queue. IAM roles are a secure way to grant permissions to EC2 instances without the need for storing or sharing AWS credentials. IAM roles can be configured to have permissions to write to the SQS queue, and the EC2 instance can automatically obtain temporary credentials from the IAM role at runtime. The credentials obtained by the EC2 instance are rotated regularly, in accordance with the corporate security policy, and are encrypted by default.\nTherefore, the correct answer is B, launching the application's Amazon EC2 instance with an IAM role.\n\n"
}, {
  "id" : 161,
  "question" : "You are a developer for a company.\nYou have been asked to deploy an application for development purposes onto an Elastic beanstalk environment.\nYou need to ensure that custom software is installed on the backend Linux servers launched as part of the Elastic Beanstalk environment.\nWhich of the following can be used to achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9dbc521de9414ade8f8b9f17ba924990",
    "option" : "Create an XML file with the required package names to be installed on the server.",
    "isCorrect" : "false"
  }, {
    "id" : "df85b766ed8e49b09cf7325a0a89bc19",
    "option" : "Create an YAML file with the required package names to be installed on the server.",
    "isCorrect" : "true"
  }, {
    "id" : "b6e816330471426496d706c7a1cf539e",
    "option" : "Place the file in the .ebextensions folder in your Application Source Bundle.",
    "isCorrect" : "true"
  }, {
    "id" : "04fbc5029a624ffd9b41e8e0ed2652a0",
    "option" : "Place the file in the .config folder in your Application Source Bundle.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - B and C.\nThe AWS Documentation mentions the following.\nAWS Elastic Beanstalk supports many configuration options that let you modify the settings applied to resources in your environment.\nSeveral of these options have default values that can be overridden to customize your environment.\nOther options can be configured to enable additional features.\nElastic Beanstalk supports two methods of saving configuration option settings.\nConfiguration files in YAML or JSON format can be included in your application's source code in a directory named .ebextensions and deployed as part of your application source bundle.\nYou create and manage configuration files locally.\nOption A is invalid because the configuration file needs to be in YAML or JSON format.\nOption D is invalid because the configuration file needs to be placed in the .ebextensions folder.\nFor more information on the environment configuration options, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environment-configuration-methods-before.html\n\nTo ensure that custom software is installed on the backend Linux servers launched as part of the Elastic Beanstalk environment, we can use AWS Elastic Beanstalk extensions. Elastic Beanstalk extensions are simple configuration files that customize the environment and applications. These files can be written in YAML, JSON, or XML format.\nTo achieve this, we need to create a file with the required package names to be installed on the server and place it in the \".ebextensions\" folder in your application source bundle.\nThe correct answers are B and C: B. Create a YAML file with the required package names to be installed on the server. C. Place the file in the .ebextensions folder in your Application Source Bundle.\nThe YAML file can be named anything, but it must have the \".config\" extension. For example, we could create a file named \"install_packages.config\" and put it in the \".ebextensions\" folder in our application source bundle.\nHere is an example YAML file that installs a custom package on the backend Linux servers:\nyaml Copy code packages:   yum:     custom-package: []\nThis YAML file can be used to install the \"custom-package\" using the yum package manager. Similarly, we can use this file to install other packages as well.\nIn summary, to install custom software on the backend Linux servers launched as part of the Elastic Beanstalk environment, we need to create a YAML file with the required package names, place it in the \".ebextensions\" folder in our application source bundle, and then upload the source bundle to Elastic Beanstalk.\n\n"
}, {
  "id" : 162,
  "question" : "You are working on a decoupled application that uses Amazon SQS for storing messages before being processed by another application deployed on the Amazon EC2 instance.\nYou are observing a surge in the number of messages being processed by this application, most of which are duplicate messages.\nWhat change needs to be done if your application takes approximately 45 secs to process each message?\n",
  "answers" : [ {
    "id" : "5e0079da77f14f748df3242459d2295d",
    "option" : "Configure Amazon SQS Visibility timeout to 43200 seconds.",
    "isCorrect" : "false"
  }, {
    "id" : "dfdaeb17234d4ae5a9c42ca0b382c204",
    "option" : "Configure Amazon SQS Visibility timeout to 0 seconds.",
    "isCorrect" : "false"
  }, {
    "id" : "a0e6354a07ac4bc4afe39c687f539040",
    "option" : "Configure Amazon SQS Visibility timeout to 60 seconds.",
    "isCorrect" : "true"
  }, {
    "id" : "2669d1d6a3ac4ee3b387eefd94177087",
    "option" : "Configure Amazon SQS Visibility timeout to default to 30 seconds.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nVisibility Timeout is a period during which Amazon SQS prevents other consumers from receiving and processing the message.\nIn the above requirement, the application takes 45 secs to process each message.\nSo Visibility Timeout should be greater than 45 seconds.\nHence, setting visibility timeout as 60 seconds is the correct option.\nOption A is incorrect.\nEven though setting Amazon SQS Visibility timeout to 43200 seconds will work, it will add delay as it must wait for 12 hours if initial message processing fails.\nOption B is incorrect since message processing takes 45 secs.\nSetting a visibility timeout of 0 seconds will increase duplicate messages as other consumers will start processing the same message when the first consumer is processing it.\nOption D is incorrect since message processing takes 45 secs.\nSetting a visibility timeout of 30 seconds will increase duplicate messages as other consumers will start processing the same message when the first consumer is processing it.\nFor more information on visibility timeout on Amazon SQS, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/working-with-messages.html#processing-messages-timely-manner\n\nTo answer this question, we need to understand the role of Amazon SQS and visibility timeout in processing messages.\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It allows you to send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\nWhen you send a message to an SQS queue, it is stored in the queue until a consumer retrieves it. When a consumer retrieves a message from a queue, the message becomes invisible to other consumers for a configurable period known as the visibility timeout. During this time, the consumer processes the message and then either deletes it from the queue or returns it to the queue if processing fails. If the message is not deleted or returned before the visibility timeout expires, it becomes visible to other consumers again.\nIn this scenario, the decoupled application uses Amazon SQS to store messages before being processed by another application on an EC2 instance. However, there is a surge in the number of messages being processed, and most of them are duplicates. If the application takes approximately 45 seconds to process each message, we need to adjust the visibility timeout to avoid processing duplicate messages.\nOption A: Configure Amazon SQS Visibility timeout to 43200 seconds. Setting the visibility timeout to 43200 seconds (12 hours) is not a good option because it is too long. If a consumer takes more than 12 hours to process a message, it will become visible again, and other consumers will try to process it, resulting in duplicate processing. This option would not address the problem of duplicate messages being processed.\nOption B: Configure Amazon SQS Visibility timeout to 0 seconds. Setting the visibility timeout to 0 seconds is also not a good option because it would make the messages immediately visible to all consumers, resulting in multiple consumers processing the same message at the same time, which would lead to concurrency issues and data inconsistency.\nOption C: Configure Amazon SQS Visibility timeout to 60 seconds. Setting the visibility timeout to 60 seconds is a better option because it gives the consumer enough time to process the message without making it visible to other consumers. This way, if a consumer fails to process the message within the timeout period, the message will become visible again, and another consumer can try to process it. This option would reduce the number of duplicate messages being processed.\nOption D: Configure Amazon SQS Visibility timeout to default to 30 seconds. The default visibility timeout for SQS is 30 seconds, which means that if we do not explicitly set the visibility timeout, messages will become visible again after 30 seconds. This option would be similar to Option C, but with a shorter visibility timeout period. It could be used if the application can process messages within 30 seconds, but in this scenario, the application takes 45 seconds to process each message.\nTherefore, Option C would be the best option to configure the Amazon SQS visibility timeout to 60 seconds to reduce the number of duplicate messages being processed while giving the consumer enough time to process the message.\n\n"
}, {
  "id" : 163,
  "question" : "A company is planning to develop an IoT application that will make use of a DynamoDB table.\nThe structure of the table is given below. Attribute Name Type Description Device ID String Automatically generated GUID Device type String The type name of the IoT device Location String Place/ Area Status code String The current status of the IoT device Which of the following should be chosen as the partition key to ensure the MOST effective distribution of keys?\n",
  "answers" : [ {
    "id" : "fc3c0b1fe4ae48dbbce2770d7a06a320",
    "option" : "Device ID",
    "isCorrect" : "true"
  }, {
    "id" : "f7eed531524a46f1926546c7ea2410d9",
    "option" : "Device type",
    "isCorrect" : "false"
  }, {
    "id" : "5d0658408d9b4022bc3f5d92ebee7784",
    "option" : "Location",
    "isCorrect" : "false"
  }, {
    "id" : "a0c395692d0d448f84e701d34811144f",
    "option" : "Status code.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe most effective one will be the Device ID since this would ideally be unique and give a better key partition.\nBecause of GUID provides programmatically unique Identity.\nFor more information on DynamoDB, please refer to the below URLs-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html,\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html\n\nTo ensure the most effective distribution of keys in a DynamoDB table, it is important to choose a partition key that distributes the data evenly across partitions. The partition key determines the partition where an item will be stored, and evenly distributing items across partitions helps to avoid hot partitions, which can result in uneven performance and increased latency.\nIn the given table structure, the best choice for the partition key would be a key that is highly diverse and evenly distributed across the table. Based on the information provided, the Device ID is an automatically generated GUID, and it is likely that this key will be unique for each item. Choosing Device ID as the partition key would distribute the data evenly across partitions, ensuring optimal performance.\nThe other attributes in the table, such as Device type, Location, and Status code, are unlikely to be as diverse and evenly distributed as the automatically generated GUID in Device ID. For example, there may be only a few device types, a limited number of locations, and a small set of status codes. Choosing any of these attributes as the partition key may result in uneven distribution of data across partitions, leading to hot partitions and increased latency.\nTherefore, the best choice for the partition key in this scenario is A. Device ID.\n\n"
}, {
  "id" : 164,
  "question" : "You are a developer who has been hired to lead the development of a new application.\nThe application needs to interact with a backend data-store.\nThe application also needs to perform many complex join operations.\nWhich of the following would be the ideal data-store option? (Select Two)\n",
  "answers" : [ {
    "id" : "d1e77e38d189453e8a85e21abbac76d7",
    "option" : "AWS DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "b12230a943014e73881146c6ead0221b",
    "option" : "AWS RDS",
    "isCorrect" : "true"
  }, {
    "id" : "3de89fcfd9fb4e378296021d57a32639",
    "option" : "AWS Redshift",
    "isCorrect" : "true"
  }, {
    "id" : "8e32797e9ba747f2af34b1646b4b69ae",
    "option" : "AWS S3",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nAmazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the cloud.\nIt provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.\nSince you need complex query design, it is better to choose one of the available relational database services.\nAmazon Redshift is an Internet hosting service and data warehouse product that forms part of the larger cloud-computing platform Amazon Web Services.\nIt is built on top of technology from the massive parallel processing data warehouse company ParAccel, to handle large scale data sets and database migrations.\nBoth of the options above support complex joins.\nOption A is incorrect since AWS DynamoDB does not support complex joins.\nOption D is incorrect since this is used for Object-level storage.\nFor more information on AWS RDS, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Welcome.html\nFor more information on AWS Redshift-\nhttps://aws.amazon.com/redshift/\n\nOf the options provided, AWS RDS and AWS Redshift would be the ideal data-store options to consider for an application that needs to perform many complex join operations.\nAWS RDS (Relational Database Service) is a managed service that provides scalable relational databases in the cloud. It supports multiple popular database engines such as MySQL, PostgreSQL, Oracle, SQL Server, and Amazon Aurora. RDS is a good option for applications that require ACID-compliant transactions, multi-AZ deployment for high availability, and read replicas for scaling read-heavy workloads. RDS is well-suited for applications that require complex join operations, as it supports SQL queries and allows for joins across multiple tables.\nAWS Redshift is a fully managed data warehouse service that can handle petabyte-scale data warehouses. Redshift uses columnar storage to optimize query performance and provides a massively parallel processing architecture that can parallelize and distribute queries across multiple nodes. Redshift also integrates with popular BI tools, such as Tableau and Power BI. Redshift is a good option for applications that need to perform complex analytics queries, including join operations across multiple tables.\nAWS DynamoDB is a NoSQL database service that provides fast and flexible document and key-value storage. DynamoDB is well-suited for applications that require low-latency and high-throughput data access, but it does not support SQL queries or joins across multiple tables. DynamoDB is a good option for applications that require predictable performance, auto-scaling, and seamless integration with other AWS services.\nAWS S3 (Simple Storage Service) is a highly scalable and durable object storage service that can store and retrieve any amount of data. However, S3 is not a database and does not support SQL queries or join operations. S3 is a good option for storing and retrieving unstructured data, such as images, videos, and log files.\nTherefore, the ideal data-store options for an application that needs to perform many complex join operations are AWS RDS and AWS Redshift.\n\n"
}, {
  "id" : 165,
  "question" : "Your company is planning to store documents in an S3 bucket.\nThe documents are sensitive, and employees should use Multi-Factor authentication when trying to access documents.\nWhich of the following must be done to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "b0da296367ca456c842f6c9f4a0e1a37",
    "option" : "Ensure that Encryption is enabled the bucket AWS server-side encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "4174c62a76354ab3a517db2913e5a495",
    "option" : "Ensure that Encryption is enabled the bucket using KMS keys.",
    "isCorrect" : "false"
  }, {
    "id" : "42fe495bf70544e19e1431462ffa3468",
    "option" : "Ensure that the a bucket policy is in place with a condition of \"aws:MultiFactorAuthPresent\":\"false\" with a Deny policy.",
    "isCorrect" : "true"
  }, {
    "id" : "7eba3edda594470ab00b8c7e9589c2b4",
    "option" : "Ensure that the a bucket policy is in place with a condition of \"aws:MultiFactorAuthPresent\":\"true\" with a Deny policy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation gives an example of adding a bucket policy.\nIt ensures that only if users are MFA authenticated, they will have access to the bucket.\nOptions A and B are incorrect since the question talks about MFA and not encryption.\nOption D is incorrect since aws:MultiFactorAuthPresent should be checked against the false value for a Deny policy.\nFor more information on this use case scenario, please refer to the below URL-\nhttps://aws.amazon.com/premiumsupport/knowledge-center/enforce-mfa-other-account-access-bucket/\n\n\nThe correct answer is D. Ensure that a bucket policy is in place with a condition of \"aws:MultiFactorAuthPresent\":\"true\" with a Deny policy.\nExplanation: To fulfill the requirement of allowing access to sensitive documents in an S3 bucket only with Multi-Factor authentication, a bucket policy should be created with a condition that checks for the presence of Multi-Factor authentication.\nA bucket policy is a JSON-based document that enables access control to S3 buckets and its contents. It provides fine-grained control over the access to S3 objects, such as read/write permissions, IP restrictions, and other parameters. In this case, the bucket policy should be set to deny access to any user who does not have Multi-Factor authentication enabled.\nOption A is incorrect because enabling server-side encryption does not ensure that Multi-Factor authentication is required to access the documents in the bucket. Server-side encryption only protects the data at rest.\nOption B is incorrect because enabling encryption with KMS keys only provides an additional layer of security to protect the data. It does not ensure that Multi-Factor authentication is required to access the documents in the bucket.\nOption C is incorrect because the condition \"aws:MultiFactorAuthPresent\":\"false\" with a Deny policy would deny access to all users who have Multi-Factor authentication enabled. It is the opposite of the requirement.\nTherefore, option D is the correct answer as it ensures that the bucket policy checks for the presence of Multi-Factor authentication before granting access to the documents in the bucket.\n\n"
}, {
  "id" : 166,
  "question" : "AWS CodeDeploy is used to configure adeployment group to automatically roll-back to the last known good revision when a deployment fails.\nDuring roll-back, files required for deployment to earlier revision cannot be retrieved by AWS CodeDeploy.\nWhich of the following actions can be executed for successful roll-back? Choose 2 correct options.\n",
  "answers" : [ {
    "id" : "d56b11e639324752926fbcca70633a5b",
    "option" : "Use Manual Roll Back instead of Automatic Roll-Back.",
    "isCorrect" : "false"
  }, {
    "id" : "75bb31e495e741cda5371a74967e4778",
    "option" : "Manually Add required files to Instance.",
    "isCorrect" : "true"
  }, {
    "id" : "bf9f91f362034d849e187dbcb1874cef",
    "option" : "Use an existing application revision.",
    "isCorrect" : "false"
  }, {
    "id" : "9044f865bff74cb48380b4de149c5046",
    "option" : "Map CodeDeploy to access those files from S3 buckets.",
    "isCorrect" : "false"
  }, {
    "id" : "731c060cc7f0461db5859cd70f37a9c7",
    "option" : "Create a new application revision.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer -B, E.\nDuring AWS CodeDeploy automatic roll-back, it will try to retrieve files that were part of previous versions.\nIf these files are deleted or missing, you need to manually add those files to Instance or create a new application revision.\nOption A is incorrect as if files required for deployment are overwritten by earlier deployment.\nFiles will not be available for CodeDeploy &amp; deployment will fail even in case of Manual roll-back.\nOption C is incorrect as AWS CodeDeploy will not find missing files using an existing application revision.\nOption D is incorrect as CodeDeploy does not automatically access these files from S3 buckets.\nFor more information on CodeDeploy Rollback, refer to the following URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployments-rollback-and-redeploy.html\n\nAWS CodeDeploy is a fully-managed deployment service that automates software deployments to various computing services such as Amazon EC2 instances, AWS Fargate, AWS Lambda, and on-premises servers. It simplifies the deployment process by automating the deployment process and reducing the deployment time. One of the key features of AWS CodeDeploy is the ability to roll-back to the last known good revision in case of deployment failures.\nHowever, during the roll-back process, files required for deployment to the earlier revision cannot be retrieved by AWS CodeDeploy. To perform a successful roll-back, you need to perform one of the following actions:\nUse Manual Roll-Back instead of Automatic Roll-Back: AWS CodeDeploy provides two types of roll-back mechanisms: Automatic and Manual. By default, the roll-back process is set to Automatic. In this case, AWS CodeDeploy automatically rolls back to the last known good revision when a deployment failure occurs. If files required for deployment to an earlier revision are missing, you can switch to Manual Roll-Back. In this case, you need to manually select the revision to which you want to roll back. Manual Roll-Back provides more control over the roll-back process, and you can choose the revision that has all the required files. Create a new application revision: If you cannot retrieve the required files from the previous revision, you can create a new application revision that contains all the required files. You can use the AWS CodeDeploy CLI, API, or Console to create a new revision. Once you have created a new revision, you can deploy it to your instances. This ensures that all the required files are available during the deployment process.\nIn conclusion, if files required for deployment to an earlier revision are missing, you can switch to Manual Roll-Back or create a new application revision that contains all the required files.\n\n"
}, {
  "id" : 167,
  "question" : "An application is being designed to make use of DynamoDB.\nAs per the requirements, the application will read the DynamoDB items of 6 KB of size per second.\nThe number of requests per second is estimated to be around 10\nIf strong consistency is required, what should be the read capacity set for the table?\n",
  "answers" : [ {
    "id" : "6f5a0e4521c74702a3937ddc24397d31",
    "option" : "5",
    "isCorrect" : "false"
  }, {
    "id" : "6c4e85fab9094042bb98aeab4a2acbf3",
    "option" : "10",
    "isCorrect" : "false"
  }, {
    "id" : "3b9769b3af844c0bb9d11b8f7b234d02",
    "option" : "20",
    "isCorrect" : "true"
  }, {
    "id" : "2de414cf91074bf2b66187e07fc3f458",
    "option" : "40",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe calculation of throughput capacity for the table would be as follows.\nSince 6KB's is the item size, we need to consider it in chunks of 4KB.\nHence that would be 2.\nSince there are around 10 requests per second, that means = 2*10=20.\nSince it's required at a strong consistency level, the read capacity would be 20.\nBased on the calculations, all other options are incorrect.\nFor more information on DynamoDB throughput, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html\n\nTo determine the read capacity required for the DynamoDB table to satisfy the given requirements, we need to consider the following:\nItem size: 6 KB Read requests per second: 10 Strong consistency required\nFor strongly consistent reads, DynamoDB returns a response with the most up-to-date data, reflecting all writes that received a successful response prior to the read. Strongly consistent reads require more capacity compared to eventually consistent reads, which return a response with the latest data that is usually a few seconds behind the actual write.\nThe formula to calculate the read capacity required for strongly consistent reads is:\nRequired read capacity = (item size / 4 KB) * (read requests per second)\nNote that DynamoDB measures throughput capacity in terms of read capacity units (RCUs) and write capacity units (WCUs). One RCU represents one strongly consistent read per second, or two eventually consistent reads per second, of an item up to 4 KB in size.\nUsing the formula above, we can calculate the required read capacity:\nRequired read capacity = (6 KB / 4 KB) * 10 Required read capacity = 15\nThis means that we need a read capacity of 15 RCUs to satisfy the given requirements.\nFrom the available answer options, the closest match to the required read capacity is option C, which is 20. Option D, which is 40, is twice the required read capacity and therefore not necessary. Option A, which is 5, is too low and would not meet the application's requirements. Option B, which is 10, is the same as the estimated read requests per second and does not take into account the item size, so it is also too low.\nTherefore, the correct answer is option C, which is 20.\n\n"
}, {
  "id" : 168,
  "question" : "You've created a Lambda function with the default settings.\nYou add code to this function which makes calls to DynamoDB.\nYou try and deploy the function.\nBut the function is not completing its execution.\nWhich of the following might be probable causes for this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "7142934c0de44f81874548657e94a371",
    "option" : "The IAM Role attached to the function does not have access to DynamoD.",
    "isCorrect" : "true"
  }, {
    "id" : "b36eaef84f9742cb9b6e034fe1424d88",
    "option" : "The timeout of the function has been reached.",
    "isCorrect" : "true"
  }, {
    "id" : "0a1ae5bcef5d49e6ade156ce52650bd1",
    "option" : "You need to deploy the function first.",
    "isCorrect" : "false"
  }, {
    "id" : "13d23246f4e040febf918fb873155485",
    "option" : "You need to create a version for the function first.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThese are given as some of the requirements in the AWS Documentation.\nOption C is incorrect since deployment is not needed from the AWS Console.\nOption D is incorrect since this is not a pre-requisite for the function to run.\nFor more information on the AWS Lambda resource model, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/resource-model.html\n\n\nThe probable causes for the Lambda function not completing its execution after adding code to make calls to DynamoDB are:\nA. The IAM Role attached to the function does not have access to DynamoDB: Lambda functions need an IAM role with appropriate permissions to interact with other AWS resources. If the IAM role attached to the function does not have access to DynamoDB, then it will not be able to make calls to it. Therefore, it's important to check whether the IAM role has the necessary permissions for DynamoDB operations.\nB. The timeout of the function has been reached: Lambda functions have a default timeout of 3 seconds, after which they will terminate the execution. If the function is making calls to DynamoDB, it may take longer to execute, which can cause the function to timeout before completing the execution. Therefore, it's important to check whether the function's timeout value is sufficient for the expected execution time.\nC. You need to deploy the function first: This option is incorrect. Lambda functions can only be invoked after they have been deployed. Therefore, if you are trying to invoke a function that has not been deployed, you will get an error message stating that the function does not exist.\nD. You need to create a version for the function first: This option is also incorrect. Creating a version for the function is not necessary for it to be deployed and executed. However, creating a version can be useful for tracking changes made to the function code and its configuration.\nTo resolve the issue, you should ensure that the IAM role attached to the Lambda function has the necessary permissions for DynamoDB operations. Additionally, you should increase the function's timeout value if it's making calls to DynamoDB, and the expected execution time is longer than the default timeout value.\n\n"
}, {
  "id" : 169,
  "question" : "You've just configured a Lambda function that sits behind the API gateway service.\nWhen you try to invoke the Lambda function via the API gateway service from Javascript in your HTML page, you receive the following error. No 'Access-Control-Allow-Origin' header is present on the requested resource.\nOrigin 'null' is therefore not allowed access. What can be done to resolve this error?\n",
  "answers" : [ {
    "id" : "b85259bd91a0436f94abf888d0c0fb23",
    "option" : "Enable CORS for the lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "be382dba40b2458db4bec1a02ac17eeb",
    "option" : "Enable CORS for the methods in the API gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "0583fac87c4e4567831d8c19115543de",
    "option" : "Change the IAM policy for the Lambda function to enable anonymous access.",
    "isCorrect" : "false"
  }, {
    "id" : "e0b0bd0bb15440ffb8368eff4b5c85d4",
    "option" : "Change the IAM policy for the API gateway to enable anonymous access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nWhen your API's resources receive requests from a domain other than the API's own domain, you must enable cross-origin resource sharing (CORS) for selected methods on the resource.\nThis amounts to having your API respond to the OPTIONS preflight request with at least the following CORS-required response headers:\nAccess-Control-Allow-Methods.\nAccess-Control-Allow-Headers.\nAccess-Control-Allow-Origin.\nOption A is incorrect because CORS is set on the API gateway level and not the Lambda function.\nOptions C and D are incorrect since IAM Policy is not the reason as to why the error is occurring.\nFor more information on CORS for the API gateway, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\n\nThe error message indicates that the API Gateway service has denied access to the Lambda function due to missing Access-Control-Allow-Origin header. This is a Cross-Origin Resource Sharing (CORS) error that occurs when a webpage makes a request to a resource on a different domain or port.\nTo resolve this error, you need to configure CORS for the API Gateway service. CORS is a security feature that allows web pages to request resources from different domains without compromising security.\nOption A: Enable CORS for the Lambda function This option is not correct because the CORS configuration needs to be done for the API Gateway service, not the Lambda function.\nOption B: Enable CORS for the methods in the API Gateway This option is the correct answer. To enable CORS for the API Gateway service, you need to add the necessary headers to the HTTP response returned by the API Gateway. This can be done by enabling CORS for the API Gateway methods using the following steps:\nOpen the API Gateway console and select your API. Select the resource and method you want to enable CORS for. Click on the 'Actions' drop-down menu and select 'Enable CORS'. This will create a new CORS configuration for the selected method. You can modify the default configuration if necessary. Save the CORS configuration and redeploy the API.\nOption C: Change the IAM policy for the Lambda function to enable anonymous access This option is not correct because enabling anonymous access would not resolve the CORS error. The error is related to the HTTP headers and not the Lambda function access policy.\nOption D: Change the IAM policy for the API gateway to enable anonymous access This option is not correct because enabling anonymous access would not resolve the CORS error. The error is related to the HTTP headers and not the API Gateway access policy.\nIn summary, to resolve the No Access-Control-Allow-Origin header is present on the requested resource error, you need to enable CORS for the API Gateway methods that the Lambda function is attached to. This can be done by configuring the necessary headers in the HTTP response returned by the API Gateway.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 171,
  "question" : "You are planning on hosting a static website using the features available with S3\nWhich of the following steps need to be carried out to ensure that you can host your static website in S3? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "503d00c3367340949a5fc764fc281025",
    "option" : "Enable WebSite hosting.",
    "isCorrect" : "true"
  }, {
    "id" : "b01d3ded1d424af6b4af93d3b29112c0",
    "option" : "Enable versioning for the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "85eadb8e05b5447ea4129f538ee8b024",
    "option" : "Configure an Index document.",
    "isCorrect" : "true"
  }, {
    "id" : "e4b20e4abc2c48d0b74c1a110efe3ea2",
    "option" : "Ensure that permissions are set for Website access.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, C and D.\nThis is given in the AWS Documentation.\nConfiguring a Bucket for Website Hosting.\nYou can host a static website in an Amazon Simple Storage Service (Amazon S3) bucket.\nHowever, to do so requires some configuration.\nSome optional configurations are also available, depending on your website requirements.\nRequired configurations:\nEnabling Website Hosting.\nConfiguring Index Document Support.\nPermissions Required for Website Access.\nOption B is invalid since this is not a pre-requisite to have web sites hosted in S3.\nFor more information on S3 web site hosting, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/HowDoIWebsiteConfiguration.html\n\nTo host a static website in S3, the following three steps need to be carried out:\nEnable WebSite Hosting: This is the first step in hosting a static website in S3. You need to enable website hosting on the S3 bucket that you want to use to host your website. This can be done by navigating to the bucket properties and selecting the \"Static website hosting\" option. Once enabled, you need to specify the index document and error document for your website. The index document is the file that S3 serves when someone accesses your website's root URL. Configure an Index Document: Once you have enabled website hosting, you need to configure an index document for your website. The index document is the file that S3 serves when someone accesses your website's root URL. You can configure the index document by specifying the name of the file that you want to use as your website's index document. Ensure that Permissions are set for Website access: To make your website accessible to the public, you need to ensure that permissions are set for website access. You can do this by creating a bucket policy that grants read access to everyone. Once you have set the bucket policy, anyone can access your website using its URL.\nTherefore, the correct answers are: A. Enable WebSite Hosting. C. Configure an Index Document. D. Ensure that Permissions are set for Website access.\n\n"
}, {
  "id" : 172,
  "question" : "You are developing an application that will use Kinesis Firehose to stream the records onto the Simple Storage Service.\nYour company policy mandates that all data needs to be encrypted at rest.\nHow can you achieve this with Kinesis Firehose? Choose 2 answers for the options given below.\n",
  "answers" : [ {
    "id" : "d9214b1288f5410ca8d30d38c8d2ba38",
    "option" : "Enable Encryption on the Kinesis Data Firehose.",
    "isCorrect" : "true"
  }, {
    "id" : "b2fa8678551249eab506277ed27dbe69",
    "option" : "Install an SSL certificate in Kinesis Data Firehose.",
    "isCorrect" : "false"
  }, {
    "id" : "9ca8dca4af8646eeb04e8619f846a9d2",
    "option" : "Ensure that all data records are transferred via SSL.",
    "isCorrect" : "false"
  }, {
    "id" : "a6dbd93d3ed741c989091fc0a423c0be",
    "option" : "Ensure that Kinesis streams are used to transfer the data from the producers.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThis is given in the AWS Documentation.\nOption A is correct because you can enable encryption ( encryption of data at rest ) for Kinesis Data Firehose.\nOption D is correct because when Kinesis Streams are chosen as source, then encryption of data at rest is enabled automatically.\nOptions B and C are invalid because this is used for encrypting data in transit.\nFor more information on Data encryption with Kinesis Firehose, please refer to the below URL-\nhttps://docs.aws.amazon.com/firehose/latest/dev/encryption.html\n\nThe correct answers to this question are A and C.\nKinesis Firehose is a managed service that allows you to capture, transform, and load streaming data into AWS services such as S3, Redshift, and Elasticsearch. To ensure that data is encrypted at rest when streamed through Kinesis Firehose, the following steps can be taken:\nA. Enable Encryption on the Kinesis Data Firehose: Kinesis Firehose provides an option to enable Server-Side Encryption (SSE) using Amazon S3-Managed Keys (SSE-S3) or AWS Key Management Service (KMS) customer master keys (CMKs). When SSE-S3 is selected, Kinesis Firehose encrypts data at rest using S3-managed keys. When KMS CMKs are used, Kinesis Firehose encrypts the data using a customer-managed key that is stored in KMS.\nC. Ensure that all data records are transferred via SSL: SSL/TLS is a protocol used to secure data in transit by providing encryption and authentication between clients and servers. When transferring data from Kinesis Data Firehose to S3, ensure that SSL/TLS is used to secure the data in transit. Kinesis Firehose supports SSL/TLS encryption for the delivery of data to S3.\nB. Install an SSL certificate in Kinesis Data Firehose: Installing an SSL certificate in Kinesis Data Firehose is not a valid option because SSL certificates are not used to encrypt data at rest. SSL certificates only encrypt data in transit between clients and servers.\nD. Ensure that Kinesis streams are used to transfer the data from the producers: Using Kinesis streams to transfer data from producers is not a valid option because Kinesis streams are a separate AWS service that provides real-time streaming data processing. Kinesis Firehose is designed for capturing and delivering streaming data to storage and analytics services.\n\n"
}, {
  "id" : 173,
  "question" : "You are the team lead for an application.\nYou have been instructed to make use of Jenkins as the Build provider in AWS CodePipeline.\nWhich of the following options is the best practice?\n",
  "answers" : [ {
    "id" : "059c7c37818a4a4fbc23bbad71d2c32e",
    "option" : "Install Jenkins on an EC2 instance and make sure the instance role has the \"codepipeline:*\" permission.",
    "isCorrect" : "false"
  }, {
    "id" : "8e5f39d02b3340a7ab6ffd67eafb2ed3",
    "option" : "Add Access Keys in EC2 for the Jenkins server to access CodePipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "1f4691605eb94c9eb141119674e9d2de",
    "option" : "Install Jenkins on an Amazon EC2 instance. Make sure the instance profile grants Jenkins, only the AWS permissions required to perform tasks for your project.",
    "isCorrect" : "true"
  }, {
    "id" : "8b27bdd584aa400f8312932d5b24c4ed",
    "option" : "Install Jenkins on a Lambda function so that it is serverless and cost-effective.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is given in the AWS Documentation.\nAs a best practice, when you use a Jenkins build provider for your pipeline's build or test action, install Jenkins on an Amazon EC2 instance and configure a separate EC2 instance profile.\nMake sure the instance profile grants Jenkins, only the AWS permissions required to perform tasks for your project, such as retrieving files from Amazon S3.\nThe instance profile provides applications running on an Amazon EC2 instance with the credentials to access other AWS services.\nAs a result, you do not need to configure AWS credentials (AWS access key and secret key).\nOption A is incorrect because the \"codepipeline:*\" permission is not suitable as it allows more actions than required.\nOption B is incorrect because an instance profile should be used to assign the permissions for Jenkins.\nThere is no need to add access keys.\nOption D is incorrect because when adding Jenkins as the build provider in AWS CodePipeline, users cannot install Jenkins on a Lambda function.\nFor more information on best practices of AWS CodePipeline, see below.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/best-practices.html\n\nThe best practice for integrating Jenkins as the Build provider in AWS CodePipeline is option C, which involves installing Jenkins on an Amazon EC2 instance and configuring its instance profile to grant only the AWS permissions required to perform tasks for your project.\nOption A is not a recommended approach because granting the EC2 instance role the \"codepipeline:*\" permission provides it with too many permissions, which can result in security risks. It's generally best practice to provide the minimum required permissions for a given task to reduce potential attack surface.\nOption B is also not recommended as it involves managing access keys for the Jenkins server, which can be a security risk if not properly secured. Access keys can be used by unauthorized individuals to gain access to AWS resources if they are stolen or compromised.\nOption D is not a feasible approach because Jenkins requires a server environment to run, which is not provided by AWS Lambda. Lambda functions are designed for short-lived, event-driven computing tasks, while Jenkins requires a persistent, long-running environment.\nTherefore, the recommended approach is to use option C, where Jenkins is installed on an EC2 instance and its instance profile is configured to grant only the required permissions for the specific project. This ensures that the Jenkins server only has the necessary permissions to perform its designated tasks and reduces potential security risks.\n\n"
}, {
  "id" : 174,
  "question" : "You are developing a system that will be sending messages to an SQS queue.\nAnother application will be running on an EC2 Instance that will be used to process the messages.\nWhich of the following are BEST practices for making COST effective use of the SQS queues? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "458a9620a9694500a8f36904fccd9b4c",
    "option" : "Use short polling for SQS queues.",
    "isCorrect" : "false"
  }, {
    "id" : "18b9a7de09b84bf985364f3a60878c92",
    "option" : "Use long polling for SQS queues.",
    "isCorrect" : "true"
  }, {
    "id" : "e3ea7fc79923457d8296f73e016866de",
    "option" : "Group the SQS API operations in batches.",
    "isCorrect" : "true"
  }, {
    "id" : "68ef0d45a3f44513baf9a6df045e1baf",
    "option" : "Use single queue operations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThis is given in the AWS Documentation.\nReducing Amazon SQS Costs.\nThe following best practices can help you reduce costs and take advantage of additional potential cost reduction and near-instantaneous response.\nBatching Message Actions.\nTo reduce costs, batch your message actions:\nTo send, receive, and delete messages, and to change the message visibility timeout for multiple messages with a single action, use the Amazon SQS batch API actions.\nTo combine client-side buffering with request batching, use long polling together with the buffered asynchronous client included with the AWS SDK for Java.\nNote.\nThe Amazon SQS Buffered Asynchronous Client doesn't currently support FIFO queues.\nBecause of what is mentioned in the AWS Documentation as best practices, other options are invalid.\nFor more information on reducing costs for SQS, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/reducing-costs.html\n\nTo make cost-effective use of SQS queues when sending messages to an EC2 instance for processing, the following two best practices should be followed:\nUse Long Polling for SQS Queues: Long polling is a technique that allows the SQS queue to wait for a specified time for messages to arrive in the queue before returning a response. This reduces the number of requests made to the SQS queue, which in turn reduces costs. Long polling also increases the efficiency of the message processing application on the EC2 instance by reducing the amount of idle time between message retrievals. Hence, using long polling is a cost-effective way of managing SQS queues. Group the SQS API Operations in Batches: When sending messages to an SQS queue, grouping the API operations in batches reduces the number of requests made to the SQS queue, which in turn reduces costs. Batch operations also help to improve message throughput by reducing the time taken to send messages to the queue. Therefore, grouping the SQS API operations in batches is another cost-effective way of managing SQS queues.\nOptions A and D are incorrect because short polling and single queue operations do not reduce the number of requests made to the SQS queue and are, therefore, not cost-effective.\nHence, the correct answers are B and C.\n\n"
}, {
  "id" : 175,
  "question" : "You have created anew provisioned table in DynamoDB.\nYou want to read 60 items per second from this table, with each item having 6 KB size.\nAlso, you want to write 80 items per second, with each item 1.5 KB in size.\nWhich of the following will provisioned read capacity units for strongly consistent read per second &amp; write capacity units will be set to avoid request throttling? (Select Two)\n",
  "answers" : [ {
    "id" : "6922bfd7bafd48c8aa0e839aec69248e",
    "option" : "240 WCU",
    "isCorrect" : "false"
  }, {
    "id" : "96f02a9c242a4b6c9a0c6dcbe99fb182",
    "option" : "120 RCU",
    "isCorrect" : "true"
  }, {
    "id" : "5b789000f0b34e2fb60a62f243612448",
    "option" : "80 WCU",
    "isCorrect" : "false"
  }, {
    "id" : "f96faa5c2b324b519d38e9e6d1c219e3",
    "option" : "60 RCU",
    "isCorrect" : "false"
  }, {
    "id" : "5fe9f746c92c492bb5c6ca00c3176368",
    "option" : "160 WCU",
    "isCorrect" : "true"
  }, {
    "id" : "029c6372933a4b9f85a620938d956f54",
    "option" : "240 RCU.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B, E.\nTo determine provisioned Read Capacity Units, the following calculation steps are performed,\n1)Item Size / 4KB, rounding to nearest whole number.\na.\nSo, in the above case, 6KB / 4 KB = 1.5 or 2 read capacity units.\n2)1 read capacity unit per item (since strongly consistent read)× No of reads per second.\na.\nSo, in the above case, 2 x 60 = 120 read capacity units.\nTo determine provisioned Write Capacity Units, the following calculation steps are performed,\n1)Item Size / 1KB, rounding to nearest whole number.\na.\nSo, in the above case, 1.5KB / 1 KB = 1.5 or 2 write capacity units.\n2)1 write capacity unit per item × No of write per second.\na.\nSo, in the above case, 2 x 80 = 160 write capacity units.\nOptions A, C, D,&amp; F are incorrect as these values are not as per the above calculations.\nFor more information on calculations for Read Capacity &amp; Write Capacity Units, refer to the following URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\n\nTo avoid request throttling in DynamoDB, we need to provision the required read and write capacity units based on the expected read and write traffic to the table. Each provisioned read capacity unit (RCU) allows us to read one item per second, up to a maximum item size of 4 KB. Similarly, each provisioned write capacity unit (WCU) allows us to write one item per second, up to a maximum item size of 1 KB.\nGiven that we want to read 60 items per second, each of 6 KB in size, we need to provision (60 * 6) / 4 = 90 RCUs. Since we want to write 80 items per second, each of 1.5 KB in size, we need to provision (80 * 1.5) / 1 = 120 WCUs.\nFrom the given options, we can select two that will meet our requirements:\nOption A: 240 WCU - This will provide enough write capacity units, but not enough read capacity units. We need 90 RCUs for the desired read throughput, so this option is not sufficient.\nOption B: 120 RCU - This will provide enough read capacity units, but not enough write capacity units. We need 120 WCUs for the desired write throughput, so this option is not sufficient.\nOption C: 80 WCU - This will not provide enough write capacity units, we need 120 WCUs for the desired write throughput, so this option is not sufficient.\nOption D: 60 RCU - This will not provide enough read capacity units, we need 90 RCUs for the desired read throughput, so this option is not sufficient.\nOption E: 160 WCU - This will provide enough write capacity units, but not enough read capacity units. We need 90 RCUs for the desired read throughput, so this option is not sufficient.\nOption F: 240 RCU - This will provide enough read capacity units and also more than enough write capacity units. Therefore, option F is the correct answer to the question.\nIn summary, to meet the requirement of reading 60 items per second, each of 6 KB in size, and writing 80 items per second, each of 1.5 KB in size, we need to provision 240 RCUs and 120 WCUs to avoid request throttling.\n\n"
}, {
  "id" : 176,
  "question" : "Your application is making requests to a DynamoDB table.\nDue to the certain surge of requests, you are now getting throttling errors in your application.\nWhich of the following can be used to resolve such errors? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "0f6e8650148345e88238cca2307ac460",
    "option" : "Use exponential backoff in your requests from the application.",
    "isCorrect" : "true"
  }, {
    "id" : "cd854ed62fd94f6081eba5e2f2614c74",
    "option" : "Consider using multiple sort keys.",
    "isCorrect" : "false"
  }, {
    "id" : "1e020a5cb8f9417993eed52a104caf94",
    "option" : "Change the throughput capacity on the tables.",
    "isCorrect" : "true"
  }, {
    "id" : "64617c9d418d482f9afad396932569c5",
    "option" : "Consider using global tables.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nUsing exponential backoff in your requests can put some retries for your application to help with your surge of requests.\nAlternatively, you can increase the throughput capacity defined for your table.\nOption B is invalid because better use of partition keys could help.\nOption D is invalid because this is used for having multiple copies of your table in additional regions.\nFor more information on API retries, please refer to the below URL-\nhttps://docs.aws.amazon.com/general/latest/gr/api-retries.html\nFor more information on DynamoDB Throughput capacity, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html\n\nDynamoDB is a managed NoSQL database service provided by AWS, which is designed to provide high performance and scalability for read and write operations. When the throughput capacity of a table is exceeded, DynamoDB responds with throttling errors, indicating that the table is temporarily unable to handle additional requests. In order to resolve these errors, the following solutions can be considered:\nA. Use exponential backoff in your requests from the application: Exponential backoff is a technique used to retry failed requests by increasing the delay between each retry, using an exponential function. This approach can help prevent overloading the DynamoDB table with too many requests at once, and reduce the likelihood of receiving throttling errors. The exponential backoff can be implemented in the application code to automatically retry requests after a delay, and gradually increase the delay if the requests continue to fail.\nC. Change the throughput capacity on the tables: DynamoDB provides a way to increase the throughput capacity of a table by adjusting the provisioned read and write capacity units. Provisioned capacity can be increased by either increasing the number of read/write capacity units or by switching to on-demand capacity mode. Increasing the provisioned capacity will allow DynamoDB to handle more requests, reducing the likelihood of throttling errors.\nB. Consider using multiple sort keys: In DynamoDB, data is organized into tables, which can have one or more indexes. Each index can have one partition key and one or more sort keys. Sort keys can be used to perform range queries, which can be useful when querying data in a table with a large number of items. By using multiple sort keys, you can distribute the query workload across multiple partitions, which can help reduce the likelihood of throttling errors.\nD. Consider using global tables: Global tables can be used to replicate data across multiple regions, allowing users to access data with low latency from any region. This can help improve the performance of the application and reduce the likelihood of throttling errors.\nIn conclusion, A and C can be considered as possible solutions to resolve throttling errors in DynamoDB. Exponential backoff can be used to reduce the number of requests made to the table at once, while increasing the provisioned capacity will allow DynamoDB to handle more requests. Option B can also help to distribute query workload across multiple partitions, while option D can be used to replicate data across regions.\n\n"
}, {
  "id" : 177,
  "question" : "Your company is developing an application that users on their mobile devices will primarily use.\nThe users need to have the ability to authenticate themselves via identity providers through Security Assertion Markup Language 2.0\nWhich of the following service should be used for user management?\n",
  "answers" : [ {
    "id" : "f2bbf62f5d2641ca94b90fdec50f1dd3",
    "option" : "AWS STS with IAM",
    "isCorrect" : "false"
  }, {
    "id" : "13d6db4ae96d4c31bca242a685e2241c",
    "option" : "AWS Cognito Identity Pools",
    "isCorrect" : "true"
  }, {
    "id" : "1df50417f3e24691b871e5ae9fa75e10",
    "option" : "AWS Security Pools",
    "isCorrect" : "false"
  }, {
    "id" : "7b0164cb39054fcc94859b60e5e7b201",
    "option" : "AWS IAM Pools.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is also given in the AWS Documentation.\nAmazon Cognito supports authentication with identity providers through Security Assertion Markup Language 2.0 (SAML 2.0)\nYou can use an identity provider that supports SAML with Amazon Cognito to provide a simple onboarding flow for your users.\nYour SAML-supporting identity provider specifies the IAM roles that can be assumed by your users so that different users can be granted different sets of permissions.\nBecause of what is mentioned in the AWS Documentation, all other options are invalid.\nFor more information on SAML Identity provider, please refer to the below URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/saml-identity-provider.html\n\nThe correct answer is B. AWS Cognito Identity Pools.\nExplanation:\nAWS Cognito Identity Pools is a fully managed service that enables developers to create unique identities for their users across multiple identity providers and provides support for Security Assertion Markup Language (SAML) 2.0. This service provides a secure and scalable way to authenticate and manage user access to AWS resources and mobile applications.\nAWS STS with IAM (Answer A) is a service that enables users to request temporary security credentials that can be used to access AWS resources. This service can be used in conjunction with IAM (Identity and Access Management) to provide granular access control over AWS resources. However, STS does not provide user management capabilities or support for SAML 2.0 authentication.\nAWS Security Pools (Answer C) and AWS IAM Pools (Answer D) are not valid AWS services. Therefore, they are not the correct answer.\nIn summary, AWS Cognito Identity Pools should be used for user management when implementing SAML 2.0 authentication in a mobile application.\n\n"
}, {
  "id" : 178,
  "question" : "You have been told to make use of Cloudformation templates for deploying applications on EC2 Instances.\nThese Instances need to be preconfigured with the NGINX web server to host the application.\nHow could you accomplish this with Cloudformation?\n",
  "answers" : [ {
    "id" : "3a622313b5c24bbf80659f0966035920",
    "option" : "Use the cfn-init helper script in Cloudformation.",
    "isCorrect" : "true"
  }, {
    "id" : "c4c0026456774a8181c6e9dd65d07653",
    "option" : "Use the Output resource type in Cloudformation.",
    "isCorrect" : "false"
  }, {
    "id" : "1a86c3b5b75d46ce8783a3133cb49a49",
    "option" : "Use the Parameter resource type in Cloudformation.",
    "isCorrect" : "false"
  }, {
    "id" : "90ddcce85189485ca66b2d24a3c10b45",
    "option" : "Use SAML to deploy the template.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - A.\nThe AWS Documentation mentions the following.\nWhen you launch stacks, you can install and configure software applications on Amazon EC2 instances by using the cfn-init helper script and the AWS::CloudFormation::Init resource.\nBy using AWS::CloudFormation::Init, you can describe the configurations that you want rather than scripting procedural steps.\nBecause of what the AWS documentation clearly mentions, all other options are invalid.\nFor more information on the best practices for Cloudformation, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html\n\nThe correct answer for this question is option A: Use the cfn-init helper script in Cloudformation.\nExplanation:\nAWS CloudFormation is an Infrastructure as Code (IaC) service that allows users to model and provision AWS resources using templates. CloudFormation templates are JSON or YAML-formatted text files that describe the infrastructure and applications that users want to deploy on AWS.\nTo preconfigure EC2 Instances with the NGINX web server, CloudFormation provides a helper script called cfn-init. The cfn-init script is a part of the AWS CloudFormation Helper Scripts, which is a collection of scripts that are used to bootstrap and manage EC2 Instances. The cfn-init script allows users to install and configure software packages on EC2 Instances when they are launched.\nTo use the cfn-init script, users must first add it to the UserData section of the EC2 instance resource in the CloudFormation template. The UserData section is a script that is run by the EC2 instance when it is launched. The cfn-init script can then be used to install and configure NGINX on the EC2 Instance.\nExample of how to use the cfn-init script in a CloudFormation template:\nyaml Copy code Resources:   EC2Instance:     Type: AWS::EC2::Instance     Properties:       ImageId: ami-0c55b159cbfafe1f0       InstanceType: t2.micro       UserData:         Fn::Base64: !Sub |           #!/bin/bash -xe           yum update -y           yum install -y aws-cfn-bootstrap           /opt/aws/bin/cfn-init -v \\             --stack ${AWS::StackName} \\             --resource EC2Instance \\             --region ${AWS::Region}\nIn the example above, the cfn-init script is installed and run on the EC2 Instance in the UserData section of the EC2 instance resource. The script installs the AWS CloudFormation Helper Scripts, and then runs the cfn-init script to configure NGINX on the EC2 Instance.\nOption B, C, and D are incorrect:\nB. Use the Output resource type in CloudFormation: The Output resource type is used to export values from the CloudFormation stack. It is not used to preconfigure software on EC2 Instances.\nC. Use the Parameter resource type in CloudFormation: The Parameter resource type is used to pass input values to the CloudFormation stack. It is not used to preconfigure software on EC2 Instances.\nD. Use SAML to deploy the template: SAML (Security Assertion Markup Language) is not used to deploy CloudFormation templates. It is a protocol used for web-based Single Sign-On (SSO) authentication.\n\n"
}, {
  "id" : 179,
  "question" : "You've developed an AWS Lambda function but are running into a lot of performance issues.\nYou decide to use the AWS X-Ray service to diagnose the issues.\nWhich of the following must be done to ensure that you can use the X-Ray service with your Lambda function?\n",
  "answers" : [ {
    "id" : "e4be6fde454f4d64b77f690d107b7b7d",
    "option" : "Ensure that the X-Ray daemon process is installed with the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "d24f883df36b49b0a97b39532550f518",
    "option" : "Ensure that the Lambda function is registered with X-Ray.",
    "isCorrect" : "false"
  }, {
    "id" : "6f5e50b450da4aada8315e0c4d9b7b52",
    "option" : "Ensure that the IAM Role assigned to the Lambda function has access to the X-Ray service.",
    "isCorrect" : "true"
  }, {
    "id" : "4ff5e36c495540b6b94fed967ccabca9",
    "option" : "Ensure that the IAM Role assigned to the X-Ray function has access to the Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nSetting Up AWS X-Ray with Lambda.\nFollowing, you can find detailed information on how to set up an X-Ray with Lambda.\nBefore You Begin.\nTo enable tracing on your Lambda function using the Lambda CLI, you must first add tracing permissions to your function's execution role.\nTo do so, take the following steps.\nSign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/.\nFind the execution role for your Lambda function.\nAttach the following managed policy: AWSXrayWriteOnlyAccess.\nOption A is incorrect since this is used if you need to use an X-Ray with an application that is hosted on an EC2 Instance.\nOption B is incorrect since this is not required to begin using the X-Ray service with AWS Lambda.\nOption D is incorrect since the permissions need to be assigned the other way around.\nFor more information on enabling X-Ray with AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/enabling-x-ray.html\n\nAWS X-Ray is a service that helps developers to analyze and debug distributed applications such as those running on AWS Lambda. With X-Ray, developers can trace requests made to their application and visualize the service map of their application components, helping to identify performance bottlenecks and issues.\nTo use AWS X-Ray with your Lambda function, you need to ensure that the following steps are taken:\nEnsure that the X-Ray SDK is installed in your Lambda function code: To use X-Ray, you need to have the X-Ray SDK installed in your Lambda function code. The X-Ray SDK provides a set of libraries and tools that help you to instrument your code, so that X-Ray can trace requests and analyze performance data. Ensure that the Lambda function is registered with X-Ray: Once you have the X-Ray SDK installed in your code, you need to ensure that your Lambda function is registered with X-Ray. This can be done either through the AWS Management Console or through the AWS CLI. When you register your function, AWS X-Ray automatically starts tracing requests to your Lambda function. Ensure that the IAM Role assigned to the Lambda function has access to the X-Ray service: To enable X-Ray tracing, you need to ensure that the IAM Role assigned to the Lambda function has permissions to write trace data to the X-Ray service. This can be done by attaching the appropriate policy to the IAM Role.\nTherefore, the correct answer to the question is B. Ensure that the Lambda function is registered with X-Ray. Although the other options are also important for enabling X-Ray tracing, they are not essential to ensure that X-Ray can be used with your Lambda function. The X-Ray daemon process is not required when using X-Ray with Lambda functions. The IAM Role assigned to the Lambda function needs to have access to the X-Ray service, but this can be achieved by attaching the appropriate policy to the role. The IAM Role assigned to the X-Ray function is not relevant to this question.\n\n"
}, {
  "id" : 180,
  "question" : "Your application is currently hosted in an Elastic Beanstalk environment.\nConfiguration changes need to be made to the environment.\nYou have been told that the changes should not affect the current environment since downtime needs to be minimized.\nThe code needs to be deployed only to the new instances.\nWhich of the following Elastic Deployment mechanisms would you consider using?\n",
  "answers" : [ {
    "id" : "5df101c4c61c452c885a6abef34e3aeb",
    "option" : "All at Once",
    "isCorrect" : "false"
  }, {
    "id" : "b1d2cffabe6346f3a23f9a7b5f174d93",
    "option" : "Rolling",
    "isCorrect" : "false"
  }, {
    "id" : "b1d1c426d23a4af7bc5ed20474d42322",
    "option" : "Rolling with Additional Batch",
    "isCorrect" : "false"
  }, {
    "id" : "ac0b889b968042e7b006c749f28735cd",
    "option" : "Immutable.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nImmutable updates are an alternative to rolling updates where a temporary Auto Scaling group is launched outside of your environment with a separate set of instances running on the new configuration, which are placed behind your environment's load balancer.\nOld and new instances both serve traffic until the new instances pass health checks.\nThe new instances are then moved into your environment's Auto Scaling group and the temporary group and old instances are terminated.\nAll other options are invalid since it clearly mentions that the current environment should not be changed.\nFor more information on updating environments, please refer to the below URL.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-updating.html\nNote:\nRolling updates have downtime.\nHowever, it is not for the entire infrastructure.\nIt will be divided into batches.\nAWS Docs says that \"When a configuration change requires instances to be replaced, Elastic Beanstalk can perform the update in batches to avoid downtime while the change is propagated.\nDuring a rolling update, capacity is only reduced by a single batch size, which you can configure.\nElastic Beanstalk takes one batch of instances out of service, terminates them, and then launches a batch with the new configuration.\nAfter the new batch starts serving requests, Elastic Beanstalk moves on to the next batch.\"\n\nTo make configuration changes to an Elastic Beanstalk environment with minimal downtime and deploy the code only to new instances, you would consider using the Rolling with Additional Batch deployment mechanism.\nHere's an overview of each of the four deployment mechanisms available in Elastic Beanstalk:\nA. All at Once: This deployment mechanism deploys the new version of your application to all instances at once. It's best used for applications that can tolerate some downtime, such as non-critical internal applications. This deployment mechanism may not be suitable for applications with a large number of instances or applications that have critical dependencies.\nB. Rolling: This deployment mechanism deploys the new version of your application in batches, one batch at a time. Each batch of instances is taken out of service while the new version is deployed, and then returned to service once the deployment is complete. This deployment mechanism is best used for applications that can tolerate some downtime, but not as much as the All at Once deployment mechanism.\nC. Rolling with Additional Batch: This deployment mechanism is similar to the Rolling deployment mechanism, but it includes an additional batch of instances. The new version of your application is deployed to the first batch of instances, which are taken out of service and returned to service once the deployment is complete. Then, the second batch of instances is taken out of service and the new version is deployed to those instances. This deployment mechanism is best used for applications that need to minimize downtime, as it ensures that at least one batch of instances is always available to handle traffic.\nD. Immutable: This deployment mechanism deploys the new version of your application to a completely new set of instances, leaving the old instances in place. Once the new instances are running and have passed health checks, traffic is shifted from the old instances to the new instances. This deployment mechanism is best used for applications that require zero downtime, as it ensures that traffic is always handled by a healthy set of instances.\nBased on the requirement to minimize downtime and deploy the code only to new instances, the Rolling with Additional Batch deployment mechanism would be the best choice. This mechanism ensures that at least one batch of instances is always available to handle traffic while the new version of the application is being deployed to the other batches of instances.\n\n"
}, {
  "id" : 181,
  "question" : "You've deployed an application using AWS Lambda and the API gateway service.\nYou need to deploy a newer version of the application.\nManagement has instructed that the newer version should be tested before being fully deployed, while the base version is still in continuity.\nHow can you accomplish this in the easiest way possible?\n",
  "answers" : [ {
    "id" : "a2fdc22e676f489fb14d59f98ee5a5b7",
    "option" : "Create a new Lambda function and a new API gateway. Give the users the new URL.",
    "isCorrect" : "false"
  }, {
    "id" : "c5d5b15d84584f3e8d2917b609b7cd0f",
    "option" : "Create a new version of the existing Lambda function and a new API gateway. Give the users the new URL.",
    "isCorrect" : "false"
  }, {
    "id" : "a04651b6085945139f7326b7605c6e2d",
    "option" : "Create a canary release in the API gateway service.",
    "isCorrect" : "true"
  }, {
    "id" : "6e20e3edee2b493c88eeb7036c66db8a",
    "option" : "Create another resource and method. Deploy the API. Give the users the new URL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nCanary release deployment in API Gateway.\nIn API Gateway, a canary release deployment uses the deployment stage for the production release of the base version of an API and attaches to the stage a canary release for the new versions, relative to the base version, of the API.\nThe stage is associated with the initial deployment and the canary with subsequent deployments.\nIn the beginning, both the stage and the canary point to the same API version.\nWe use stage and production release interchangeably and use canary and canary release interchangeably throughout this section.\nWhen you enable API execution logging, the canary release has its own logs and metrics generated for all canary requests.\nThey are reported to a production stage CloudWatch Logs log group as well as a canary-specific CloudWatch Logs log group.\nThe same applies to access logging.\nThe separate canary-specific logs are helpful to validate new API changes and decide whether to accept the changes and promote the canary release to the production stage or to discard the changes and revert the canary release from the production stage.\nOptions A and B are incorrect.\nEven though possible, they would add too much of maintenance overhead.\nOption C is CORRECT because a small portion of traffic goes to the canary version before the traffic is fully switched to the new release.\nOption D is incorrect because this is not the right way for a new deployment.\nFor more information on the canary release, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\n\nThe correct answer is C. Create a canary release in the API Gateway service.\nExplanation:\nWhen deploying a new version of an application, it is important to test it before fully deploying it to avoid any issues or unexpected behavior. In this case, the easiest way to accomplish this is by creating a canary release in the API Gateway service.\nA canary release is a deployment technique where a small subset of users or traffic is routed to the new version of the application while the majority of the traffic still uses the old version. This allows for testing the new version in production without affecting all users.\nTo create a canary release in the API Gateway service, you can follow these steps:\nCreate a new version of the existing Lambda function: This can be done by creating a new version in the AWS Lambda console or by using the AWS CLI. This new version will contain the updated code. Create a new deployment in the API Gateway: In the API Gateway console, create a new deployment that uses the new version of the Lambda function. Create a canary release configuration: In the API Gateway console, create a canary release configuration that specifies the percentage of traffic to route to the new deployment. Test the new version: Monitor the traffic to the new deployment and test the new version with a small subset of users. Gradually increase traffic to the new version: Once the new version has been tested and verified, gradually increase the percentage of traffic routed to the new deployment until all traffic is using the new version.\nOption A is not the best approach as it would require creating a new API Gateway endpoint and a new Lambda function, which could cause confusion for users and result in unnecessary maintenance overhead.\nOption B is also not the best approach as it only creates a new version of the existing Lambda function and does not provide any way to test the new version before fully deploying it.\nOption D is not the best approach as it requires creating a new resource and method, which could also cause confusion for users and result in unnecessary maintenance overhead. Additionally, it does not provide any way to test the new version before fully deploying it.\n\n"
}, {
  "id" : 182,
  "question" : "You're developing an application that is going to be hosted in AWS Lambda.\nThe function will make calls to a database.\nA requirement is that all database connection strings should be kept secure.\nWhich of the following is the MOST secure way to implement this?\n",
  "answers" : [ {
    "id" : "66c0ecbda10b4aac8c017a26d1bc72d2",
    "option" : "Put the connection strings values in a CloudFormation template.",
    "isCorrect" : "false"
  }, {
    "id" : "9d4dcdbc9b494dcaa00aacee0dbdd2ec",
    "option" : "Put the database connection string in the app.json file and store it in a Git repository.",
    "isCorrect" : "false"
  }, {
    "id" : "d8b9204ee3eb48fc924a0d753ad9270a",
    "option" : "Lambda needs to reference the AWS Systems Manager Parameter Store for the encrypted database connection string.",
    "isCorrect" : "true"
  }, {
    "id" : "2b3880fbdddf49b3829e3304c15c5bca",
    "option" : "Place the database connection string in the AWS Lambda function itself since all Lambda functions are encrypted at rest.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAWS Systems Manager Parameter Store provides secure, hierarchical storage for configuration data management and secrets management.\nYou can store data such as passwords, database strings, and license codes as parameter values.\nYou can store values as plain text or encrypted data.\nYou can then reference values by using the unique name that you specified when you created the parameter.\nHighly scalable, available, and durable, Parameter Store is backed by the AWS Cloud.\nParameter Store is offered at no additional charge.\nOption A is incorrect because the connection strings values are exposed in the CloudFormation template.\nOption B is incorrect because the string is stored in the code repository and is unsecure.\nOption D is incorrect because the string is stored together with the Lambda function code, which is not secure.\nFor more information on the Systems Manager Parameter Store, please refer to the below URL-\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-paramstore.html\n\nThe most secure way to store and access database connection strings in an AWS Lambda function is to use the AWS Systems Manager Parameter Store. This option is provided by option C.\nOption A is not the most secure way to implement this because CloudFormation templates are typically stored in plain text, which can be a security risk. This means that anyone with access to the CloudFormation template can view the database connection string.\nOption B is also not the most secure way to implement this because storing the database connection string in a Git repository is not secure. Git repositories are typically public or shared with a team of developers, which means that anyone with access to the repository can view the database connection string.\nOption D is also not the most secure way to implement this because storing the database connection string in the AWS Lambda function itself is not secure. While Lambda functions are encrypted at rest, anyone with access to the function can view the database connection string.\nOption C is the most secure way to implement this because the AWS Systems Manager Parameter Store allows you to store encrypted strings, including database connection strings, as parameters. The parameters can be accessed by the Lambda function securely, and you can control who has access to the encrypted strings using AWS Identity and Access Management (IAM). By using the AWS Systems Manager Parameter Store, you can ensure that your database connection strings are encrypted and secure.\n\n"
}, {
  "id" : 183,
  "question" : "You are working on building microservices using Amazon ECS.\nThis ECS will be deployed in an Amazon EC2 instance along with its Amazon ECS container agent.\nAfter successfully launching the EC2 instance, the Amazon ECS container agent has registered this instance in a cluster.\nWhat would be the status of the container instance &amp; its corresponding agent connection when an ECS container is stopped?\n",
  "answers" : [ {
    "id" : "22e9e6de6cbc4750bac499643bd082d0",
    "option" : "Container instance status remains as ACTIVE and Agent connection status as TRUE after a few minutes.",
    "isCorrect" : "false"
  }, {
    "id" : "ad76c84e77934ec894627ef31f509c47",
    "option" : "Container instance status remains as ACTIVE and Agent connection status as FALSE after a few minutes.",
    "isCorrect" : "true"
  }, {
    "id" : "274ee35ac6694638ad3f35ac1cc900ea",
    "option" : "Container instance changes status as INACTIVE and Agent connection status as FALSE after a few minutes.",
    "isCorrect" : "false"
  }, {
    "id" : "60c0cc825a6f4e2b8f3512af8eac940e",
    "option" : "Container instance changes status as INACTIVE and Agent connection status as TRUE after a few minutes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nWhen an ECS container is stopped, the Container instance status remains Active, but the container Agent status changes to FALSE after a few minutes.\nOption A is incorrect as when the ECS container is stopped, Agent connection status changes to False &amp; not True.\nOptions C &amp; D are incorrect as when the ECS container is stopped, the Container instance remains active &amp; not inactive.\nFor more information on enabling ECS on EC2 instance, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html\n\nIn Amazon ECS, a container instance is an Amazon EC2 instance that is running the Amazon ECS container agent and has been registered into an Amazon ECS cluster. The Amazon ECS container agent runs on each container instance within a cluster and is responsible for registering the instance with the Amazon ECS service, reporting resource utilization and status updates for running tasks, and handling task state changes.\nWhen an ECS container is stopped, the task running within the container is terminated, and the container is removed from the container instance. The Amazon ECS container agent sends a message to the Amazon ECS service indicating that the task has stopped.\nBased on this behavior, the correct answer is option B: Container instance status remains as ACTIVE and Agent connection status as FALSE after a few minutes.\nThe container instance status remains as ACTIVE because the EC2 instance is still running and available for other tasks to be scheduled on it. However, the agent connection status changes to FALSE because the Amazon ECS container agent is no longer able to communicate with the stopped container. After a few minutes, the Amazon ECS service updates the status of the container instance and agent connection to reflect the stopped container.\nOption A is incorrect because the agent connection status changes to FALSE when a container is stopped. Option C is incorrect because the container instance does not change status to INACTIVE when a container is stopped, but rather remains as ACTIVE. Option D is incorrect because the agent connection status changes to FALSE when a container is stopped, and does not remain as TRUE.\n\n"
}, {
  "id" : 184,
  "question" : "You have created an Amazon DynamoDB table with Global Secondary Index.\nWhich of the following can be used to get the latest results quickly with the least impact on RCU (Read Capacity Unit)?\n",
  "answers" : [ {
    "id" : "9df21d7031664c4aa3fbd2c14beb7939",
    "option" : "Query with ConsistentRead",
    "isCorrect" : "false"
  }, {
    "id" : "13f089dbd4fa4af58e53b5f45e8aa497",
    "option" : "Scan with ConsistentRead",
    "isCorrect" : "false"
  }, {
    "id" : "3f9ed9546df6496db4615f89944813b4",
    "option" : "Query with EventualRead",
    "isCorrect" : "true"
  }, {
    "id" : "970e15c8a12943738102f823f998eee4",
    "option" : "Scan with EventualRead.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nGlobal Secondary Index does not support Consistent read.\nIt only supports Eventual Read.\nFor other tables, Query with Consistent Read will provide the latest results without scanning the whole table.\nOption A is incorrect as Global Secondary Index does not support Consistent read.\nOption B is incorrect as Scan will impact performance as it will scan the whole table.\nOption D is incorrect as Scan will impact performance as it will scan the whole table.\nFor more information for Query with DynamoDB, refer to the following URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Query.html\n\nTo get the latest results quickly with the least impact on RCU, you should use Query with ConsistentRead.\nQuery with ConsistentRead returns a strongly consistent result, meaning it returns the most up-to-date data. When you use ConsistentRead, DynamoDB queries all the nodes in the local region to provide a consistent view of the data. This operation requires more RCU than Query with EventualRead, but it returns the most up-to-date data with high accuracy.\nOn the other hand, Query with EventualRead returns a result with eventual consistency, which means it may not provide the most up-to-date data. When you use EventualRead, DynamoDB queries a subset of nodes in the local region to provide a result, and it may take some time to catch up with the latest data. This operation requires fewer RCU than ConsistentRead, but it may return stale data.\nSimilarly, Scan with ConsistentRead returns a strongly consistent result, but it scans the entire table and may require a significant amount of RCU. Scan with EventualRead returns a result with eventual consistency and scans the entire table, which is inefficient and requires a significant amount of RCU.\nIn summary, to get the latest results quickly with the least impact on RCU, you should use Query with ConsistentRead.\n\n"
}, {
  "id" : 185,
  "question" : "You've created a local Java-based Lambda function.\nYou then package and upload the function to AWS.\nYou try to run the function with the default settings.\nBut the function does not run as expected.\nWhich of the following could be the reasons for the issue? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ccac4a0ce20d4988a2389fd37bceab17",
    "option" : "The name assigned to the function is not correct.",
    "isCorrect" : "false"
  }, {
    "id" : "4b2fbe1968af47cbbe955d2ac3d1c4fd",
    "option" : "The amount of CPU assigned to the function is not enough.",
    "isCorrect" : "false"
  }, {
    "id" : "bc0b4382c91d429799e49712cf0beab4",
    "option" : "The amount of memory assigned to the function is not enough.",
    "isCorrect" : "true"
  }, {
    "id" : "c29608b7496048eda455ca3ae73ba9d1",
    "option" : "The timeout specified for the function is too short.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nSince the function is created with the default settings, the timeout for the function would be 3 seconds and the memory would default to 128 MB.\nFor a Java-based function, this would be too less.\nHence you need to ensure that the right settings are put in place for the function.\nQ: How are compute resources assigned to an AWS Lambda function?\nIn the AWS Lambda resource model, you choose the amount of memory you want for your function, and are allocated proportional CPU power and other resources.\nFor example, choosing 256MB of memory allocates approximately twice as much CPU power to your Lambda function as requesting 128MB of memory and half as much CPU power as choosing 512MB of memory.\nYou can set your memory from 128 MB to 3,008 MB, in 64 MB increments.\nOption A is invalid since the name is not a reason for the function not working.\nOption B is invalid since the CPU is allocated by AWS automatically.\nFor more information on creating a function, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/get-started-create-function.html\n\nWhen a local Java-based Lambda function is packaged and uploaded to AWS, it is important to ensure that the configuration settings are correct for the function to run as expected. If the function does not run as expected, there could be several reasons for the issue. Here are the possible reasons:\nA. The name assigned to the function is not correct: It is possible that the function is not running as expected because the name assigned to the function is incorrect. The function name must match the name specified in the handler class. If there is a mismatch between the function name and the handler class name, the function will not run.\nB. The amount of CPU assigned to the function is not enough: Lambda functions are executed in a containerized environment that is isolated from other functions. The amount of CPU allocated to a function depends on the size of the container and the amount of memory allocated to the function. If the function is running slow, it could be because the allocated CPU is not enough to handle the workload.\nC. The amount of memory assigned to the function is not enough: Lambda functions are executed in a containerized environment, and the amount of memory allocated to the function determines the amount of CPU allocated. If the function does not have enough memory allocated, it may not be able to handle the workload, resulting in slow execution times or unexpected behavior.\nD. The timeout specified for the function is too short: The timeout specified for a function determines how long AWS waits for the function to complete its execution. If the function takes longer than the specified timeout, AWS will terminate the function. If the timeout specified for the function is too short, the function may not have enough time to complete its execution, resulting in unexpected behavior.\nIn summary, the possible reasons why a local Java-based Lambda function may not run as expected after packaging and uploading to AWS include incorrect function name, inadequate CPU and memory allocation, and short function timeout.\n\n"
}, {
  "id" : 186,
  "question" : "You need to set up a RESTful API service in AWS that would be serviced via the following URL- https://democompany.com/customers?ID=1 So customers should be able to get their details whilst providing the ID to the API.\nWhich of the following would you define to fulfill this requirement? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "4684cf5d588049eda50ce7feeebdbfb2",
    "option" : "A Lambda function and expose the Lambda function to the customers. Pass the ID as a parameter to the function.",
    "isCorrect" : "false"
  }, {
    "id" : "51908dcb2d5b467cb975e94816a9a3d8",
    "option" : "An API gateway with a Lambda function to process the customer information.",
    "isCorrect" : "true"
  }, {
    "id" : "0a25031ca1e44929a11f9ba07c2d410b",
    "option" : "Expose the GET method in the API Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "c89fb9d3c1824ee69c89b6bfcd0222d9",
    "option" : "Expose the GET method in the Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe ideal approach would be to define the code to get the customer information in the Lambda function.\nThen attach the Lambda function to the API gateway service.\nExpose the GET method in the API gateway so that users can call the API accordingly.\nFor more information on methods for the API gateway, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-method-settings-method-request.html\n\nTo fulfill the requirement of setting up a RESTful API service in AWS that would be serviced via the following URL - https://democompany.com/customers?ID=1, there are a few different options available. The correct options are A and B.\nOption A: A Lambda function and expose the Lambda function to the customers. Pass the ID as a parameter to the function. This option involves creating a Lambda function that can accept a customer ID as a parameter and return the relevant customer details. The Lambda function can then be exposed to customers via an API endpoint, such as an API Gateway endpoint. Customers can pass their ID as a query parameter in the URL, and the Lambda function can retrieve the details and return them in the response.\nOption B: An API gateway with a Lambda function to process the customer information. This option involves setting up an API Gateway to act as a front-end for the Lambda function that processes the customer information. The API Gateway can be configured with a resource for \"customers\" and a GET method to retrieve the details. The Lambda function can be connected to the API Gateway via an integration, and customers can pass their ID as a query parameter in the URL to retrieve their details.\nOption C: Expose the GET method in the API Gateway. This option is not sufficient on its own to fulfill the requirement. While exposing a GET method in the API Gateway is necessary, it is not enough to retrieve the customer details based on their ID.\nOption D: Expose the GET method in the Lambda function. This option is also not sufficient to fulfill the requirement. While exposing a GET method in the Lambda function may be possible, it is not the recommended approach for implementing a RESTful API service in AWS. The Lambda function should be connected to an API Gateway to handle incoming requests and provide a scalable and flexible way to manage the API.\n\n"
}, {
  "id" : 187,
  "question" : "You are currently managing deployments for a Lambda application via Code Deploy.\nYou have a new version of the Lambda function in place.\nYou have been told that all traffic needs to be shifted instantaneously to the new function.\nWhich deployment technique would you employ in CodeDeploy?\n",
  "answers" : [ {
    "id" : "5bf9a1b190bd480f91306a682a7353a2",
    "option" : "Canary",
    "isCorrect" : "false"
  }, {
    "id" : "7e8611109d9848e4ae108a1470553d1d",
    "option" : "Gradual",
    "isCorrect" : "false"
  }, {
    "id" : "5c5a959b5c564ab2a1759fc668ece978",
    "option" : "Linear",
    "isCorrect" : "false"
  }, {
    "id" : "f2209559317b4ce991dfca3b11c3d618",
    "option" : "All-at-Once.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nThere are three ways traffic can shift during a deployment.\nCanary: Traffic is shifted in two increments.\nYou can choose from predefined canary options that specify the percentage of traffic shifted to your updated Lambda function version in the first increment and the interval, in minutes, before the remaining traffic is shifted in the second increment.\nLinear: Traffic is shifted in equal increments with an equal number of minutes between each increment.\nYou can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number of minutes between each increment.\nAll-at-once: All traffic is shifted from the original Lambda function to the updated Lambda function version at once.\nBecause of the options present in the documentation, all other options become invalid.\nFor more information on deployment configurations, please refer to the below URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html\n\nThe deployment technique that should be used in this scenario is \"All-at-Once\" (Option D).\nIn the \"All-at-Once\" deployment technique, the new version is deployed to all instances simultaneously, and all traffic is shifted instantly to the new version. This method is suitable when the Lambda function has a small or negligible impact on the overall system performance and does not require any special attention during deployment.\nCanary deployment (Option A) is a deployment technique that gradually introduces the new version to a small subset of users to test its functionality and performance before rolling out the update to the entire user base. This technique is suitable for critical applications where any unexpected behavior or errors could cause significant issues for users.\nGradual deployment (Option B) is a deployment technique that introduces the new version gradually to a small percentage of users and slowly increases the traffic to the new version over time. This method is suitable for applications that require additional time for testing and validation before being rolled out to the entire user base.\nLinear deployment (Option C) is a deployment technique that introduces the new version to a small percentage of users and gradually increases the traffic to the new version over time in equal increments. This technique is suitable for applications where there is a need to manage traffic to the new version and gradually test its functionality and performance.\nIn summary, \"All-at-Once\" deployment technique (Option D) is the most suitable option for this scenario because the requirement is to shift all traffic instantaneously to the new Lambda function.\n\n"
}, {
  "id" : 188,
  "question" : "You have an application that needs to encrypt data using the KMS service.\nThe company has already defined the customer master key in AWS for usage in the application.\nWhich of the following steps must be followed in the encryption process? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "cc8448f886354e118fae38ec8275257c",
    "option" : "Use the GenerateDataKey to get the data key to encrypt the data.",
    "isCorrect" : "true"
  }, {
    "id" : "797f2aa39d034869bc773171b09d0697",
    "option" : "Use CustomerMaster Key to encrypt the data.",
    "isCorrect" : "false"
  }, {
    "id" : "80ac74371d814d6793947c9c9810e325",
    "option" : "Delete the plaintext data encryption key after the data is encrypted.",
    "isCorrect" : "true"
  }, {
    "id" : "cc35612267fc45218556913ad40c3986",
    "option" : "Delete the Customer Master Key after the data is encrypted.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nOptions B and D are incorrect because you will not use the Customer Key to encrypt and decrypt data directly.\nThe AWS Documentation mentions the following.\nWe recommend that you use the following pattern to encrypt data locally in your application.\nUse this operation (GenerateDataKey) to get a data encryption key.\nUse the plaintext data encryption key (returned in the Plaintext field of the response) to encrypt data locally, then erase the plaintext data key from memory.\nStore the encrypted data key (returned in the CiphertextBlob field of the response) alongside the locally encrypted data.\nFor more information on generating data key, please refer to the below URL-\nhttps://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\n\nWhen encrypting data using the AWS KMS service, the following two steps must be followed:\nA. Use the GenerateDataKey to get the data key to encrypt the data. The first step is to use the GenerateDataKey API call to retrieve a plaintext data encryption key. The plaintext data encryption key is used to encrypt the data, and it is encrypted under the specified customer master key. The customer master key is never used directly to encrypt data, as it is only used to encrypt data encryption keys. By using a data encryption key for each piece of data, you can improve security by limiting the amount of data that could be exposed if a key is compromised.\nB. Use CustomerMaster Key to encrypt the data. Once you have the plaintext data encryption key, you can use it to encrypt the data. The plaintext data encryption key is never stored, and is deleted immediately after use. The encrypted data encryption key is stored alongside the encrypted data, and is needed to decrypt the data.\nC. Delete the plaintext data encryption key after the data is encrypted. The plaintext data encryption key should be deleted immediately after it is used to encrypt the data. This ensures that the key is not accidentally exposed, and reduces the risk of it being compromised. You can use the DeleteKey API call to delete the key.\nD. Delete the Customer Master Key after the data is encrypted. You should not delete the customer master key after the data is encrypted. The customer master key is needed to decrypt the data encryption key, which is required to decrypt the data. If the customer master key is deleted, the encrypted data and data encryption key cannot be decrypted.\nIn summary, after retrieving the plaintext data encryption key using GenerateDataKey, you should use it to encrypt the data and then delete it immediately. The customer master key should not be deleted after the data is encrypted, as it is required to decrypt the data.\n\n"
}, {
  "id" : 189,
  "question" : "Your team is currently working on source code that's defined in a Subversion repository.\nThe company has just started using AWS tools for their CI/CD process and has now mandated that source code be migrated to AWS CodeCommit.\nWhich of the following steps would you perform to fulfill this requirement? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "b2a33278b9c948418715a26934ac610a",
    "option" : "Migrate the code as it is to the AWS CodeCommit Repository.",
    "isCorrect" : "false"
  }, {
    "id" : "62e6800c45d94b2c868c05fb28ed1a2b",
    "option" : "Migrate the code to a Git Repository first.",
    "isCorrect" : "true"
  }, {
    "id" : "ad1be3edcd6e45dc819a607ea7b58e7d",
    "option" : "Migrate Git code to AWS Code Commit.",
    "isCorrect" : "true"
  }, {
    "id" : "77f9a9be4452465b828c596664eb9f81",
    "option" : "Ensure to clone the current repository from Subversion before committing it directly to AWS CodeCommit.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe AWS Documentation mentions the following.\nMigrate to AWS CodeCommit.\nYou can migrate a Git repository to an AWS CodeCommit repository in many ways: by cloning it, mirroring it, migrating all or just some of the branches, and so on.\nYou can also migrate local, unversioned content on your computer to AWS CodeCommit.\nThe following topics demonstrate some of the ways you can choose to migrate a repository.\nYour steps may vary, depending on your repository type, style, or complexity and the decisions you make about what and how you want to migrate.\nFor very large repositories, you might want to consider migrating incrementally.\nNote.\nYou can migrate to AWS CodeCommit from other version control systems, such as Perforce, Subversion, or TFS, but you will have to migrate to Git first.\nOptions A and D are incorrect since you need to migrate the repository to Git first.\nFor more information on migrating a repository, please refer to the below URL-\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/how-to-migrate-repository.html\n\nIn order to migrate source code from Subversion to AWS CodeCommit for the company's CI/CD process, the following steps can be performed:\nMigrate the code to a Git Repository first: As CodeCommit is a managed Git repository service, the first step is to migrate the code from Subversion to a Git repository. This can be done using a tool like git-svn, which allows for the migration of the complete Subversion repository history to Git. Migrate Git code to AWS CodeCommit: Once the code has been migrated to a Git repository, it can be migrated to AWS CodeCommit. This can be done by creating a new CodeCommit repository and pushing the Git code to the new repository.\nTherefore, the correct answers are B and C.\nOption A is not a recommended approach as it suggests directly migrating the Subversion repository to CodeCommit, which can cause issues as CodeCommit only supports Git repositories.\nOption D is not necessary as cloning the current repository from Subversion before committing it directly to CodeCommit is not a requirement. The migration process involves creating a new Git repository and pushing the migrated code to it.\n\n"
}, {
  "id" : 190,
  "question" : "A company currently has an application that works with DynamoDB.\nThe application is a high revenue generating application for the company.\nTheir current response time for their read workloads is in the order of milliseconds.\nBut to bump up hits to their pages, they want to reduce the response time to microseconds.\nWhich of the following would you suggest to be most preferably used with DynamoDB to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "c1baea7003614fd7982ed2ce5f5e557f",
    "option" : "Consider deploying an ElastiCache in front of DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "8cdb472115b5476d896a92247e2e6d91",
    "option" : "Consider using DynamoDB global tables.",
    "isCorrect" : "false"
  }, {
    "id" : "596b3734d8d0478fa19c40cb6fd39c36",
    "option" : "Consider using DynamoDB accelerator.",
    "isCorrect" : "true"
  }, {
    "id" : "9b84597f23c7472cb71c6a4444e8afb8",
    "option" : "Consider using a higher throughput for the tables.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nDynamoDB Accelerator(DAX) is a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications.\nDAX addresses three core scenarios.\nAs an in-memory cache, DAX reduces the response times of eventually-consistent read workloads by order of magnitude, from single-digit milliseconds to microseconds.\nDAX reduces operational and application complexity by providing a managed service that is API-compatible with Amazon DynamoDB.\nThus it requires only minimal functional changes to use with an existing application.\nOption A is invalid because ElastiCache would not guarantee for certain such a great reduction in response times.\nThe question also asks for a \"preferred\" service to be used along with DynamoDB, for which DAX would be a better answer than Elasticache.\nOption B is invalid because it is used when you want to make replicas of the tables in different regions.\nOption D is invalid because this option is used only when you have throttling errors for the table.\nFor more information on DAX, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\n\nTo reduce the response time to microseconds, we need to improve the performance of the DynamoDB application. The following are the available options:\nA. Deploying an ElastiCache in front of DynamoDB: Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store. ElastiCache can be used to store frequently accessed data in memory, which can help reduce the response time of the application. However, ElastiCache cannot reduce the response time to microseconds. The typical response time for ElastiCache is in the range of milliseconds.\nB. Using DynamoDB global tables: DynamoDB Global Tables can be used to replicate data across multiple regions, which can help reduce the response time of the application by providing faster access to data. However, this solution does not directly address the requirement to reduce the response time to microseconds.\nC. Using DynamoDB Accelerator (DAX): DynamoDB Accelerator (DAX) is an in-memory cache for DynamoDB that can help reduce the response time to microseconds. DAX caches frequently accessed data in memory, which can help reduce the number of requests to DynamoDB and improve the response time of the application.\nD. Using a higher throughput for the tables: Increasing the provisioned throughput for the DynamoDB tables can help improve the performance of the application. However, this solution does not directly address the requirement to reduce the response time to microseconds.\nBased on the above options, the most suitable option to reduce the response time to microseconds for the DynamoDB application is to use DynamoDB Accelerator (DAX).\n\n"
}, {
  "id" : 191,
  "question" : "To enhance browser security, you are planning to enable CORS.\nTo enable CORS on a resource using API gateway for GET method, which of the following action needs to be performed for all types of response except 200 response in case of Lambda custom integration?\n",
  "answers" : [ {
    "id" : "e3e6b583c45c4720a2e1ee8aad9ab1f3",
    "option" : "Backend devices will be responsible to return Access-Control-Allow-Origin` header with `*` or specific origins to fulfil pre-flight handshakes.",
    "isCorrect" : "false"
  }, {
    "id" : "7acdc437bb6e4c42969c516257a68d34",
    "option" : "An OPTIONS method is added to the resource which is automatically configured to return the three Access-Control-Allow-* headers to fulfil pre-flight handshakes.",
    "isCorrect" : "false"
  }, {
    "id" : "4b8f7b7d88af49df8c9d9cd259835d3b",
    "option" : "Check if all Methods are added in “AllowMethods” along with GET.",
    "isCorrect" : "false"
  }, {
    "id" : "28678a98c38c4f2ca2e8d2c0702df5d8",
    "option" : "Manually configure to return Access-Control-Allow-Origin` header with `*` or specific origins to fulfil pre-flight handshakes.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nWhile enabling CORS on resources using API Gateway, for all responses apart from 200 responses of the OPTIONS method, we need to manually configure to return Access-Control-Allow-Origin' header with '*' or specific origins to fulfill pre-flight handshakes.\nOption A is incorrect as this is valid only for Lambda Proxy integration &amp; not for Lambda custom integration.\nOption B is incorrect as this is set only for the 200 responses of the Options method.\nFor all other methods, we need to manually configure to return Access-Control-Allow-Origin' header with '*' or specific origins to fulfill pre-flight handshakes.\nOption C is incorrect as even though this is set, we will also need to Manually configure to return Access-Control-Allow-Origin' header with '*' or specific origins to fulfill pre-flight handshakes.\nFor more information on enabling CORS for an API Gateway, refer to the following URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\n\nCross-Origin Resource Sharing (CORS) is a mechanism that allows web pages to make XMLHttpRequests to a domain that is different from the originating domain. It is a security feature implemented by modern browsers to prevent cross-site scripting (XSS) attacks.\nTo enable CORS on a resource using API Gateway for GET method, the following actions need to be performed:\nAdd an OPTIONS method to the resource: An OPTIONS method needs to be added to the resource to handle pre-flight requests. A pre-flight request is an HTTP request that is used to determine whether the actual request is safe to send. It is used to check whether the server is willing to accept the request, based on the origin, request headers, and the HTTP method used. Configure the OPTIONS method to return the Access-Control-Allow-* headers: When an OPTIONS request is received by API Gateway, it automatically returns the three Access-Control-Allow-* headers (Access-Control-Allow-Origin, Access-Control-Allow-Methods, and Access-Control-Allow-Headers) to fulfill pre-flight handshakes. This is done by default, so you don't need to manually configure it. Configure the backend to return the Access-Control-Allow-Origin header: For all types of responses except the 200 response in case of Lambda custom integration, the backend devices will be responsible for returning the Access-Control-Allow-Origin header with * or specific origins to fulfill pre-flight handshakes. This is necessary because the API Gateway cannot add this header to the response. Check if all Methods are added in \"AllowMethods\" along with GET: It is important to check if all methods are added in \"AllowMethods\" along with GET to ensure that the API Gateway allows all the necessary methods to access the resource.\nTherefore, the correct answer is option A: \"Backend devices will be responsible for returning Access-Control-Allow-Origin header with * or specific origins to fulfill pre-flight handshakes.\" Option B is partially correct but not necessary to configure as it is done by API Gateway automatically. Option C is also important but not directly related to enabling CORS on a resource using API Gateway for GET method. Option D is incorrect as the API Gateway cannot add the Access-Control-Allow-Origin header to the response.\n\n"
}, {
  "id" : 192,
  "question" : "Your company is planning on using the Simple Storage Service to host objects that users will access.\nTo ensure optimal performance when requests are made to get the objects from the bucket, which of the following is the right way to define the keys for optimal performance?\n",
  "answers" : [ {
    "id" : "b4746963d2a74d43818266df6289beaf",
    "option" : "demoawsbucket/Image1.jpg",
    "isCorrect" : "false"
  }, {
    "id" : "7fe527e653134c6eba7c1f43bc2a6235",
    "option" : "demoawsbucket/Images/2019-14-03-15-00-00Image1.jpg",
    "isCorrect" : "false"
  }, {
    "id" : "46624a770427424c8d8ffebafa1e3837",
    "option" : "demoawsbucket/2019-14-03-15-00-00/Image1.jpg",
    "isCorrect" : "false"
  }, {
    "id" : "4778d0f16f5c4f4493e18db3a0eeb066",
    "option" : "All the options have the same performance.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nLatest Update: Based on the New S3 announcement (S3 performance) Amazon S3 now provides increased request rate performance.\nIt removes any previous guidance to randomize object prefixes to achieve faster performance.\nThat means you can now use logical or sequential naming patterns in S3 object naming without any performance implications.\nhttps://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/\n\nWhen using Amazon Simple Storage Service (S3) to host objects, defining the keys correctly is essential for optimal performance when requests are made to get the objects from the bucket.\nThe key of an object in Amazon S3 is the unique identifier for the object within the bucket. A key is composed of a prefix, a delimiter, and an object name. The prefix and delimiter are optional, but they can help you organize and search for objects more efficiently.\nOption A: demoawsbucket/Image1.jpg This key has no prefix or delimiter, which means that all objects are located in the root of the bucket. This can make it difficult to organize and search for objects, especially if there are a large number of objects in the bucket.\nOption B: demoawsbucket/Images/2019-14-03-15-00-00Image1.jpg This key includes a prefix (Images/) and an object name (2019-14-03-15-00-00Image1.jpg). Using a prefix can help organize and categorize objects in the bucket, which can improve searchability and performance.\nOption C: demoawsbucket/2019-14-03-15-00-00/Image1.jpg This key includes both a prefix (2019-14-03-15-00-00/) and an object name (Image1.jpg). This is similar to Option B, but the prefix is more specific, which can help improve searchability and performance even further.\nOption D: All the options have the same performance. This is incorrect because the performance can vary depending on how the keys are defined.\nIn conclusion, Option C (demoawsbucket/2019-14-03-15-00-00/Image1.jpg) is the best option for defining keys for optimal performance when requests are made to get the objects from the bucket. However, the best approach may vary depending on the specific use case and the number of objects in the bucket.\n\n"
}, {
  "id" : 193,
  "question" : "You are a team lead for the development of an application that will be hosted in AWS.\nThe application will consist of a front end that will allow users to upload files.\nPart of the application will consist of sending and processing of messages by a backend service.\nYou have been told to reduce the cost for the backend service but also ensure efficiency.\nWhich of the following would you consider in the implementation of the backend service? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9bf9e80b18824e058cd3415bfb4b69a7",
    "option" : "Create an SQS queue to handle the processing of messages.",
    "isCorrect" : "true"
  }, {
    "id" : "fef28ad545624d36b7334fd05911cd93",
    "option" : "Create an SNS topics to handle the processing of messages.",
    "isCorrect" : "false"
  }, {
    "id" : "ebfe852eca9742b495acc2f39716c627",
    "option" : "Create a Lambda function to process the messages from the queue.",
    "isCorrect" : "true"
  }, {
    "id" : "ee84f2d41208450094378e9cdc345ad7",
    "option" : "Create an EC2 Instance to process the messages from the queue.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe SQS queue can be used to handle the sending and receiving of messages.\nTo reduce costs, you can use Lambda functions to process the messages.\nThe below is also given in the AWS Documentation.\nUsing AWS Lambda with Amazon SQS.\nAttaching an Amazon SQS queue as an AWS Lambda event source is an easy way to process the queue's content using a Lambda function.\nLambda takes care of:\nAutomatically retrieving messages and directing them to the target Lambda function.\nDeleting them once your Lambda function successfully completes.\nOption B is incorrect since you should use SQS for the handling of messages.\nSNS has no persistence.\nWhichever consumer is present at the time of message arrival, get the message and the message is deleted.\nIf no consumers available, then the message is lost.\nOption D is incorrect since this would not be a cost-effective option.\nFor more information on working with SQS, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\nGood Discussion on StackOverflow here- : https://stackoverflow.com/questions/13681213/what-is-the-difference-between-amazon-sns-and-amazon-sqs.\n\nThe two options that would be considered in the implementation of the backend service to reduce cost but ensure efficiency are:\nA. Create an SQS queue to handle the processing of messages. C. Create a Lambda function to process the messages from the queue.\nExplanation:\nA. Create an SQS queue to handle the processing of messages: Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS eliminates the complexity and overhead associated with managing and operating message oriented middleware, and empowers developers to focus on differentiating work. By using SQS to handle the processing of messages, you can reduce the cost of running a backend service because SQS is a fully managed service and you only pay for what you use. Additionally, it ensures efficiency because SQS is designed to handle any volume of messages, and it allows for message delivery between distributed application components and microservices, which can help in scaling and managing the application.\nC. Create a Lambda function to process the messages from the queue: AWS Lambda is a compute service that lets you run code without provisioning or managing servers. Lambda automatically scales and runs your code in response to each trigger, ensuring that you only pay for the compute time that you consume. By creating a Lambda function to process the messages from the queue, you can reduce the cost of running a backend service because you only pay for the compute time that the function uses, which can be less expensive than running a dedicated server or EC2 instance. Additionally, Lambda ensures efficiency because it can be triggered by events, such as messages in an SQS queue, and it can process the messages in parallel, which can help in handling high volumes of messages.\nD. Create an EC2 Instance to process the messages from the queue: Amazon Elastic Compute Cloud (EC2) is a web service that provides resizable compute capacity in the cloud. By creating an EC2 instance to process the messages from the queue, you can have more control over the environment in which the messages are processed. However, this option may not be the most cost-effective option, because you need to pay for the EC2 instance and the associated infrastructure costs, such as storage and network bandwidth. Additionally, you need to manage the EC2 instance yourself, which can be time-consuming and require more resources. Therefore, this option does not help in reducing the cost of running a backend service, but it can ensure efficiency if you need more control over the environment in which the messages are processed.\nB. Create an SNS topic to handle the processing of messages: Amazon Simple Notification Service (SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. SNS enables you to create topic to which multiple recipients can subscribe and receive published messages. While SNS is useful for broadcasting messages to multiple endpoints, it does not provide a queuing mechanism to handle message processing or to ensure reliable message delivery. Therefore, it is not an appropriate option to handle the processing of messages in this scenario.\nIn conclusion, options A and C would be considered in the implementation of the backend service to reduce cost but ensure efficiency.\n\n"
}, {
  "id" : 194,
  "question" : "You are using Amazon DynamoDB for storing all product details for an online Furniture store.\nWhich of the following expression can be used to return the Colour &amp; Size Attribute of the table during query operations?\n",
  "answers" : [ {
    "id" : "707a5f7ed58540c79e150982250aff0b",
    "option" : "Update Expressions",
    "isCorrect" : "false"
  }, {
    "id" : "8d72a787983948f7800c933574ba7b00",
    "option" : "Condition Expressions",
    "isCorrect" : "false"
  }, {
    "id" : "4f5d51edb0ec4c3e8cb9a0c2123c7c70",
    "option" : "Projection Expressions",
    "isCorrect" : "false"
  }, {
    "id" : "3af6024a87a1472790a3522586f23f78",
    "option" : "Expression Attribute Names.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\n\nThe correct answer is C. Projection Expressions.\nProjection Expressions allow you to specify which attributes should be included in the query result. In the context of DynamoDB, a Projection Expression can be used to select a subset of attributes to be returned when querying a table. This can help reduce the amount of data that needs to be read and transferred, improving performance and reducing costs.\nIn this case, if you want to return only the Color and Size attributes when querying the product details table, you can use a Projection Expression to specify those attributes. For example, the following code snippet shows how to use a Projection Expression in a query operation:\njavascript Copy code const AWS = require('aws-sdk'); const dynamodb = new AWS.DynamoDB();  const params = {     TableName: 'ProductDetails',     KeyConditionExpression: 'ProductID = :pid',     ExpressionAttributeValues: {         ':pid': { S: '12345' }     },     ProjectionExpression: 'Color, Size' };  dynamodb.query(params, (err, data) => {     if (err) {         console.error(err);     } else {         console.log(data.Items);     } });\nIn this example, the ProjectionExpression is set to 'Color, Size', which tells DynamoDB to only return those two attributes for each item that matches the specified KeyConditionExpression. The result of the query will be an array of objects, each containing only the Color and Size attributes for the corresponding item in the table.\nUpdate Expressions, Condition Expressions, and Expression Attribute Names are all used in different contexts within DynamoDB, but none of them are relevant to the task of selecting specific attributes during a query operation.\n\n"
}, {
  "id" : 195,
  "question" : "Which of the following is true with respect to strongly consistent read requests from an application to a DynamoDB with a DAX cluster?\n",
  "answers" : [ {
    "id" : "1cd433d5b196463b968dc15d1d49da4d",
    "option" : "All requests are forwarded to DynamoDB &amp; results are cached.",
    "isCorrect" : "false"
  }, {
    "id" : "4fac7a7fb9154075944b6f6f5f931bf6",
    "option" : "All requests are forwarded to DynamoDB &amp; results are stored in Item Cache before passing to application.",
    "isCorrect" : "false"
  }, {
    "id" : "b5e2c74e7b614bd2b6af3361d4360b64",
    "option" : "All requests are forwarded to DynamoDB &amp; results are stored in Query Cache before passing to application.",
    "isCorrect" : "false"
  }, {
    "id" : "b9e1ab4960c94fc58ab896a7f281125e",
    "option" : "All requests are forwarded to DynamoDB &amp; results are not cached.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nDynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that supports strongly consistent read requests. When an application sends a strongly consistent read request to a DAX cluster, DAX will forward the request to DynamoDB and cache the response. The cached data is then used to serve subsequent read requests, which can improve the application's read performance and reduce the workload on DynamoDB.\nAnswer: A is correct. All strongly consistent read requests are forwarded to DynamoDB and the results are cached in the DAX cluster. The cache is maintained until the item is updated or deleted, or until the cache is evicted due to memory constraints.\nAnswer B is incorrect because the results are not stored in the Item Cache. The Item Cache is a separate cache that is used to cache individual items, rather than query results.\nAnswer C is incorrect because the results are not stored in the Query Cache. The Query Cache is a separate cache that is used to cache the results of specific queries, rather than all strongly consistent read requests.\nAnswer D is incorrect because the results are cached in the DAX cluster. DAX is specifically designed to cache the results of strongly consistent read requests, so that subsequent read requests can be served from the cache without querying DynamoDB again.\n\n"
}, {
  "id" : 196,
  "question" : "A DynamoDB table is set to have a read capacity unit (RCU) of 1\nWhich of the following situations is the RCU suitable for?\n",
  "answers" : [ {
    "id" : "cbec769a39454c65b6b328a4cb5deb99",
    "option" : "For two strongly consistent read of 4KB data.",
    "isCorrect" : "false"
  }, {
    "id" : "187a1f7b3bf3422f8725c36cecb6d94b",
    "option" : "For two eventually consistent read of 4KB data.",
    "isCorrect" : "true"
  }, {
    "id" : "44fe25bb32ba4842813ec8810c5a4dc9",
    "option" : "For three strongly consistent read of 4KB data.",
    "isCorrect" : "false"
  }, {
    "id" : "7bb629f7d58c46fdaa064014a780527d",
    "option" : "For two transactional read of 4KB data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nAs per the documentation,\n\"One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size.\nTwo read request units represent one transactional read for items up to 4 KB.\"\nOption A is incorrect because it should be one strongly consistent read.\nOption B is CORRECT because 1 RCU represents two eventually consistent reads of 4KB data.\nOption C is incorrect because, for three strongly consistent reads of 4KB data, RCU needs to be three.\nOption D is incorrect because two transactional reads of 4KB data need the RCU to be 4(2x2) according to the above reference.\nPlease refer to the below link for DynamoDB read (strongly consistent and eventually consistent) and write requests.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nDynamoDB is a NoSQL database service provided by Amazon Web Services (AWS) that offers fast and flexible NoSQL database solutions. DynamoDB has two capacity modes: provisioned and on-demand. In the provisioned capacity mode, the user needs to specify the read and write capacity units (RCUs and WCUs) required for their table.\nAn RCU is the capacity that enables reading one item per second, where the size of the item is up to 4 KB in size.\nIn this question, the DynamoDB table is set to have an RCU of 1. Let's analyze each option:\nA. For two strongly consistent read of 4KB data. In strongly consistent read, the read operation returns a result that reflects all writes that received a successful response before the read. In other words, the read operation is guaranteed to return the most recent version of the data. Each strongly consistent read requires one RCU. Therefore, two strongly consistent reads of 4KB data require two RCUs, which is greater than the provisioned RCU of 1. Therefore, Option A is not suitable.\nB. For two eventually consistent read of 4KB data. In eventually consistent read, the read operation might not reflect the latest version of the data, as it returns a result that is a composite of all writes that were made before the read, plus some fraction of writes that were made after the read. The fraction of such writes is determined by the consistency level. Eventually consistent reads require half the RCU needed for strongly consistent reads. Therefore, two eventually consistent reads of 4KB data require one RCU, which is equal to the provisioned RCU of 1. Therefore, Option B is suitable.\nC. For three strongly consistent read of 4KB data. Each strongly consistent read requires one RCU. Therefore, three strongly consistent reads of 4KB data require three RCUs, which is greater than the provisioned RCU of 1. Therefore, Option C is not suitable.\nD. For two transactional read of 4KB data. Transactional read is a type of strongly consistent read that is used in transactional requests. In transactional requests, the read operation returns a result that reflects all writes that received a successful response before the read, plus any writes made by the same transaction that are not yet committed. Each transactional read requires two RCUs. Therefore, two transactional reads of 4KB data require four RCUs, which is greater than the provisioned RCU of 1. Therefore, Option D is not suitable.\nIn conclusion, the suitable option is Option B, which is for two eventually consistent reads of 4KB data.\n\n"
}, {
  "id" : 197,
  "question" : "You are developing an application that is working with a DynamoDB table.\nYou need to create a query that has search criteria.\nWhich of the following must be done to work with search queries? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "b4bddd0a0c614b6988a0de9ce41aa33e",
    "option" : "Specify a key condition expression in the query.",
    "isCorrect" : "true"
  }, {
    "id" : "253faaee933d442ca3d9220546e9b2a6",
    "option" : "Specify a partition key name and value in the equality condition.",
    "isCorrect" : "true"
  }, {
    "id" : "9f22dd87cd514b928c5e89c8a3ef900f",
    "option" : "Specify a sort key name and value in the equality condition.",
    "isCorrect" : "false"
  }, {
    "id" : "6f936cac87a741f5b3389a780882beaa",
    "option" : "Specify a filter expression.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS Documentation mentions the following.\nKey Condition Expression.\nTo specify the search criteria, you use a key condition expression-a string that determines the items to be read from the table or index.\nYou must specify the partition key name and value as an equality condition.\nOption C is incorrect since you need to mention the partition key and not the sort key.\nOption D is incorrect since this is used to filter results further.\nFor more information on working with queries, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html\n\nWhen working with DynamoDB, search queries can be performed using the Query API. To perform a search query in DynamoDB, you must specify the partition key (or the partition key and sort key) that will be used to retrieve the items from the table.\nTo work with search queries, the following must be done:\nSpecify a partition key name and value in the equality condition: When using the Query API, you must specify the partition key value for the items you want to retrieve. This is done using the KeyConditionExpression parameter. The partition key value must be an exact match, and you can only specify one partition key value at a time. Specify a sort key name and value in the equality condition: If your table has a sort key, you can also use it to refine your search by specifying a sort key value in the KeyConditionExpression. The sort key value must also be an exact match, and you can only specify one sort key value at a time.\nTherefore, options B and C are the correct answers. Option A is not sufficient on its own, as it does not specify the partition key value. Option D refers to the FilterExpression, which is used to further refine the results of a Query operation after the initial KeyConditionExpression has been evaluated.\n\n"
}, {
  "id" : 198,
  "question" : "You are working on an application that saves strings in the DynamoDB table.\nFor strings with a size more than 400KB, you are getting an item size exceeded error.\nWhich of the following is a recommended option to store strings with a larger size using AWS services?\n",
  "answers" : [ {
    "id" : "1d2e90ab01d34fc78880da43349e5c32",
    "option" : "Compress large size strings to fit in DynamoDB table.",
    "isCorrect" : "false"
  }, {
    "id" : "6404ab12c3284cf7975aa39921eb850d",
    "option" : "Split strings between multiple tables.",
    "isCorrect" : "false"
  }, {
    "id" : "6c2fdf4d6a4d4a11986e0ca89dc00bce",
    "option" : "Save string object in S3 with object identifiers inDynamoD.",
    "isCorrect" : "true"
  }, {
    "id" : "879b261ecab44e44a8424fcb9278bf1a",
    "option" : "Open a ticket with AWS support to increase Item size to more than 400 K.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon S3 can be used to saved items which are exceeding 400 KB.\nIn this option, Items are saved in S3 buckets while an object identifier is saved in the DynamoDB table which points to an item in S3.\nOption A is incorrect.\nCompressing large-size strings to fit in the DynamoDB table can be used to resolve this error.\nStill, it will be a short-term solution &amp; not a recommended option for permanently resolving item size error.\nOption B is incorrect.\nSplitting strings in multiple tables will incur inconsistency while updating items in multiple tables, &amp; also this will not be a permanent resolution.\nOption D is incorrect as the Item size of 400 KB is a hard limit &amp; cannot be expanded.\nFor more information on using S3 with DynamoDB for saving larger files, refer to the following URL-\nttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html.\n\nThe correct option to store strings with a size larger than 400KB using AWS services is option C - Save string object in S3 with object identifiers in DynamoDB.\nDynamoDB is a NoSQL database service provided by AWS, which is optimized for fast and predictable performance with seamless scalability. However, it has a limit on the item size which is 400KB, and items larger than this size are not supported.\nOption A, compressing large size strings, may not be a feasible option as it will require decompression every time the string is read from the table, which could lead to performance issues.\nOption B, splitting strings between multiple tables, is also not recommended as it can add complexity to the application design and querying data across multiple tables can be challenging.\nOption D, opening a ticket with AWS support to increase the item size, is not feasible as item size limit is a hard limit, and AWS cannot increase this limit.\nOption C, saving the string object in S3 with object identifiers in DynamoDB, is the recommended option. With this option, the application can store the large strings in an S3 bucket, which can handle objects with a size of up to 5TB. The object identifiers of the stored strings can be stored in the DynamoDB table, and the application can retrieve the string object by reading the object identifier from DynamoDB and retrieving the object from S3.\nTherefore, option C provides a scalable and efficient solution to store large strings in DynamoDB without violating the item size limit.\n\n"
}, {
  "id" : 199,
  "question" : "You are the team lead for an application that is already in production and using S3 buckets.\nUsers from another country have now started actively using the objects in the S3 bucket.\nWhich of the following can be done to reduce the latency of access to objects for the new users?\n",
  "answers" : [ {
    "id" : "4bb3aced2beb4f8ea294874ccfd2b6c2",
    "option" : "Enable cross region replication for the bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "0cc958474d9b4d698697b2dcd36d3ee4",
    "option" : "Enable Encryption for the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "e9b065185282481181038b0e621ccdfa",
    "option" : "Host a static web site.",
    "isCorrect" : "false"
  }, {
    "id" : "1f76a43327c74e878f35f9fac90122ed",
    "option" : "Change the storage class.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is given as a use case in the documentation.\nWhen to Use CRR.\nCross-region replication can help you do the following.\nComply with compliance requirements-Although Amazon S3 stores your data across multiple geographically distant Availability Zones by default, compliance requirements might dictate that you store data at even greater distances.\nCross-region replication allows you to replicate data between distant AWS Regions to satisfy these requirements.\nMinimize latency-If your customers are in two geographic locations, you can minimize latency in accessing objects by maintaining object copies in AWS Regions that are geographically closer to your users.\nIncrease operational efficiency-If you have compute clusters in two different AWS Regions that analyze the same set of objects, you might choose to maintain object copies in those Regions.\nMaintain object copies under different ownership-Regardless of who owns the source object, you can tell Amazon S3 to change replica ownership to the AWS account that owns the destination bucket.\nThis is referred to as the owner override option.\nYou might use this option to restrict access to object replicas.\nOption B is invalid since this is only used when you want to secure data at rest.\nOption C is invalid since this is only used when you want to have a static web site in place.\nOptionD is invalid since this will not help reduce latency.\nFor more information on cross-region replication, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html\n\nOption A: Enable cross region replication for the bucket.\nEnabling cross-region replication allows you to automatically replicate data between S3 buckets in different regions. This can be useful for disaster recovery purposes or for reducing latency for users in different regions. In this scenario, enabling cross-region replication could reduce the latency of access to objects for users in the new country by replicating the objects to a bucket in a region that is closer to them.\nOption B: Enable Encryption for the bucket.\nEnabling encryption for the bucket will not directly reduce latency for users in the new country. However, it is still a best practice to enable encryption to protect sensitive data and meet compliance requirements.\nOption C: Host a static web site.\nHosting a static website in S3 bucket can be useful for reducing latency for users in different regions. By hosting the website in an S3 bucket in the same region as the user, the latency can be reduced. However, this option assumes that the application can be modified to use a static website instead of the existing application.\nOption D: Change the storage class.\nChanging the storage class will not directly reduce latency for users in the new country. However, it may reduce costs and optimize performance based on the access pattern of the objects. For example, if the objects are infrequently accessed, changing the storage class to S3 Glacier may be more cost-effective. If the objects are frequently accessed, changing the storage class to S3 Standard-Infrequent Access may be more cost-effective.\nOverall, Option A is the best option for reducing latency for users in the new country. However, it's important to consider the cost implications of cross-region replication and ensure that the replication configuration is set up correctly to avoid data inconsistencies.\n\n"
}, {
  "id" : 200,
  "question" : "You are creating a Lambda function that will be accessing a database.\nDue to compliance reasons, all database connecting strings must be stored encrypted at rest.\nHow can you accomplish this in the Lambda function? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9d74cd7dfeb94ea38980e26c18dff7d1",
    "option" : "Put the database connection string in the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "2e61375ed3c5404fbfc7b016b1b35d7f",
    "option" : "Put the database connecting string as an environment variable.",
    "isCorrect" : "true"
  }, {
    "id" : "3e861c7513384ad1bde564b45eb6bf89",
    "option" : "Encrypt the entire Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "edd3db48dcbc4f2fa84730c1a566a4e3",
    "option" : "Encrypt the environment variable.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nOption A is incorrect.\nThe connection string is assigned to an environment variable within a Lambda function.\nWe cannot directly put the connection string into the lambda function.\nOption B is correct.\nThe connection string is assigned to an environment variable within a Lambda function.\nOption C is incorrect.\nYou need to enable encryption for the environment variable and not the lambda function.\nOption D is correct.\nYou need to enable encryption for the environment variable.\nYou can do this via Environment variables as mentioned in the AWS Documentation.\nExpand the Environment variables section.\nEnter your key-value pair.\nExpand the Encryption configuration section.\nNote that Lambda provides a default service key under the KMS key to encrypt at rest which encrypts your information after uploading.\nIf the information you provided is sensitive, you can additionally check the Enable helpers for encryption in transit checkbox and supply a custom key.\nThis masks the value you entered and results in a call to AWS KMS to encrypt the value and return it as Ciphertext.\nIf you haven't created a KMS key for your account, you will be provided a link to the AWS IAM console to create one.\nThe account must have encrypt and decrypt permissions for that key.\nNote that the Encrypt button toggles to Decrypt after you choose it.\nThis affords you the option to update the information.\nOnce you have done that, choose the Encrypt button.\nThe Code button provides sample decrypt code specific to the runtime of your Lambda function that you can use with your application.\nBecause of what is mentioned in the documentation, all other options are invalid.\nFor more information on environment variables in Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/tutorial-env_console.html\n\nOption A is not a good choice as it would mean storing the database connection string in plaintext inside the Lambda function code. This can be a security risk as anyone with access to the function code could see the database credentials.\nOption B is a better choice because you can use AWS Key Management Service (KMS) to encrypt the environment variable holding the database connection string. This way, the database credentials are encrypted at rest and are only decrypted when needed by the Lambda function. To do this, you can create a KMS key, grant the Lambda function permission to use the key, and then encrypt the environment variable holding the database connection string with that key.\nOption C is not necessary because it would mean encrypting the entire function code, including the environment variables, which would make it difficult to use and maintain the function.\nOption D is a valid choice as well. If you only want to encrypt the environment variable holding the database connection string, you can create a KMS key and use it to encrypt that specific environment variable. This way, the other environment variables are not encrypted and can be used without decryption.\nOverall, the best approach would be to use Option B or Option D, depending on whether you want to encrypt all environment variables or just the one holding the database connection string.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 202,
  "question" : "You are a developer for your company responsible for the development and deployment of AWS Lambda functions.\nYou have been told to start the automated deployment of Lambda based applications.\nWhich of the following collated services can be used for automated deployment? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "14042e653a2f48acbf2d492e784b820b",
    "option" : "AWS API gateway",
    "isCorrect" : "false"
  }, {
    "id" : "9185dd865a364a1a99696bc8a26ce76b",
    "option" : "AWS Code Pipeline",
    "isCorrect" : "true"
  }, {
    "id" : "df574d3c39fc418b86bf609bd702149a",
    "option" : "AWS Code Build",
    "isCorrect" : "true"
  }, {
    "id" : "9393b152464341be85655174d4e32219",
    "option" : "AWS Code Deploy.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B, C and D.\nThe AWS Documentation mentions the following.\nAutomating Deployment of Lambda Applications.\nIn the previous section, you learned how to create a SAM template, generate your deployment package, and use the AWS CLI to deploy your serverless application manually.\nIn this section, you will leverage the following AWS services to automate the deployment process fully.\nAWS CodePipeline: You use AWS CodePipeline to model, visualize, and automate the steps required to release your serverless application.\nFor more information, see What is AWS CodePipeline?\nAWS CodeBuild: You use AWS CodeBuild to build, locally test, and package your serverless application.\nFor more information, see What is AWS CodeBuild?\nAWS CloudFormation: You use AWS CloudFormation to deploy your application.\nFor more information, see What is AWS CloudFormation?\nAWS CodeDeploy: You use AWS CodeDeploy to deploy updates to your serverless applications gradually.\nFor more information on how to do this, see Gradual Code Deployment.\nOption A is invalid because it can be used in front of the Lambda function but cannot do the automated deployment.\nFor more information on automating deployment, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/automating-deployment.html\n\nAs a developer, automating the deployment process for AWS Lambda functions can save time and reduce the likelihood of errors during the deployment process. AWS offers several services to facilitate this process.\nThe three services that can be used for automated deployment are:\nB. AWS CodePipeline - This is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. With CodePipeline, you can define a workflow that includes different stages such as building, testing, and deploying your application. CodePipeline integrates with other AWS services such as AWS CodeBuild and AWS CodeDeploy to provide a complete CI/CD solution.\nC. AWS CodeBuild - This is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. CodeBuild can be used to build and test your Lambda functions before they are deployed. CodeBuild can also be integrated with other AWS services such as AWS CodePipeline and AWS CodeDeploy to provide a complete CI/CD solution.\nD. AWS CodeDeploy - This is a fully managed deployment service that automates software deployments to a variety of compute services such as EC2 instances, Lambda functions, and on-premises servers. CodeDeploy can be used to deploy your Lambda functions automatically. CodeDeploy supports rolling updates, blue-green deployments, and can be integrated with other AWS services such as AWS CodePipeline and AWS CodeBuild to provide a complete CI/CD solution.\nA. AWS API Gateway - While AWS API Gateway is a powerful service for building and managing APIs, it is not primarily used for automated deployment of Lambda-based applications. However, you can use AWS API Gateway in conjunction with AWS CodePipeline and AWS CodeDeploy to build and deploy your API Gateway and Lambda functions automatically.\nIn conclusion, the three services that can be used for automated deployment of AWS Lambda-based applications are AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy.\n\n"
}, {
  "id" : 203,
  "question" : "An Amazon DynamoDB table is populated with a large amount of data from an application capturing client data from various servers across various regions.\nWhich of the following is a preferred option for deletion of old items in a DynamoDB table?\n",
  "answers" : [ {
    "id" : "d4aa9580e0064c5184e4e813ce17d727",
    "option" : "Schedule a cron Job during off peak hours to delete unwanted items.",
    "isCorrect" : "false"
  }, {
    "id" : "f7c25048040a41d4a58904652de5e776",
    "option" : "Run Periodic jobs to delete old items.",
    "isCorrect" : "false"
  }, {
    "id" : "21958e3aa5c04749a52c76899d80ceb9",
    "option" : "Pre-Create a DynamoDB table with specific time.",
    "isCorrect" : "false"
  }, {
    "id" : "7f6a508701bc4d4fb97cd1674cb4fc4b",
    "option" : "Enable TTL on each item to automatically delete old items.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAmazon DynamoDB TTL can delete items in a DynamoDB table based upon the TTL values you specify.\nTTL is specified as a timestamp for items in a table.\nThis is an easier way of deleting old items &amp; thus freeing up space &amp; saving cost.\nOption A is incorrect as although this will work.\nBut additional admin work will be required to create &amp; run jobs.\nOption B is incorrect as this will require additional manual jobs to run to delete old items.\nOption C is incorrect.\nThis will require adding additional logic in an application to switch over to new tables after a specific time for which DynamoDB tables are pre-created.\nFor more information on TTL for DynamoDB, refer to the following URLs-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/howitworks-ttl.html\nhttps://aws.amazon.com/blogs/database/automatically-archive-items-to-s3-using-dynamodb-time-to-live-with-aws-lambda-and-amazon-kinesis-firehose/\n\nThe preferred option for deletion of old items in a DynamoDB table is to enable TTL (Time to Live) on each item to automatically delete old items.\nTTL is a feature in DynamoDB that automatically deletes items from a table after a specified amount of time has passed. This feature can be enabled on a per-item basis by adding a TTL attribute to the item. The value of this attribute is a timestamp that specifies when the item should be deleted.\nEnabling TTL on each item has several benefits. Firstly, it eliminates the need for manual deletion of old items, which can be time-consuming and error-prone. Secondly, it ensures that old items are deleted in a timely manner, which helps keep the size of the DynamoDB table under control. Finally, it allows you to focus on more important tasks, such as analyzing the data that remains in the table.\nIn contrast, the other options are less desirable. Scheduling a cron job during off-peak hours to delete unwanted items requires manual intervention, which can be error-prone and time-consuming. Running periodic jobs to delete old items is also less desirable because it requires more overhead and can impact the performance of the DynamoDB table. Pre-creating a DynamoDB table with a specific time is not a viable solution because it is not dynamic enough to handle changing data over time.\nIn summary, enabling TTL on each item is the preferred option for deleting old items in a DynamoDB table because it is automatic, timely, and requires minimal overhead.\n\n"
}, {
  "id" : 204,
  "question" : "You are developing an application that is going to make use of Docker containers.\nTraffic needs to be routed based on demand to the application.\nDynamic host port mapping would be used for the docker containers.\nWhich of the following two options would you use for the distribution of traffic to the Docker containers?\n",
  "answers" : [ {
    "id" : "14601b384a844d13b61556d9692c68e5",
    "option" : "AWS Application Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "8147cf6f63804defbf7f2ffe64f5776f",
    "option" : "AWS Network Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "572513db51e644c29a1740bac248fd64",
    "option" : "AWS Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "187af315d38f4a7c95f67440429e6f82",
    "option" : "AWS Classic Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A &amp; B.\nThe AWS Documentation mentions the following.\nApplication Load Balancers offer several features that make them attractive for use with Amazon ECS services.\nApplication Load Balancers allow containers to use dynamic host port mapping (so that multiple tasks from the same service are allowed per container instance).\nApplication Load Balancers support path-based routing and priority rules (so that multiple services can use the same listener port on a single Application Load Balancer).\nNetwork Load Balancers do support dynamic host port mapping.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/load-balancer-types.html#nlb\nOptions D is invalid since the Application Load balancer is ideal when you have the requirement for path-based routing.\nOption C is incorrect since this is used for DNS Routing.\nFor more information on Service Load Balancing, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html\n\nTo distribute traffic to Docker containers using dynamic port mapping, you would need to use an AWS Load Balancer. Load Balancers are a service provided by AWS that automatically distributes incoming traffic across multiple targets, such as EC2 instances or containers.\nThere are three types of Load Balancers available on AWS:\nApplication Load Balancer (ALB): Used for HTTP and HTTPS traffic, it routes traffic based on application-level content (such as URL paths or hostnames). It is designed to handle traffic for microservices-based architectures and is often used for web applications, APIs, and containers. Network Load Balancer (NLB): Used for TCP, UDP, and TLS traffic, it routes traffic based on IP protocol data. It is designed to handle traffic for applications that require extremely high performance and static IP addresses, such as gaming or streaming. Classic Load Balancer (CLB): Used for HTTP, HTTPS, TCP, and SSL traffic, it is a legacy service that is still supported but has been mostly replaced by the newer ALB and NLB services.\nIn this case, since you are using Docker containers with dynamic port mapping, the best option would be to use an Application Load Balancer (ALB). The ALB can dynamically route traffic to different containers based on the port mappings specified in the container definitions.\nAWS Route 53 is a DNS service that can be used to route traffic to different resources based on DNS records. However, it is not designed for load balancing and cannot dynamically route traffic based on port mappings like an ALB can.\nTherefore, the correct answer to the question is A. AWS Application Load Balancer.\n\n"
}, {
  "id" : 205,
  "question" : "You are working on an application that provides an online Car booking service using Amazon DynamoDB.\nThis is a read-heavy application that reads car &amp; driver location details &amp; provides the latest position to prospective car booking customers.\nWhich of the following can be used to have consistent data writes &amp; avoid unpredictable spikes in DynamoDB requests during peak hours?\n",
  "answers" : [ {
    "id" : "e781edc81f9d473d94731cf199412592",
    "option" : "Write Around Cache using DynamoDB DAX.",
    "isCorrect" : "false"
  }, {
    "id" : "a5223f7e5d32474f92fa4836628b8549",
    "option" : "Write Through Cache using DynamoDB DAX.",
    "isCorrect" : "true"
  }, {
    "id" : "4830aac26c8d4cfd81d0a7ff64004527",
    "option" : "Use Side Cache using Redis along with DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "853c4a420cf44fd4afd3b9e3145099ab",
    "option" : "Write Through Cache using Redis along with DynamoD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nDAX is intended for applications that require high-performance reads.\nAs a write-through cache, DAX allows you to issue writes directly so that your writes are immediately reflected in the item cache.\nYou do not need to manage cache invalidation logic because DAX handles it for you.\nFor more information, please check the below link-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/dynamodb-dg.pdf\nOption A is incorrect.\nWrite Around Cache is generally useful when there is a considerable amount of data to be written to the database.\nIn this case, data is directly written to DynamoDB instead of DAX.\nOption C is incorrect as Using Side Cache using Redis is eventually consistent and non-durable which may add additional delay.\nOption D is incorrect.\nAs with Write Through cache using Redis, there are chances of missing data during new scaling out.\nFor more information on Caching with DAX, refer to the following URL-\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-accelerator-dax-a-read-throughwrite-through-cache-for-dynamodb/\n\nTo have consistent data writes and avoid unpredictable spikes in DynamoDB requests during peak hours for an online Car booking service application that reads car and driver location details, we need a caching solution that can handle high traffic and offer predictable performance.\nA. Write Around Cache using DynamoDB DAX: DynamoDB Accelerator (DAX) is a caching service that can be used with DynamoDB to improve read performance. Write Around Cache is a cache strategy in which write operations are not cached. Therefore, using Write Around Cache with DynamoDB DAX might not be an effective solution for a read-heavy application that requires consistent data writes.\nB. Write Through Cache using DynamoDB DAX: Write Through Cache is a cache strategy in which write operations are first written to the cache before writing them to the database. This strategy ensures that data is always consistent between the cache and the database. DynamoDB DAX supports Write Through Cache, which can be an effective solution for a read-heavy application that requires consistent data writes.\nC. Use Side Cache using Redis along with DynamoDB: Redis is an in-memory data structure store that can be used as a caching solution. Side Cache is a cache strategy in which the cache sits alongside the database and receives copies of the data for reading operations. Redis can be used as a side cache with DynamoDB to provide consistent read performance. This strategy can be effective for a read-heavy application that requires consistent data writes.\nD. Write Through Cache using Redis along with DynamoDB: Write Through Cache can be used with Redis along with DynamoDB to provide consistent data writes and read performance. In this strategy, write operations are first written to the cache and then to the database, ensuring consistency between the cache and the database. This can be an effective solution for a read-heavy application that requires consistent data writes.\nTherefore, option D - Write Through Cache using Redis along with DynamoDB is the most effective solution for this scenario.\n\n"
}, {
  "id" : 206,
  "question" : "You are developing a set of Lambda functions for your application.\nThe company mandates that all API calls to Lambda functions be recorded.\nWhich of the below service can help achieve this?\n",
  "answers" : [ {
    "id" : "9831fd87d6e64b5a8c0651ff8d383c1c",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "dd04b5436721471ca537b39913fd2142",
    "option" : "AWS CloudTrail",
    "isCorrect" : "true"
  }, {
    "id" : "a1d63d54f5f14e5b864267ede94213a3",
    "option" : "AWS VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "1772b259b4a045129d1eb3302c727e4e",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAWS Lambda is integrated with AWS CloudTrail, a service that captures API calls made by or on behalf of AWS Lambda in your AWS account and delivers the log files to an Amazon S3 bucket that you specify.\nCloudTrail captures API calls made from the AWS Lambda console or from the AWS Lambda API.\nUsing the information collected by CloudTrail, you can determine what request was made to AWS Lambda, the source IP address from which the request was made, who made the request, when it was made, and so on.\nOption A is incorrect since this can only give information on the logs from Cloudwatch but not who called the Lambda function itself.\nOption C is incorrect since this is used for logging network traffic to the VPC.Option D is incorrect since this cannot give API logging information.\nFor more information on using Cloudtrail, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/logging-using-cloudtrail.html\n\nThe correct answer to the question is B. AWS CloudTrail.\nAWS CloudTrail is a service that helps in recording all the API calls made to other AWS services within your AWS account. It provides a complete history of all the API calls made to your account, including who made the call, when they made it, and what actions were performed. CloudTrail logs are stored in an S3 bucket, where they can be retrieved and analyzed later.\nAWS Cloudwatch is a service that helps in monitoring AWS resources and applications that run on AWS. It provides a comprehensive view of the performance and operational health of your resources, but it does not record all the API calls made to your AWS account.\nAWS VPC Flow Logs is a service that helps in capturing information about the IP traffic going to and from network interfaces in your VPC. It records details such as source and destination IP addresses, ports, and protocols. However, it does not record API calls made to AWS services.\nAWS Trusted Advisor is a service that helps in optimizing your AWS infrastructure by providing best practices and recommendations. However, it does not record API calls made to AWS services.\nTherefore, the correct answer is B. AWS CloudTrail.\n\n"
}, {
  "id" : 207,
  "question" : "As an API developer, you have just configured an API with the AWS API gateway service.\nYou are testing out the API and getting the below response whenever an action is made to an undefined API resource. { “message”: “Missing Authentication Token” } You want to customize the error response and make it more user-readable.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "e17afab1c984490db3a62976c3861460",
    "option" : "By setting up the appropriate method in the API gateway",
    "isCorrect" : "false"
  }, {
    "id" : "6f6c1d5066534cbdbe0d402346ed6a77",
    "option" : "By setting up the appropriate method integration request in the API gateway",
    "isCorrect" : "false"
  }, {
    "id" : "913776d6362c41d580c9167051601528",
    "option" : "By setting up the appropriate gateway response in the API gateway",
    "isCorrect" : "true"
  }, {
    "id" : "ed6508b503ee4fcbaa67b2166542765a",
    "option" : "By setting up the appropriate gateway request in the API gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - C.\nThis is mentioned in the AWS Documentation.\nSet up Gateway Responses to Customize Error Responses.\nIf API Gateway fails to process an incoming request, it returns the client an error response without forwarding the request to the integration backend.\nBy default, the error response contains a short descriptive error message.\nFor example, if you attempt to call an operation on an undefined API resource, you receive an error response with the { \"message\": \"Missing Authentication Token\" } message.\nIf you are new to API Gateway, you may find it difficult to understand what actually went wrong.\nFor some of the error responses, API Gateway allows customization by API developers to return the responses in different formats.\nFor the Missing Authentication Token example, you can add a hint to the original response payload with the possible cause, as in this example: {\"message\":\"Missing Authentication Token\", \"hint\":\"The HTTP method or resources may not be supported.\"}.\nThe documentation clearly mentions how this should be configured.\nHence the other options are all invalid.\nFor more information on the gateway response, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/customize-gateway-responses.html\n\nThe correct answer to this question is C. By setting up the appropriate gateway response in the API gateway.\nWhen an API resource is not defined, the API Gateway returns a standard error response that says \"Missing Authentication Token.\" This message is not user-friendly and does not provide any useful information about the error. To make the error response more user-readable, you can customize it using the Gateway Response feature of API Gateway.\nThe Gateway Response feature allows you to customize error responses for different HTTP status codes returned by the API. To customize the error response for the \"Missing Authentication Token\" error, you need to create a new Gateway Response for the 401 HTTP status code.\nHere are the steps to customize the error response:\nOpen the API Gateway console and select your API. Click on \"Gateway Responses\" under the \"API Gateway\" section in the left navigation pane. Select \"Create a Gateway Response.\" Choose \"Unauthorized\" for the response status. Enter a Response Body Mapping Template that provides a more user-friendly message. For example:\njson Copy code {   \"message\": \"Sorry, you are not authorized to access this resource. Please check your authentication credentials and try again.\" }\nClick on \"Create.\"\nAfter creating the Gateway Response, any request that results in a 401 HTTP status code will return the customized error message instead of the default \"Missing Authentication Token\" message.\nOption A is incorrect because setting up the appropriate method in the API gateway is used to define the HTTP methods supported by your API resource.\nOption B is incorrect because setting up the appropriate method integration request is used to configure how API Gateway should handle requests to your backend integration.\nOption D is incorrect because setting up the appropriate gateway request in the API gateway is not a valid option.\n\n"
}, {
  "id" : 208,
  "question" : "A company is developing an application that interacts with a DynamoDB table.\nThere is now a security mandate that all data must be encrypted at rest.\nHow can you achieve this requirement? (Select TWO)\n",
  "answers" : [ {
    "id" : "1506e442993d4a548606e6aceb0f4155",
    "option" : "Enable encryption using AWS owned key.",
    "isCorrect" : "true"
  }, {
    "id" : "ebc21ccbe78b4956a2a2b6a44f284929",
    "option" : "Enable encryption using AWS managed key.",
    "isCorrect" : "true"
  }, {
    "id" : "3f4611cc7351447fba418f5a95c72182",
    "option" : "Enable encryption using client keys.",
    "isCorrect" : "false"
  }, {
    "id" : "0a02829b725c42f1b70ccb30f68c2962",
    "option" : "Enable your application to use the SDK to decrypt the data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and B.\nDynamoDB encryption is mandatory at the time of table creation itself and it is of two types.\ni.\nDEFAULT method using 'AWS owned key'\nii.\nKMS method using 'AWS managed key '\nTherefore the following options are correct.\nA.\nEnable encryption using AWS-owned CMK.\nB.\nEnable encryption using AWS-managed CMK.\nFor more information on Encryption at rest for DynamoDB, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html\n\nTo achieve the requirement of encrypting data at rest in a DynamoDB table, the following two options can be used:\nA. Enable encryption using AWS owned key: Amazon DynamoDB provides server-side encryption using AWS managed keys that are unique to your AWS account, and automatically rotate on a regular basis. This encryption is transparent to the application and the SDKs, as DynamoDB automatically encrypts and decrypts the data as needed. By enabling this option, you can ensure that all data stored in the DynamoDB table is encrypted at rest.\nB. Enable encryption using AWS managed key: Alternatively, you can enable server-side encryption using AWS Key Management Service (KMS) customer managed keys. This option allows you to use your own keys to encrypt and decrypt the data stored in DynamoDB. With this option, you have greater control over the keys and can audit their usage. This option can also meet compliance requirements for certain regulations.\nC. Enable encryption using client keys: Client-side encryption involves encrypting the data before it is sent to DynamoDB and then decrypting it after it is retrieved. This option requires the application to manage the encryption keys and can add additional complexity and management overhead.\nD. Enable your application to use the SDK to decrypt the data: This option is not relevant for encrypting data at rest in DynamoDB, as it focuses on decrypting the data after it has been retrieved from the table.\nIn summary, the recommended options to achieve the requirement of encrypting data at rest in a DynamoDB table are to enable encryption using AWS-owned or AWS-managed keys.\n\n"
}, {
  "id" : 209,
  "question" : "You are developing a .Net Core application that is going to be hosted in an Elastic beanstalk environment.\nThe application is going to make backend calls to a database.\nYou need to increase the performance of the application during the testing phase.\nHow can you diagnose any performance issues in the easiest way possible?\n",
  "answers" : [ {
    "id" : "64f118a27ac24bd5b1f9ad822f614c09",
    "option" : "Place a Load balancer in front of Elastic beanstalk.",
    "isCorrect" : "false"
  }, {
    "id" : "ff54cd5afb21414f813e461ba9eb3d39",
    "option" : "Place instrumentation calls in your code.",
    "isCorrect" : "false"
  }, {
    "id" : "7f062928aa2b444ea209a1b98bb58d08",
    "option" : "Make use of traces using the X-Ray service.",
    "isCorrect" : "true"
  }, {
    "id" : "d62852e7f0754f45bfe6eb48db7bc1da",
    "option" : "Use Cloudwatch logs to debug issues.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe best way is to use the X-Ray service which can be used to automatically see the call trace and time spend in each layer.\nThe below snapshot from the AWS Documentation showcases this.\nAWS X-Ray is a service that collects data about requests that your application serves.\nX-Ray provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.\nFor any traced request to your application, you can see detailed information in the AWS X-Ray console.\nThis includes information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases, and HTTP web APIs.\nOption A is incorrect since this will not help you diagnose ways to improve your application's performance.\nOption B is partially correct.\nYou can do this, but the easiest way is to use the X-Ray service.\nOption D is incorrect since the logs will not give detailed time tracing of the calls of your application.\nFor more information on an example on this, please refer to the below URL-\nhttps://aws.amazon.com/blogs/developer/new-aws-x-ray-net-core-support/\n\n\nThe best way to diagnose performance issues in a .Net Core application that is going to be hosted in an Elastic Beanstalk environment is to make use of traces using the X-Ray service (Option C).\nAWS X-Ray is a service that allows developers to trace requests made to their applications and identify performance bottlenecks. By using X-Ray, you can gain visibility into how your application is performing and identify any areas that need optimization.\nX-Ray supports .NET Core applications, and you can easily integrate it into your application by installing the AWS X-Ray SDK for .NET. Once installed, you can use the SDK to add trace information to your application's requests and responses. This trace information is then sent to X-Ray, where you can view it in the AWS Management Console.\nWith X-Ray, you can see the time taken by each component of your application, such as the time taken to make a database call or process a request. You can also see how requests are flowing through your application and identify any issues that are causing delays or bottlenecks.\nTherefore, option C is the best choice for diagnosing performance issues in a .Net Core application hosted in an Elastic Beanstalk environment. Option B (placing instrumentation calls in your code) can also help, but it requires more manual work and can be less effective than using X-Ray. Option A (placing a load balancer in front of Elastic Beanstalk) and option D (using CloudWatch logs to debug issues) are not directly related to diagnosing performance issues, although they may be useful in other aspects of application development and management.\n\n"
}, {
  "id" : 210,
  "question" : "You're planning on using the AWS CodeDeploy tool for the deployment of your application.\nWhich of the following is used to specify how your application will be deployed to the underlying instances?\n",
  "answers" : [ {
    "id" : "5acc8c99b3d040aa802e6747dde7ab7a",
    "option" : "appConfig.json",
    "isCorrect" : "false"
  }, {
    "id" : "ee8d811cf9724bcf9e8ac0be0b95f3fb",
    "option" : "DeploymentGroup",
    "isCorrect" : "false"
  }, {
    "id" : "c3d6189cb5274be49138c7fb4cf9c09a",
    "option" : "appConfig.YAML",
    "isCorrect" : "false"
  }, {
    "id" : "fbb88dfe204d43c792cb480b96ce339c",
    "option" : "appspec.yml.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThis is mentioned in the AWS Documentation.\nBecause of what is mentioned in the documentation on what is the functionality of the appspec.yml file, all other options are incorrect.\nRefer to page 19 and 25 of the below in.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-user.pdf\nFor more information on Code Deploy, please refer to the below URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\nNOTE:\nAs per AWS docs, AppSpec Files on an AWS Lambda Compute Platform is a YAML-formatted or JSON-formatted file used by AWS CodeDeploy to manage deployment.\nHowever, according to AWS, an application specification file (AppSpec file) is unique to the AWS CodeDeploy service.\nThe AppSpec file is used to manage each deployment as a series of lifecycle event hooks, which are defined in the file.\nAccording to our question options, A &amp; C are referring to an appConfig file that is wrong.\n\n\nThe correct answer is D. appspec.yml.\nAWS CodeDeploy is a fully-managed deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions. The appspec.yml file is used to specify the deployment instructions for an application. This file contains the necessary instructions for AWS CodeDeploy to deploy the application to the underlying instances.\nThe appspec.yml file is located in the root directory of the application code and provides CodeDeploy with information on how to deploy the application to the instances in the deployment group. It specifies the files to be copied, the scripts to be run, and the order in which they should be executed.\nThe appspec.yml file consists of several sections, including:\nFiles: This section specifies the source and destination paths for the files that will be deployed to the instances. Hooks: This section specifies the scripts that will be run at various stages of the deployment process, such as before or after the application is installed or updated. Permissions: This section specifies the permissions that the deployment process requires to perform certain actions, such as running scripts or accessing files.\nOverall, the appspec.yml file plays a critical role in the deployment process and ensures that the application is deployed correctly to the underlying instances.\n\n"
}, {
  "id" : 211,
  "question" : "You are currently deploying an application that needs to have a sign-up and sign-in functionality added.\nAs much as possible, you would want to reduce the coding effort required for these modules.\nYou also need to ensure that code is executed automatically after the sign-in process is complete.\nHow can you achieve this? Choose 3 answers from the options below.\n",
  "answers" : [ {
    "id" : "13f633a8793343e2b240fd26eada6f77",
    "option" : "Use the AWS Cognito service to provide the sign-up and sign-in functionality.",
    "isCorrect" : "true"
  }, {
    "id" : "1633690e40214206944dd92c94edc285",
    "option" : "Use the AWS IAM service to provide the sign-up and sign-in functionality.",
    "isCorrect" : "false"
  }, {
    "id" : "621c717a0f394f518114ba1413eec520",
    "option" : "Use AWS Cloudwatch events to trigger code that will be run after the user sign-in process is complete.",
    "isCorrect" : "true"
  }, {
    "id" : "0dbdb9afdb354056a981bcb0a25a9556",
    "option" : "Trigger a lambda function to execute the code associated with the post authentication event.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, C and D.\nThis is mentioned in the AWS Documentation.\n###############\nYou can create an AWS Lambda function and then trigger that function during user pool operations such as user sign-up, confirmation, and sign-in (authentication) with a Lambda trigger.\nYou can add authentication challenges, migrate users, and customize verification messages.\n##################\nOption B is incorrect since IAM cannot simulate the sign-in and sign-up process that the application would require.\nFor more information on using Lambda triggers with AWS Cognito, please refer to the below URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools-working-with-aws-lambda-triggers.html\n\nThe best way to add sign-up and sign-in functionality to your application while minimizing coding effort is to use an existing service that provides these features. AWS provides two services that can help you achieve this: Cognito and IAM. Both services provide user authentication and access control mechanisms, but they differ in their intended use cases.\nOption A: Use the AWS Cognito service to provide the sign-up and sign-in functionality. Amazon Cognito is a managed service that provides authentication, authorization, and user management for web and mobile applications. It makes it easy to add user sign-up and sign-in functionality to your applications, and also supports social identity providers such as Facebook, Google, and Amazon. With Cognito, you can create user pools to manage your users, and use the provided SDKs and APIs to authenticate your users and authorize access to your resources.\nOption B: Use the AWS IAM service to provide the sign-up and sign-in functionality. Amazon IAM (Identity and Access Management) is a service that helps you securely control access to AWS resources. While IAM is primarily intended for controlling access to AWS resources, it can also be used to provide user authentication and authorization for your own applications. IAM allows you to create users and groups, assign policies to control access to resources, and use the provided APIs to authenticate your users.\nOption C: Use AWS Cloudwatch events to trigger code that will be run after the user sign-in process is complete. AWS CloudWatch is a monitoring and observability service that provides real-time insights into your application and infrastructure. It also supports events, which can be used to trigger code in response to certain events. You can configure CloudWatch to trigger a Lambda function or other code when a user signs in to your application, allowing you to perform additional actions such as logging or sending notifications.\nOption D: Trigger a lambda function to execute the code associated with the post authentication event. AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. You can use Lambda to execute code in response to various events, including user authentication. With Lambda, you can create a function that will be triggered when a user signs in to your application, and then perform additional actions such as logging, sending notifications, or interacting with other AWS services.\nIn conclusion, the three options that can be used to achieve the required functionality are:\nUse the AWS Cognito service to provide the sign-up and sign-in functionality. Use AWS Cloudwatch events to trigger code that will be run after the user sign-in process is complete. Trigger a lambda function to execute the code associated with the post authentication event.\n\n"
}, {
  "id" : 212,
  "question" : "Your company is planning to use the AWS CodePipeline service for their CI/CD process.\nThey have their own propriety build process that needs to be incorporated in CodePipeline.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "f742170bcc4b4e7bb18251914c208c1a",
    "option" : "Create a default action for your Pipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "105ba40ed8e5474b95ee7ef82ebb15b4",
    "option" : "Create a custom action for your Pipeline.",
    "isCorrect" : "true"
  }, {
    "id" : "e589f28718364e8eb0cea757b4099ad9",
    "option" : "Create a primary action for your Pipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "a6ace200d3cc4dd894e88c87d1408bf8",
    "option" : "Create a secondary action for your Pipeline.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is mentioned in the AWS Documentation.\nAWS CodePipeline includes several actions that help you configure build, test, and deploy resources for your automated release process.\nIf your release process includes activities that are not included in the default actions, such as an internally developed build process or a test suite, you can create a custom action for that purpose and include it in your pipeline.\nYou can use the AWS CLI to create custom actions in pipelines associated with your AWS account.\nOption A is incorrect because the question asks to build your own proprietary build process that needs to be incorporated in CodePipeline.\nOptions C and D are incorrect because these 2 actions do not exist in the pipeline.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions.html\nFor more information on creating custom actions, please refer to the below URL-\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-custom-action.html\n\nTo incorporate a proprietary build process in AWS CodePipeline, you need to create a custom action for your pipeline.\nAWS CodePipeline is a fully managed continuous delivery service that helps automate your release pipelines for fast and reliable application and infrastructure updates. It allows you to define a series of stages, each of which can have multiple actions that perform different tasks, such as building code, deploying infrastructure, and testing applications.\nA default action is a predefined action that is available for every pipeline stage. It can be used as is, or you can modify its configuration to suit your requirements. However, it may not be suitable for your proprietary build process.\nA primary action is the first action in a stage that must complete successfully before the next action can be executed. Similarly, a secondary action is the second or subsequent action in a stage. They are also predefined actions and may not be suitable for your proprietary build process.\nTherefore, to incorporate your proprietary build process, you should create a custom action for your pipeline. A custom action allows you to define a custom process that meets your specific needs, such as building a proprietary application, running a custom script, or executing a third-party tool. You can also define the input and output artifacts for the custom action, which allow you to pass data between actions and stages.\nOnce you have created the custom action, you can add it to a stage in your pipeline and configure it as required. You can also specify the conditions under which the action should be executed, such as when a specific file is modified in your source code repository.\nIn summary, to incorporate your proprietary build process in AWS CodePipeline, you should create a custom action for your pipeline, which allows you to define a custom process that meets your specific needs.\n\n"
}, {
  "id" : 213,
  "question" : "You've just started developing an application on your On-premise network.\nThis application will interact with the Simple Storage Service and some DynamoDB tables.\nAs the developer, how would you ensure that your SDK can interact with the AWS services on the cloud in the most secure manner?\n",
  "answers" : [ {
    "id" : "59ff15bc30b8427b8ce53657a5a0827d",
    "option" : "Create an IAM Role with the required permissions and add it to your workstation.",
    "isCorrect" : "false"
  }, {
    "id" : "245ecfe8bd76466bb3629e03dcb2a25a",
    "option" : "Create an IAM Role with the required permissions and make a call to the STS service.",
    "isCorrect" : "true"
  }, {
    "id" : "3c440398b37148379affdbb8310926e3",
    "option" : "Create an IAM User, generate access keys, then use them from within your program.",
    "isCorrect" : "false"
  }, {
    "id" : "5b5157bee0df4d5995604c4af4016a68",
    "option" : "Create an IAM User, generate a security token, and use the Security Token from within your program.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAn IAM role can be created to ensure the SDK can interact with AWS services on the cloud.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html\nOption A is incorrect because an on-premise workstation should not use IAM Roles.\nOption C is incorrect because IAM User should not be used in this scenario because the question is asking for the most secure option here.\nOption D is incorrect since we should not generate a security token to interact with the various AWS services during the development phase.\nFor more information on the usage of credentials in AWS, please refer to the below link-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\n\n\nAs a developer, you can ensure that your SDK interacts with AWS services in a secure manner by using the AWS Identity and Access Management (IAM) service. IAM enables you to create and manage AWS users, groups, and roles and define their permissions to access AWS resources.\nOption A: Creating an IAM Role with the required permissions and adding it to your workstation is not recommended as it could compromise the security of your AWS account.\nOption B: Creating an IAM Role with the required permissions and making a call to the Security Token Service (STS) is a better approach. STS provides temporary security credentials that you can use to access AWS resources. You can assume an IAM Role and obtain temporary security credentials to interact with AWS services. By using STS, you can reduce the risk of exposing long-term access keys, which can be easily compromised.\nOption C: Creating an IAM User and generating access keys, then using them from within your program is also not recommended as the access keys can be easily compromised. It is recommended to use IAM Roles and temporary security credentials to access AWS resources instead of IAM Users and long-term access keys.\nOption D: Creating an IAM User, generating a security token, and using the Security Token from within your program is not a recommended approach as it requires you to manage the security token, which can be a cumbersome process. It is recommended to use STS to obtain temporary security credentials instead.\nTherefore, the correct answer is B. Create an IAM Role with the required permissions and make a call to the STS service.\n\n"
}, {
  "id" : 214,
  "question" : "You've been asked to move an existing development environment on the AWS Cloud.\nThis environment consists mainly of Docker-based containers.\nYou need to ensure that minimum effort is taken during the migration process.\nWhich of the following step would you consider for this requirement?\n",
  "answers" : [ {
    "id" : "1f5d83d2e971404e9213f16a4c8f9758",
    "option" : "Create an Opswork stack and deploy the Docker containers.",
    "isCorrect" : "false"
  }, {
    "id" : "527a221f1fd2499486f948e659a2abec",
    "option" : "Create an application and Environment for the Docker containers in the Elastic Beanstalk service.",
    "isCorrect" : "true"
  }, {
    "id" : "3650284cafa248f89a57f0042b6fca0e",
    "option" : "Create an EC2 Instance. Install Docker and deploy the necessary containers.",
    "isCorrect" : "false"
  }, {
    "id" : "f6b4828ead8741eba6f22082b876a03f",
    "option" : "Create an EC2 Instance. Install Docker and deploy the necessary containers. Add an Autoscaling Group for scalability of the containers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe Elastic Beanstalk service is the ideal service to quickly provision development environments.\nYou can also create environments that can be used to host Docker-based containers.\nOption A is incorrect since using Opswork is best suited when you have multiple stacks and want to use configuration tools for the environment.\nOptions C and D are incorrect since this would involve a lot of effort in deployment.\nFor more information on using Docker containers in Elastic Beanstalk, please refer to the below link-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html\n\nThe correct answer for this question would be B. Create an application and Environment for the Docker containers in the Elastic Beanstalk service.\nExplanation: Elastic Beanstalk is a fully-managed service provided by AWS that allows developers to deploy applications quickly and easily. Elastic Beanstalk supports a wide variety of programming languages and platforms, including Docker containers. Elastic Beanstalk abstracts the underlying infrastructure, allowing developers to focus on writing code and deploying their applications.\nWhen migrating an existing development environment consisting mainly of Docker-based containers, using Elastic Beanstalk would ensure that minimum effort is taken during the migration process. The Elastic Beanstalk service abstracts the underlying infrastructure, allowing developers to focus on deploying their Docker containers, without worrying about the underlying infrastructure.\nCreating an application and environment for the Docker containers in Elastic Beanstalk is a straightforward process. Developers can simply upload their Docker containers to Elastic Beanstalk, and Elastic Beanstalk will handle the deployment and scaling of the containers. Elastic Beanstalk also provides a web-based console and a command-line interface (CLI) for managing the environment, including scaling and monitoring.\nOption A, creating an Opswork stack and deploying the Docker containers, is not the best option in this scenario. While OpsWorks is another fully-managed service provided by AWS, it is better suited for more complex applications and infrastructures. Using OpsWorks to deploy Docker containers would require more effort than using Elastic Beanstalk.\nOption C and D, creating an EC2 instance, installing Docker, and deploying the necessary containers, is a viable option. However, this approach would require more effort and would not be as scalable or automated as using Elastic Beanstalk. Additionally, using EC2 instances would require more management and maintenance than using Elastic Beanstalk. Option D, adding an Autoscaling Group, would provide scalability for the containers, but would also require more management and configuration than using Elastic Beanstalk.\n\n"
}, {
  "id" : 215,
  "question" : "You've currently developed an application that makes use of AWS RDS - MySQL service.\nDuring the testing phase, you can see that the database is taking a performance hit.\nAfter further investigation, you can see that the same queries are causing the performance bottleneck on the application.\nWhich of the following development steps should be taken to resolve this issue?\n",
  "answers" : [ {
    "id" : "e6b4c97d4a3444db84af325dd66e7223",
    "option" : "Use the Multi-AZ feature for the underlying database.",
    "isCorrect" : "false"
  }, {
    "id" : "ac7c9523bdf74a908d168cba56e1c67f",
    "option" : "Change the underlying instance type for the database.",
    "isCorrect" : "false"
  }, {
    "id" : "f72a4c70cdbd4c67997b2da77edf9866",
    "option" : "Use SQS queues to store the results of the query for faster access.",
    "isCorrect" : "false"
  }, {
    "id" : "a7e2ee18439d4014baf9022e3291c88a",
    "option" : "Use AWS ElastiCache to store the results of the query for faster access.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nOption A is invalid because this is used for the high availability of the database.\nOption B is invalid because we don't know the server's current configuration.\nWe guess that this could be the underlying issue.\nOption C is invalid because this service is normally used for messaging across distributed components of an application.\nThe AWS Documentation states the following which makes sense to use AWS ElasticCache for this scenario.\nAmazon ElastiCache offers fully managed Redis and Memcached.\nSeamlessly deploy, operate, and scale popular open source compatible in-memory data stores.\nBuild data-intensive apps or improve your existing apps' performance by retrieving data from high throughput and low latency in-memory data stores.\nFor more information on AWS ElastiCache, please refer to the below link-\nhttps://aws.amazon.com/elasticache/\n\nWhen an application utilizing Amazon RDS - MySQL service experiences performance issues, it may be due to multiple factors, such as insufficient instance size or database configuration issues. It is crucial to identify the source of the issue before making any changes.\nIn this scenario, the source of the bottleneck is a specific query that is causing performance issues. Therefore, we need to focus on optimizing this query rather than making changes to the database instance itself.\nOption A suggests using Multi-AZ feature for the underlying database. The Multi-AZ feature is a high availability and data protection solution that replicates data synchronously to a standby replica in a different Availability Zone (AZ). While this feature improves availability, it does not directly address the performance issues caused by a particular query.\nOption B suggests changing the underlying instance type for the database. This option may improve performance by providing additional CPU, memory, and I/O resources to the database instance. However, it is not targeted towards optimizing the problematic query that is causing the performance bottleneck.\nOption C suggests using SQS queues to store the results of the query for faster access. This option is not recommended because SQS is a distributed message queue service that is designed for asynchronous message communication between various components of an application. It is not ideal for storing and retrieving the results of a specific query.\nOption D suggests using AWS ElastiCache to store the results of the query for faster access. AWS ElastiCache is a fully managed in-memory data store and cache service that supports two popular open-source in-memory caching engines: Redis and Memcached. Caching query results in ElastiCache can improve the performance of the application by reducing the number of requests to the database instance. This option may be the most appropriate solution to the problem described in the question.\nIn conclusion, option D, using AWS ElastiCache to store the results of the query for faster access, is the most appropriate development step to resolve the issue described in the question.\n\n"
}, {
  "id" : 216,
  "question" : "You've written an application that uploads objects onto an S3 bucket.\nThe size of the object varies between 200 - 500 MB.\nYou've seen that the application sometimes takes a longer than expected time to upload the object.\nYou want to improve the performance of the application.\nWhich of the following would you consider?\n",
  "answers" : [ {
    "id" : "b0a4ee409aca4b83890d2142742e8e88",
    "option" : "Create multiple threads and upload the objects in the multiple threads.",
    "isCorrect" : "false"
  }, {
    "id" : "134ed1242d9c443a961cabcd447e45b9",
    "option" : "Write the items in batches for better performance.",
    "isCorrect" : "false"
  }, {
    "id" : "241e7fa173d84627a9f1c1987694dc1b",
    "option" : "Use the Multipart upload API.",
    "isCorrect" : "true"
  }, {
    "id" : "31031543c4a84785bb5b46ac3920a50c",
    "option" : "Enable versioning on the Bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following to support this.\nThe Multipart Upload API enables you to upload large objects in parts.\nYou can use this API to upload new large objects or copy an existing object (see Operations on Objects).\nMultipart uploading is a three-step process: You initiate the upload, you upload the object parts, and after you have uploaded all the parts, you complete the multipart upload.\nUpon receiving the complete multipart upload request, Amazon S3 constructs the object from the uploaded parts.\nYou can then access the object just as you would any other object in your bucket.\nOption A and B are incorrect because both need more changes in the application and are not easier than option.\nC.Option D is incorrect because enabling versioning does not help to improve the performance of uploading objects.\nFor more information on Amazon S3 Multipart file upload, please refer to the below link-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html\n\nThe best solution to improve the performance of the application for uploading objects of varying size between 200 - 500 MB would be to use the Multipart upload API. Option C is the correct answer.\nMultipart upload allows the upload of large objects in smaller parts. This means that instead of uploading the entire object as a single entity, the object is divided into smaller parts and uploaded individually. This approach provides several benefits, including improved reliability, the ability to upload parts in parallel, and the ability to retry individual parts that may have failed.\nUsing multiple threads (Option A) to upload objects in parallel may improve performance to some extent, but it may not be as efficient as the Multipart upload API. Also, managing threads and ensuring data consistency across them can be challenging.\nWriting items in batches (Option B) may be useful in some scenarios, but it may not be the optimal solution for uploading large objects. It could be more suited for a scenario where many small objects need to be uploaded.\nEnabling versioning on the bucket (Option D) is a good practice to maintain a history of object versions. However, it does not improve the performance of uploading large objects.\nIn summary, using the Multipart upload API (Option C) is the best solution to improve the performance of the application for uploading large objects to an S3 bucket.\n\n"
}, {
  "id" : 217,
  "question" : "Your team lead has finished creating a build project in the console.\nYou have access to run the build but not to access the project.\nYou want to specify a different source location for the build.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "254b2b1749ce427fa415018783ef4c5f",
    "option" : "Issue the update project command and specify the new location of the build.",
    "isCorrect" : "false"
  }, {
    "id" : "66aea82b240f4ac080a841dee14d9dd4",
    "option" : "Specify the new location of the build in a new buildspec.yml file and issue the update-project command.",
    "isCorrect" : "false"
  }, {
    "id" : "c5ceadb24de54bc38b10ce359896a958",
    "option" : "Specify the new location of the build in a new buildspec.yml file and use the start-build command.",
    "isCorrect" : "true"
  }, {
    "id" : "1c6a902ddf0a43e58fb9d34da39ea195",
    "option" : "Specify the new location of the build in a new buildspec.yml file and use the update-build command.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOptions A and B are incorrect since the question mentions that you don't have access to the project.\nOption D is incorrect since you have to use the start-build command.\nThe AWS Documentation mentions the following.\nTo override the default build spec file name, location, or both, do one of the following.\nRun the AWS CLI create-project or update-project command, setting the buildspec value to the path to the alternate build spec file relative to the value of the built-in environment variable CODEBUILD_SRC_DIR.\nYou can also do the equivalent with the create project operation in the AWS SDKs.\nFor more information, see Create a Build Project or Change a Build Project's Settings.\nRun the AWS CLI start-build command, setting the buildspecOverride value to the path to the alternate build spec file relative to the value of the built-in environment variable CODEBUILD_SRC_DIR.\nYou can also do the equivalent with the start build operation in the AWS SDKs.\nFor more information on the build specification for AWS CodeBuild, please refer to the below link-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\n\nThe correct answer is C. Specify the new location of the build in a new buildspec.yml file and use the start-build command.\nTo explain in more detail, let's break down the options:\nA. Issue the update project command and specify the new location of the build. This option is incorrect because it assumes you have access to update the project configuration, which is not the case according to the question. Also, the update-project command is used to update project-level settings like the source code provider, environment variables, and artifacts settings.\nB. Specify the new location of the build in a new buildspec.yml file and issue the update-project command. This option is incorrect because it also assumes you have access to update the project configuration. Additionally, the update-project command is not used to update the buildspec.yml file.\nC. Specify the new location of the build in a new buildspec.yml file and use the start-build command. This is the correct option. You can specify the new source location in a new buildspec.yml file and use the start-build command to run the build. The buildspec.yml file is a declarative file that specifies the build phases, including the source location. You can create a new buildspec.yml file and specify the new source location, then use the start-build command to initiate the build process with the updated buildspec.yml file.\nD. Specify the new location of the build in a new buildspec.yml file and use the update-build command. This option is also incorrect because the update-build command is used to update a build that is already in progress, and not for updating the buildspec.yml file. Additionally, it assumes you have access to update the build configuration, which is not the case according to the question.\nIn conclusion, the correct way to specify a different source location for the build is to create a new buildspec.yml file with the updated source location and then use the start-build command to initiate the build process.\n\n"
}, {
  "id" : 218,
  "question" : "You are working as a team lead for your company.\nYou have been told to manage the Blue Green Deployment methodology for one of the applications.\nWhich of the following are some of the approaches for implementing this methodology? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "92f4bde23da64389a817b96a84180c9b",
    "option" : "Using Autoscaling Groups to scale on demands for both deployments.",
    "isCorrect" : "false"
  }, {
    "id" : "9119c3fa86f54e8983be5e2bb2908dc7",
    "option" : "Using Route 53 with Weighted Routing policies.",
    "isCorrect" : "true"
  }, {
    "id" : "9f63c9ebe7b5467f917fccf31493e81e",
    "option" : "Using Route 53 with Latency Routing policies.",
    "isCorrect" : "false"
  }, {
    "id" : "6a7be6befbf84bbd959ce9641be47149",
    "option" : "Using Elastic Beanstalk with the swap URL feature.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe AWS Documentation mentions the following.\nWeighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource.\nThis can be useful for various purposes, including load balancing and testing new versions of software.\nBecause AWS Elastic Beanstalk performs an in-place update when you update your application versions, your application can become unavailable to users for a short period of time.\nYou can avoid this downtime by performing a blue/green deployment where you deploy the new version to a separate environment and then swap CNAMEs of the two environments to instantly redirect traffic to the new version.\nOption A is incorrect as on its own Autoscaling should be used to shift traffic and not on demand for such deployments.\nOption C is incorrect since you need to use Route 53 with Weighted Routing policies.\nFor more information on weighted routing policy, please refer to the below link-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\nFor more information on the Swap URL feature, please refer to the below link.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\n\nBlue-Green Deployment is a methodology that involves creating two identical environments, where one environment is used for production (Green) while the other environment (Blue) is used for testing and development. In this methodology, the team deploys the application code changes to the Blue environment first, tests them thoroughly, and then switches the traffic to the Blue environment once it has been verified to work as expected.\nThis ensures that any issues in the new code deployment are caught before the traffic is redirected to the production environment. If any issues arise, the traffic can easily be switched back to the Green environment, which remains untouched.\nTo implement Blue-Green Deployment methodology, there are several approaches available:\nA. Using Autoscaling Groups to scale on demands for both deployments: This approach involves using Autoscaling groups for both Blue and Green environments. The Autoscaling group can be configured to launch instances based on the traffic demand, and it can also ensure that the group maintains a minimum and maximum number of instances running. This approach allows for easy scaling of the environments based on demand and ensures that both environments are always up-to-date.\nB. Using Route 53 with Weighted Routing policies: This approach involves using Route 53, which is Amazon's DNS service, with Weighted Routing policies. The team can create two identical DNS records for the application, with one pointing to the Green environment and the other pointing to the Blue environment. The traffic is then routed to the two environments based on a weight assigned to each DNS record. This approach allows for a gradual transition of traffic from one environment to another, ensuring that there is no sudden surge of traffic on the new environment.\nC. Using Route 53 with Latency Routing policies: This approach is similar to the above approach, but instead of using Weighted Routing policies, the team uses Latency Routing policies. This ensures that the traffic is routed to the environment with the lowest latency, providing a better user experience for customers.\nD. Using Elastic Beanstalk with the swap URL feature: This approach involves using Elastic Beanstalk, which is a platform that allows developers to deploy, manage, and scale web applications. The swap URL feature allows the team to easily switch the traffic from one environment to another. The Blue environment is deployed, tested, and then the URL is swapped with the Green environment, making the Blue environment the production environment.\nIn conclusion, the two approaches that can be used to implement the Blue-Green Deployment methodology are:\nUsing Autoscaling Groups to scale on demands for both deployments. Using Route 53 with Weighted Routing policies.\n\n"
}, {
  "id" : 219,
  "question" : "You have a set of developers that need to work with applications hosted on the Elastic Beanstalk environment.\nYou need to ensure they can work with the beanstalk environments but not give them access to the AWS Console.\nHow can you achieve this in the BEST way possible?\n",
  "answers" : [ {
    "id" : "4b78916f21134e76a5a0b2f5ec4e080e",
    "option" : "Ask them to manage the environments via the SDK.",
    "isCorrect" : "false"
  }, {
    "id" : "47e50f44e8fc4768a121026e3681ddc4",
    "option" : "Ask them to manage the environments via the EB CLI.",
    "isCorrect" : "true"
  }, {
    "id" : "298c6fc02dfa48b784af61b5ee6b8b5f",
    "option" : "Ask them to manage the environments via an EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "1b625243605e4f9f926c8d4244365f7d",
    "option" : "Ask them to manage the environments via an ECS cluster.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nThe EB CLI is a command-line interface for Elastic Beanstalk that provides interactive commands that simplify creating, updating and monitoring environments from a local repository.\nUse the EB CLI as part of your everyday development and testing cycle as an alternative to the AWS Management Console.\nBecause of what the AWS Documentation mentions, all other options are invalid.\nFor more information on using the Elastic Beanstalk CLI, please refer to the below link-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3.html\n\nThe BEST way to ensure that developers can work with Elastic Beanstalk environments without giving them access to the AWS Console is to ask them to manage the environments via the Elastic Beanstalk Command Line Interface (EB CLI).\nThe EB CLI is a command-line tool that simplifies the process of deploying, managing, and scaling applications in Elastic Beanstalk environments. It provides a set of commands that developers can use to interact with their applications, such as deploying new versions, viewing logs, and scaling instances.\nBy using the EB CLI, developers can work with Elastic Beanstalk environments from their local development environments, without the need to access the AWS Console. They can also use their preferred programming languages and tools to manage the environments, as the EB CLI supports multiple languages and platforms.\nOption A, asking developers to manage the environments via the SDK, is also a valid solution. However, it requires developers to write code to interact with Elastic Beanstalk, which may not be as straightforward as using the EB CLI. Additionally, developers may need to authenticate their requests to the AWS API, which can add complexity to the solution.\nOptions C and D, asking developers to manage the environments via an EC2 instance or an ECS cluster, are not ideal solutions for this scenario. Both options require developers to have access to AWS resources, which goes against the requirement of not giving them access to the AWS Console. Additionally, managing Elastic Beanstalk environments through an EC2 instance or an ECS cluster can add unnecessary complexity to the solution.\nTherefore, the BEST solution for this scenario is to ask developers to manage the environments via the Elastic Beanstalk Command Line Interface (EB CLI).\n\n"
}, {
  "id" : 220,
  "question" : "You are defining a Redis cluster using the AWS Elasticache service.\nYou need to define common values across the nodes for memory usage and item sizes.\nWhich of the following components of the ElastiCache service allows you to define this?\n",
  "answers" : [ {
    "id" : "a6e5bf9f970c4eb9866300d96a060637",
    "option" : "Endpoints",
    "isCorrect" : "false"
  }, {
    "id" : "54d55f38b45647b3bf8f1d9f5a30fdbe",
    "option" : "Parameter Groups",
    "isCorrect" : "true"
  }, {
    "id" : "42ea37d5b8f24c259e464376e06b725b",
    "option" : "Security Groups",
    "isCorrect" : "false"
  }, {
    "id" : "059915355e3c411e8d3995d1f21f422b",
    "option" : "Subnet Groups.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nCache parameter groups are an easy way to manage runtime settings for supported engine software.\nParameters are used to control memory usage, eviction policies, item sizes, and more.\nAn ElastiCache parameter group is a named collection of engine-specific parameters that you can apply to a cluster.\nBy doing this, you make sure that all of the nodes in that cluster are configured exactly in the same way.\nOption A is incorrect because an endpoint is the unique address your application uses to connect to an ElastiCache node or cluster.\nOption C is incorrect because the security group is used to controls access and does not help with this.\nOption D is incorrect because the subnet group cannot be used to define values in Elasticache.\nFor more information on the components for Elasticache, please refer to the below link-\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.Components.html\n\nThe correct answer is B. Parameter Groups.\nParameter Groups are a key feature of the AWS ElastiCache service that allows you to define and manage a set of configuration parameters that are applied to all nodes in a Redis cluster. These parameters are used to control various aspects of the cluster, such as memory usage, item sizes, and other performance-related settings.\nWhen you create a Redis cluster in ElastiCache, you can specify a Parameter Group that contains a set of pre-defined configuration parameters. You can also create your own custom Parameter Group that contains your own set of configuration parameters that are specific to your use case.\nUsing Parameter Groups, you can ensure that all nodes in the Redis cluster are configured consistently and with the same set of parameters, which can help ensure optimal performance and reliability of the cluster.\nEndpoints, Security Groups, and Subnet Groups are also important components of the ElastiCache service, but they do not allow you to define common values for memory usage and item sizes across the nodes in a Redis cluster.\nEndpoints are used to access the Redis cluster from your application code or other AWS services.\nSecurity Groups are used to control network access to the Redis cluster by defining inbound and outbound rules that specify which IP addresses or security groups are allowed to access the cluster.\nSubnet Groups are used to specify the subnets in which the Redis cluster will be deployed, and to specify which Availability Zones the cluster will use for high availability.\n\n"
}, {
  "id" : 221,
  "question" : "You've just created an AWS Lambda function.\nYou're running the function, but the output of the function is not as expected.\nYou need to check and see what the issue is.\nWhich of the following can help the developer debug the issue with the Lambda function?\n",
  "answers" : [ {
    "id" : "1b5ac4b5302542c7840f02802c9ef133",
    "option" : "Check Cloudwatch logs",
    "isCorrect" : "true"
  }, {
    "id" : "2de7b24796eb4eaabbbd5680340706f4",
    "option" : "Check VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "e8ef1084de4d4323bbd2a8090adc65b9",
    "option" : "Check AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "bcb6341d8aef49eaa57a56c76192024a",
    "option" : "Check AWS Inspector.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS Lambda automatically monitors Lambda functions on your behalf, reporting metrics through Amazon CloudWatch.\nTo help you troubleshoot failures in a function, Lambda logs all requests handled by your function and automatically stores logs generated by your code through Amazon CloudWatch Logs.\nOption B is incorrect because VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.Option C is incorrect because AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices.\nIt cannot be used to debug Lambda functions.\nOption D is incorrect because Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.\nIt is not a troubleshooting tool for the Lambda function.\nFor more information on monitoring functions, please refer to the below link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-logs.html\n\nThe correct answer is A. Check CloudWatch logs.\nWhen developing AWS Lambda functions, it's important to have a way to debug issues that may arise. AWS provides several tools for debugging Lambda functions, but the most common one is CloudWatch logs.\nCloudWatch is a monitoring and logging service provided by AWS. When you create a Lambda function, AWS automatically sets up a CloudWatch log group for that function. Every time the function runs, it generates logs that are sent to the log group.\nTo check the logs, you can go to the AWS Management Console, navigate to the CloudWatch service, and then select the log group associated with your Lambda function. From there, you can view the logs in real-time and search for specific events or error messages.\nVPC Flow Logs are used to capture information about IP traffic going to and from network interfaces in a VPC, and can be used to diagnose connectivity issues. However, they are not related to debugging Lambda functions.\nAWS Trusted Advisor is a service that provides recommendations to help optimize your AWS infrastructure, but it does not provide debugging tools for Lambda functions.\nAWS Inspector is a security assessment service that helps you identify security issues in your AWS resources, but it also does not provide debugging tools for Lambda functions.\nTherefore, the correct answer is A. Check CloudWatch logs.\n\n"
}, {
  "id" : 222,
  "question" : "You're a developer for a company that is developing a .net based application.\nThis application will be hosted in AWS.\nThere is a need to encrypt data.\nCurrently, the company does not have a key store for managing encryption.\nWhich of the following could the developer use in this code for encrypting data?\n",
  "answers" : [ {
    "id" : "e41cf21c42af4bc3b29119838e76b1f8",
    "option" : "Use S3 Server-side encryption to work with encryption keys.",
    "isCorrect" : "false"
  }, {
    "id" : "d0506fe415a948e98c4fdfa5bc5d801d",
    "option" : "Use the AWS KMS service to generate data keys.",
    "isCorrect" : "true"
  }, {
    "id" : "7ffe787732914d37b373a3238f062d41",
    "option" : "Use the AWS Config service to generate data keys.",
    "isCorrect" : "false"
  }, {
    "id" : "3b426bdf867c4dd29d4d6e71b6fafec9",
    "option" : "Use S3 client-side encryption to work with encryption keys.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data.\nThe master keys you create in AWS KMS are protected by FIPS 140-2 validated cryptographic modules.\nOptions A and D are incorrect since here there is no mention of working with the S3 service.\nOption C is incorrect because the AWS Config service can't be used to work with encryption keys.\nFor more information on the KMS service, please refer to the below link-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/overview.html\n\nAs a developer for a company developing a .net based application that will be hosted on AWS, you are tasked with encrypting data. Currently, the company does not have a key store for managing encryption. Which of the following options should you use to encrypt data?\nThe best option for encrypting data in this scenario would be to use the AWS KMS (Key Management Service) to generate data keys.\nAWS KMS is a fully managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. KMS allows you to create customer master keys (CMKs) and data keys that can be used to encrypt and decrypt your data. You can also use KMS to manage and rotate your encryption keys, and audit your key usage to meet your compliance requirements.\nUsing S3 server-side encryption is an option, but it is more appropriate when storing data in S3, rather than encrypting data in your application. S3 server-side encryption provides a way to encrypt data at rest in S3 buckets using keys managed by AWS. This can provide additional security for data stored in S3, but it may not meet the specific needs of encrypting data in your application.\nAWS Config service is not a service that provides encryption or key management. It is a service that provides you with a detailed inventory of your AWS resources, configuration history, and configuration change notifications.\nS3 client-side encryption can also be used to encrypt data, but it may not be the best option for this scenario since it requires the application to manage the encryption keys, which can be more complex and risky than using a managed service like AWS KMS.\nIn conclusion, the best option for encrypting data in a .net based application hosted in AWS, would be to use the AWS KMS service to generate data keys.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 224,
  "question" : "You've been hired as a developer to work on an application.\nThis application is hosted on an EC2 Instance and interacts with an SQS queue.\nIt's been noticed that when the application is pulling messages, a lot of empty responses are being returned.\nWhat change can you make to ensure that the application uses the SQS queue effectively?\n",
  "answers" : [ {
    "id" : "1dc4adc48422488a9feba88c8d5a3489",
    "option" : "Use long polling",
    "isCorrect" : "true"
  }, {
    "id" : "4cbc9e7cb8464e81b5411d6e31cf91b9",
    "option" : "Set a custom visibility timeout",
    "isCorrect" : "false"
  }, {
    "id" : "79ecb203934242a2b591e957153708ee",
    "option" : "Use short polling",
    "isCorrect" : "false"
  }, {
    "id" : "e4e18cdb6a534524999efede1294d715",
    "option" : "Implement exponential backoff.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nOption B is invalid because this is valid only for the processing time for the Messages.\nOption C is invalid because this would not be a cost-effective option.\nOption D is invalid because this is not a practice for SQS queues.\nThe AWS Documentation mentions the following.\nLong polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessagerequest) and false empty responses (when messages are available but aren't included in a response).\nFor more information on long polling in SQS, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html\n\nThe best answer for this scenario is A. Use long polling.\nHere's why:\nWhen an application pulls messages from an SQS queue, it can either use short polling or long polling.\nShort polling is the default behavior. In this approach, the application sends a request to the SQS queue, and if there is a message available, the queue returns it immediately. If there is no message available, the queue returns an empty response. Short polling is not the most efficient way to interact with an SQS queue because it can result in a lot of empty responses being returned, as the application is constantly checking the queue for new messages.\nLong polling is an alternative approach to interacting with an SQS queue. In this approach, the application sends a request to the queue, but instead of immediately returning an empty response if there is no message available, the queue waits for a specified amount of time for a message to become available. If a message is available during that time, the queue immediately returns it to the application. If no message becomes available during that time, the queue returns an empty response.\nBy using long polling, the application can reduce the number of empty responses it receives from the SQS queue. This is because the queue is actively waiting for a message to become available before returning a response to the application. This approach can improve the performance of the application and reduce the load on the SQS queue.\nOption B, \"Set a custom visibility timeout,\" is not the best answer to this scenario because it does not address the issue of empty responses being returned from the SQS queue. Visibility timeout is the amount of time that a message is hidden from other consumers after it has been consumed. This option may be useful if the issue was related to messages being consumed too quickly, but it does not address the issue of empty responses.\nOption C, \"Use short polling,\" is not the best answer to this scenario because it is the default behavior and is the cause of the issue the application is experiencing. Using short polling would not solve the problem of empty responses being returned from the SQS queue.\nOption D, \"Implement exponential backoff,\" may be useful in some scenarios but does not address the issue of empty responses being returned from the SQS queue. Exponential backoff is a technique used to retry failed requests with progressively longer wait times between each attempt. This option may be useful if the application is experiencing errors or timeouts, but it does not address the issue of empty responses.\n\n"
}, {
  "id" : 225,
  "question" : "You've just deployed an AWS Lambda function.\nThis Lambda function would be invoked via the API gateway service.\nYou want to know if there were any errors while the Lambda function was being invoked.\nWhich of the following service would allow you to check the performance of your underlying Lambda function?\n",
  "answers" : [ {
    "id" : "380212dcc1d94ba39e182b3652c9cafc",
    "option" : "VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "c5a842f8171d401b8a67a79581fd3600",
    "option" : "Cloudwatch",
    "isCorrect" : "true"
  }, {
    "id" : "b87f0e9eb4944db4a44af2a2578617aa",
    "option" : "Cloudtrail",
    "isCorrect" : "false"
  }, {
    "id" : "53c8a307127e4ac592f270c9c2f9e26b",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nIn AWS Lambda, you can use Cloudwatch metrics to see the number of Invocation errors.\nThe below snapshot from the AWS Documentation shows an example of this.\nAccessing Amazon CloudWatch Metrics for AWS Lambda.\nAWS Lambda automatically monitors functions on your behalf, reporting metrics through Amazon CloudWatch.\nThese metrics include total requests, latency, and error rates.\nFor more information about Lambda metrics, see AWS Lambda Metrics.\nFor more information about CloudWatch, see the Amazon CloudWatch User Guide.\nYou can monitor metrics for Lambda and view logs by using the Lambda console, the CloudWatch console, the AWS CLI, or the CloudWatch API.\nThe following procedures show you how to access metrics using these different methods.\nTo access metrics using the Lambda console.\nSign in to the AWS Management Console and open the AWS Lambda console at https://console.aws.amazon.com/lambda/.\nOn the Functions page, choose the function name and then choose the Monitoring tab.\nOption A is invalid because this service is used to get the network traffic entering your VPC.Option C is invalid because this service is used to monitor API Activity.\nOption D is invalid because this service does not have the ability to give you the metrics for the Lambda service.\nFor more information on accessing metrics for AWS Lambda, please refer to the below link-\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-access-metrics.html\n\n\nThe correct answer is B. Cloudwatch.\nWhen an AWS Lambda function is invoked via API Gateway, it is important to monitor the function's performance to ensure that it is running correctly and without any errors. AWS CloudWatch is a monitoring service that allows you to monitor and collect metrics, logs, and events from AWS resources, including AWS Lambda functions.\nCloudWatch can be used to monitor metrics such as function invocation count, execution duration, and error rates. By monitoring these metrics, you can quickly identify any issues or bottlenecks in your Lambda function and take corrective action to ensure that it is running efficiently.\nIn addition to monitoring metrics, CloudWatch can also be used to monitor Lambda function logs. Lambda function logs are generated by the Lambda function and contain information about the function's execution, including any errors or exceptions that occurred during execution. By using CloudWatch Logs, you can centralize and analyze these logs, making it easier to identify and troubleshoot issues with your Lambda function.\nTherefore, B. Cloudwatch is the correct answer as it is the AWS service that allows you to check the performance of your underlying Lambda function.\n\n"
}, {
  "id" : 226,
  "question" : "You are the lead for your development team.\nThere is a requirement to provision an application using the Elastic beanstalk service.\nIt's a custom application with unique configuration files and software to include.\nWhich of the following would be the best way to provision the environment in the least time possible?\n",
  "answers" : [ {
    "id" : "f8548972196644658b6eab5adaa9f5d8",
    "option" : "Use a custom AMI for the underlying instances.",
    "isCorrect" : "true"
  }, {
    "id" : "ed3e5367350541ea8ac091fa3da72338",
    "option" : "Use configuration files to download and install the updates.",
    "isCorrect" : "false"
  }, {
    "id" : "1e994edb3fc947db9edbfce94312f9d5",
    "option" : "Use the User data section for the Instances to download the updates.",
    "isCorrect" : "false"
  }, {
    "id" : "64377e20576145e998f9f68e35fd113b",
    "option" : "Use the metadata data section for the Instances to download the updates.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nWhen you create an AWS Elastic Beanstalk environment, you can specify an Amazon Machine Image (AMI) to use instead of the standard Elastic Beanstalk AMI included in your platform configuration's solution stack.\nA custom AMI can improve provisioning times when instances are launched in your environment if you need to install a lot of software that isn't included in the standard AMIs.\nUsing configuration files is great for configuring and customizing your environment quickly and consistently.\nApplying configurations, however, can start to take a long time during environment creation and updates.\nIf you do a lot of server configuration in configuration files, you can reduce this time by making a custom AMI that already has the software and configuration that you need.\nOptions B and C are invalid since these options would not result in the least amount of time for setting up the environment.\nOption D is invalid since the metadata data section is used for getting information about the underlying instances.\nFor more information on working with custom environments, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.customenv.html\n\nIn order to provision an application using Elastic Beanstalk, there are several ways to ensure that the process is done in the least amount of time possible. However, the best approach would be to use a custom Amazon Machine Image (AMI) for the underlying instances.\nOption A: Use a custom AMI for the underlying instances. This approach involves creating a custom AMI that contains the unique configuration files and software to be included in the application. This AMI can then be used to launch the underlying EC2 instances required for the Elastic Beanstalk environment. Using a custom AMI can reduce the amount of time needed to install and configure the necessary software on the instances.\nOption B: Use configuration files to download and install the updates. This option involves using configuration files to download and install the updates required for the application. While this approach can work, it may take longer than using a custom AMI since the instances need to be launched and configured before the updates can be downloaded and installed.\nOption C: Use the User data section for the Instances to download the updates. This approach involves using the user data section of the instances to download and install the updates required for the application. This can be done by specifying scripts or commands that will be executed when the instance launches. While this approach can work, it may take longer than using a custom AMI since the instances need to be launched and configured before the updates can be downloaded and installed.\nOption D: Use the metadata data section for the Instances to download the updates. This option is not valid, as there is no such thing as a metadata data section for instances in Elastic Beanstalk.\nIn summary, the best approach to provision an application using Elastic Beanstalk with unique configuration files and software to include is to use a custom AMI for the underlying instances. This can save time and ensure that the application is up and running as quickly as possible.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 228,
  "question" : "Your development team is working with Docker containers.\nThese containers need to encrypt data.\nThe data key needs to be generated using the KMS service.\nThe data key should be in the encrypted format.\nWhich of the following would you most ideally use? (Choose 2 options)\n",
  "answers" : [ {
    "id" : "d43c12df881d4a7cbc09e81e958b9200",
    "option" : "The GenerateDataKey command",
    "isCorrect" : "false"
  }, {
    "id" : "b7ac9bb89fea456f81c14050b2c2b73d",
    "option" : "The GenerateDataKeyWithoutPlaintext command",
    "isCorrect" : "true"
  }, {
    "id" : "73aad6bf200f40aba0a9d0678b3d0b0b",
    "option" : "Use the CMK Keys",
    "isCorrect" : "true"
  }, {
    "id" : "6d42eb928ff642cfbda15335b27cdc96",
    "option" : "Use client-side keys.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe AWS Documentation mentions the following.\nGenerateDataKeyWithoutPlaintext returns a data encryption key encrypted under a customer master key (CMK)\nThis operation is identical to GenerateDataKey but returns only the encrypted copy of the data key.\nOption A is incorrect because the GenerateDataKey command returns both the original plaintext key and the encrypted copy of the key.\nOption B is CORRECT because we need the command \"GenerateDataKeyWithoutPlaintext\" only to return the encrypted key.\nOption C is CORRECT because the CMK is required to encrypt the data keys with the above command.\nOption D is invalid since the question states that you need to use the KMS service.\nFor more information on Generating data keys, please refer to the below URL-\nhttps://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKeyWithoutPlaintext.html\n\nTo encrypt data in Docker containers, we can generate a data key using the AWS Key Management Service (KMS) and then use this key to encrypt and decrypt the data. There are two options available to generate an encrypted data key in KMS:\nGenerateDataKeyWithoutPlaintext command: This option generates an encrypted data key without the plaintext version of the key. This means that the key is never exposed in its unencrypted form. The encrypted key can be used to encrypt data within the Docker containers. Use CMK Keys: AWS KMS allows you to create and manage customer master keys (CMKs) that can be used to generate data keys. CMKs can be used to encrypt and decrypt data within the Docker containers.\nTherefore, options B and C are the correct answers. Option A, the GenerateDataKey command, generates an unencrypted data key which is not suitable for encrypting data. Option D, client-side keys, are keys that are generated outside of AWS and are not suitable for use with KMS.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 230,
  "question" : "You are starting to develop an application using AWS services.\nYou are testing the authentication to the AWS services by querying them using REST API.\nWhich of the following would be needed to make successful calls to AWS services using REST API?\n",
  "answers" : [ {
    "id" : "a89c9423a876460f9d4eac03b50cbc09",
    "option" : "User name and password",
    "isCorrect" : "false"
  }, {
    "id" : "6a1f4f5bd59941e49bcb73c11fca7af1",
    "option" : "SSL certificates",
    "isCorrect" : "false"
  }, {
    "id" : "81fa68f1a94849dd826ba272983d9327",
    "option" : "Access Keys",
    "isCorrect" : "true"
  }, {
    "id" : "58f6aaa03d6a436e8d5aba98b6a1f3d8",
    "option" : "X.509 certificates.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAccess keys consist of an access key ID (for example, AKIAIOSFODNN7EXAMPLE) and a secret access key (for example, wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY)\nYou use access keys to sign programmatic requests that you make to AWS if you use the AWS SDKs, REST, or Query API operations.\nOption A is incorrect because it works with APIs or SDK.\nYou need to use Access Key ID and Secret Access Key.\nOptions B and D are incorrect because SSL/TLS certificates or X.509 certificates are used to secure network communications.\nThey are not used to access AWS resources.\nFor more information on Access Keys, please refer to the below URL-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html\n\nTo make successful calls to AWS services using REST API, you would need Access Keys. Access Keys consist of an Access Key ID and a Secret Access Key. These are long-term credentials used to identify and authenticate your application with AWS services.\nAccess Keys provide programmatic access to AWS services through the AWS API. They can be used to make programmatic calls to AWS services using tools such as the AWS SDKs and REST APIs.\nThe Access Key ID is used to identify your application or user account when making API calls to AWS services. The Secret Access Key is used to sign API requests to AWS services, which is used to verify that the request comes from an authorized source.\nIt is important to keep the Secret Access Key confidential and never share it with anyone or include it in any code or configuration files that are shared or stored in a public place.\nUser name and password are not typically used for authentication to AWS services through REST API. SSL certificates and X.509 certificates may be used for secure communication, but they are not required for basic authentication with AWS services through REST API.\n\n"
}, {
  "id" : 231,
  "question" : "You've currently set up an API gateway service in AWS.\nThe API gateway is calling a custom API hosted on an EC2 Instance.\nThere are severe latency issues, and you need to diagnose the reason for those latency issues.\nWhich of the following could be used to address this concern?\n",
  "answers" : [ {
    "id" : "be107b4598e04ff486c4439be635fd38",
    "option" : "AWS X-Ray",
    "isCorrect" : "true"
  }, {
    "id" : "f8292c70939a446db7077e904410e8cd",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "12d5900317374de4b224817cace39d27",
    "option" : "AWS Cloudtrail",
    "isCorrect" : "false"
  }, {
    "id" : "79c205b49a76420cb1f58f14a559e01a",
    "option" : "AWS VPC Flow logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS X-Ray is an AWS service that allows you to trace latency issues with your Amazon API Gateway APIs.\nX-Ray collects metadata from the API Gateway service and any downstream services that make up your API.\nX-Ray uses this metadata to generate a detailed service graph that illustrates latency spikes and other issues that impact your API's performance.\nOption B is invalid since this is used for logging API execution operations.\nOption C is invalid since this is used for logging API Gateway API management operations.\nOption D is invalid since this is used for logging calls into the VPC.For more information on the API monitoring overview, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/monitoring_overview.html\n\nThe correct answer is A. AWS X-Ray.\nAWS X-Ray is a service that helps developers analyze and debug distributed applications, including those built using microservices architecture. It provides end-to-end tracing of requests as they flow through your application, so you can identify performance bottlenecks and troubleshoot issues quickly.\nIn this scenario, the API gateway is calling a custom API hosted on an EC2 Instance, and there are severe latency issues. By using AWS X-Ray, you can trace the request flow and identify which component(s) of your application are causing the latency issues.\nAWS CloudWatch is a monitoring and observability service that provides metrics, logs, and alarms for AWS resources and applications. While it can help you monitor the overall health of your API Gateway and EC2 instances, it does not provide detailed tracing of requests.\nAWS CloudTrail is a service that logs all API calls made to AWS resources in your account. While it can help you track changes and troubleshoot issues related to IAM policies or resource access, it does not provide detailed tracing of requests.\nAWS VPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. While it can help you monitor network traffic and troubleshoot issues related to network connectivity, it does not provide detailed tracing of requests through your application.\nTherefore, AWS X-Ray is the most appropriate service for diagnosing the severe latency issues in this scenario.\n\n"
}, {
  "id" : 232,
  "question" : "Your company has set up an application in AWS.\nThe application has been developed in-house which consists of many distributed components.\nYou would need a new service to deliver and manage messages for the distributed system.\nWhich of the following services should you suggest?\n",
  "answers" : [ {
    "id" : "f331f3f3c5614a1dabfdde8c503091b0",
    "option" : "Suggest the usage of the SQS service for the messaging across the distributed components.",
    "isCorrect" : "true"
  }, {
    "id" : "29988bfba4d14546b655bc60c131a459",
    "option" : "Suggest the usage of the SNS service for the messaging across the distributed components.",
    "isCorrect" : "false"
  }, {
    "id" : "5038318c5af74639ae55f36104ac7c82",
    "option" : "Suggest the use of CloudWatch logs to deliver and manage the messages.",
    "isCorrect" : "false"
  }, {
    "id" : "65298c9423c841fc855208f3bec423de",
    "option" : "Suggest the use of CloudTrail logs to forward and manage the logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\nSQS eliminates the complexity and overhead associated with managing and operating message oriented middleware and empowers developers to focus on differentiating work.\nUsing SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\nGet started with SQS in minutes using the AWS console, Command Line Interface or SDK of your choice, and three simple commands.\nOption B is invalid since this is used to send notifications to different endpoints.\nOptions C and D are invalid because either CloudWatch or CloudTrail is not a messaging system and cannot deliver or manage messages.\nFor more information on SQS queues, please refer to the below URL-\nhttps://aws.amazon.com/sqs\n\nFor a distributed application with multiple components, a messaging service is necessary to ensure reliable and scalable communication between the components. Among the AWS services available for this purpose, the two most commonly used services are Amazon Simple Queue Service (SQS) and Amazon Simple Notification Service (SNS).\nOption A - Suggest the usage of the SQS service for the messaging across the distributed components: Amazon SQS is a fully-managed message queuing service that enables decoupling and scaling of distributed systems and applications. It allows you to send, store, and receive messages between software components, without requiring them to be online simultaneously. With SQS, you can create message queues that store messages waiting to be processed. Each queue can have multiple producers and consumers, making it ideal for a distributed architecture. When a component finishes processing a message, it can delete it from the queue, so other components can process the same message. This ensures reliable and asynchronous communication between the components. SQS offers a variety of features, including standard and FIFO queues, dead-letter queues, message timers, and message encryption.\nOption B - Suggest the usage of the SNS service for the messaging across the distributed components: Amazon SNS is a fully-managed messaging service that enables the publication and subscription of messages to topics or endpoints. It supports multiple delivery protocols, including HTTP/HTTPS, email, SMS, mobile push notifications, and AWS Lambda functions. SNS is designed for broadcasting messages to multiple recipients, rather than direct communication between components. It allows you to publish a message to a topic, which can have multiple subscribers, such as email addresses, phone numbers, or SQS queues. When a subscriber receives a message, it can trigger a process or notification. SNS offers several features, including message filtering, message attributes, mobile push notifications, and cross-region replication.\nOption C - Suggest the use of CloudWatch logs to deliver and manage the messages: Amazon CloudWatch is a monitoring and observability service that enables you to collect and analyze metrics, logs, and events from AWS resources and applications. CloudWatch logs allow you to centralize the collection and storage of logs from different sources, such as EC2 instances, Lambda functions, and containers. However, CloudWatch logs are not designed for message delivery or management. They can be used to monitor the activity of a distributed system, but they do not provide the necessary features for message queuing, such as message order, delivery retries, and dead-letter queues.\nOption D - Suggest the use of CloudTrail logs to forward and manage the logs: Amazon CloudTrail is a service that enables you to record and monitor API calls and events in AWS services and resources. It provides a log of all API activities, including the identity of the caller, the time of the API call, the request parameters, and the response elements. However, CloudTrail logs are not designed for message delivery or management. They are useful for auditing and compliance purposes, but they do not provide the necessary features for message queuing, such as message ordering, delivery retries, and dead-letter queues.\nIn conclusion, the best option for delivering and managing messages across distributed components in AWS is to use Amazon SQS. It provides a reliable and scalable message queuing service that enables asynchronous communication between components. Amazon SNS is a useful service for broadcasting messages to multiple endpoints, but it is not suitable for direct communication between components. Amazon CloudWatch and CloudTrail logs are not designed for message queuing and management, but they can be used for monitoring and auditing activities in a distributed system.\n\n"
}, {
  "id" : 233,
  "question" : "Your company has a set of EC2 Instances and On-premise.\nThey now want to automate the deployment of their applications using the AWS Code Deploy tool in AWS.\nWhich of the following is not needed in this scenario?\n",
  "answers" : [ {
    "id" : "a1f74999faaf412394a9665a1baa0a61",
    "option" : "Ensure both EC2 Instances and On-premise servers have the Code Deploy agent installed.",
    "isCorrect" : "false"
  }, {
    "id" : "b137c2d6cd244a718606a852c3f8183a",
    "option" : "Ensure both EC2 Instances and On-premise servers can connect to the Code Deploy service.",
    "isCorrect" : "false"
  }, {
    "id" : "abfe98e79d29420aa055805174df816c",
    "option" : "Ensure that On-premise servers use an IAM role ARN to authenticate requests.",
    "isCorrect" : "false"
  }, {
    "id" : "80cf63544f724a67b5fd70fcd8c3b401",
    "option" : "Ensure both EC2 Instances and On-premise servers have instance profiles attached to them.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nOn-premises instances do not use IAM instance profiles.\nAll other options are incorrect because the question asks which one is not needed.\nPlease refer to the below links in section 'Pre-requisites' for instance profile related to On-premises servers.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html\nhttps://docs.aws.amazon.com/en_pv/codedeploy/latest/userguide/instances-ec2-configure.html\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/tutorials-on-premises-instance.html\n\nThe correct answer is D. Ensure both EC2 Instances and On-premise servers have instance profiles attached to them.\nExplanation:\nAWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services, including EC2 instances and on-premises servers. CodeDeploy requires an agent to be installed on the instances that will receive the application deployments. The agent enables the instances to communicate with the CodeDeploy service and receive the deployment instructions. Therefore, both EC2 instances and on-premises servers need to have the CodeDeploy agent installed (Option A) and be able to connect to the CodeDeploy service (Option B).\nAdditionally, to authenticate requests from on-premises servers, an IAM role ARN needs to be used (Option C). An IAM role is an AWS Identity and Access Management (IAM) entity that defines a set of permissions for making AWS service requests. An IAM role ARN is required when using an on-premises instance with CodeDeploy. This role must be granted permissions to call CodeDeploy APIs on behalf of the on-premises instance.\nHowever, instance profiles (Option D) are not required in this scenario. Instance profiles are a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. CodeDeploy does not use instance profiles to manage deployments, and they are not required for the CodeDeploy agent to function.\nIn summary, to automate the deployment of applications using AWS CodeDeploy, you need to ensure both EC2 instances and on-premises servers have the CodeDeploy agent installed, can connect to the CodeDeploy service, and use an IAM role ARN to authenticate requests. Instance profiles are not needed.\n\n"
}, {
  "id" : 234,
  "question" : "You are in charge of developing Cloudformation templates that would be used to deploy databases in different AWS Accounts.\nTo ensure that the passwords for the database are passed in a secure manner, which of the following could you use with Cloudformation?\n",
  "answers" : [ {
    "id" : "a99eb1b919514795b27217363acf73dd",
    "option" : "Outputs",
    "isCorrect" : "false"
  }, {
    "id" : "5a87401ab9714f2cabd1a8e49f86a18b",
    "option" : "Metadata",
    "isCorrect" : "false"
  }, {
    "id" : "7d330315cbe24c6fa03ef7ea4fed187c",
    "option" : "Parameters",
    "isCorrect" : "true"
  }, {
    "id" : "24b2391c0776402fb4eb75682c06a338",
    "option" : "Resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nOption A is incorrect since this is used to describes the values that are returned whenever you view your stack's properties.\nOption B is incorrect since this is used to specify Objects that provide additional information about the template.\nOption D is incorrect since here you would need to add the hard-coded passwords.\nFor more information on best practices for Cloudformation, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html\n\n\nTo ensure that the passwords for the database are passed in a secure manner with CloudFormation, you can use parameters.\nParameters are variables that are passed to the CloudFormation stack at runtime. Parameters enable you to specify inputs to your template when you create or update a stack. With parameters, you can pass values such as passwords, AMI IDs, and instance types to your template.\nBy using parameters to pass passwords, you can ensure that they are not hard-coded in your template, which would make them visible to anyone who has access to your template. Instead, you can prompt users for the password when they launch the CloudFormation stack, or you can store the password in AWS Systems Manager Parameter Store or AWS Secrets Manager and retrieve it at runtime.\nOutputs, metadata, and resources are not designed to be used for passing passwords securely. Outputs are used to export values from your stack, metadata is used to provide additional information about your template, and resources are used to define the AWS resources that you want to create with your stack.\nTherefore, the correct answer is C. Parameters.\n\n"
}, {
  "id" : 235,
  "question" : "Your team needs to develop an application that needs to make use of SQS queues.\nThere is a requirement that when a message is added to the queue, the message is not visible for 5 minutes to consumers.\nHow can you achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9276c74f91fe48089019e6970c2399fa",
    "option" : "Increase the visibility timeout of the queue.",
    "isCorrect" : "false"
  }, {
    "id" : "c0734f96979f4a33b0659dcfe1e38863",
    "option" : "Implement long polling for the SQS queue.",
    "isCorrect" : "false"
  }, {
    "id" : "04d623ebae994bb8be3e6f5492476735",
    "option" : "Implement delay queues in AWS.",
    "isCorrect" : "true"
  }, {
    "id" : "14c236783fec40498e76c278bb3f4bfa",
    "option" : "Change the message timer value for each individual message.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nThe AWS Documentation mentions the following.\nDelay queues let you postpone the delivery of new messages to a queue for several seconds.\nIf you create a delay queue, any messages you send to the queue remain invisible to consumers for the duration of the delay period.\nThe default (minimum) delay for a queue is 0 seconds.\nThe maximum is 15 minutes.\nTo set delay seconds on individual messages, rather than on an entire queue, use message timers to allow Amazon SQS to use the message timer's DelaySeconds value instead of the delay queue's DelaySeconds value.\nOption A is invalid since this can only make the message invisible after the message has been read and not in the beginning.\nOption B is invalid since this is used to reduce the cost of using Amazon SQS by eliminating the number of empty responses from SQS queues.\nFor more information on SQS delay queues, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\n\nTo achieve the requirement of delaying visibility of messages for 5 minutes in SQS queues, we can make use of the following two options:\nA. Increase the visibility timeout of the queue: The visibility timeout of an SQS message is the time period during which the message is invisible to consumers after being retrieved from the queue by a consumer. By default, the visibility timeout is set to 30 seconds, which means that after a consumer retrieves a message from the queue, the message is invisible to other consumers for the next 30 seconds. If the consumer doesn't delete the message within this period, it will become visible to other consumers after the timeout period expires.\nTo achieve the requirement, we can increase the visibility timeout of the SQS queue to 5 minutes (300 seconds). This will ensure that the messages added to the queue are not visible to consumers for the next 5 minutes.\nB. Implement delay queues in AWS: Delay queues are a feature of Amazon SQS that enables you to postpone the delivery of new messages to a queue for a specified number of seconds. This means that messages sent to a delay queue aren't immediately available to consumers. Instead, they're held for the specified delay period, after which they become available to consumers.\nTo achieve the requirement, we can create a delay queue in SQS and set the delay period to 5 minutes. When messages are sent to this queue, they will not be visible to consumers for the next 5 minutes. Once the delay period is over, the messages will become visible to consumers.\nNote: Option D is not a valid solution as changing the message timer value for each individual message would be cumbersome and error-prone, especially if the number of messages is large. Similarly, option B (Implement long polling for the SQS queue) is not directly related to delaying the visibility of messages, although it can help reduce the number of requests to the queue and improve efficiency.\n\n"
}, {
  "id" : 236,
  "question" : "You're currently working with a table in DynamoDB.\nYour program has inserted several rows in the table.\nYou now need to read the data from the table and only retrieve certain attributes in the query.\nWhich of the following would you use for this purpose?\n",
  "answers" : [ {
    "id" : "ad8edb0e8ba94811becc4d214ae5f161",
    "option" : "The scan operation only since only this supports the filtering of certain attributes.",
    "isCorrect" : "false"
  }, {
    "id" : "f039c4c4df8b41aea64bee8acac2b48c",
    "option" : "The query operation only since only this supports the filtering of certain attributes.",
    "isCorrect" : "false"
  }, {
    "id" : "54786dce196a4e4d97e77cd0dad28c6e",
    "option" : "Use the Projection Expression.",
    "isCorrect" : "true"
  }, {
    "id" : "068f2fd01f85462ea2f4f281f0891985",
    "option" : "Use the Index name.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - C.\nThe AWS Documentation mentions the following.\nTo read data from a table, you use operations such as GetItem, Query, or Scan.\nDynamoDB returns all of the item attributes by default.\nTo get just some, rather than all of the attributes, use a projection expression.\nA projection expression is a string that identifies the attributes you want.\nTo retrieve a single attribute, specify its name.\nFor multiple attributes, the names must be comma-separated.\nOptions A and B are incorrect since Projection Expression is available in both the scan and query operation.\nOption D is invalid since you need to use the Projection Expression.\nFor more information on projection expressions, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Expressions.ProjectionExpressions.html\n\nThe correct answer is C. Use the Projection Expression.\nExplanation:\nDynamoDB is a NoSQL database service provided by AWS that provides fast and predictable performance with seamless scalability. In DynamoDB, data is stored in tables, and each table can have one or more items (rows) and each item can have one or more attributes (columns). To retrieve data from a DynamoDB table, you can use two operations: Scan and Query.\nScan operation: The scan operation reads all the items in a table and returns the result. This operation can be slow and expensive, especially for large tables, and it requires the use of filtering to narrow down the results. Query operation: The query operation retrieves items from a table that match a specific partition key value. It is faster and more efficient than the scan operation, especially for large tables.\nWhen using the query operation, you can use a Projection Expression to retrieve only specific attributes from the table. A projection expression is a string that identifies the attributes to be retrieved from a table. The query operation supports the use of a Projection Expression to retrieve only specific attributes from the table.\nExample:\nSuppose you have a DynamoDB table named \"Users\" with the following items:\nUserId Name Age Gender City 1 Alice 28 Female New York 2 Bob 35 Male London 3 Charlie 42 Male Sydney 4 Dave 19 Male New York\nIf you want to retrieve only the names and ages of all the users from the table where the City is \"New York,\" you can use the following query with a Projection Expression:\njavascript Copy code var params = {     TableName: \"Users\",     KeyConditionExpression: \"City = :city\",     ExpressionAttributeValues: {         \":city\": \"New York\"     },     ProjectionExpression: \"Name, Age\" };  docClient.query(params, function(err, data) {     if (err) {         console.log(\"Error\", err);     } else {         console.log(\"Success\", data.Items);     } });\nIn this example, the Projection Expression \"Name, Age\" is used to retrieve only the Name and Age attributes of the items matching the query condition. The result will be:\nyaml Copy code Success [   { Name: 'Alice', Age: 28 },   { Name: 'Dave', Age: 19 } ]\nTherefore, the correct answer is C. Use the Projection Expression.\n\n"
}, {
  "id" : 237,
  "question" : "Your company currently is maintaining excel sheets with data that now needs to be ported to a DynamoDB table.\nThe excel sheet contains the following headers for the data. · Customer ID · Customer Name · Customer Location · Customer Age Which of the following would be the ideal partition key for the data table in DynamoDB?\n",
  "answers" : [ {
    "id" : "54698e037bc340e495aa3f68c44f3c06",
    "option" : "Customer ID",
    "isCorrect" : "true"
  }, {
    "id" : "edca8212062343799e3e4dc2655b567a",
    "option" : "Customer Name",
    "isCorrect" : "false"
  }, {
    "id" : "86652ca72a6b41c180fe9b2e4051ecd5",
    "option" : "Customer Location",
    "isCorrect" : "false"
  }, {
    "id" : "56700e91a25f4edd8c2aadba5dc9f7d7",
    "option" : "Customer Age.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nTo get better performance on your underlying DynamoDB table, you should choose a partition key that would give an even distribution of values, and this would be the Customer ID.The below snapshot from the AWS Documentation gives some of the recommended partition key values.\nBecause of the AWS Documentation's recommendations on better partition key values, all other options are invalid.\nFor more information on partition key design, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html\n\n\nThe partition key in DynamoDB is used to partition the table's data across multiple nodes in the database, allowing for efficient access and scalability. It's important to choose the right partition key for the table to optimize performance and minimize costs.\nIn this case, the ideal partition key for the DynamoDB table would be \"Customer ID\" (option A).\nHere are a few reasons why:\nUniqueness: The partition key should be unique for each item in the table to ensure even distribution of data across partitions. \"Customer ID\" is likely to be unique for each customer, making it a good candidate for the partition key. Queryability: The partition key should also be used in queries to retrieve specific items from the table efficiently. \"Customer ID\" would be a good choice for queries that retrieve data for a specific customer. Uniformity: The partition key should distribute data evenly across partitions to ensure that no single partition becomes a performance bottleneck. \"Customer ID\" is likely to distribute data evenly, especially if it's a randomly generated identifier.\nIn contrast, the other options would not be as good for the following reasons:\nCustomer Name (Option B): Customer names may not be unique, and there's no guarantee that queries for specific names would distribute evenly across partitions. Customer Location (Option C): Customer locations may not be evenly distributed, which could result in uneven data distribution across partitions. Customer Age (Option D): Age is not likely to be unique for each customer, and there's no guarantee that queries for specific ages would distribute evenly across partitions.\nTherefore, Option A (Customer ID) would be the best partition key for the DynamoDB table.\n\n"
}, {
  "id" : 238,
  "question" : "A large multi-media company is hosting their application on AWS.\nThe application is currently using DynamoDB table for storage and retrieval of data.\nThe data access for items in the table is currently in milliseconds, but the company wants to improve further on the access times.\nHow can this be achieved?\n",
  "answers" : [ {
    "id" : "a47c943b351f4cc7b5b6f4d1e6401ae0",
    "option" : "Use larger DynamoDB tables.",
    "isCorrect" : "false"
  }, {
    "id" : "1789a374ebda4cc1a98209a0a9e1b1ee",
    "option" : "Increase the Read capacity on the tables.",
    "isCorrect" : "false"
  }, {
    "id" : "d29f710c70324985b39331b504cc3c82",
    "option" : "Increase the Write capacity on the tables.",
    "isCorrect" : "false"
  }, {
    "id" : "2faf125e3c254c02adeb041ce47a0e65",
    "option" : "Make use of the DAX feature.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAmazon DynamoDB is designed for scale and performance.\nIn most cases, the DynamoDB response times can be measured in single-digit milliseconds.\nHowever, certain use cases require response times in microseconds.\nDynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data for these use cases.\nOption A is incorrect because using a larger table means providing more capacity, which does not help improve access times.\nOptions B and C are incorrect because the read and write capacities specify the number of reads and writes per second required for the application.\nThis does not help to improve the access time.\nFor more information on DynamoDB Acceleration, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html\n\nThe best solution to further improve the access times of a DynamoDB table for a large multimedia company is to make use of the DAX (DynamoDB Accelerator) feature, which is a fully managed, highly available, and in-memory cache for DynamoDB. DAX is designed to improve the read performance of DynamoDB tables by caching frequently accessed data in memory.\nDAX reduces the response times of read-heavy workloads by caching data and storing it in memory, thereby eliminating the need to repeatedly query the database. This reduces the read latencies and improves the response times for read requests. DAX can handle millions of requests per second, making it ideal for applications that need to deliver fast and consistent read performance.\nUsing larger DynamoDB tables does not necessarily improve access times, as the size of the table does not have a direct impact on performance. The table's read and write capacity, on the other hand, affects the table's ability to handle requests. Increasing the read or write capacity on the table will increase the number of requests that can be processed concurrently, but it will not necessarily improve access times.\nIn summary, to improve the access times of a DynamoDB table for a large multimedia company, it is best to use the DAX feature. DAX reduces the read latencies by caching frequently accessed data in memory, thereby improving the response times for read requests.\n\n"
}, {
  "id" : 239,
  "question" : "Which of the following are the types supported while defining Parameters in a CloudFormation template? (Select THREE)\n",
  "answers" : [ {
    "id" : "1e0887a07725405190ba257fdfeda0c3",
    "option" : "String",
    "isCorrect" : "true"
  }, {
    "id" : "5085a7e7ea494508a417a8df3144817d",
    "option" : "Binary Number",
    "isCorrect" : "false"
  }, {
    "id" : "4a95c104dc5645bba2d1d2ef9abf064b",
    "option" : "Number",
    "isCorrect" : "true"
  }, {
    "id" : "f812231551204c9b802a78665cfa517a",
    "option" : "Array",
    "isCorrect" : "false"
  }, {
    "id" : "897a66733e594834a25399693f44d8c9",
    "option" : "List.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - A, C, E.\nParameters within an AWS CloudFormation Template allow to input custom values while creating or updating a stack.\nSupported data types for Parameters are as follows.\n· String.\n· Number.\n· List.\n· CommaDelimitedList.\n· AWS-Specific Parameter Types.\n· SSM Parameter Types.\nOptions B &amp; D are incorrect as they are not the supported Data Types for Parameters in CloudFormation Templates.\nFor more information on Parameters in AWS CloudFormation Templates, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/parameters-section-structure.html\n\nCloudFormation is an AWS service that allows you to define and provision infrastructure as code. When creating a CloudFormation stack, you define the resources you want to create and their dependencies, as well as any necessary inputs or parameters to configure those resources.\nTo define parameters in a CloudFormation template, you can specify their type, default value, and allowed values. Parameters can be used to make your templates more flexible and reusable, as they allow you to customize the stack at runtime.\nHere are the explanations for the supported parameter types:\nA. String: A String parameter allows you to pass in any arbitrary string value as a parameter. This can be useful for things like setting a hostname or other configuration values.\nB. Binary: A Binary parameter allows you to pass in binary data as a parameter. This is useful if you need to pass in files or other binary data to your stack.\nC. Number: A Number parameter allows you to pass in a numerical value as a parameter. This can be useful for setting things like port numbers or other numerical configuration values.\nD. Array: An Array parameter allows you to pass in an array of values as a parameter. This can be useful if you need to pass in a list of values to your stack.\nE. List: A List parameter allows you to pass in a list of values as a parameter. This is similar to an Array parameter, but with slightly different syntax.\nTherefore, the correct answers to the question are A. String, C. Number, and D. Array.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 241,
  "question" : "You're developing an application that will need to do the following. · Upload images via a front end from users. · Store the images in S3. · Add the location of the images to a DynamoDB table. Which of the following two options would be part of the implementation process? Please choose two correct answers.\n",
  "answers" : [ {
    "id" : "521701179c4543fd96f41cf93b439727",
    "option" : "Add a Lambda function that would respond to events in S3.",
    "isCorrect" : "true"
  }, {
    "id" : "b1f6bc1130d043d18e7fab63e0b4a182",
    "option" : "Add a message to an SQS queue after the object is inserted into the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "e54cc0b09d714aa89fcfccc609e7b6bd",
    "option" : "Ensure that the Lambda function has access to the DynamoDB table.",
    "isCorrect" : "true"
  }, {
    "id" : "f9aa71dfc2e8450e87b81d0e2d7a0541",
    "option" : "Ensure that the SQS service has access to the DynamoDB table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe ideal approach would be to automate the entire flow using AWS Lambda.\nEnsure that the Lambda function has a role to access data in the DynamoDB table.\nThe AWS Documentation also mentions the following.\nYou can write Lambda functions to process S3 bucket events, such as the object-created or object-deleted events.\nFor example, when a user uploads a photo to a bucket, you might want Amazon S3 to invoke your Lambda function so that it reads the image and creates a thumbnail for the photo.\nYou can use the bucket notification configuration feature in Amazon S3 to configure the event source mapping, identifying the bucket events that you want Amazon S3 to publish and which Lambda function to invoke.\nOptions B and D would not be the ideal combination for this sort of requirement.\nFor more information on AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html\n\nTo implement the given requirements, the following steps are necessary:\nCreate an S3 bucket: The first step is to create an S3 bucket to store the uploaded images. Create a DynamoDB table: The second step is to create a DynamoDB table to store the metadata about the uploaded images. The table can have attributes such as Image ID, Image Name, S3 Object Key, and S3 Bucket Name. Create an IAM Role: The third step is to create an IAM role that the Lambda function will assume. This role should have permissions to write to the DynamoDB table. Create a Lambda Function: The fourth step is to create a Lambda function that will be triggered by S3 events whenever a new image is uploaded to the S3 bucket. The Lambda function will read the metadata of the uploaded image and write it to the DynamoDB table. Ensure Lambda function has access to DynamoDB table: The fifth step is to ensure that the Lambda function has access to the DynamoDB table by assigning the IAM role created in step 3 to the Lambda function. (Optional) Add an SQS Queue: An additional step could be to add an SQS queue to ensure that there is no data loss if the Lambda function fails. If an SQS queue is added, the Lambda function can write a message to the queue after the object is inserted into the bucket. Another Lambda function can read messages from the queue and process them independently. Ensure SQS service has access to DynamoDB table: If an SQS queue is added, it is necessary to ensure that the SQS service has access to the DynamoDB table. This can be done by modifying the IAM policy of the role assigned to the Lambda function to allow the SQS service to write to the DynamoDB table.\nIn conclusion, the correct answers are A and C. Answer A is correct because adding a Lambda function that responds to events in S3 is necessary to trigger the function whenever a new image is uploaded. Answer C is correct because it is necessary to ensure that the Lambda function has access to the DynamoDB table in order to write the metadata about the uploaded images. Answer B is incorrect because adding a message to an SQS queue is optional, and answer D is incorrect because it is unnecessary to give the SQS service access to the DynamoDB table if an SQS queue is not used.\n\n"
}, {
  "id" : 242,
  "question" : "Your team is planning on hosting an application in Elastic Beanstalk.\nThe underlying environment will contain multiple Linux EC2 Instances.\nYou need these instances to share data via a shared file system.\nWhich of the following would you use for this purpose?\n",
  "answers" : [ {
    "id" : "0c5c4fee73874175906719102ed1d059",
    "option" : "AWS S3",
    "isCorrect" : "false"
  }, {
    "id" : "0a8dde7005f14218a97fcbe9059d0403",
    "option" : "AWS Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "243edc3357344f93bc50aa28e9b931e8",
    "option" : "AWS EBS",
    "isCorrect" : "false"
  }, {
    "id" : "8ed9a00b1d3d4f20b51ea43deb089a7e",
    "option" : "AWS EFS.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation also mentions the following.\nWith Amazon Elastic File System (Amazon EFS), you can create network file systems that can be mounted by instances across multiple Availability Zones.\nAn Amazon EFS file system is an AWS resource that uses security groups to control access over the network in your default or custom VPC.In an Elastic Beanstalk environment, you can use Amazon EFS to create a shared directory that stores files uploaded or modified by users of your application.\nYour application can treat a mounted Amazon EFS volume like local storage.\nSo you don't have to change your application code to scale up to multiple instances.\nOption A is incorrect since this is used for object-based storage.\nOption B is incorrect since this is used for archive storage.\nOption C is incorrect since this is used for local block storage for EC2 Instances.\nFor more information on Elastic beanstalk and EFS, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/services-efs.html\n\nThe correct option for this scenario is D. AWS EFS.\nAWS Elastic File System (EFS) is a fully-managed, scalable, and highly-available file storage service that is designed to provide shared file storage across multiple EC2 instances. With EFS, multiple EC2 instances can share common data, which makes it an ideal choice for scenarios such as web content management, media processing workflows, big data analytics, and more.\nAWS S3 (Simple Storage Service) is an object storage service that is designed to store and retrieve large amounts of data. While S3 can be used to share files between multiple EC2 instances, it is not a file system and is not designed for shared file storage.\nAWS Glacier is a low-cost, long-term data archival service that is designed for data that is infrequently accessed and can tolerate retrieval times of several hours.\nAWS EBS (Elastic Block Store) is a block-level storage service that is designed to provide persistent storage volumes that can be attached to EC2 instances. While EBS volumes can be shared between multiple EC2 instances, they are not designed for shared file storage.\nIn summary, for the scenario described in the question, AWS EFS is the best option to provide shared file storage across multiple EC2 instances in Elastic Beanstalk.\n\n"
}, {
  "id" : 243,
  "question" : "Your team has currently developed an application using Docker containers.\nAs the development lead, you now need to host this application in AWS.\nYou also need to ensure that the AWS service has orchestration services built-in.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "f5fa76dc6f0e4da8afe99136b9719345",
    "option" : "Consider building a Kubernetes cluster on EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "b317a9bf71824185a66873756bba4061",
    "option" : "Consider building a Kubernetes cluster on your on-premise infrastructure.",
    "isCorrect" : "false"
  }, {
    "id" : "34e9c397d63743a68e33aa40a0da319f",
    "option" : "Consider using the Elastic Container Service.",
    "isCorrect" : "true"
  }, {
    "id" : "d462ae3b19054eb08578113b0054ad8e",
    "option" : "Consider using the Simple Storage service to store your docker containers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation also mentions the following.\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster.\nYou can host your cluster on a serverless infrastructure that Amazon ECS manages by launching your services or tasks using the Fargate launch type.\nYou can host your tasks on a cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances that you manage by using the EC2 launch type for more control.\nOptions A and B are invalid since these would involve additional maintenance activities.\nOption D is incorrect since this is Object-based storage.\nFor more information on the Elastic Container service, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n\nTo host the application using Docker containers in AWS, you can consider using Elastic Container Service (ECS). ECS is a fully managed container orchestration service provided by AWS that supports Docker containers. It allows you to easily run and scale Docker containers on AWS infrastructure.\nOption A suggests building a Kubernetes cluster on EC2 instances. While it is possible to run Kubernetes on EC2 instances, it requires additional effort and maintenance from the development team to manage the infrastructure. ECS, on the other hand, is a fully managed service that abstracts away the underlying infrastructure and provides automatic scaling, load balancing, and monitoring.\nOption B suggests building a Kubernetes cluster on your on-premise infrastructure. This option requires additional hardware and maintenance, which can be expensive and time-consuming. Moreover, hosting the application on-premise can result in latency and network issues, making it less reliable.\nOption D suggests using Simple Storage Service (S3) to store Docker containers. However, S3 is a file storage service and is not designed to host and run Docker containers. Instead, ECS provides a dedicated container registry that allows you to store and manage Docker images.\nIn summary, to host the application using Docker containers in AWS with orchestration services built-in, you should consider using Elastic Container Service (ECS).\n\n"
}, {
  "id" : 244,
  "question" : "Your development team is planning to deploy an application using the Elastic Beanstalk service.\nAs part of the deployment, you need to ensure that a high-end instance type is used to deploy the underlying instances.\nWhich of the following would you use to enable this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "10ab8e60f19f45d8b6403bf54c54d3aa",
    "option" : "The Launch configuration",
    "isCorrect" : "true"
  }, {
    "id" : "2dbff9bf343a47be8d8dccc06518bf54",
    "option" : "The Environment manifest file",
    "isCorrect" : "true"
  }, {
    "id" : "ec7aec8b9ce64f45ab0bbccc34eeb75a",
    "option" : "Instance Profile section",
    "isCorrect" : "false"
  }, {
    "id" : "0d2619e30578423694886b2ef0ffe400",
    "option" : "In the AWS Config section.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS Documentation mentions the following.\nYour Elastic Beanstalk includes an Auto Scaling group that manages the Amazon EC2 instances in your environment.\nIn a single-instance environment, the Auto Scaling group ensures that there is always one instance running.\nIn a load-balanced environment, you configure the group with a range of instances to run, and Amazon EC2 Auto Scaling adds or removes instances as needed, based on load.\nThe Auto Scaling group also manages the launch configuration for the instances in your environment.\nYou can modify the launch configuration to change the instance type, key pair, Amazon Elastic Block Store (Amazon EBS) storage, and other settings that can only be configured when you launch an instance.\nYou can include a YAML formatted environment manifest in the root of your application source bundle to configure the environment name, solution stack and environment links to use when creating your environment.\nAn environment manifest uses the same format as Saved Configurations.\nOption C is invalid since this is used to ensure that the environment can interact with other AWS resources.\nOption D is invalid since this is used to monitor the configuration changes of resources.\nFor more information on using the features of Elastic Beanstalk, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as.html\n\nTo ensure that a high-end instance type is used for deployment of underlying instances when deploying an application using Elastic Beanstalk, we need to make use of the following two options:\nThe Launch Configuration: Elastic Beanstalk uses launch configurations to determine the type of Amazon EC2 instances to launch for your application. A launch configuration is a blueprint that defines the settings for the EC2 instances that are launched. To use a high-end instance type, we can create a launch configuration with the desired instance type and then specify this launch configuration when creating a new environment or updating an existing environment. The Environment Manifest File: Elastic Beanstalk uses environment manifest files to specify the configuration settings for an environment. These files are written in YAML or JSON and contain key-value pairs that define the options for an environment. To use a high-end instance type, we can specify the instance type in the environment manifest file using the \"instance_type\" option.\nOption C, the Instance Profile section, is not used to specify the instance type for Elastic Beanstalk instances. Instance profiles are used to provide AWS credentials to EC2 instances running in an Elastic Beanstalk environment.\nOption D, the AWS Config section, is not used to specify the instance type for Elastic Beanstalk instances. The AWS Config service is used to assess, audit, and evaluate the configurations of your AWS resources to ensure compliance with your organization's policies and standards.\n\n"
}, {
  "id" : 245,
  "question" : "Your development team is currently planning on moving an on-premise data store to AWS DynamoDB.\nThere were triggers defined in the prior database which was used for updates to existing items.\nHow can you achieve the same when the movement is made to DynamoDB in the easiest way possible?\n",
  "answers" : [ {
    "id" : "d2231ed3e02c417fbfcab83e551f5279",
    "option" : "Define triggers in DynamoDB for each table.",
    "isCorrect" : "false"
  }, {
    "id" : "e641e0692d934a58942c4509767f5750",
    "option" : "Define Lambda functions in response to events from DynamoDB Streams.",
    "isCorrect" : "true"
  }, {
    "id" : "e211815257dc4381963939de2835ce53",
    "option" : "Define SNS topics in response to events from DynamoDB Streams.",
    "isCorrect" : "false"
  }, {
    "id" : "eb4bb7f6d48043a09d5495a571efc0e0",
    "option" : "Define SQS topics in response to events from DynamoDB Streams.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon DynamoDB is integrated with AWS Lambda so that you can create triggers-pieces of code that automatically respond to events in DynamoDB Streams.\nWith triggers, you can build applications that react to data modifications in DynamoDB tables.\nIf you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write.\nImmediately after an item in the table is modified, a new record appears in the table's stream.\nAWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.\nOption A is invalid since triggers cannot be defined by default in DynamoDB.Options C and D are invalid since the streams need to be integrated with Lambda.\nFor more information on using streams with AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\n\nWhen migrating an on-premises data store to Amazon DynamoDB, triggers defined in the prior database can be replicated using DynamoDB Streams. DynamoDB Streams is a feature that enables real-time streaming of data from a table to other AWS services, such as AWS Lambda, Amazon SNS, or Amazon SQS.\nOption A is incorrect because DynamoDB does not support triggers natively.\nOption B is the correct answer. Developers can define AWS Lambda functions that respond to events from DynamoDB Streams. When a change occurs in a DynamoDB table, such as an item being created, updated, or deleted, a record is written to the table's associated stream. The Lambda function can be configured to trigger when a specific event is written to the stream. The Lambda function can then perform the necessary logic, such as updating other data stores, sending notifications, or executing a business process.\nOption C is incorrect because while SNS can be used to notify subscribers about changes in DynamoDB tables, it does not provide the ability to process or manipulate the data in the same way as AWS Lambda.\nOption D is also incorrect because SQS is not designed to execute code in response to events like DynamoDB Streams. It's a message queuing service that enables asynchronous communication between decoupled systems. Developers could use SQS to buffer or batch changes to DynamoDB tables for processing later by AWS Lambda or other services.\nIn summary, when moving an on-premises data store to DynamoDB, developers can use AWS Lambda functions in response to events from DynamoDB Streams to replicate triggers defined in the prior database.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 248,
  "question" : "Your development team is planning on working with Amazon Step Functions.\nWhich of the following is a recommended practice when working with activity workers and tasks in Step Functions? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "4db6336048cc45b88ff3b70bb355b472",
    "option" : "Ensure to specify a timeout in state machine definitions.",
    "isCorrect" : "true"
  }, {
    "id" : "7165833d24794c7dafa10c7444827432",
    "option" : "We can use only 1 transition per state.",
    "isCorrect" : "false"
  }, {
    "id" : "ebe3975e3b0c40d891f206afbe39e614",
    "option" : "If you are passing larger payloads between states, consider using the Simple Storage Service.",
    "isCorrect" : "true"
  }, {
    "id" : "3f85e6339d5b436bb7ac4dc9839da94c",
    "option" : "If you are passing larger payloads between states, consider using EBS volumes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS Documentation mentions the following.\nBy default, the Amazon States Language doesn't set timeouts in state machine definitions.\nWithout an explicit timeout, Step Functions often relies solely on a response from an activity worker to know that a task is complete.\nIf something goes wrong and TimeoutSeconds isn't specified, an execution is stuck waiting for a response that will never come.\nExecutions that pass large payloads of data between states can be terminated.\nIf the data you pass between states might grow to over 32 KB, use Amazon Simple Storage Service (Amazon S3) to store the data, and pass the Amazon Resource Name instead of the raw data.\nAlternatively, adjust your implementation so that you pass smaller payloads in your executions.\nOption B is incorrect since States can have multiple incoming transitions from other states.\nBecause the documentation clearly mentions the best practices, the other options are invalid.\nFor more information on the best practices, please refer to the below URL-\nhttps://docs.aws.amazon.com/step-functions/latest/dg/sfn-best-practices.html\n\nWhen working with Amazon Step Functions, it is essential to follow some recommended practices to ensure the success of your workflow execution. Two of these recommended practices include:\nSpecifying a timeout in state machine definitions: When defining a state machine in Amazon Step Functions, it is crucial to specify a timeout for each state. This timeout will define the maximum time that a task can run before it is considered failed. If the task exceeds the defined timeout, the state machine will move to the next state and mark the task as failed. Specifying a timeout is essential to ensure that your state machine progresses correctly and does not get stuck in a state due to a task that is taking too long to complete. Using Simple Storage Service (S3) for passing larger payloads between states: If you are passing larger payloads between states, it is recommended to use Amazon S3 to store these payloads instead of passing them directly between the states. Passing large payloads between states can cause performance issues and slow down the execution of your state machine. Storing the payloads in Amazon S3 and passing only the S3 object's key between states can significantly improve the performance and speed of your state machine execution.\nIn summary, it is recommended to specify a timeout for each state in your state machine definition and use Amazon S3 for passing larger payloads between states when working with Amazon Step Functions.\n\n"
}, {
  "id" : 249,
  "question" : "There is a new Lambda Function developed using AWS CloudFormation Templates.\nWhich of the following attributes can be used to test the new Function with migrating 5% of traffic to the new version?\n",
  "answers" : [ {
    "id" : "7dfc0de1771549fe8dcf276150a49640",
    "option" : "aws lambda create-alias --name alias name --function-name function-name \\--routing-config AdditionalVersionWeights={\"2\"=0.05}",
    "isCorrect" : "true"
  }, {
    "id" : "3659a02710d94b67b1119213fdc9094d",
    "option" : "aws lambda create-alias --name alias name --function-name function-name \\--routing-config AdditionalVersionWeights={\"2\"=5}",
    "isCorrect" : "false"
  }, {
    "id" : "d61cce15431f466c9435c24302dd304f",
    "option" : "aws lambda create-alias --name alias name --function-name function-name \\--routing-config AdditionalVersionWeights={\"2\"=0.5}",
    "isCorrect" : "false"
  }, {
    "id" : "a258a33d8ca24c9a8a4e6344ae8a514c",
    "option" : "aws lambda create-alias --name alias name --function-name function-name \\--routing-config AdditionalVersionWeights={\"2\"=5%}",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nRouting-Config parameter of the Lambda alias allows one to point to two different versions of the Lambda function and determine what percentage of incoming traffic is sent to each version.\nIn the above case, a new version will be created to test the new function with 5 % of the traffic, while the original version will be used for the remaining 95% traffic.\nOption B is incorrect since 5% of traffic needs to shift to a new function.\nThe routing-config parameter should be 0.05 &amp; not 5.\nOption C is incorrect since 5% of traffic needs to shift to a new function.\nThe routing-config parameter should be 0.05 &amp; not 0.5.\nOption D is incorrect since 5% of traffic needs to shift to a new function.\nThe routing-config parameter should be 0.05 &amp; not 5%.\nFor more information on Pointing to the right version of Lambda function in CloudFormation, refer to the following URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-traffic-shifting-using-aliases.html\n\nTo test the new Lambda Function with migrating 5% of traffic to the new version, we need to create an alias for the Lambda Function and use the routing configuration to specify the percentage of traffic that should be sent to the new version.\nThe correct answer is A. aws lambda create-alias --name alias name --function-name function-name --routing-config AdditionalVersionWeights={\"2\"=0.05}\nHere's a detailed explanation of why:\nAn AWS Lambda Function can have multiple versions, and each version is identified by a unique version number. To control the traffic between different versions of a function, we can create an alias, which is a pointer to a specific version of the function.\nWhen we create an alias, we can specify the percentage of traffic that should be routed to each version of the function using the routing configuration. In this case, we want to route 5% of traffic to the new version, so we need to set the AdditionalVersionWeights attribute to {\"2\"=0.05}, where \"2\" is the version number of the new version.\nHere's what each of the options means:\nA. aws lambda create-alias --name alias name --function-name function-name --routing-config AdditionalVersionWeights={\"2\"=0.05} This option is correct. It creates an alias named \"alias name\" for the function named \"function-name\" and sets the percentage of traffic routed to version 2 to 5%.\nB. aws lambda create-alias --name alias name --function-name function-name --routing-config AdditionalVersionWeights={\"2\"=5} This option is incorrect. It sets the percentage of traffic routed to version 2 to 500%, which is not a valid value.\nC. aws lambda create-alias --name alias name --function-name function-name --routing-config AdditionalVersionWeights={\"2\"=0.5} This option is incorrect. It sets the percentage of traffic routed to version 2 to 50%, which is too high for our requirements.\nD. aws lambda create-alias --name alias name --function-name function-name --routing-config AdditionalVersionWeights={\"2\"=5%} This option is incorrect. The syntax of the value is not correct. We need to use a decimal value instead of a percentage value.\n\n"
}, {
  "id" : 250,
  "question" : "You've created a Code Commit Repository in AWS.\nYou need to share the repository with the developers in your team.\nWhich of the following would be a secure and easier way to share the repository with the development team? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "b3822e80e0e5443d880a196d1c3a8a4a",
    "option" : "Create Git credentials for the IAM users.",
    "isCorrect" : "true"
  }, {
    "id" : "7f36c89341dd44f9bc9b22e7d2d52550",
    "option" : "Allow the developers to connect via HTTPS using the Git credentials.",
    "isCorrect" : "true"
  }, {
    "id" : "b31cd5ecead540bba5515d1609f65ee5",
    "option" : "Allow the developers to connect via SSH.",
    "isCorrect" : "false"
  }, {
    "id" : "ebddec0e69354c81a5d3650de2645c67",
    "option" : "Create a public private key pair.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS Documentation mentions the following.\nYour repository users must configure in the credential helper included in the AWS CLI.\nBut it is the only method available for root account or federated users.\nGit credentials are the easiest method for users of your repository to set up and use.\nSSH connections require your users to generate a public-private key pair, store the public key, associate the public key with their IAM user, configure their known hosts file on their local computer, and create and maintain a config file on their local computers.\nBecause this is a more complex configuration process, we recommend you choose HTTPS and Git credentials for connections to AWS CodeCommit.\nThe easiest way to set up AWS CodeCommit is to configure HTTPS Git credentials for AWS CodeCommit.\nThis HTTPS authentication method:\nUses a static user name and password.\nWorks with all operating systems supported by AWS CodeCommit.\nIs also compatible with integrated development environments (IDEs) and other development tools that support Git credentials.\nThe simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for AWS CodeCommit in the IAM console and then use those credentials for HTTPS connections.\nFor more information, please refer to the below URL-\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html\nOptions C and D are incorrect because these are the least easy way to connect to the repository.\nFor more information on sharing a repository, please refer to the below URL-\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/how-to-share-repository.html\n\nSure, I'd be happy to help!\nWhen it comes to sharing a CodeCommit repository with your development team, you want to make sure that the process is both secure and easy for your team members to use. There are a few options available to you, but the two that are considered the most secure and easy to use are:\nA. Create Git credentials for the IAM users: You can create Git credentials for each member of your development team, which will allow them to securely access the repository. This involves creating IAM users for each team member, and then creating Git credentials for those users. Once the credentials are created, the developers can use them to authenticate with the repository and perform Git operations.\nB. Allow the developers to connect via HTTPS using the Git credentials: You can also allow your team members to connect to the repository via HTTPS using the Git credentials. This involves creating Git credentials as described above, and then configuring the repository to allow HTTPS connections. Once this is done, the developers can clone the repository using the HTTPS URL and authenticate using the Git credentials.\nC. Allow the developers to connect via SSH: While this is also a secure way to connect to the repository, it is not as easy to set up as the previous two options. This requires each team member to generate an SSH key pair, and then provide the public key to the repository owner. The repository owner would then need to add the public key to the repository's SSH keys list, which can be a cumbersome process. Once this is done, the developers can connect to the repository using the SSH URL.\nD. Create a public-private key pair: While this is a secure option, it is not a recommended method for sharing a CodeCommit repository with your development team. This is because it involves sharing a private key with each team member, which can be a security risk. Additionally, this method is more difficult to set up than the other options listed above.\nIn summary, options A and B are the recommended methods for sharing a CodeCommit repository with your development team as they are both secure and easy to set up. Option C is also secure, but it can be a more difficult process to set up. Option D is not recommended due to the security risks involved.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 252,
  "question" : "Your team has just moved from their Jenkins setup to using the AWS Code Pipeline service in AWS.\nThey have a requirement to ensure triggers are in place during various stages in the pipeline to take actions based on those triggers.\nWhich of the following can help you achieve this?\n",
  "answers" : [ {
    "id" : "8502694fc2cd46b2be4c7266eca3513a",
    "option" : "AWS Cloudwatch Events",
    "isCorrect" : "true"
  }, {
    "id" : "9a8851c042be4d63b15998f0b6073b34",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "ffabb7f82de2404394259f09789dcaff",
    "option" : "AWS Cloudtrail",
    "isCorrect" : "false"
  }, {
    "id" : "d9f6e97413a04820a411265eb9a55362",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon CloudWatch Events is a web service that monitors your AWS resources and the applications you run on AWS.\nYou can use Amazon CloudWatch Events to detect and react to changes in the state of a pipeline, stage, or action.\nThen, based on the rules you create, CloudWatch Events invokes one or more target actions when a pipeline, stage, or action enters the state you specify in a rule.\nDepending on the type of state change, you might want to send notifications, capture state information, take corrective action, initiate events, or take other actions.\nOption B is incorrect since this service is used to monitor configuration changes.\nOption C is incorrect since this service is used for API monitoring.\nOption D is incorrect since this service is used to give recommendations.\nFor more information on Cloudwatch events and Code Pipeline, please refer to the below URL-\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html\n\nThe correct answer for this question is A. AWS Cloudwatch Events.\nAWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. AWS CodePipeline can help you deliver your application or infrastructure updates to production faster and more reliably. It models, visualizes, and automates the steps required to release your software.\nAWS CodePipeline allows you to define a series of stages, such as source, build, test, and deploy, which are used to build and release your software. Each stage is composed of one or more actions. Actions are the tasks that AWS CodePipeline performs on your behalf, such as retrieving your application source code, building your application, or deploying your application to production.\nAWS CodePipeline integrates with several other AWS services to provide a complete solution for continuous delivery. AWS CodePipeline can use AWS CodeBuild for building and testing your application, AWS CodeDeploy for deploying your application to production, and AWS CloudFormation for creating and managing your infrastructure.\nIn order to ensure triggers are in place during various stages in the pipeline to take actions based on those triggers, AWS Cloudwatch Events is the right service to use. AWS CloudWatch Events delivers a near real-time stream of system events that describe changes to AWS resources. Using simple rules, you can match events and route them to one or more target functions or streams.\nAWS CloudWatch Events allow you to create custom rules that match events and route them to targets like AWS Lambda functions, Amazon SNS topics, and AWS Step Functions state machines. You can use CloudWatch Events to detect and respond to changes in your AWS resources, including AWS CodePipeline, AWS CodeBuild, AWS CodeDeploy, and AWS CloudFormation.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config provides a detailed view of the configuration of AWS resources and their relationships to other resources in your environment. However, it is not suitable for triggering actions in AWS CodePipeline.\nAWS CloudTrail is a service that enables you to log, monitor, and retain account activity related to actions taken in your AWS account. AWS CloudTrail provides visibility into user activity, resource changes, and API calls made in your AWS account. However, it is not suitable for triggering actions in AWS CodePipeline.\nAWS Trusted Advisor is a service that helps you optimize your AWS infrastructure and improve your security posture. AWS Trusted Advisor provides guidance to help you optimize costs, improve performance, and enhance security in your AWS environment. However, it is not suitable for triggering actions in AWS CodePipeline.\n\n"
}, {
  "id" : 253,
  "question" : "Your company has an application that is interacting with a DynamoDB table.\nAfter reviewing the logs for the application, it has been noticed that there quite a few “ProvisionedThroughputExceededException” occurring in the logs.\nWhich of the following can be implemented to overcome these errors?\n",
  "answers" : [ {
    "id" : "6428ce9dd9a34662b36adf91fa3dba45",
    "option" : "Implement global tables.",
    "isCorrect" : "false"
  }, {
    "id" : "b3f82fca0d0f4ef38219ab8145168565",
    "option" : "Use exponential backoff in the program.",
    "isCorrect" : "true"
  }, {
    "id" : "ea04c2e1f1c84b0baf275a9720f93a0b",
    "option" : "Ensure the correct permissions are set for the Instance profile for the instance hosting the application.",
    "isCorrect" : "false"
  }, {
    "id" : "5b6383de1f3e48dab6a5c988d830371f",
    "option" : "Ensure to use indexes instead.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nOption A is incorrect since this is used for deploying a multi-region, multi-master database.\nOption C is incorrect since this is not a permissions issue.\nOption D is incorrect since this is not an indexing issue.\nFor more information on handling programming errors, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html\n\n\nThe \"ProvisionedThroughputExceededException\" error in DynamoDB indicates that the read or write request rate to the table or partition key is exceeding the provisioned throughput limit. The provisioned throughput is the amount of capacity reserved for a table or partition key to handle requests. When the limit is exceeded, DynamoDB returns this error.\nTo overcome this error, we can implement the following solutions:\nA. Implement global tables: If the application is accessing DynamoDB from multiple AWS regions, we can use global tables to replicate the table data across multiple regions, thus allowing for higher read and write request rates. This can increase the provisioned throughput capacity and avoid the \"ProvisionedThroughputExceededException\" error.\nB. Use exponential backoff in the program: When a request to DynamoDB is throttled, the application can retry the request with an exponential backoff strategy, which involves waiting for an increasing amount of time between each retry attempt. This can reduce the request rate and give the provisioned throughput capacity time to recover.\nC. Ensure the correct permissions are set for the Instance profile for the instance hosting the application: If the application is running on an EC2 instance, we need to ensure that the instance profile associated with the instance has the necessary permissions to access DynamoDB. Without the correct permissions, the application may be unable to make requests to DynamoDB, resulting in errors.\nD. Ensure to use indexes instead: If the application is performing a large number of queries or scans, we can create indexes on the table to improve the performance of the queries. Indexes allow the application to access specific data in the table without scanning the entire table, thus reducing the request rate and avoiding the \"ProvisionedThroughputExceededException\" error.\nIn summary, the best solution to overcome \"ProvisionedThroughputExceededException\" errors in DynamoDB would be to implement a combination of A, B, and D. By using global tables, implementing exponential backoff, and optimizing queries with indexes, we can increase the provisioned throughput capacity, reduce the request rate, and improve the application's performance.\n\n"
}, {
  "id" : 254,
  "question" : "Your development team requires a message to be consumed by multiple consumers in an application.\nYou also need to ensure that the metadata can be sent along with the messages.\nWhich of the following would you implement for this purpose?\n",
  "answers" : [ {
    "id" : "bacdfe5bb914461daf8d034bc07d6bb4",
    "option" : "Implement as SNS topic and use different endpoints for the different types of metadata.",
    "isCorrect" : "false"
  }, {
    "id" : "e69e798eb6564090afe19feed24341f2",
    "option" : "Use SQS queues and create different queues for the different type of metadata.",
    "isCorrect" : "false"
  }, {
    "id" : "561e1ecc5919481b9725db15c998726d",
    "option" : "Use a FIFO SQS queue to maintain order of messages and fan them out.",
    "isCorrect" : "false"
  }, {
    "id" : "75d085cf65d440d6951d5140d32fa127",
    "option" : "Use an SNS topic and add message attributes to the messages.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAmazon SNS supports the delivery of message attributes which let you provide structured metadata items (such as timestamps, geospatial data, signatures, and identifiers) about the message.\nEach message can have up to 10 attributes.\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-message-attributes.html\nOptions A is not correct because you are asked to send metadata information along with the message.\nHere it's referring to work with many custom endpoints (which is much more overhead).\nOption B is invalid since you cannot mimic the functionality of SNS Topics through many SQS queues.\nThis would be a bad practice.\nOption C is not the intended use for FIFO SQS.\nWhile it does maintain the order of items in the queue and their underlying message attributes, it does not fan output to endpoints.\nFor more information specifically on SQS message attributes, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-message-attributes.html\n\nThe requirement is to consume a message by multiple consumers in an application while ensuring that metadata can be sent along with the messages. AWS provides multiple messaging services to address this requirement, and among them, the most suitable option would depend on the specific needs and use case.\nOption A suggests implementing an SNS topic and using different endpoints for different types of metadata. Amazon SNS (Simple Notification Service) is a fully managed messaging service that enables message delivery to multiple subscribers or endpoints, such as email, SMS, and HTTP/S. It supports filtering of messages based on message attributes, which can be used to route messages to specific endpoints. This option could work if there is a limited number of metadata types and specific endpoints can be associated with each metadata type. However, this may not be scalable or flexible enough for complex applications.\nOption B suggests using SQS (Simple Queue Service) queues and creating different queues for different types of metadata. Amazon SQS is a fully managed message queuing service that allows decoupling of message producers and consumers, enabling asynchronous and distributed processing. Each queue can have its own set of message attributes, which can be used to identify the metadata type. This option is more scalable and flexible compared to Option A, as it allows multiple consumers to consume messages from the same queue, and the number of queues can be dynamically adjusted based on the workload. However, it may require additional logic to ensure that messages are processed in the correct order.\nOption C suggests using a FIFO (First-In-First-Out) SQS queue to maintain the order of messages and fan them out. A FIFO queue guarantees that messages are processed in the order they are received, and it also provides deduplication of messages to ensure that each message is processed only once. This option could work if the order of messages is critical and multiple consumers need to consume messages in the same order. However, it may not be suitable if the workload is highly variable or if multiple metadata types need to be processed concurrently.\nOption D suggests using an SNS topic and adding message attributes to the messages. This option is similar to Option A, but it allows adding metadata to individual messages rather than routing messages to specific endpoints. Message attributes are key-value pairs that can be added to a message, and they can be used for filtering and routing purposes. This option could work if the metadata needs to be attached to individual messages rather than the entire topic, and if the number of metadata types is relatively small.\nIn summary, the most suitable option depends on the specific needs and use case. Option B using SQS queues and creating different queues for different types of metadata is likely the most scalable and flexible option.\n\n"
}, {
  "id" : 255,
  "question" : "You are planning on using the Serverless Application Model to deploy a Lambda function.\nBelow is a normal construct for the template to be used. AWSTemplateFormatVersion: '2010-09-09' Transform: AWS::Serverless-2016-10-31 Resources: PutFunction: Type: AWS::Serverless::Function Properties: Handler: index.handler Runtime: nodejs6.10 CodeUri: Where would the code base for the CodeUri normally point to?\n",
  "answers" : [ {
    "id" : "e904c4223f934ddc98a770010979cbfe",
    "option" : "The code as a zip package in Amazon Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "471f2c4ef3d74470bc1cdf3563a6956f",
    "option" : "The code as a zip package in Amazon EBS Volumes",
    "isCorrect" : "false"
  }, {
    "id" : "d37f1710bc684875938c661b26e343e2",
    "option" : "The code as a zip package in Amazon S3",
    "isCorrect" : "true"
  }, {
    "id" : "a52ce219ca114deb9cf0ece68a51689f",
    "option" : "The code as a zip package in Amazon Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe documentation shows a snippet of the template for the YAML file which is used for the deployment.\nBecause of what is mentioned in the documentation, all other options are invalid.\nFor more information on Serverless App deployment, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/serverless_app.html\n\n\nWhen deploying a Lambda function using the Serverless Application Model (SAM), the CodeUri property is used to specify the location of the code for the Lambda function. The code must be provided as a zip package.\nThe possible options for where to store the zip package include:\nA. Amazon Glacier - This is a storage service designed for long-term cold data archiving. It is not designed for storing actively used code packages and is not a suitable option for storing code for Lambda functions.\nB. Amazon EBS Volumes - This is a storage service for attaching storage volumes to EC2 instances. It is not designed for storing code packages and is not a suitable option for storing code for Lambda functions.\nC. Amazon S3 - This is a highly scalable object storage service. It is a commonly used option for storing code packages for Lambda functions. The S3 bucket used for storing code packages must be in the same AWS region as the Lambda function.\nD. Amazon Config - This is a service for monitoring and recording configuration changes to AWS resources. It is not designed for storing code packages and is not a suitable option for storing code for Lambda functions.\nTherefore, the correct answer is C. The code as a zip package in Amazon S3.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 257,
  "question" : "Your team is considering the deployment of their applications using Opsworks stacks.\nThey want to ensure they use the right configuration management tools which can be used with Opsworks.\nWhich of the below are officially supported as configuration management tools with Opsworks? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "7ee5ea5e5a794844bb8b4583a16f1a37",
    "option" : "Chef",
    "isCorrect" : "true"
  }, {
    "id" : "c855df1feb3e4a80b40419034c0029a3",
    "option" : "Ansible",
    "isCorrect" : "false"
  }, {
    "id" : "9f267de52b6b44e1bbc52d6434771922",
    "option" : "SaltStack",
    "isCorrect" : "false"
  }, {
    "id" : "9e3e9e24335d40a48e39892a46074661",
    "option" : "Puppet.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThe AWS Documentation mentions the following.\nAWS OpsWorks is a configuration management service that helps you configure and operate applications in a cloud enterprise using Puppet or Chef.\nAWS OpsWorks Stacks and AWS OpsWorks for Chef Automate let you use Chef cookbooks and solutions for configuration management.\nIn contrast, OpsWorks for Puppet Enterprise lets you configure a Puppet Enterprise master server in AWS.\nPuppet offers a set of tools for enforcing the desired state of your infrastructure and automating on-demand tasks.\nOptions B and C are incorrect because Ansible and SaltStack are automation tools and cannot be used with AWS OpsWorks.\nFor more information on Opsworks, please refer to the below URL-\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/welcome.html\n\nOpsWorks is a configuration management service offered by AWS for managing and deploying applications on the cloud. It provides an integrated experience for DevOps engineers to automate the deployment, configuration, and management of their applications on AWS infrastructure. OpsWorks supports various configuration management tools, including Chef, Puppet, and Ansible.\nThe supported configuration management tools for OpsWorks are:\nA. Chef: Chef is a powerful automation platform that allows you to automate the process of configuring, deploying, and managing infrastructure. OpsWorks has deep integration with Chef and provides support for both Chef Solo and Chef Server.\nB. Ansible: Ansible is an open-source automation tool that enables you to automate your application deployment and management. OpsWorks supports Ansible integration through custom cookbooks.\nC. SaltStack: SaltStack is an open-source automation tool that automates the configuration and management of infrastructure. Although SaltStack is not officially supported by OpsWorks, it is possible to use SaltStack with OpsWorks through custom cookbooks.\nD. Puppet: Puppet is a configuration management tool that automates the deployment and management of infrastructure. OpsWorks provides native support for Puppet and allows you to manage your infrastructure using Puppet manifests.\nIn summary, OpsWorks supports multiple configuration management tools, including Chef, Ansible, and Puppet. While SaltStack is not officially supported, it can be used through custom cookbooks. DevOps engineers can choose the configuration management tool that best suits their requirements and use it to automate their application deployment and management on AWS infrastructure.\n\n"
}, {
  "id" : 258,
  "question" : "Your team is planning on deploying an application on an ECS cluster.\nThey also need to ensure that the X-Ray service can trace the application deployed on the cluster.\nWhich of the following is the right set of steps that are needed to accomplish this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "44d2bb65b1374a8aada82951f513bf29",
    "option" : "Create a Docker image with the X-Ray daemon.",
    "isCorrect" : "true"
  }, {
    "id" : "4cfe6c5078ec475b95c316a93d205950",
    "option" : "Attach an IAM role with permissions to the ECS Cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "1931b5d81d66452ea5f257e560ad3245",
    "option" : "Deploy the EC2 Instance to the ECS Cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "76972224596e4211b52663eaa2821d1c",
    "option" : "Assign a role to the docker container instance in ECS which has a policy that allows it to write to xray.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThis is given in the AWS Documentation.\nOptions B and C are invalid since these are irrelevant to the requirements given in the question.\nFor more information on X-Ray and ECS, please refer to the below URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html\n\n\nTo enable X-Ray tracing for an application deployed on an ECS cluster, you need to take the following steps:\nCreate a Docker image with the X-Ray daemon: You can create a custom Docker image that includes the X-Ray daemon. This image should be based on the official AWS X-Ray daemon image available on DockerHub. You can then use this image to run your application on ECS. Attach an IAM role with permissions to the ECS Cluster: You need to attach an IAM role to the ECS cluster, which has permissions to communicate with the X-Ray service. This role should have the AWSXRayDaemonWriteAccess policy attached to it. Deploy the EC2 Instance to the ECS Cluster: You need to deploy an EC2 instance to the ECS cluster, which will run the Docker container instances that host your application. This EC2 instance must have an IAM instance profile assigned to it that includes the IAM role with the AWSXRayDaemonWriteAccess policy. Assign a role to the Docker container instance in ECS which has a policy that allows it to write to xray: When you create a task definition for your application in ECS, you can specify an IAM task role for the container instances. This role should include the AWSXRayWriteOnlyAccess policy, which grants the container instance permission to write data to X-Ray.\nTherefore, the correct answers are A and D:\nA. Create a Docker image with the X-Ray daemon. D. Assign a role to the Docker container instance in ECS which has a policy that allows it to write to xray.\n\n"
}, {
  "id" : 259,
  "question" : "Your team is developing an API that they want to host using the AWS API gateway service.\nThey don't want to allow anonymous access and want to have an authentication mechanism in place.\nWhich of the following can be used for authentication purposes for the API gateway? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ff21eb88294e4c3d9577525ab10e5b3b",
    "option" : "Lambda authorizers",
    "isCorrect" : "true"
  }, {
    "id" : "d187460469b2452e94fe079b9f3a8b87",
    "option" : "AWS Cognito",
    "isCorrect" : "true"
  }, {
    "id" : "bb41ccaadc464f41af308e7395383c22",
    "option" : "API keys",
    "isCorrect" : "true"
  }, {
    "id" : "d5eefd88635b41d2830882c8d1c2d2b6",
    "option" : "User names and passwords.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A, B and C.\nThe AWS Documentation mentions the following.\nAPI Gateway supports multiple mechanisms for controlling access to your API.\nResource policies let you create resource-based policies to allow or deny access to your APIs and methods from the specified source IP addresses or VPC endpoints.\nStandard AWS IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods.\nCross-origin resource sharing (CORS) lets you control how your API responds to cross-domain resource requests.\nLambda authorizers are Lambda functions that control access to your API methods using bearer token authentication as well as information described by headers, paths, query strings, stage variables, or context variables request parameters.\nAmazon Cognito user pools let you create customizable authentication and authorization solutions.\nClient-side SSL certificates can be used to verify that HTTP requests to your backend system are from API Gateway.\nUsage plans let you provide API keys to your customers - and then track and limit the usage of your API stages and methods for each API key.\nFor more information on controlling access to the API, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\n\nSure, I'd be happy to help you understand this question.\nAWS API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It offers multiple options for authentication, authorization, and access control to your API, ensuring that only authorized users or applications can access your API.\nIn this question, the requirement is to have an authentication mechanism in place to prevent anonymous access to the API. The options provided are:\nA. Lambda authorizers: A Lambda authorizer is a custom authorizer that uses a Lambda function to authenticate requests to your API. It can be used to perform a wide range of authentication and authorization tasks, such as verifying an OAuth token, checking if a user is authorized to access a particular resource, or validating a JSON Web Token (JWT). Lambda authorizers are flexible and can be used with any identity provider that supports the OAuth 2.0 or OpenID Connect protocols.\nB. AWS Cognito: AWS Cognito is a managed service that provides user authentication, authorization, and user management for web and mobile applications. It supports various authentication mechanisms, including username and password, social login, and multi-factor authentication (MFA). Cognito can be used as an identity provider (IDP) for your API Gateway, enabling you to authenticate users before they access your API.\nC. API keys: API keys are simple tokens that can be used to authenticate requests to your API. They are usually passed as query parameters or headers in the HTTP request. API keys are suitable for simple authentication scenarios where you only need to restrict access to your API to a set of trusted applications or users. However, API keys are not secure enough for more complex scenarios, as they can be easily shared or stolen.\nD. User names and passwords: Usernames and passwords are a common form of authentication that requires users to provide a unique identifier (username) and a secret credential (password) to access a system or resource. While this option may work for authentication in some scenarios, it is not recommended for API Gateway as it can be difficult to manage at scale and can expose security risks such as credential stuffing attacks.\nTherefore, the correct answers for this question are A, B, and C. Lambda authorizers, AWS Cognito, and API keys are all valid options for authentication in API Gateway, depending on your specific requirements and use case.\n\n"
}, {
  "id" : 260,
  "question" : "Your team has started configuring CodeBuild to run builds in AWS.\nThe source code is stored in a bucket.\nWhen the build is run, you are getting the below error. Error: \"The bucket you are attempting to access must be addressed using the specified endpoint...\" When Running a Build. Which of the following could be the cause of the error?\n",
  "answers" : [ {
    "id" : "d174245a3ed44a9f9ab6bb8387360eaf",
    "option" : "The bucket is not in the same region as the Code Build project.",
    "isCorrect" : "true"
  }, {
    "id" : "5fce21233b5743a789f6acd303ba0a5d",
    "option" : "Code should ideally be stored on EBS Volumes.",
    "isCorrect" : "false"
  }, {
    "id" : "6d3bf8c84ef144d3b5ab4bddfbca1894",
    "option" : "Versioning is enabled for the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "187f2269c95e483cb4108db5d926e004",
    "option" : "MFA is enabled on the bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis error is specified in the AWS Documentation.\nBecause the error is clearly mentioned, all other options are invalid.\nFor more information on troubleshooting Code Builds, please refer to the below URL-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/troubleshooting.html\n\n\nThe error message, \"The bucket you are attempting to access must be addressed using the specified endpoint,\" typically occurs when there is a mismatch between the region in which the CodeBuild project is located and the region in which the S3 bucket storing the source code is located. In other words, the CodeBuild project is trying to access a bucket that is located in a different region, and the bucket's endpoint is not valid for the region in which the CodeBuild project is located.\nOption A is therefore the correct answer. The bucket is not in the same region as the CodeBuild project, which is causing the error message.\nOption B, \"Code should ideally be stored on EBS Volumes,\" is not relevant to the error message, as it does not address the issue of the mismatched regions.\nOption C, \"Versioning is enabled for the bucket,\" is also not relevant to the error message, as it does not address the issue of the mismatched regions.\nOption D, \"MFA is enabled on the bucket,\" is also not relevant to the error message, as it does not address the issue of the mismatched regions. MFA (multi-factor authentication) is a security measure used to protect against unauthorized access to resources, and it is not related to the issue of accessing a bucket in a different region than the CodeBuild project.\nTo resolve the error message, the S3 bucket containing the source code should be located in the same region as the CodeBuild project. This can be accomplished by either creating a new bucket in the same region as the CodeBuild project or by copying the contents of the existing bucket to a new bucket in the same region.\n\n"
}, {
  "id" : 261,
  "question" : "Your development team is testing out an application that is being deployed onto AWS Elastic Beanstalk.\nThe application needs to have an RDS Instance provisioned as part of the Elastic Beanstalk setup.\nThey also want to ensure that the database is preserved for analysis even after the environment is torn down.\nHow can you achieve both the requirements? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "358dd8df955746018db5f7f79285f95e",
    "option" : "Ensure the database is created as part of the Elastic Beanstalk development or testing environment.",
    "isCorrect" : "true"
  }, {
    "id" : "9a3699789fc0474f9216fb7e3fa1d0a7",
    "option" : "Ensure that you only choose the MySQL engine type.",
    "isCorrect" : "false"
  }, {
    "id" : "8ff5ea8e020f47d28ea9373821514f75",
    "option" : "Ensure that the retention for the database is marked as “Create Snapshot”.",
    "isCorrect" : "true"
  }, {
    "id" : "cb1bf3431c9e422dba0cc560ca3812a1",
    "option" : "Ensure that the retention for the database is marked as “Automated Backup”.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS documentation mentions that you should ensure the Retention field is marked as \"Create Snapshot\" for saving a snapshot before the termination of the environment.\nThis will ensure that we can create a database from the snapshot that is created.\nAWS Elastic Beanstalk provides support for running RDS instances in your Elastic Beanstalk environment.\nThis works great for development and testing environments.\nHowever, it isn't ideal for a production environment because it ties the lifecycle of the database instance to the lifecycle of your application's environment.\nOption B is incorrect since it is not necessary that the engine type only has to be MySQL.\nOption D is incorrect since the option should be “Create Snapshot”.\nFor more information on managing a database in Elastic Beanstalk, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.db.html\nNote:\nFor a production environment, AWS recommends launching a database instance outside of your environment and configure your application to connect to it outside of the functionality provided by Elastic Beanstalk.\nUsing a database instance that is external to your environment requires additional security groups and connection string configuration.\nHowever, it also lets you connect to the database from multiple environments, use database types not supported with integrated databases, perform blue/green deployments, and tear down your environment without affecting the database instance.\n\n\nTo achieve the requirements of provisioning an RDS instance as part of the Elastic Beanstalk setup and preserving the database even after the environment is torn down, we can follow the below steps:\nEnsure the database is created as part of the Elastic Beanstalk development or testing environment:\nWe can create an RDS instance and attach it to the Elastic Beanstalk environment using the AWS Management Console or Elastic Beanstalk CLI. This ensures that the database is part of the environment and can be easily managed and scaled along with the application. When the environment is terminated, the RDS instance associated with the environment will also be terminated.\nEnsure that the retention for the database is marked as “Create Snapshot”:\nBy selecting the “Create Snapshot” retention option, we can preserve the database data even after the environment is torn down. When the RDS instance is terminated, a final snapshot of the database is created, which can be used to restore the database later. We can also specify a retention period for snapshots, which will automatically delete snapshots older than the specified period.\nEnsure that you only choose the MySQL engine type:\nThis option is not relevant to the requirements mentioned in the question. Elastic Beanstalk supports multiple database engines, including MySQL, PostgreSQL, and SQL Server. We can choose the database engine that best suits our application's requirements.\nEnsure that the retention for the database is marked as “Automated Backup”:\nWhile this option can be used to preserve the database, it does not satisfy the requirement of preserving the database even after the environment is torn down. Automated backups are incremental backups taken daily, which can be used to restore the database to a specific point in time. However, these backups are stored in the same region as the RDS instance and will be deleted if the RDS instance is terminated.\nIn conclusion, options A and C are the correct answers to achieve the requirements of provisioning an RDS instance as part of the Elastic Beanstalk setup and preserving the database even after the environment is torn down.\n\n"
}, {
  "id" : 262,
  "question" : "Your team is looking towards uploading different versions of an application using the AWS Elastic Beanstalk.\nHow can they achieve this in the easiest possible way?\n",
  "answers" : [ {
    "id" : "cddf8373f22a415797a62a948ee7b44d",
    "option" : "Create multiple applications in Elastic Beanstalk.",
    "isCorrect" : "false"
  }, {
    "id" : "86b2c49abbe2425b8813afb90b2656ac",
    "option" : "Create multiple environments in Elastic Beanstalk.",
    "isCorrect" : "false"
  }, {
    "id" : "6b5879962b004f99a895bc07342ab35f",
    "option" : "Upload the application`s source code or source bundle in the Elastic Beanstalk console.",
    "isCorrect" : "true"
  }, {
    "id" : "98a815900d9c401fbb65e7bbee142843",
    "option" : "Use CodePipeline to stream line the various application versions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nElastic Beanstalk creates an application version whenever you upload source code.\nThis usually occurs when you create an environment or upload and deploy code using the environment management console or EB CLI.\nElastic Beanstalk deletes these application versions according to its life cycle policy and when you delete the application.\nYou can also upload a source bundle without deploying it from the application management console or with the EB CLI command eb appversion.\nElastic Beanstalk stores source bundles in Amazon Simple Storage Service (Amazon S3) and doesn't automatically delete them.\nOptions A and B are incorrect since this would be the least efficient way to maintain application revisions.\nOption D is incorrect since you would not use CodePipeline for application versions.\nFor more information on application versions, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-versions.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.deploy-existing-version.html\n\n\nElastic Beanstalk is a fully managed service provided by AWS that helps developers deploy and run their applications at scale. It takes care of the underlying infrastructure, including the application server, database, and other resources needed to run the application. Elastic Beanstalk provides an easy-to-use interface for deploying and managing applications.\nIf your team is looking to upload different versions of an application using Elastic Beanstalk, there are different approaches you can take, such as creating multiple applications, creating multiple environments, uploading the application's source code or source bundle, or using CodePipeline to streamline the various application versions.\nHere's a detailed explanation of each of these approaches:\nA. Create multiple applications in Elastic Beanstalk: Creating multiple applications in Elastic Beanstalk is one way to manage different versions of an application. Each application would have its own resources, such as a web server, database, and other dependencies, and would be deployed independently. This approach provides greater isolation between different versions of the application but may result in higher costs and greater management overhead.\nB. Create multiple environments in Elastic Beanstalk: Creating multiple environments in Elastic Beanstalk is another way to manage different versions of an application. An environment is a collection of AWS resources, such as EC2 instances and databases, that are used to deploy and run an application version. Each environment can have its own configuration settings, such as instance types, security groups, and database settings. This approach allows you to deploy different versions of the application to different environments and provides a high degree of flexibility and control over the deployment process.\nC. Upload the application's source code or source bundle in the Elastic Beanstalk console: Uploading the application's source code or source bundle in the Elastic Beanstalk console is the most straightforward approach to deploying a new version of an application. Once the application's source code or source bundle has been uploaded, Elastic Beanstalk automatically creates the necessary resources, such as EC2 instances, load balancers, and databases, to run the application. This approach is ideal for smaller teams or applications that do not require complex deployment workflows.\nD. Use CodePipeline to streamline the various application versions: Using CodePipeline is a more advanced approach to managing different versions of an application. CodePipeline is a fully managed continuous integration and continuous delivery (CI/CD) service that automates the entire software release process. It provides a graphical interface for defining the various stages of the deployment pipeline, including building, testing, and deploying the application. This approach is ideal for larger teams or applications that require a more complex deployment workflow.\nIn conclusion, there are different approaches you can take to manage different versions of an application using Elastic Beanstalk, depending on your team's requirements and level of expertise. Creating multiple applications or environments provides greater isolation and flexibility but may result in higher costs and greater management overhead. Uploading the application's source code or source bundle in the Elastic Beanstalk console is the most straightforward approach, while using CodePipeline provides a more advanced and automated deployment workflow.\n\n"
}, {
  "id" : 263,
  "question" : "Your team is planning on creating a DynamoDB table and use it with their application.\nThey are planning to place the initial Read Capacity Unit (RCU) to 10 with the eventual read consistency.\nYou want to read 10 items from the table per second where each item is of size 2 KB.What is the total size of data that can be read per second using the above read throughput capacity?\n",
  "answers" : [ {
    "id" : "3f28bd1090ad47ffab53d9c1794dc92f",
    "option" : "20KB",
    "isCorrect" : "false"
  }, {
    "id" : "1a96304909be46138d7d4c7e6b598732",
    "option" : "10KB",
    "isCorrect" : "false"
  }, {
    "id" : "0f0eb0d1728d480ba3c883de90a6df83",
    "option" : "4KB",
    "isCorrect" : "false"
  }, {
    "id" : "085300d72d024b3fb16d46d1907fe875",
    "option" : "40KB.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOne read capacity unit represents one strongly consistent read per second,\nor two eventually consistent reads per second, for an item up to 4 KB in size.\nAs per the question, for each 2 KB we need 1 RCU.\n(means 2kb/4kb=1 RCU - rounded to the larger whole number)\nSo, for 10 RCU we can have up to 20KB per sec.\nFor eventually consistent read we need to multiply the number of items by '2', So the size of the item would be 40kB.Below is an example from AWS documentation:\nSuppose that you create a provisioned table with 6 read capacity units.\nWith these settings, your application could do the following:\nPerform strongly consistent reads of up to 24 KB per second (4 KB × 6 read capacity units).\nPerform eventually consistent reads of up to 48 KB per second (twice as much read throughput).\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\n\nTo calculate the total size of data that can be read per second, we need to consider the read capacity unit (RCU) and the size of each item.\nA read capacity unit (RCU) is defined as the maximum number of strongly consistent reads per second, or the maximum number of eventually consistent reads per second, of an item up to 4 KB in size. So, if an item is larger than 4 KB, more than one RCU will be required to read it.\nIn this case, the table has an initial Read Capacity Unit (RCU) of 10 with eventual consistency, which means we can read 10 items of 4 KB per second. However, each item in this case is only 2 KB, so we can read twice as many items per second.\nTherefore, the total size of data that can be read per second is:\n10 RCU x 2 KB per item = 20 KB per second\nHence, the correct answer is option A.\n\n"
}, {
  "id" : 264,
  "question" : "A developer has created a script that accesses an S3 bucket.\nThe script will run on an EC2 Instance at regular intervals.\nWhat is the authentication mechanism that should be employed to ensure that the script works as desired?\n",
  "answers" : [ {
    "id" : "d809bca7f63a465dac1a4c1165d11550",
    "option" : "Create an IAM user. Ensure the IAM user has access to the S3 bucket via IAM policies. Embed the user name and password in the script.",
    "isCorrect" : "false"
  }, {
    "id" : "c9377ed00dfd4fe2af63b7b0e52eb86f",
    "option" : "Create an IAM Role. Ensure the IAM Role has access to the S3 bucket via IAM policies. Attach the role to the instance.",
    "isCorrect" : "true"
  }, {
    "id" : "2c3d8508ac3842a688ad8c8824290191",
    "option" : "Create an IAM user. Ensure the IAM user has access to the S3 bucket via IAM policies. Embed the Access keys to the program.",
    "isCorrect" : "false"
  }, {
    "id" : "a20d00e925f345f49e67b843e4021f62",
    "option" : "Create an IAM user. Ensure the IAM user has access to the S3 bucket via IAM policies. Embed the Access keys as environment variables for the Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nYou have an application or AWS CLI scripts running on an Amazon EC2 instance.\nDo not pass an access key to the application, embed it in the application, or have the application read a key from a source such as an Amazon S3 bucket (even if the bucket is encrypted)\nInstead, define an IAM role with appropriate permissions for your application and launch the Amazon EC2 instance with roles for EC2\nThis associates an IAM role with the Amazon EC2 instance and lets the application get temporary security credentials that it can, in turn, use to make AWS calls.\nThe AWS SDKs and the AWS CLI can get temporary credentials from the role automatically.\nAll other options are incorrect since the most secure way is to create and use IAM Roles.\nFor more information on best practices for Access Keys, please refer to the below URL-\nhttps://docs.aws.amazon.com/general/latest/gr/aws-access-keys-best-practices.html\n\nThe correct answer to the question is B. Create an IAM Role. Ensure the IAM Role has access to the S3 bucket via IAM policies. Attach the role to the instance.\nExplanation: To access an S3 bucket from an EC2 instance, you need to have credentials with sufficient permissions to access the S3 bucket. However, embedding credentials in the script or the code is not recommended as it is a security risk. Instead, AWS provides an authentication mechanism that is more secure and convenient - IAM Roles.\nIAM Roles are a secure way to grant temporary permissions to AWS services or EC2 instances to access other AWS services. IAM Roles are preferred over IAM Users because they do not require the management of long-term access keys. IAM Roles can be assigned to an EC2 instance during its launch or after the instance is running.\nTo create an IAM Role that can access the S3 bucket, you will need to create an IAM policy that defines the permissions required to access the S3 bucket. Once the policy is created, you can create an IAM Role and attach the policy to the role. You can then assign the IAM Role to the EC2 instance. The IAM Role will be used to provide temporary credentials to the EC2 instance to access the S3 bucket.\nIn summary, embedding credentials in the script or code is not recommended as it is a security risk. The recommended way to access an S3 bucket from an EC2 instance is to use an IAM Role. An IAM Role provides temporary credentials to the EC2 instance to access the S3 bucket.\n\n"
}, {
  "id" : 265,
  "question" : "You are configuring Cross-Origin Resource Sharing for your S3 bucket.\nYou need to ensure that external domain sites can only issue the GET requests against your bucket.\nWhich of the following would you modify as part of the CORS configuration for this requirement?\n",
  "answers" : [ {
    "id" : "e9626bfcc2204b9f998f2b6f46172354",
    "option" : "AllowedOrigin Element",
    "isCorrect" : "false"
  }, {
    "id" : "bada509f28e542859b75ae005d47a45a",
    "option" : "AllowedHeader Element",
    "isCorrect" : "false"
  }, {
    "id" : "8092823203a54cc99221b3f8c9e6f11c",
    "option" : "AllowedMethod Element",
    "isCorrect" : "true"
  }, {
    "id" : "1c2de1c773494ce5b013dadf4fec7059",
    "option" : "MaxAgeSeconds Element.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is mentioned in the AWS Documentation.\nOption A is invalid since it is used to specify the origins you want to allow cross-domain requests.\nOption B is invalid since this is used to specify which headers are allowed in a preflight request through the Access-Control-Request-Headers header.\nOption D is invalid since it is used to specify the time in seconds that your browser can cache the response for a preflight request identified by the resource, the HTTP method, and the origin.\nFor more information on CORS, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\n\n\nThe answer is (C) AllowedMethod Element.\nCross-Origin Resource Sharing (CORS) is a security feature implemented by web browsers that restricts web pages from making requests to a different domain than the one that served the web page. CORS is a way for servers to inform browsers about which domains are allowed to access its resources.\nIn S3, CORS configuration is done using an XML file that specifies rules for cross-origin access to resources in the bucket. The CORS configuration XML file can be uploaded to the S3 bucket via the AWS Management Console, CLI, or SDKs.\nThe CORS configuration file includes the following elements:\nAllowedOrigin: Specifies the domain names that are allowed to make cross-origin requests to the S3 bucket. AllowedHeader: Specifies which headers are allowed to be included in cross-origin requests. AllowedMethod: Specifies which HTTP methods (e.g., GET, POST, PUT) are allowed for cross-origin requests. This is the element that needs to be modified in this scenario to allow only GET requests. MaxAgeSeconds: Specifies the time period, in seconds, that the browser can cache the CORS response.\nIn the given scenario, the requirement is to allow external domain sites to issue only GET requests against the S3 bucket. Therefore, the AllowedMethod element needs to be modified to include only the GET HTTP method.\nSo, the correct answer is (C) AllowedMethod Element.\n\n"
}, {
  "id" : 266,
  "question" : "Your team is performing load testing for an application.\nThis application is making use of DynamoDB tables.\nThey need to monitor the throughput for the table to ensure that the Consumed capacity does no go beyond the throughput capacity.\nWhich of the following service would you use for this purpose?\n",
  "answers" : [ {
    "id" : "ca0d61feb9594761a6386e838b26c64e",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "true"
  }, {
    "id" : "8c326f01b19d4903a12f5a931048718c",
    "option" : "AWS Cloudtrail",
    "isCorrect" : "false"
  }, {
    "id" : "dcdeee01dbab4c53a1cd4c94c4e3175c",
    "option" : "AWS SQS",
    "isCorrect" : "false"
  }, {
    "id" : "5e331a155f0b4bd599512a53d9fa0ed7",
    "option" : "AWS SNS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is mentioned in the AWS Documentation.\nOption B is invalid since this is an API monitoring service.\nOption C is invalid since this is a Simple Queue Service.\nOption D is invalid since this is a Simple Notification Service.\nFor more information on metrics for DynamoDB, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/metrics-dimensions.html\n\n\nThe correct answer for this question is A. AWS CloudWatch.\nExplanation:\nAWS CloudWatch is a monitoring and management service that provides data and insights into various AWS resources and applications. It allows you to monitor metrics, collect and track log files, and set alarms. It provides a detailed view of resource utilization, operational health, and performance.\nIn the given scenario, the team needs to monitor the throughput for the DynamoDB table to ensure that the consumed capacity does not exceed the throughput capacity. This means that they need to monitor the provisioned throughput and consumed capacity of the DynamoDB table.\nAWS CloudWatch provides metrics for DynamoDB that include consumed read and write capacity units, provisioned read and write capacity units, throttled requests, and more. You can use these metrics to monitor the throughput of your DynamoDB table and ensure that the consumed capacity does not exceed the provisioned throughput.\nIn addition to metrics, AWS CloudWatch also allows you to set alarms based on the metrics. You can set an alarm to notify you when the consumed capacity exceeds a certain threshold, allowing you to take corrective action before it impacts your application's performance.\nTherefore, AWS CloudWatch is the correct service to use for monitoring the throughput of the DynamoDB table and ensuring that the consumed capacity does not go beyond the throughput capacity.\n\n"
}, {
  "id" : 267,
  "question" : "You have created an application for real-time analysis of website clickstreams using Kinesis Data Streams.\nThere are multiple EC2 instance acting as consumers which process data &amp; save analyzed data in Amazon DynamoDB using Enhanced Fan-out.\nWhich of the following can be achieved using Enhanced Fan-out for each registered consumer?\n",
  "answers" : [ {
    "id" : "01c604827dc34fbbacff5be89f07ad9a",
    "option" : "Enables registered consumers to receive records with a throughput of up to 2 MiB of data per second for all shards with an average message propagation delay of 70 ms for all consumers.",
    "isCorrect" : "false"
  }, {
    "id" : "86fb486b79994ae5914db2371bb2777a",
    "option" : "Enables registered consumers to receive records with a throughput of up to 2 MiB of data per second per shard with an average message propagation delay of 70 ms for all consumers.",
    "isCorrect" : "true"
  }, {
    "id" : "eab902b79ef74416916508ed33810a7a",
    "option" : "Enables registered consumers to receive records with a throughput of a total of 2 MiB of data per second per shard shared between all consumers with an average message propagation delay of 70 ms for all consumers.",
    "isCorrect" : "false"
  }, {
    "id" : "0a06d41830f7424a9a04599d91e34823",
    "option" : "Enables registered consumers to receive records with a throughput of up to 2 MiB of data per second per shard with an average message propagation delay of 70 ms which surges based upon many consumers paralleling accessing the shard.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nWith Enhanced Fan-out, each registered consumer gets 2MiB/sec throughput per shard &amp; message propagation delay of 70 ms independent of other consumers.\nOption A is incorrect as With Enhanced Fan-out, each registered consumer gets 2MiB/sec throughput per shard &amp; not shared for all shards.\nOption C is incorrect as With Enhanced Fan-out, each registered consumer gets 2MiB/sec throughput per shard irrespective of the total number of consumers.\nOption D is incorrect as With Enhanced Fan-out, each registered consumer gets a message propagation delay of 70 ms independent of other consumers accessing the shard.\nFor more information on Enhanced Fan-Out with Kinesis Data Streams, refer to the following URL-\nhttps://docs.aws.amazon.com/streams/latest/dev/building-consumers.html\n\nEnhanced Fan-out is a feature of Amazon Kinesis Data Streams that enables multiple consumers to simultaneously read data from a single shard with no duplication of data. This feature increases the number of consumers that can simultaneously read data from a single shard by orders of magnitude when compared to the Classic model.\nIn this scenario, you have multiple EC2 instances acting as consumers, which process data and save analyzed data in Amazon DynamoDB using Enhanced Fan-out. Enhanced Fan-out allows you to register multiple consumers to read data from a single shard in parallel without compromising performance.\nThe correct answer is B. Enhanced Fan-out enables registered consumers to receive records with a throughput of up to 2 MiB of data per second per shard with an average message propagation delay of 70 ms for all consumers.\nThis means that each registered consumer can receive up to 2 MiB of data per second from a single shard. If you have multiple shards, you can register multiple consumers to read data from each shard simultaneously. The average message propagation delay is 70 ms for all consumers, which means that all registered consumers can receive data with the same delay.\nOption A is incorrect because it implies that all shards can deliver 2 MiB of data per second to all consumers, which is not true. Option C is incorrect because it implies that the total throughput of data is shared between all consumers, which is not true. Option D is incorrect because it suggests that message propagation delay surges based on the number of consumers paralleling accessing the shard, which is not true.\nIn conclusion, Enhanced Fan-out is a powerful feature of Amazon Kinesis Data Streams that allows you to register multiple consumers to read data from a single shard simultaneously without compromising performance.\n\n"
}, {
  "id" : 268,
  "question" : "Your company is planning on moving its on-premise data to AWS.\nThey have some Node.js code that needs to be ported onto AWS with the least amount of administrative headache.\nYou also need to ensure that cost is minimized for hosting the code base.\nWhich of the following service would you use for this purpose?\n",
  "answers" : [ {
    "id" : "f5030888b68e4eb0ba0dbecf34207653",
    "option" : "AWS API gateway",
    "isCorrect" : "false"
  }, {
    "id" : "3c9183297aa64375902df1208f4551ac",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "8311cbbb99324f86a7f02ea0d24c15dd",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "e93cc8c272fa447aae10bf39a93eea38",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is mentioned in the AWS Documentation.\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers.\nAWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running.\nWith AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration.\nAWS Lambda runs your code on a high-availability compute infrastructure and performs all of the compute resources administration, including server and operating system maintenance, capacity provisioning and automatic scaling, code monitoring and logging.\nAll you need to do is supply your code in one of the languages that AWS Lambda supports (currently Node.js, Java, C#, Go and Python).\nOption A is incorrect since this is used to manage API.\nOption C is incorrect since this would incur a higher cost for hosting.\nOption D is incorrect since this is a messaging service.\nFor more information on AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html\n\nBased on the requirements mentioned in the question, the most appropriate service to use would be AWS Lambda.\nAWS Lambda is a compute service that allows developers to run code without provisioning or managing servers. It is a serverless service, meaning you don't have to worry about managing the infrastructure required to run your code.\nNode.js is one of the many programming languages supported by AWS Lambda, so porting the code onto AWS Lambda should be relatively straightforward.\nUsing AWS Lambda also ensures that cost is minimized since you only pay for the actual compute time used by your code, and not for any idle time when the code is not running. This is in contrast to services like AWS EC2, where you have to pay for the entire server instance regardless of how much of it you are using.\nAWS API Gateway is a service that allows you to create, publish, and manage APIs for your backend services, and AWS SQS is a message queue service. While both of these services could be used in conjunction with AWS Lambda, they are not directly relevant to the requirement of porting Node.js code to AWS with the least amount of administrative headache and minimizing costs.\nTherefore, the correct answer is B. AWS Lambda.\n\n"
}, {
  "id" : 269,
  "question" : "Your company is currently managing the deployment of its applications using Code Deploy.\nThey want to automate the deployment of the Code Deploy environment itself.\nWhich of the following service can help you achieve this?\n",
  "answers" : [ {
    "id" : "248b72bf132a40a2ad5fb82052e008b3",
    "option" : "AWS Cloudformation",
    "isCorrect" : "true"
  }, {
    "id" : "3c271a306193465593f4b4b431c4b2b5",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "190097d33e104a90ab359e46964fe5da",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "992f31033a6b490dbdaa3e8e55b1afc0",
    "option" : "AWS Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is mentioned in the AWS Documentation.\nAWS CloudFormation is a service that helps you model and set up your AWS resources using templates.\nAn AWS CloudFormation template is a text file whose format complies with the JSON standard.\nYou create a template that describes all of the AWS resources you want, and AWS CloudFormation takes care of provisioning and configuring those resources for you.\nOption B is incorrect because Cloudwatch is a monitoring service and does not help to automate the deployment.\nOption C is incorrect because Elastic Beanstalk cannot automate the deployment for the CodeDeploy service.\nOption D is incorrect because AWS Config is not a service to automate the deployments.\nFor more information on using Cloudformation templates for CodeDeploy, please refer to the below URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-cloudformation-templates.html\n\n\nThe service that can help automate the deployment of the Code Deploy environment itself is AWS CloudFormation (Option A).\nAWS CloudFormation is a service that allows you to create and manage AWS resources using templates. These templates are written in either YAML or JSON and define the infrastructure and resources needed for your application. CloudFormation automates the provisioning of resources and manages the entire lifecycle of those resources.\nUsing CloudFormation, you can create a template that includes all the resources needed for your Code Deploy environment, such as Amazon EC2 instances, Amazon S3 buckets, and IAM roles. Once the template is defined, you can use it to create and manage Code Deploy environments automatically, without any manual intervention.\nAWS CloudWatch (Option B) is a monitoring and observability service that collects and analyzes metrics, logs, and events from AWS resources and applications. It can help you monitor the health and performance of your Code Deploy environment, but it cannot automate the deployment of the environment itself.\nAWS Elastic Beanstalk (Option C) is a service that helps you deploy and manage web applications, but it is not specific to Code Deploy environments.\nAWS Config (Option D) is a service that provides you with a detailed inventory of your AWS resources and their configurations, and also allows you to track changes to those resources over time. While it can help you monitor and manage the configuration of your Code Deploy environment, it does not provide the ability to automate the deployment of the environment itself.\nTherefore, the correct answer is A. AWS CloudFormation.\n\n"
}, {
  "id" : 270,
  "question" : "While choosing an Instance for your web application, which of the following features are available additionally with M5 instance in comparison with T2 instance?\n",
  "answers" : [ {
    "id" : "9b79623d58ba417c9035defa8f41a88a",
    "option" : "Network performance of only up to 5 Gbps &amp; Enhanced Networking Support with EN.",
    "isCorrect" : "false"
  }, {
    "id" : "6cf55d719ef145a5b68698bd3419234a",
    "option" : "Network performance of up to 1 Gbps &amp; Enhanced Networking support with Intel 82599 Virtual Function (VF) interface.",
    "isCorrect" : "false"
  }, {
    "id" : "d1f4527f22d6486d90e4d2f3a5a8e5ed",
    "option" : "Network performance of 10- 25 Gbps based upon instance type &amp; Enhanced Networking Support with EN.",
    "isCorrect" : "true"
  }, {
    "id" : "55bb8ae6ab0e4e18bb8e0a82c86c9932",
    "option" : "Network performance up to 5 Gbps &amp; Enhanced Networking support with Intel 82599 Virtual Function (VF) interface.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nWith M5 General-Purpose Instance, Elastic Network Adaptor is used to support Enhance Networking.\nAlso, M5 General-Purpose Instance support network performance with 10 Gbps to 25 Gbps based upon instance type.\nT2 instance does not support Enhance Networking &amp; support network performance only up to 1 Gbps.\nOption A is incorrect as M5 General-Purpose Instance provides Network Performance of 10 Gbps to 25 Gbps based upon various instance types &amp;not only up to 5 Gbps.\nOptionB is incorrect as M5 General-Purpose Instance provides Enhance Networking Support with Elastic Network Adaptor &amp; not with Intel 82599 Virtual Function (VF) interface.\nAlso, Network Performance of 10 Gbps to 25 Gbps based upon various instance types.\nOptionD is incorrect as M5 General-Purpose Instance provides Enhance Networking Support with Elastic Network Adaptor &amp; not with Intel 82599 Virtual Function (VF) interface.\nFor more information on General Purpose Instance features, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/general-purpose-instances.html\n\nWhen it comes to choosing an instance for your web application, it's essential to understand the different features and capabilities that each instance type offers. In this case, the question is specifically asking about the differences between the M5 and T2 instance types, with a focus on network performance and Enhanced Networking Support.\nFirst, let's take a look at the T2 instance type. T2 instances are burstable, meaning that they provide a baseline level of performance with the ability to burst above that baseline when needed. They are a good choice for applications that don't require sustained high performance, but occasionally need to handle bursts of traffic.\nIn terms of network performance, T2 instances provide up to 1 Gbps of network throughput. They also support Enhanced Networking, which is a feature that provides higher packet-per-second (PPS) performance, lower network jitter, and lower latency. Enhanced Networking on T2 instances is enabled through the use of the Intel 82599 Virtual Function (VF) interface.\nNow let's look at the M5 instance type. M5 instances are general-purpose instances that provide a balance of compute, memory, and networking resources. They are a good choice for a wide variety of workloads, including web applications, databases, and more.\nIn terms of network performance, M5 instances provide up to 5 Gbps of network throughput. However, it's important to note that the actual network performance will depend on the instance size. For example, an m5.24xlarge instance provides up to 25 Gbps of network throughput.\nLike T2 instances, M5 instances also support Enhanced Networking. However, the Enhanced Networking support on M5 instances is provided through the Elastic Network Adapter (ENA), which offers higher bandwidth, higher PPS performance, and lower network jitter and latency than the Intel 82599 VF interface used by T2 instances.\nSo, to summarize the answer to the question, the correct option is D: \"Network performance up to 5 Gbps & Enhanced Networking support with Intel 82599 Virtual Function (VF) interface\" is only available with T2 instances, whereas M5 instances offer up to 5 Gbps of network throughput and Enhanced Networking support with the Elastic Network Adapter (ENA).\n\n"
}, {
  "id" : 271,
  "question" : "Your team is deploying a set of applications onto AWS.\nThese applications work with multiple databases.\nYou need to ensure that the database passwords are stored securely.\nWhich of the following is the ideal way to store the database passwords?\n",
  "answers" : [ {
    "id" : "d3cca748d54f49f7bba291b8c957a1e5",
    "option" : "Store them in separate Lambda functions which can be invoked via HTTPS.",
    "isCorrect" : "false"
  }, {
    "id" : "332b43c4391a4f868d996be7bf97a330",
    "option" : "Store them as secrets in AWS Secrets Manager.",
    "isCorrect" : "true"
  }, {
    "id" : "2ed8453308be4891b3a6050525efb7d6",
    "option" : "Store them in separate DynamoDB tables.",
    "isCorrect" : "false"
  }, {
    "id" : "e33f6d6ca89645d3bd5737eda4ba51ea",
    "option" : "Store them in separate S3 buckets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is mentioned in the AWS Documentation.\nAWS Secrets Manager is an AWS service that makes it easier for you to manage secrets.\nSecrets can be database credentials, passwords, third-party API keys, and even arbitrary text.\nYou can store and control access to these secrets centrally by using the Secrets Manager console, the Secrets Manager command-line interface (CLI), or the Secrets Manager API and SDKs.\nOption A is incorrect because the Lambda function is a compute service and not used for storing credentials.\nOption C is incorrect because DynamoDB is a NoSQL database service and not suitable for storing credentials.\nOption D is incorrect because the S3 bucket is used to store objects and is not particularly designed to store credentials.\nIt may have some security issues.\nFor more information on the Secrets Manager, please refer to the below URL-\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\n\nThe ideal way to store database passwords on AWS is by using AWS Secrets Manager, which is a service that enables you to securely store and manage secrets, such as database passwords, API keys, and other sensitive data. AWS Secrets Manager helps you protect access to your applications, services, and IT resources without the upfront investment and on-going maintenance costs of operating your own infrastructure.\nStoring database passwords as secrets in AWS Secrets Manager provides several benefits:\nEnhanced security: AWS Secrets Manager provides end-to-end encryption for your secrets, which means that your secrets are encrypted at rest and in transit. Additionally, you can define granular access policies to control who can access your secrets. Easy integration: AWS Secrets Manager integrates with a variety of AWS services, such as Amazon RDS, Amazon DocumentDB, and Amazon Redshift. This integration makes it easy to retrieve secrets from AWS Secrets Manager and use them in your applications. Automatic rotation: AWS Secrets Manager enables you to automatically rotate your secrets on a schedule that you define. This feature helps you improve your security posture by ensuring that your secrets are frequently updated.\nOption A, storing database passwords in separate Lambda functions which can be invoked via HTTPS, is not ideal because it requires you to manage your own infrastructure to host and run the Lambda functions. Additionally, you would need to implement your own access controls to secure access to the Lambda functions.\nOption C, storing database passwords in separate DynamoDB tables, is also not ideal because it requires you to manage your own infrastructure to host and run the DynamoDB tables. Additionally, you would need to implement your own access controls to secure access to the DynamoDB tables.\nOption D, storing database passwords in separate S3 buckets, is not ideal because S3 is designed to store and retrieve data, not secrets. Additionally, S3 does not provide end-to-end encryption for your data by default, which means that you would need to implement your own encryption and access controls to secure your secrets.\n\n"
}, {
  "id" : 272,
  "question" : "Your team is currently managing a set of applications for a company in AWS.\nThere is now a requirement to carry out Blue Green deployments for the future set of applications.\nWhich of the following can help you achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9013b123cdbb4aec9bb6b25d2a009e10",
    "option" : "Use Route 53 with the failover routing policy.",
    "isCorrect" : "false"
  }, {
    "id" : "7f89d3e5d160411280e29ff688504bc2",
    "option" : "Use Route 53 with the weighted routing policy.",
    "isCorrect" : "true"
  }, {
    "id" : "de5255be0e6742d5b51974bc962d50e1",
    "option" : "Ensure that the application is placed behind an EL.",
    "isCorrect" : "true"
  }, {
    "id" : "7f90eab471c943ae8fe994bb8f22c8e7",
    "option" : "Ensure that the application is placed in a single AZ.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThis is mentioned in an AWS whitepaper.\nYou can shift traffic all at once, or you can do a weighted distribution.\nWith Amazon Route 53, you can define a percentage of traffic to go to the green environment and gradually update the weights until the green environment carries the full production traffic.\nA weighted distribution provides the ability to perform canary analysis where a small percentage of production traffic is introduced to a new environment.\nIt also allows the green environment to scale out to support the full production load if you're using Elastic Load Balancing, for example.\nElastic Load Balancing automatically scales its request-handling capacity to meet the inbound application traffic.\nOption A is invalid since this is used for failover purposes.\nOption D is invalid since you should not deploy your applications using just a single AZ.\nFor more information on Blue Green deployments, please refer to the below URL-\nhttps://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\n\nBlue Green Deployment is a technique used for deploying applications in a way that reduces downtime, risk and provides a more seamless deployment experience. It involves creating two identical environments (Blue and Green) and directing traffic between them through a load balancer. Once the new version of the application is tested and validated in the Green environment, traffic is switched from the Blue environment to the Green environment, and the Blue environment is retired.\nHere are the options that can help you achieve Blue Green deployments:\nA. Use Route 53 with the failover routing policy: Route 53 is a DNS (Domain Name System) service provided by AWS that can be used to direct traffic between two identical environments (Blue and Green) using failover routing policy. This policy is used to direct traffic to a healthy environment (Blue or Green) and automatically switch traffic to a secondary environment if the primary environment fails. Failover routing policy works best when the primary and secondary environments are in different Availability Zones (AZs) to ensure high availability.\nB. Use Route 53 with the weighted routing policy: Route 53 can also be used to direct traffic between two identical environments (Blue and Green) using the weighted routing policy. In this policy, traffic is distributed between the two environments based on a specified weight. During a Blue Green deployment, the Green environment can be given a higher weight, and as the application is validated in the Green environment, the weight can be gradually increased, and the Blue environment can be eventually retired.\nC. Ensure that the application is placed behind an ELB: An Elastic Load Balancer (ELB) is a service provided by AWS that can be used to distribute traffic across multiple instances of an application deployed in different Availability Zones. ELB can be used to direct traffic between two identical environments (Blue and Green) during a Blue Green deployment. Once the new version of the application is validated in the Green environment, traffic can be directed to the Green environment through ELB, and the Blue environment can be retired.\nD. Ensure that the application is placed in a single AZ: Placing the application in a single Availability Zone (AZ) is not recommended as it increases the risk of downtime due to single point of failure. In a Blue Green deployment, it is recommended to place the Blue and Green environments in different AZs to ensure high availability and reduce the risk of downtime.\nIn conclusion, options A and B can help achieve Blue Green deployments using Route 53 with failover routing policy and weighted routing policy, respectively. Option C using ELB is also a viable solution, while option D is not recommended as it increases the risk of downtime.\n\n"
}, {
  "id" : 273,
  "question" : "Your team is developing a mobile-based application.\nThe users who are going to use this application will first be authenticated using an external provider such as Facebook.\nThe application would then need to get temporary access credentials to work with AWS resources.\nWhich of the following actions would you ideally use for this purpose?\n",
  "answers" : [ {
    "id" : "9752286607e94ca3b07005aebb313b28",
    "option" : "AssumeRoleWithWebIdentity",
    "isCorrect" : "false"
  }, {
    "id" : "c0cfd00f5e454088854a71ce351c1d2c",
    "option" : "AssumeRoleWithSAML",
    "isCorrect" : "false"
  }, {
    "id" : "d589100971d8488a8c8e76a5b28e8fa1",
    "option" : "GetCallerIdentity",
    "isCorrect" : "false"
  }, {
    "id" : "7c3301e7c5144a50804b7d5d503bc437",
    "option" : "GetSessionToken.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nThe ideal AWS service that can be used to grant temporary access credentials to a mobile-based application whose users are authenticated using an external provider such as Facebook is AssumeRoleWithWebIdentity. Therefore, the correct answer is A.\nHere's an explanation of each of the options:\nA. AssumeRoleWithWebIdentity: This AWS service allows a user to assume a role within an AWS account, using web identity federation. Web identity federation enables users to access AWS resources after they authenticate with an identity provider such as Facebook, Google, or Amazon. Once authenticated, the identity provider issues a security token that can be exchanged for temporary security credentials. These credentials can be used to access AWS resources such as Amazon S3 or Amazon DynamoDB.\nB. AssumeRoleWithSAML: This AWS service allows a user to assume a role within an AWS account, using Security Assertion Markup Language (SAML) 2.0. SAML is an XML-based standard used for exchanging authentication and authorization data between parties, in particular, between an identity provider and a service provider. This service is typically used for enterprise applications where a user needs to authenticate using an enterprise identity provider.\nC. GetCallerIdentity: This AWS service returns the AWS account ID of the caller making the request. This service is used to determine the AWS account associated with an access key, an IAM role, or a federated user. This service does not grant temporary access credentials.\nD. GetSessionToken: This AWS service returns a set of temporary security credentials that can be used to access AWS resources. These credentials are typically used by AWS Identity and Access Management (IAM) users who require temporary access to perform specific tasks. This service requires a user's access key and secret access key to generate temporary security credentials.\nTherefore, from the above explanations, AssumeRoleWithWebIdentity is the best choice for this scenario since it allows temporary access credentials to be granted to users who have been authenticated using an external identity provider such as Facebook.\n\n"
}, {
  "id" : 274,
  "question" : "Your company currently has an S3 bucket hosted in an AWS Account.\nIt holds information that needs to be accessed by a partner account.\nWhich is the MOST secure way to allow the partner account to access the S3 bucket in your account? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ff17ee1d79b24cdc828d639b37964960",
    "option" : "Ensure an IAM role is created which can be assumed by the partner account.",
    "isCorrect" : "true"
  }, {
    "id" : "dc83385de884437e8b1eee9e1e7cadd7",
    "option" : "Ensure an IAM user is created which can be assumed by the partner account.",
    "isCorrect" : "false"
  }, {
    "id" : "72f4cbb06a624d4a8184ca9c0e1888dc",
    "option" : "Ensure the partner uses an external id when making the request.",
    "isCorrect" : "true"
  }, {
    "id" : "f2965d2388e441e0aa9a2ba747c4bb70",
    "option" : "Provide the ARN for the role to the partner account.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, C and D.\nThe below diagram from the AWS documentation showcases an example of this wherein an IAM role and external ID can access an AWS account resources.\nOption B is invalid because Roles are assumed and not IAM users.\nFor more information on creating roles for external ID's, please visit the following URL-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html\n\n\nTo allow a partner account to access an S3 bucket hosted in your AWS account in the most secure way, there are three options:\nCreate an IAM role that can be assumed by the partner account: This option allows you to create an IAM role that grants access to the S3 bucket and then allows the partner account to assume that role. This approach is preferred as it provides a higher level of security as compared to creating an IAM user with access keys. The partner account can assume the IAM role using the AWS Security Token Service (STS). Use an external ID: When creating the IAM role, you can also specify an external ID. This external ID acts as a shared secret between your account and the partner account, and is used to prevent unauthorized access. When the partner account requests access to assume the IAM role, they must include the external ID in their request. Provide the ARN for the role to the partner account: Once the IAM role is created, you can provide the Amazon Resource Name (ARN) for the role to the partner account. This allows the partner account to assume the role and access the S3 bucket using the AWS Management Console or SDKs.\nOption B, creating an IAM user with access keys, is not recommended because access keys can be easily shared or compromised, resulting in a potential security breach.\nIn summary, the most secure way to allow a partner account to access an S3 bucket hosted in your AWS account is to create an IAM role that can be assumed by the partner account, use an external ID for authentication, and provide the ARN for the role to the partner account.\n\n"
}, {
  "id" : 275,
  "question" : "Your team has configured an environment in Elastic beanstalk using the following configuration. Java 7 with tomcat 7 They now want to change the configuration to Java 8 with Tomcat 8.5\nHow can they achieve this in the easiest way possible?\n",
  "answers" : [ {
    "id" : "4aa53aef117e4ae49535f39f37c91b5f",
    "option" : "Change the configuration using the AWS console.",
    "isCorrect" : "true"
  }, {
    "id" : "495bae230313435196571b0eeccf2948",
    "option" : "Create a new application revision.",
    "isCorrect" : "false"
  }, {
    "id" : "86850b3582c74ae69f26e20b55fc35d9",
    "option" : "Use the Change configuration environment.",
    "isCorrect" : "false"
  }, {
    "id" : "a0863ca879864530968947005cc86854",
    "option" : "Migrate the environment to Opswork.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is mentioned in the AWS Documentation.\nPlease refer to page 523 on the below link.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/awseb-dg.pdf\nOptions B, C and D are incorrect because they need extra or unnecessary work and are not as easy as option A.For more information on using Elastic beanstalk configuration changes, please visit the following URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.platform.upgrade.html\n\n\nThe easiest way to change the configuration in Elastic Beanstalk from Java 7 with Tomcat 7 to Java 8 with Tomcat 8.5 is by using the \"Change configuration\" feature in Elastic Beanstalk.\nOption A, changing the configuration using the AWS console, is possible but may require additional steps to ensure the correct version of Java and Tomcat are used together.\nOption B, creating a new application revision, may also work but it would require deploying the new revision and possibly reconfiguring the environment manually.\nOption D, migrating the environment to OpsWorks, is not relevant to this task since it involves using a different service.\nTo use the \"Change configuration\" feature, follow these steps:\nLog in to the AWS Management Console and navigate to the Elastic Beanstalk console. Select the environment that needs to be updated. In the Environment Overview page, click the \"Configuration\" button. In the \"Software Configuration\" section, click the \"Edit\" button. In the \"Base configuration\" section, select \"Java\" version \"8\" and \"Tomcat\" version \"8.5\" from the drop-down menus. Click \"Apply\" to save the changes. Elastic Beanstalk will now create a new environment with the updated configuration. Once the new environment is ready, you can swap it with the old environment to complete the update. To swap the environments, go back to the Environment Overview page and click the \"Swap environment URLs\" button. Select the new environment from the drop-down menu and click \"Swap environments\". Elastic Beanstalk will swap the URLs between the two environments, making the updated environment the live one.\nBy using the \"Change configuration\" feature in Elastic Beanstalk, the team can update their environment to use Java 8 with Tomcat 8.5 with minimal disruption and manual configuration.\n\n"
}, {
  "id" : 276,
  "question" : "Your development team is planning on using AWS ElastiCache - Redis for their caching implementation.\nIt needs to be ensured that data is only filled in the cache when it is required.\nWhich of the following cache strategy can be used for this purpose?\n",
  "answers" : [ {
    "id" : "2871b5b47bea4135b808473505b068e1",
    "option" : "Lazy loading",
    "isCorrect" : "true"
  }, {
    "id" : "d2889c81981f4be1b53bd12d0398c1f0",
    "option" : "Write through",
    "isCorrect" : "false"
  }, {
    "id" : "98a74384dfa14b1bbd13dd9db54ca39b",
    "option" : "Adding a TTL",
    "isCorrect" : "false"
  }, {
    "id" : "0ae38cc2c42e4c08bc36508413cfa9d0",
    "option" : "Use Redis AOF.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nOption B is incorrect since this is used to add data or updates data in the cache whenever data is written to the database.\nOption C is incorrect since this is used to specify the number of seconds (Redis can specify seconds or milliseconds) until the key expires.\nOption D is incorrect since this is used in useful in recovery scenarios.\nThis is mentioned in the AWS Documentation.\nAdvantages of Lazy Loading.\nOnly requested data is cached.\nSince most data is never requested, lazy loading avoids filling up the cache with data that isn't requested.\nNode failures are not fatal.\nWhen a node fails and is replaced by a new, empty node, the application continues to function, though with increased latency.\nAs requests are made to the new node, each cache miss results in a query of the database and adding the data copy to the cache so that subsequent requests are retrieved from the cache.\nFor more information on the caching strategies, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html\n\nThe caching strategy that can be used to ensure that data is only filled in the cache when it is required is \"Lazy loading.\" Lazy loading is a technique where data is only loaded into the cache when it is requested by the application, and not preloaded into the cache. This technique helps to reduce the memory usage of the cache, as it only stores data that is actually used by the application.\nThe other cache strategies listed in the question are as follows:\nB. Write through: In this strategy, data is written to the cache and the underlying database at the same time. This ensures that data is always up-to-date in the cache, but can be less performant than lazy loading.\nC. Adding a TTL: A time-to-live (TTL) value can be set for cached data, which determines how long the data will remain in the cache before it is automatically removed. This strategy can be useful for controlling the size of the cache, but does not necessarily ensure that data is only loaded into the cache when it is required.\nD. Use Redis AOF: Redis AOF (append-only file) is a persistence mechanism that logs every write operation to a file. This allows Redis to recover data in the event of a crash, but does not specifically relate to cache loading strategies.\nIn summary, lazy loading is the caching strategy that can be used to ensure that data is only filled in the cache when it is required.\n\n"
}, {
  "id" : 277,
  "question" : "Your team has just deployed an API behind the AWS API gateway service.\nThey want to ensure minimum latency of requests to the API gateway service.\nWhich of the following feature can help fulfill this requirement?\n",
  "answers" : [ {
    "id" : "b584eb3c982a400c9dd9398151d09c2c",
    "option" : "Setup X-Ray tracing.",
    "isCorrect" : "false"
  }, {
    "id" : "e771632ff80c4fecace70006e712c9ae",
    "option" : "Use API caching.",
    "isCorrect" : "true"
  }, {
    "id" : "f58327ebb3e9485fb2ebe5fc755c5212",
    "option" : "Enable CORS.",
    "isCorrect" : "false"
  }, {
    "id" : "31f48f6870ac4937b1406a01b601befd",
    "option" : "Use Lambda Authorizers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is mentioned in the AWS Documentation.\nYou can enable API caching in Amazon API Gateway to cache your endpoint's responses.\nWith caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.\nWhen you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds.\nAPI Gateway then responds to the request by looking up the endpoint response from the cache instead of requesting your endpoint.\nThe default TTL value for API caching is 300 seconds.\nThe maximum TTL value is 3600 seconds.\nTTL=0 means caching is disabled.\nOption A is incorrect since this is used to help debug issues related to API request latency.\nOption C is incorrect since it allows API resources to receive requests from a domain other than the API's own domain.\nOption D is incorrect since this is used to control access to your API methods.\nFor more information on API gateway cache, please visit the following URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\n\nB. Use API caching.\nAPI caching can help reduce the latency of requests to the API gateway service by caching responses for a specified period of time. This means that if a client makes a request for the same resource within the caching period, the API gateway can return the cached response instead of making another request to the backend service. This can significantly reduce the response time and the load on the backend service.\nA. Setup X-Ray tracing.\nX-Ray tracing can help diagnose and troubleshoot issues with the API gateway service by providing end-to-end visibility into the requests and the services they interact with. However, setting up X-Ray tracing alone may not necessarily improve the latency of requests to the API gateway service.\nC. Enable CORS.\nEnabling CORS (Cross-Origin Resource Sharing) allows clients to make cross-origin requests to the API gateway service, but it does not directly impact the latency of requests.\nD. Use Lambda Authorizers.\nLambda authorizers can be used to authenticate and authorize requests to the API gateway service using AWS Lambda functions. However, using Lambda authorizers alone may not necessarily improve the latency of requests to the API gateway service.\nTherefore, the correct answer is B. Use API caching, as it directly impacts the latency of requests to the API gateway service.\n\n"
}, {
  "id" : 278,
  "question" : "You plan to use an instance in a Cluster Placement group to achieve a low latency network performance along with NVMe EBS.\nWhich of the following latest General-Purpose Instance would you choose?\n",
  "answers" : [ {
    "id" : "1c21a2d101c54f09a4acde1b2ee3df36",
    "option" : "Mac",
    "isCorrect" : "false"
  }, {
    "id" : "337f274fb5e941d596a5fb9292ec3bd4",
    "option" : "M5",
    "isCorrect" : "true"
  }, {
    "id" : "ee36d2f9e768461b95c0680a8d221846",
    "option" : "R5",
    "isCorrect" : "false"
  }, {
    "id" : "7e83d827e746450b95d9d1a843b440d8",
    "option" : "C5",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOptions A, C and D are incorrect as they are not General Purpose instance types.\nMac is the Apple Mac system and is suitable for applications that require the Xcode IDE.R5 is for memory optimized instances.\nC5 is for compute optimized instances.\nFor more information on EC2 instance types, refer to the following URL:\nhttps://aws.amazon.com/ec2/instance-types/\n\nTo achieve low latency network performance along with NVMe EBS, we need to select the latest General-Purpose Instance that supports these features.\nCluster placement group is a logical grouping of instances within a single Availability Zone in which instances are placed in such a way that they can communicate with each other with low latency and high throughput.\nNVMe EBS (Elastic Block Store) is a type of storage volume that provides low-latency access to data and high throughput for read and write operations.\nOut of the given options, the latest general-purpose instances are M5, R5, and C5.\nMac instances are not general-purpose instances but are designed specifically for macOS-based workloads. Therefore, Option A is not a valid option. M5 instances are the latest generation of Amazon EC2 General Purpose Instances and provide a balance of compute, memory, and networking resources. They offer high network performance with low-latency EBS storage volumes. Therefore, Option B is a valid option. R5 instances are optimized for memory-intensive workloads and provide high network performance with low-latency EBS storage volumes. However, they may not be the best fit for general-purpose workloads. Therefore, Option C is not the best choice. C5 instances are compute-optimized and provide high network performance with low-latency EBS storage volumes. However, they may not be the best fit for general-purpose workloads. Therefore, Option D is not the best choice.\nTherefore, the best option for achieving low latency network performance along with NVMe EBS for a general-purpose workload is M5 instance.\n\n"
}, {
  "id" : 279,
  "question" : "A company has hired you for its on-going development project.\nThe project entails streaming data onto Amazon Kinesis streams from various log sources.\nYou need to analyze real-time data using standard SQL.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "cd7121ba9b4241f59a656a6ec1e77494",
    "option" : "Amazon Kinesis Firehose",
    "isCorrect" : "false"
  }, {
    "id" : "a25c9a08d8d64153a131793bb420bc51",
    "option" : "Amazon Kinesis Data Analytics",
    "isCorrect" : "true"
  }, {
    "id" : "f581776aff224ff59e72854fa810f03d",
    "option" : "Amazon Athena",
    "isCorrect" : "false"
  }, {
    "id" : "685ea1c3256447eab3472c863c43474f",
    "option" : "Amazon EMR.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nOption A is incorrect since it delivers real-time streaming data to destinations such as Amazon Simple Storage Service.\nThis is mentioned in the AWS Documentation.\nWith Amazon Kinesis Data Analytics, you can process and analyze streaming data using standard SQL.\nThe service enables you to quickly author and run powerful SQL code against streaming sources to perform time-series analytics, feed real-time dashboards, and create real-time metrics.\nOption A is incorrect since it delivers real-time streaming data to destinations such as Amazon Simple Storage Service.\nOption C is incorrect since this is used to make it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL.\nOption D is incorrect since this is used as a fully managed service for Big Data.\nFor more information on Kinesis Data Analytics, please visit the following URL-\nhttps://docs.aws.amazon.com/kinesisanalytics/latest/dev/what-is.html\n\nThe correct answer to the question is B. Amazon Kinesis Data Analytics.\nAmazon Kinesis is a platform for real-time streaming data on AWS. It can be used to ingest and process large amounts of data in real-time from various sources such as web applications, mobile devices, IoT devices, and log files.\nAmazon Kinesis Data Analytics is a service that allows you to analyze streaming data with SQL queries. It provides an easy-to-use interface for querying streaming data using standard SQL statements. With Kinesis Data Analytics, you can easily analyze, process, and transform real-time data streams, and gain insights from the data.\nAmazon Kinesis Firehose is a service that allows you to load streaming data into data stores and analytics tools. It is designed to be a simple way to move data from Kinesis streams to other services such as Amazon S3, Redshift, and Elasticsearch. However, Firehose does not provide the ability to analyze data with SQL.\nAmazon Athena is a service that allows you to run ad-hoc SQL queries against data stored in Amazon S3. Athena is designed for interactive analysis of large datasets and provides a SQL-like interface for querying data. However, Athena is not specifically designed for real-time streaming data and may not be the best option for analyzing real-time data.\nAmazon EMR (Elastic MapReduce) is a managed service that allows you to run big data frameworks such as Hadoop, Spark, and Hive on AWS. EMR is designed for batch processing of large datasets and is not optimized for real-time streaming data analysis.\nIn summary, Amazon Kinesis Data Analytics is the best option for analyzing real-time data with SQL queries. It provides a simple and easy-to-use interface for querying streaming data and gaining insights from the data.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 281,
  "question" : "You are currently developing an application that consists of a web layer hosted on an EC2 Instance.\nThis is interacting with the database layer using the AWS RDS Instance.\nYou are noticing that the same query reads are causing performance issues.\nWhich of the following can be used to alleviate this issue?\n",
  "answers" : [ {
    "id" : "a1de36c290214e9ba61a2f434426f7c0",
    "option" : "Place an ELB in front of the web layer.",
    "isCorrect" : "false"
  }, {
    "id" : "1010f6e50e74406bba2046e154c9171d",
    "option" : "Place an ELB in front of the database layer.",
    "isCorrect" : "false"
  }, {
    "id" : "d2d7acf8d8724a1fa311c12223c8347c",
    "option" : "Place an ElastiCache in front of the database layer.",
    "isCorrect" : "true"
  }, {
    "id" : "288f47e2750e40c29113346838bff898",
    "option" : "Place an ElastiCache in front of the web layer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is mentioned in the AWS Documentation.\nAmazon ElastiCache offers fully managed Redis and Memcached.\nSeamlessly deploy, run, and scale popular open source compatible in-memory data stores.\nBuild data-intensive apps or improve the performance of your existing apps by retrieving data from high throughput and low latency in-memory data stores.\nAmazon ElastiCache is a popular choice for Gaming, Ad-Tech, Financial Services, Healthcare, and IoT apps.\nOptions A and B are incorrect since the ELB will not resolve the issue.\nIt can only direct the traffic and not reduce the latency issues.\nOption D is incorrect since this should ideally be placed in front of a data store.\nFor more information on ElastiCache, please visit the following URL-\nhttps://aws.amazon.com/elasticache/\n\nThe performance issue of the application is caused by the same read queries to the database. To alleviate this issue, we need to reduce the load on the database and increase the speed of read queries. Here are the available options:\nA. Place an ELB in front of the web layer: An Elastic Load Balancer (ELB) can distribute incoming traffic across multiple instances of the web layer. While this can improve the scalability and fault tolerance of the web layer, it does not address the performance issue with the database.\nB. Place an ELB in front of the database layer: Placing an ELB in front of the database layer can help distribute traffic across multiple RDS instances. However, this can only help if the performance issue is related to high traffic, and not caused by the query patterns. Moreover, read queries are typically handled by the primary RDS instance, so distributing read traffic across multiple instances may not improve performance.\nC. Place an ElastiCache in front of the database layer: Amazon ElastiCache is a managed in-memory caching service that can be used to improve the performance of read queries. By caching frequently accessed data in memory, ElastiCache can reduce the number of read queries that hit the database, thus reducing the load on the RDS instance and improving query response times.\nD. Place an ElastiCache in front of the web layer: Placing an ElastiCache in front of the web layer can improve the speed of read queries by caching frequently accessed data in memory. However, this does not address the root cause of the performance issue, which is the load on the RDS instance.\nTherefore, the best option to alleviate the performance issue caused by the same read queries is to place an ElastiCache in front of the database layer (option C). This can significantly reduce the load on the RDS instance and improve query response times.\n\n"
}, {
  "id" : 282,
  "question" : "You have developed an application that is putting custom metrics into Cloudwatch.\nYou need to generate alarms on a 10 second interval based on the published metrics.\nWhich of the following needs to be done to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "c558d63f8a734105889885b2a1a59a93",
    "option" : "Enable basic monitoring.",
    "isCorrect" : "false"
  }, {
    "id" : "613b2465cc7344e6a968b8788e0d00df",
    "option" : "Enable detailed monitoring.",
    "isCorrect" : "false"
  }, {
    "id" : "54a0edb39b654e128b672fb114b5affb",
    "option" : "Create standard resolution metrics.",
    "isCorrect" : "false"
  }, {
    "id" : "b27e4d76be49490fb8066391d6e1dea2",
    "option" : "Create high-resolution metrics.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThis is mentioned in the AWS Documentation.\nUsing the existing PutMetricData API, you can now publish Custom Metrics down to 1-second resolution.\nThis gives you more immediate visibility and greater granularity into the state and performance of your custom applications, such as observing short-lived spikes and functions.\nBesides, you can also alert sooner with High-Resolution Alarms, as frequently as 10-second periods.\nHigh-Resolution Alarms allow you to react and take actions faster and support the same actions available today with standard 1-minute alarms.\nYou can add these high-resolution metrics and alarms widgets to your Dashboard, giving you easy observability of critical components.\nAdditionally, if you use collectd to gather your metrics, you can publish these metrics to CloudWatch using our updated collectd plugin supporting high-resolution periods down to 1-second.\nOptions A and B are incorrect since they are more pertinent to existing services available in AWS.\nOption C is incorrect.\nSince for such a high degree of resolution, you need to use high-resolution metrics.\nFor more information on high-resolution metrics, please visit the following URL-\nhttps://aws.amazon.com/about-aws/whats-new/2017/07/amazon-cloudwatch-introduces-high-resolution-custom-metrics-and-alarms/\n\nTo fulfill the requirement of generating alarms on a 10-second interval based on the published metrics in CloudWatch, the following steps need to be taken:\nCreate Custom Metrics: The first step is to create custom metrics for the application. These custom metrics should capture the relevant data points for the application that need to be monitored. This can be done by using CloudWatch APIs or SDKs to publish metrics to CloudWatch. Choose the Right Resolution: The next step is to choose the right resolution for the custom metrics. CloudWatch offers two types of resolutions for metrics: standard resolution and high-resolution. The standard resolution metrics have a resolution of one minute, while high-resolution metrics have a resolution of one second. Choose the Right Monitoring Level: The monitoring level determines how often CloudWatch collects data and generates alarms. CloudWatch offers two monitoring levels: basic and detailed. The basic monitoring level collects data and generates alarms every five minutes, while the detailed monitoring level collects data and generates alarms every minute. Create Alarms: After creating custom metrics, choosing the right resolution, and monitoring level, the next step is to create alarms based on the custom metrics. These alarms should be configured to trigger on a 10-second interval.\nBased on the above steps, the correct answer to the question is D. Create high-resolution metrics. By creating high-resolution metrics, the application can capture data points at a one-second interval, which can then be used to generate alarms on a 10-second interval. Answers A and B are incorrect because enabling basic or detailed monitoring does not allow for the required 10-second interval. Answer C is incorrect because standard resolution metrics have a resolution of one minute, which is not frequent enough to generate alarms on a 10-second interval.\n\n"
}, {
  "id" : 283,
  "question" : "As a developer, you are developing an application that will carry out the task of uploading objects to the Simple Storage service.\nThe size of the objects varies from 300MB - 500 MB of size.\nWhich of the following should you do to minimize the amount of time that is used to upload an item?\n",
  "answers" : [ {
    "id" : "9157494d0f8645d09f5718d983fde203",
    "option" : "Use the BatchWriteItem command.",
    "isCorrect" : "false"
  }, {
    "id" : "3eb8709291374896bdb42388f8a75037",
    "option" : "Use Multipart Upload.",
    "isCorrect" : "true"
  }, {
    "id" : "16da1664412b4c1e899c757897605cd1",
    "option" : "Use the MultiPutItem command.",
    "isCorrect" : "false"
  }, {
    "id" : "ccd760fbcf464f8bb44022778ddda320",
    "option" : "Use the BatchPutItem command.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is mentioned in the AWS Documentation.\nMultipart upload allows you to upload a single object as a set of parts.\nEach part is a contiguous portion of the object's data.\nYou can upload these object parts independently and in any order.\nIf transmission of any part fails, you can retransmit that part without affecting other parts.\nAfter all parts of your object are uploaded, Amazon S3 assembles these parts and creates the object.\nIn general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading the object in a single operation.\nOption A is incorrect because the\nBatchWriteItem.\noperation is used for DynamoDB instead of S3.\nOption C is incorrect as there is no such command present in S3.\nOption D is incorrect because there is no such command in S3\nThe APIs of BatchGetItem and BatchWriteItem belong to the DynamoDB service.\nFor more information on high-resolution metrics, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html\n\nTo minimize the amount of time that is used to upload large objects, the best approach is to use Multipart Upload.\nMultipart Upload allows you to upload large objects in parts, with each part being uploaded independently. This approach has several benefits:\nImproved upload speed: With Multipart Upload, each part can be uploaded in parallel, which can result in faster upload speeds. Fault tolerance: If a part fails to upload, you can simply retry that part without having to upload the entire object again. Resumability: If the upload process is interrupted for any reason, you can resume uploading from the point where it left off, rather than starting over from the beginning.\nBatchWriteItem, MultiPutItem, and BatchPutItem are all commands used for uploading multiple items to Amazon DynamoDB at once. They are not relevant to uploading objects to Amazon S3.\nIn conclusion, to minimize the amount of time used to upload objects ranging from 300MB - 500 MB in size, you should use Multipart Upload.\n\n"
}, {
  "id" : 284,
  "question" : "As a developer, you are writing an application that will be hosted on an EC2 Instance.\nThis application will interact with a queue defined using the Simple Queue service.\nThe messages will appear in the queue during a 20-60 second time duration.\nWhich of the following strategy should be used to query the queue for messages effectively?\n",
  "answers" : [ {
    "id" : "eb6108afbc454c768e857a19990c166e",
    "option" : "Use dead letter queues.",
    "isCorrect" : "false"
  }, {
    "id" : "462b4f6c779342278f05c923cfa0149d",
    "option" : "Use FIFO queues.",
    "isCorrect" : "false"
  }, {
    "id" : "24e78efc95c2485e9a1ebf3481a1f160",
    "option" : "Use long polling.",
    "isCorrect" : "true"
  }, {
    "id" : "c8368a2735b441ae8f4ba93aadefec05",
    "option" : "Use short polling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is mentioned in the AWS Documentation.\nLong polling offers the following benefits.\nEliminate empty responses by allowing Amazon SQS to wait until a message is available in a queue before sending a response.\nUnless the connection times out, the response to the ReceiveMessage request contains at least one of the available messages, up to the maximum number of messages specified in the ReceiveMessage action.\nEliminate false empty responses by querying all-rather than a subset of-Amazon SQS servers.\nOption A is invalid since this is used for storing undelivered messages.\nOption B is invalid since this is used for First In First Out queues.\nOption D is invalid since this is used when messages are immediately available in the queue.\nFor more information on long polling, please visit the following URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html\n\nTo effectively query the Simple Queue Service for messages, the best strategy to use in this scenario would be long polling (Option C).\nLong polling is a technique that enables the client to send a request to the server, and the server will hold the request open until a message is available, or the connection times out. This method reduces the number of requests to the server and helps to minimize the cost of data transfer.\nIn the case of an EC2 instance interacting with an SQS queue, long polling is particularly useful when the duration for which messages appear in the queue is not fixed, as in this scenario, where the messages appear in the queue during a 20-60 second time duration. Long polling helps to ensure that messages are retrieved as soon as they are available, without the need for frequent polling requests.\nDead letter queues (Option A) are used to handle messages that cannot be processed successfully. If a message cannot be processed successfully after a certain number of retries, it is sent to a dead letter queue for further analysis. This feature is not directly related to the efficient retrieval of messages from the queue.\nFIFO queues (Option B) are used when message order is important, and each message must be processed exactly once. While FIFO queues can be used to retrieve messages effectively, they do not provide any specific advantage for the scenario described in the question.\nShort polling (Option D) is a technique where the client sends a request to the server and receives an immediate response, whether or not there are messages in the queue. Short polling can be useful when there is a high rate of message traffic, and messages are expected to be available frequently. However, in this scenario, where messages appear in the queue during a 20-60 second time duration, short polling would result in frequent requests being made, which could result in increased cost and slower message retrieval.\n\n"
}, {
  "id" : 285,
  "question" : "You are a business Intelligence developer for a company.\nYour company has a large data warehouse that needs to be ported to AWS.\nWhich of the following services would you use to host the data warehouse?\n",
  "answers" : [ {
    "id" : "095f8a4f7455463bab9ffef599750cc3",
    "option" : "AWS DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "211073d048c44e1c98660d4da18e3577",
    "option" : "AWS Redshift",
    "isCorrect" : "true"
  }, {
    "id" : "22c5c98073f748e5b39e3e913460ddaa",
    "option" : "AWS Simple Storage Service",
    "isCorrect" : "false"
  }, {
    "id" : "a322cffb71b8468e9b44bfcbf6dd6c5c",
    "option" : "AWS Aurora.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is mentioned in the AWS Documentation.\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.\nYou can start with just a few hundred gigabytes of data and scale to a petabyte or more.\nThis enables you to use your data to acquire new insights for your business and customers.\nOption A is invalid since this is a fully managed NoSQL database.\nOption C is invalid since this is an object data store.\nOption D is invalid since this is a MySQL or PostgreSQL data store.\nFor more information on AWS Redshift, please visit the following URL-\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html\n\nOut of the given options, the service that is specifically designed to host large data warehouses is AWS Redshift (Option B).\nAWS Redshift is a fully managed petabyte-scale data warehouse service in the cloud that offers high scalability and performance. It is optimized for analytics workloads and supports SQL-based queries. With Redshift, users can easily store and analyze large amounts of structured and semi-structured data using existing business intelligence tools.\nAWS DynamoDB (Option A) is a NoSQL database service that is suitable for handling large amounts of unstructured data with a high degree of scalability, but it is not designed specifically for data warehousing.\nAWS Simple Storage Service (Option C) is an object storage service that provides durable and scalable object storage for data backups, archive data, and other storage needs, but it is not a data warehouse service.\nAWS Aurora (Option D) is a relational database service that is designed to be highly scalable and available. It is suitable for running transactional workloads and is not specifically optimized for analytical workloads like a data warehouse.\nTherefore, AWS Redshift is the best option to host the data warehouse in the AWS cloud.\n\n"
}, {
  "id" : 286,
  "question" : "You work as a developer for a company.\nYou have been given a requirement for the development of an application.\nAll the components that make up the application need to be developed using Docker containers and hosted on AWS.\nWhich of the following services could you use to host your application? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "43f4b725e18b4dc28c1fd0866dc30c55",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "4834fb580337436da6cb296302e6f52e",
    "option" : "AWS API gateway",
    "isCorrect" : "false"
  }, {
    "id" : "1ab587adfa6945e19844f36accf6add9",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "true"
  }, {
    "id" : "5ea32e19776449cab2ca6942d02a80f2",
    "option" : "AWS ECS.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nThis is mentioned in the AWS Documentation.\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers.\nIt allows you to run and scale containerized applications on AWS easily.\nAmazon ECS eliminates the need for you to install and operate your own container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines.\nElastic Beanstalk supports the deployment of web applications from Docker containers.\nWith Docker containers, you can define your own runtime environment.\nYou can choose your own platform, programming language, and any application dependencies (such as package managers or tools), that aren't supported by other platforms.\nDocker containers are self-contained and include all the configuration information and software your web application requires to run.\nOption A is incorrect because CloudFormation helps to manage AWS resources but it is not a service to host Docker containers.\nOption B is incorrect because AWS API gateway is not used for hosting Docker-based containers.\nFor more information on the Elastic container service and Elastic beanstalk for Docker, please visit the following URLs-\nhttps://aws.amazon.com/ecs/\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html\n\nThe two services that can be used to host Docker containers on AWS are AWS Elastic Beanstalk and AWS ECS.\nAWS Elastic Beanstalk is a fully managed service that makes it easy to deploy and run applications in multiple languages, including Java, .NET, PHP, Node.js, Python, Ruby, and Go, on AWS. With Elastic Beanstalk, you can simply upload your Docker container and the service will take care of the rest. Elastic Beanstalk provides automatic scaling, load balancing, and application health monitoring, making it a great choice for developers who want to focus on writing code rather than managing infrastructure.\nAWS ECS (Elastic Container Service) is another fully managed service that supports Docker containers. It allows you to easily run and scale containerized applications on AWS. ECS provides features such as load balancing, auto scaling, and service discovery, making it a great choice for running microservices-based applications. Additionally, ECS integrates with other AWS services like Elastic Load Balancing, EC2 Container Registry, and CloudWatch, providing a seamless and comprehensive solution for container management.\nAWS CloudFormation is an infrastructure as code service that can be used to automate the deployment and management of AWS resources. While CloudFormation can be used to create and manage Docker containers on AWS, it is not a container management service like Elastic Beanstalk or ECS.\nAWS API Gateway is a fully managed service that makes it easy to create, deploy, and manage APIs at any scale. While it can be used to manage APIs that are part of a larger application, it is not a container management service like Elastic Beanstalk or ECS.\nTherefore, the two services that can be used to host Docker containers on AWS are AWS Elastic Beanstalk and AWS ECS.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 288,
  "question" : "You are a developer for a company.\nYou have to develop an application that would transfer the logs from several EC2 Instances to an S3 bucket.\nWhich of the following would you use for this purpose?\n",
  "answers" : [ {
    "id" : "cb40f7aa94b642bf801f751e97aab6f8",
    "option" : "AWS Database Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "23e7324a3c394c1884060bc21221863e",
    "option" : "AWS Athena",
    "isCorrect" : "false"
  }, {
    "id" : "335ea4f709d24fcc8e9bfb4ce5e576ee",
    "option" : "AWS Data Pipeline",
    "isCorrect" : "true"
  }, {
    "id" : "7d0bf34d4a294465ae5067f2558ebf2b",
    "option" : "AWS EMR.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is mentioned in the AWS Documentation.\nAWS Data Pipeline is a web service that you can use to automate data movement and transformation.\nWith AWS Data Pipeline, you can define data-driven workflows so that tasks can depend on the successful completion of previous tasks.\nYou define the parameters of your data transformations, and AWS Data Pipeline enforces the logic that you've set up.\nOption A is incorrect since this is used specifically for migrating databases.\nOption B is incorrect since this is used for performing SQL queries in data stored on S3.\nOption D is incorrect since this is used for Big data applications.\nFor more information on AWS Pipeline, please visit the following URL-\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html\n\n\nThe correct answer for this question is C. AWS Data Pipeline.\nAWS Data Pipeline is a fully-managed service that enables you to move data between different AWS data stores and compute services. It allows you to create complex data processing workloads that can transfer data from various sources to your destination location such as Amazon S3, Amazon RDS, or Amazon Redshift.\nIn the given scenario, the application needs to transfer logs from several EC2 instances to an S3 bucket, and AWS Data Pipeline can perform this task efficiently. You can use AWS Data Pipeline to create a pipeline that would move data from EC2 instances to an S3 bucket using pre-built data connectors for Amazon EC2 and Amazon S3.\nAWS Database Migration Service (A) is used to migrate databases from on-premises data centers to AWS or from one type of database to another within AWS. Therefore, it's not the correct choice for transferring logs from EC2 instances to an S3 bucket.\nAWS Athena (B) is a serverless query service that enables you to analyze data stored in Amazon S3 using SQL. It's not suitable for transferring data from EC2 instances to S3 buckets.\nAWS EMR (D) is a managed big data processing service that helps you to process vast amounts of data using open-source frameworks such as Apache Hadoop, Apache Spark, and Apache Hive. It's not designed to transfer logs from EC2 instances to an S3 bucket.\n\n"
}, {
  "id" : 289,
  "question" : "Which of the following is true regarding the default Sampling rule configured using AWS X-Ray Console?\n",
  "answers" : [ {
    "id" : "3fadaef02497401182a0ce9230adeb1f",
    "option" : "Two requests per second &amp; ten percent of any additional request per host.",
    "isCorrect" : "false"
  }, {
    "id" : "0193df46db69488ab3e7c8555de3b6f2",
    "option" : "One request per second &amp; five percent of any additional requests per host.",
    "isCorrect" : "true"
  }, {
    "id" : "7d75fdb36d3b4bcab6dad6215dec9bfc",
    "option" : "Two requests per second &amp; five percent of any additional requests per host.",
    "isCorrect" : "false"
  }, {
    "id" : "e57eb142adcb4cd4a9ced132aef07e58",
    "option" : "One request per second &amp; ten percent of any additional requests per host.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer -B.\nWith Default Sampling Rule, X-Ray records one request per second &amp; five percent of any additional requests per host.\nOption A is incorrect as the Default Sampling rule records One request per second &amp; five percent of any additional request per host, &amp; not Two requests per second &amp; ten percent of additional requests.\nOption C is incorrect as the Default Sampling rule records one request per second &amp; not two requests per second.\nOption D is incorrect as the Default Sampling rule records five percent of any additional request &amp; not five percent of additional requests.\nFor more information on Sampling Rules for AWS X-Ray, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-console-sampling.html\n\nAWS X-Ray is a service that enables tracing of distributed applications. It helps in analyzing and debugging production, distributed applications, such as those built using a microservices architecture. Sampling is a method used by X-Ray to reduce the amount of data that needs to be processed and stored, to ensure cost efficiency.\nBy default, X-Ray applies a Sampling Rule with a sampling rate of 2 requests per second and 10% of any additional request per host. This means that X-Ray traces 2 requests per second from each host, and if there are any additional requests beyond this, it samples 10% of them. For example, if there are 12 requests from a particular host, X-Ray traces 2 requests, and samples 10% of the remaining 10 requests, which is 1 request. In total, X-Ray traces 3 requests (2 requests + 1 sampled request).\nTherefore, the correct answer is A: Two requests per second & ten percent of any additional request per host.\n\n"
}, {
  "id" : 290,
  "question" : "You have configured a Sampling Rule in AWS X-Ray, with a reservoir size as 60 &amp; a fixed rate at 20%\nThere are 200 requests per second matching the rule defined.\nHow many requests will be sampled per second?\n",
  "answers" : [ {
    "id" : "eef4bdc656d84904a6376d8e7de08d45",
    "option" : "88 requests per second",
    "isCorrect" : "true"
  }, {
    "id" : "335c6abe7c5f413e9139dc592991311e",
    "option" : "60 requests per second.",
    "isCorrect" : "false"
  }, {
    "id" : "e90f8ae0cb114f89bdaf7b4500d33d0b",
    "option" : "40 requests per second.",
    "isCorrect" : "false"
  }, {
    "id" : "50e05503949b42c8a11a62cf4ef574da",
    "option" : "120 requests per second.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nLet us suppose that we specify a value for Reservoir Rate as 60 and Fixed Rate as 20\nNow, if your application receives 200 requests in a second, then the total number of requests that would be traced or sampled will be: -\n\"Reservoir Rate\" + Fixed Rate % [(Total Requests - Reservoir Rate)]\n- 60 + (200-60) * 20%\n- 60 + (140) * 20%\n- 60 + 28\n- 88\nOption B, C, &amp; D are incorrect as these values are not matching the reservoir size &amp; fixed-rate sampling rate.\nFor more information on Configuring Sampling Rules in the AWS X-Ray Console, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-sampling.html\n\nAWS X-Ray is a service that enables developers to analyze and debug production, distributed applications, such as microservices or serverless applications. It helps to identify issues and opportunities for optimization by providing trace information of requests as they travel through your application.\nWhen you configure a sampling rule in AWS X-Ray, you can control the percentage of requests that are traced for analysis. Sampling helps to reduce the volume of data that needs to be processed and stored while still providing a representative sample of your application's traffic.\nIn this question, the sampling rule is configured with a reservoir size of 60 and a fixed rate of 20%. The reservoir size refers to the number of traces that can be stored at a given time. The fixed rate refers to the percentage of requests that will be traced for analysis.\nThe question states that there are 200 requests per second matching the rule defined. To calculate how many requests will be sampled per second, we need to use the following formula:\nsampled requests per second = (fixed rate * requests per second) / reservoir size\nSubstituting the values given in the question, we get:\nsampled requests per second = (20% * 200) / 60 sampled requests per second = (0.2 * 200) / 60 sampled requests per second = 40 / 60 sampled requests per second = 0.67\nTherefore, the answer is C. 40 requests per second.\n\n"
}, {
  "id" : 291,
  "question" : "You are a developer for a company that is planning on using the AWS RDS service.\nYour Database administrator spins up a new MySQL RDS Instance in AWS.\nYou now need to connect to that instance.\nHow can you achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "a129a26b284343a1ae66bb65174ed2d0",
    "option" : "Use the DescribeDBInstances API and get the endpoint for the database instance.",
    "isCorrect" : "true"
  }, {
    "id" : "fef7ee5ad6fd429e8f3003d39470b2e6",
    "option" : "Use the DescribeDBInstances API and get the IP address for the database instance.",
    "isCorrect" : "false"
  }, {
    "id" : "a548846a4cc1478c926a50772d2be42a",
    "option" : "Request an endpoint for the instance from the Database Administrator.",
    "isCorrect" : "true"
  }, {
    "id" : "ba894a46569f4d50ac67eabfe82bc636",
    "option" : "Request the IP address of the instance from the Database Administrator.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - A and C.\nThe AWS Documentation mentions the following.\nBefore you can connect to a DB instance running the MySQL database engine, you must create a DB instance.\nFor information, see Creating a DB Instance Running the MySQL Database Engine.\nOnce Amazon RDS provisions your DB instance, you can use any standard MySQL client application or utility to connect to the instance.\nIn the connection string, you specify the DNS address from the DB instance endpoint as the host parameter and specify the port number from the DB instance endpoint as the port parameter.\nYou can use the AWS Management Console, the AWS CLI describe-db-instances command, or the Amazon RDS API DescribeDBInstances action to list the details of an Amazon RDS DB instance, including its endpoint.\nOptions B and D are incorrect since even the endpoints resolve to IP addresses and the IP addresses can be used to connect to the database, this is not a recommended option because the underlying IP address could change during failover.\nFor more information on connecting to a database endpoint, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToInstance.html\n\nTo connect to a new MySQL RDS Instance in AWS, you can use the following two methods:\nUse the DescribeDBInstances API and get the endpoint for the database instance: AWS provides an API called DescribeDBInstances that provides information about all the RDS instances associated with your account. You can use this API to get the endpoint for the newly created RDS instance. The endpoint is a URL that you can use to connect to the RDS instance from your application.\nTo get the endpoint for the RDS instance, you can use the following AWS CLI command:\nsql Copy code aws rds describe-db-instances --db-instance-identifier <your-db-instance-id>\nReplace <your-db-instance-id> with the ID of the newly created RDS instance.\nRequest an endpoint for the instance from the Database Administrator: The Database Administrator who created the RDS instance can also provide you with the endpoint URL for the instance. You can use this URL to connect to the RDS instance from your application.\nAlternatively, you can also request the IP address of the instance from the Database Administrator. However, it is generally recommended to use the endpoint URL instead of the IP address, as the IP address may change over time, while the endpoint URL remains the same.\nIn summary, to connect to a newly created MySQL RDS Instance in AWS, you can either use the DescribeDBInstances API to get the endpoint URL or request the endpoint URL from the Database Administrator.\n\n"
}, {
  "id" : 292,
  "question" : "You are a developer for a company that has been given the responsibility to debug performance issues for an existing application.\nThe application connects to a MySQL RDS Instance in AWS.\nThere is suspicion that there are performance issues in the underlying queries.\nWhich of the following can help diagnose these issues?\n",
  "answers" : [ {
    "id" : "1e14ef7f4204493e97932ae77ce5ea3e",
    "option" : "The Cloudtrail logs for the region.",
    "isCorrect" : "false"
  }, {
    "id" : "e5ac8f59bf1a4f4689628d9da3224937",
    "option" : "Get the slow query logs for the RDS service.",
    "isCorrect" : "true"
  }, {
    "id" : "3392c1610f0e41eb8072b0053a890e42",
    "option" : "Use the AWS Config service to diagnose the problem areas.",
    "isCorrect" : "false"
  }, {
    "id" : "bb9c27e3d0af4445856bc108cda657dc",
    "option" : "Use the AWS Inspector service to diagnose the problem areas.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS RDS Service contains several logs, such as the ones given below.\nError log - contains diagnostic messages generated by the database engine, along with startup and shutdown times.\nGeneral query log - contains a record of all SQL statements received from clients and client connect and disconnect times.\nSlow query log - contains a record of SQL statements that took longer to execute than a set amount of time and that examined more than a defined number of rows.\nBoth thresholds are configurable.\nOption A is incorrect because this is used to monitor API activity.\nOption C is incorrect because this is used as a configuration service.\nOption D is incorrect because this is used to inspect EC2 Instances for vulnerabilities.\nFor more information on monitoring Amazon RDS, please refer to the below URL-\nhttps://aws.amazon.com/blogs/database/monitor-amazon-rds-for-mysql-and-mariadb-logs-with-amazon-cloudwatch/\n\nThe correct answer is B: Get the slow query logs for the RDS service.\nExplanation:\nWhen debugging performance issues in an application that connects to a MySQL RDS Instance in AWS, slow query logs can help diagnose the underlying queries that are causing the performance issues.\nSlow query logs contain information about the queries that took longer than a certain threshold to execute, along with the execution time, the number of rows examined, and other relevant data. By analyzing these logs, developers can identify which queries are causing the performance issues and optimize them accordingly.\nOption A, using the CloudTrail logs for the region, is not relevant for diagnosing performance issues in an RDS Instance.\nOption C, using the AWS Config service to diagnose the problem areas, is not relevant for diagnosing performance issues in an RDS Instance either. AWS Config is a service that helps you assess, audit, and evaluate the configurations of your AWS resources. It can be used to identify resources that are non-compliant with your policies, but it does not provide performance insights for RDS instances.\nOption D, using the AWS Inspector service to diagnose the problem areas, is not relevant for diagnosing performance issues in an RDS Instance either. AWS Inspector is a security assessment service that helps you identify security issues in your application stack. While it can help identify security-related performance issues, it is not designed to diagnose performance issues in an RDS Instance.\nIn summary, the correct answer is B, using the slow query logs for the RDS service, to diagnose the underlying queries causing performance issues.\n\n"
}, {
  "id" : 293,
  "question" : "Your team has an application deployed using the Elastic Beanstalk service.\nA Web environment has been configured for the production environment.\nThere is now a requirement to perform a Blue Green deployment for a new version of the application.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "ffd567c5d8dd4ab5b882ed4c9dc9491e",
    "option" : "Create a new application and swap the application environments.",
    "isCorrect" : "false"
  }, {
    "id" : "0f1de76d1ce546ca8e6afac4ec6cbe0e",
    "option" : "Create a new application version and upload the new application version.",
    "isCorrect" : "false"
  }, {
    "id" : "738fb049448d4239b08de7521438d378",
    "option" : "Create a new environment in the application with the updated application version and perform a swap.",
    "isCorrect" : "true"
  }, {
    "id" : "d6fd9e12add645f99f064eb03e4934ab",
    "option" : "Create a new environment in the application and Load the configuration of an existing environment.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThis is mentioned in the AWS Documentation.\nSince this is clearly mentioned in the AWS Documentation, all other options are invalid.\nFor more information on Blue Green deployments in Elastic Beanstalk, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html\n\n\nBlue Green deployment is a technique for releasing new versions of an application with zero downtime. In this approach, a new version of the application is deployed to a new environment (Green environment) and traffic is gradually shifted from the old environment (Blue environment) to the new one. This ensures that users can continue to use the application during the deployment process.\nTo achieve Blue Green deployment for an application deployed on Elastic Beanstalk, the following steps can be taken:\nCreate a new application version: The first step is to create a new version of the application that you want to deploy. This can be done by uploading the updated code to Elastic Beanstalk or by using a deployment tool such as AWS CodeDeploy. When creating a new version, make sure to include a descriptive label that identifies the version number and any changes that were made. Create a new environment: Once a new version of the application has been created, create a new environment in Elastic Beanstalk for the new version. This can be done by clicking on the \"Create Environment\" button in the Elastic Beanstalk console and selecting the appropriate environment type (e.g. Web Server). Test the new environment: Before you start directing traffic to the new environment, it is important to test it to ensure that it is working correctly. This can be done by accessing the environment's URL and testing the application's functionality. Route traffic to the new environment: Once you are satisfied that the new environment is working correctly, you can start routing traffic to it. This can be done by using Elastic Beanstalk's built-in swap environment feature, which allows you to swap the URLs of two environments. To do this, select the Blue environment and click on the \"Swap Environment URLs\" button in the Elastic Beanstalk console. This will swap the URLs of the Blue and Green environments, directing traffic to the new environment. Monitor the deployment: Once traffic has been routed to the new environment, monitor the deployment to ensure that it is running smoothly. This can be done by using Elastic Beanstalk's monitoring tools to track metrics such as CPU usage, network traffic, and error rates. Rollback if necessary: If any issues are discovered during the deployment process, it may be necessary to rollback to the previous version of the application. This can be done by using Elastic Beanstalk's rollback feature, which allows you to easily revert to a previous environment configuration.\nIn summary, to achieve Blue Green deployment for an application deployed on Elastic Beanstalk, you need to create a new version of the application, create a new environment for the new version, test the new environment, route traffic to the new environment using Elastic Beanstalk's built-in swap environment feature, monitor the deployment, and rollback if necessary.\n\n"
}, {
  "id" : 294,
  "question" : "You are developing an application that consists of the following architecture. · A set of EC2 Instances hosting a web layer · A database hosting a MySQL instance You need to add a layer that can be used to ensure that the most frequently accessed data from the database is fetched in a more fast and efficient manner from the database.\nWhich of the following can be used to accomplish this requirement?\n",
  "answers" : [ {
    "id" : "7d94114e30cc46feb31cb2b606b98a5c",
    "option" : "An SQS queue to store the frequently accessed data",
    "isCorrect" : "false"
  }, {
    "id" : "39ec5774dfe544448f2baaf6c185ae59",
    "option" : "An SNS topic to store the frequently accessed data",
    "isCorrect" : "false"
  }, {
    "id" : "6226bf4e4523495285692a2f6ecd3697",
    "option" : "A Cloudfront distribution to store the frequently accessed data",
    "isCorrect" : "false"
  }, {
    "id" : "ef03ada249384ba3a7a3df3ac4e55682",
    "option" : "A Elasticache instance to store the frequently accessed data.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAmazon ElastiCache offers fully managed Redis and Memcached.\nSeamlessly deploy, run, and scale popular open source compatible in-memory data stores.\nBuild data-intensive apps or improve your existing apps' performance by retrieving data from high throughput and low latency in-memory data stores.\nAmazon ElastiCache is a popular choice for Gaming, Ad-Tech, Financial Services, Healthcare, and IoT apps.\nOption A is incorrect because this is a messaging service.\nOption B is incorrect because this is a notification service.\nOption C is incorrect because this is a web distribution service.\nFor more information on Amazon ElastiCache, please refer to the below URL-\nhttps://aws.amazon.com/elasticache/\n\nOut of the given options, the most appropriate solution to the problem is to use Elasticache. Elasticache is a web service offered by AWS that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. It is designed to improve the performance of web applications by allowing frequently accessed data to be stored in a cache, which can be accessed more quickly than retrieving the data from the database.\nOption A (SQS queue) is not a suitable solution for this problem. SQS is a distributed message queue service that enables decoupling and scaling microservices, distributed systems, and serverless applications. However, it does not offer any in-memory caching capabilities and does not improve the performance of frequently accessed data.\nOption B (SNS topic) is also not a suitable solution for this problem. SNS is a managed pub/sub messaging service that enables messaging and fan-out scenarios for application components. It is not designed for caching frequently accessed data and does not offer any in-memory caching capabilities.\nOption C (Cloudfront distribution) is also not a suitable solution for this problem. Cloudfront is a content delivery network (CDN) offered by AWS that enables faster delivery of static and dynamic web content, including images, videos, and applications. While Cloudfront can cache frequently accessed data, it is not designed for caching dynamic database data, such as the data stored in a MySQL database.\nOption D (Elasticache instance) is the most suitable solution for this problem. Elasticache is a fully managed in-memory data store service that enables caching of frequently accessed data. It supports two popular open-source in-memory caching engines: Memcached and Redis. Elasticache can be integrated with a variety of AWS services, including EC2 instances, RDS databases, and Lambda functions. It can also be used to scale applications by handling a large number of requests without putting excessive load on the database.\nTherefore, the correct answer to the given question is option D (Elasticache instance).\n\n"
}, {
  "id" : 295,
  "question" : "You've just started development on an application that will make use of the ElastiCache service.\nYou need to ensure that objects are cached but not kept inadvertently for a long time.\nWhich of the following strategy would you employ for the cache service?\n",
  "answers" : [ {
    "id" : "a6bde9c6ad264b73865d423c04722a02",
    "option" : "Adding TTL",
    "isCorrect" : "true"
  }, {
    "id" : "2043d9d1f65d42f4a56e95493fd9f3f2",
    "option" : "Lazy Loading",
    "isCorrect" : "false"
  }, {
    "id" : "01123bb02b884428b91dd15b786be4b9",
    "option" : "Write Through",
    "isCorrect" : "false"
  }, {
    "id" : "0617ba017368452e99d4cc5cd5b55ea5",
    "option" : "Read Through.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nLazy loading allows for stale data but won't fail with empty nodes.\nWrite through ensures that data is always fresh but may fail with empty nodes and may populate the cache with superfluous data.\nBy adding a time to live (TTL) value to each write, we can enjoy the advantages of each strategy and largely avoid cluttering up the cache with superfluous data.\nOption B is incorrect because this is a caching strategy that loads data into the cache only when necessary.\nOption C is incorrect because this is a caching strategy that adds data or updates data in the cache whenever data is written to the database.\nOption D is incorrect because there is no such caching strategy.\nFor more information on Caching strategies, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html\n\nTo ensure that objects are cached but not kept inadvertently for a long time, you can use Time-To-Live (TTL) expiration. Therefore, the correct answer is A. Adding TTL.\nTTL is a setting that specifies the amount of time for which an object can remain in the cache before it is automatically evicted. This is a useful feature that prevents your cache from becoming stale and keeps the most frequently accessed data in the cache. When an object's TTL expires, it is evicted from the cache and must be reloaded from the original data source.\nLazy Loading, Write Through, and Read Through are caching strategies, but they do not address the requirement of ensuring that objects are cached but not kept inadvertently for a long time. Let's briefly explain each of them:\nLazy Loading: This is a technique that loads data into the cache only when it is needed. It is useful for improving performance by reducing the amount of data that needs to be loaded into the cache upfront. However, it does not address the requirement of ensuring that objects are not kept in the cache for too long. Write Through: This caching strategy involves writing data to the cache and the data source simultaneously. This ensures that the data in the cache is always consistent with the data in the data source. However, it does not address the requirement of ensuring that objects are not kept in the cache for too long. Read Through: This caching strategy involves loading data from the data source into the cache on demand. This ensures that the data in the cache is always consistent with the data in the data source. However, it does not address the requirement of ensuring that objects are not kept in the cache for too long.\nIn summary, the correct answer is A. Adding TTL, which will help you ensure that objects are cached but not kept inadvertently for a long time.\n\n"
}, {
  "id" : 296,
  "question" : "Your team has been tasked with the development of an application.\nThe backend store needs to store data on which ad-hoc queries can run, and table joins are required.\nWhich of the following would you use to host the underlying data store?\n",
  "answers" : [ {
    "id" : "9939f0de98f3473380b37696c87a7080",
    "option" : "AWS RDS",
    "isCorrect" : "true"
  }, {
    "id" : "e8f47d10d9f84b028aee1556dd586f86",
    "option" : "AWS DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "53ba1c2ad5c04ea0ab1a544c7077b962",
    "option" : "AWS S3",
    "isCorrect" : "false"
  }, {
    "id" : "4d2641d62ef14ad0a3cdde1c9e04e20b",
    "option" : "AWS Athena.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation gives an example comparison of when to use AWS RDS for SQL data and DynamoDB for NoSQL data.\nOption B is incorrect because this should not be used when you have table joins to be carried out.\nOption C is incorrect because this is used for object level storage.\nOption D is incorrect because this is used for querying data in S3.\nFor more information on when to use SQL over NoSQL, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html\n\n\nFor an application that requires the storage of data on which ad-hoc queries can run and table joins are required, the best option would be a relational database.\nAWS offers a number of database services, including RDS, DynamoDB, and Athena, but the best option would be AWS RDS.\nAWS RDS (Relational Database Service) is a managed database service that makes it easy to set up, operate, and scale a relational database in the cloud. With RDS, you can choose from six popular database engines: Amazon Aurora, MySQL, PostgreSQL, Oracle, Microsoft SQL Server, and MariaDB. RDS supports both read replicas and Multi-AZ deployments for high availability and automatic failover.\nDynamoDB is a NoSQL database, which means it is not well-suited for ad-hoc queries and table joins. While it is fast and scalable, it is designed for use cases that require low-latency access to small items of data.\nS3 is an object storage service, not a database service. While S3 can store structured data, it is not optimized for running ad-hoc queries and table joins.\nAthena is a query service that allows you to run ad-hoc queries against data stored in S3. While Athena is well-suited for ad-hoc queries, it is not optimized for table joins.\nIn summary, for an application that requires the storage of data on which ad-hoc queries can run and table joins are required, the best option would be AWS RDS.\n\n"
}, {
  "id" : 297,
  "question" : "Your development team is currently working with an application that interacts with the DynamoDB table.\nDue to the proposed extensive use of the application, the underlying DynamoDB table would undergo a steady growth in size.\nWhich of the following preferred options should be used for retrieving the data? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "c6436bf15e444ed39908669900f41d15",
    "option" : "Use the query operation.",
    "isCorrect" : "true"
  }, {
    "id" : "03c57395d19449ceac6d91653995d61f",
    "option" : "Use the Scan operation.",
    "isCorrect" : "false"
  }, {
    "id" : "a180db7bb50f4b36ae0b3bbf99f7857c",
    "option" : "Use the GetItem API command.",
    "isCorrect" : "true"
  }, {
    "id" : "10bf735846834017b961194c53638b63",
    "option" : "Use the BatchGetItem API command.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, C and D.\nThe AWS Documentation mentions the following.\nIf possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results.\nAlso, as a table or index grows, the Scan operation slows.\nThe Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation.\nFor faster response times, design your tables and indexes so that your applications can use Query instead of Scan.\n(For tables, you can also consider using the GetItem and BatchGetItem APIs.)\nOption B is incorrect since this would cause performance issues as the amount of items starts to increase.\nFor more information on best practices for the query and scan operation, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html\n\nWhen dealing with a large DynamoDB table, it's important to choose the appropriate operation to retrieve data efficiently. The three preferred options for retrieving data from a DynamoDB table with steady growth are as follows:\nUse the Query operation: The Query operation is the most efficient way to retrieve a subset of data from a DynamoDB table. It is designed to retrieve items that match specific key values. The Query operation can be used to fetch data from a single partition or a range of partitions. Since the Query operation only fetches items that match a specific key value, it can help reduce the number of read operations and lower the cost of data retrieval. Use the BatchGetItem API command: The BatchGetItem API command can be used to retrieve up to 100 items at once from one or more tables in a single request. It is best used when you know the primary key values for the items you want to retrieve. The BatchGetItem operation is more efficient than individual GetItem operations because it minimizes the number of network round trips required to retrieve data. Use the GetItem API command: The GetItem API command is best used when you need to retrieve a specific item from a DynamoDB table. It retrieves a single item from a table using its primary key. The GetItem operation is less efficient than the Query operation and the BatchGetItem operation because it requires a read operation for each item retrieved. Do NOT use the Scan operation: The Scan operation retrieves all items from a DynamoDB table and applies an optional filter to the results to refine the search. The Scan operation can be expensive and time-consuming, especially on large tables, and should be avoided whenever possible.\nIn summary, the preferred options for retrieving data from a DynamoDB table with steady growth are the Query operation, the BatchGetItem API command, and the GetItem API command. The Scan operation should be avoided due to its potential cost and performance issues.\n\n"
}, {
  "id" : 298,
  "question" : "Your development team is planning on building an application using microservices.\nDocker containers will be used to build the application.\nWhich of the following services can be used to deploy the Docker containers? (Select Two).\n",
  "answers" : [ {
    "id" : "4a2a4dba05834f99a08d01f2cdbee752",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "ad38aaedb6fd465cb3d4f7c9451c3a1d",
    "option" : "Amazon ECS",
    "isCorrect" : "true"
  }, {
    "id" : "4f40e1dc75b14964bcc312b774e9b9f6",
    "option" : "Classic Load balancer",
    "isCorrect" : "false"
  }, {
    "id" : "657eac4a54c04cbab766975e465960ff",
    "option" : "Amazon Kinesis.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nAmazon Elastic Container Service (Amazon ECS) is a highly scalable, fast container management service that makes it easy to run, stop, and manage containers on a cluster.\nYour containers are defined in a task definition that you use to run individual tasks or tasks within a service.\nSo option B is correct.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\nOption A is correct because AWS Lambda is a serverless service and supports Docker deployments.\nYou can now package and deploy Lambda functions as container images of up to 10 GB in size.\nIn this way, you can also easily build and deploy larger workloads that rely on sizable dependencies.\nhttps://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/\nOption C is incorrect because there is no need for a load balancer in deployment work.\nOption D is incorrect because Kinesis is a streaming and analytics service, which does not help in this scenario.\n\nThe correct answers are B. Amazon ECS and C. Classic Load balancer.\nAmazon Elastic Container Service (ECS) is a fully-managed container orchestration service that makes it easy to run, stop, and manage Docker containers on a cluster. ECS provides a scalable and secure platform to deploy and manage Docker containers, and it integrates with other AWS services such as Amazon EC2, Amazon ECR, and Elastic Load Balancing.\nWith ECS, you can create a cluster of EC2 instances, and deploy your Docker containers onto this cluster. ECS provides a variety of features to manage the deployment and scaling of your containers, such as auto-scaling, load balancing, and service discovery.\nClassic Load Balancer is a service that automatically distributes incoming traffic across multiple EC2 instances. The load balancer can distribute traffic to multiple containers running on the same EC2 instance, or to containers running on different EC2 instances.\nTo use Classic Load Balancer with Docker containers, you can create a Docker container image that includes a web server (such as Apache or Nginx) and a load balancer (such as HAProxy or Nginx). You can then deploy this image to an EC2 instance, and configure the load balancer to distribute incoming traffic across multiple instances.\nIn summary, Amazon ECS and Classic Load Balancer can be used to deploy Docker containers in a microservices architecture. ECS provides a fully-managed container orchestration service, while Classic Load Balancer provides load balancing capabilities for incoming traffic.\n\n"
}, {
  "id" : 299,
  "question" : "Your team has an application being served out of AWS S3\nThere is a surge of increased number of GET requests.\nAfter monitoring using Cloudwatch metrics, you can see the rate of GET requests going close to 5000 requests per second.\nWhich of the following can be used to ensure better performance?\n",
  "answers" : [ {
    "id" : "3ea30d984c024c7a9f040a50e7c568c9",
    "option" : "Add an Elasticache in front of the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "674efe9116644f4a9b7164b382c6dce9",
    "option" : "Use DynamoDB instead of using S3.",
    "isCorrect" : "false"
  }, {
    "id" : "eb7c2727c9e443808510d0685edee845",
    "option" : "Place a Cloudfront distribution in front of the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "b73e93dc4a8a4f1394df1a2d7d6d8114",
    "option" : "Place an Elastic Load balancer in front of the S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nIf your workload is mainly sending GET requests, in addition to the preceding guidelines, you should consider using Amazon CloudFront for performance optimization.\nBy integrating CloudFront with Amazon S3, you can distribute content to your users with low latency and a high data transfer rate.\nYou also send fewer direct requests to Amazon S3, which reduces your costs.\nSince the documentation clearly mentions this, all other options are invalid.\nFor more information, please check the below link.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/s3-dg.pdf\n\nThe best option to improve the performance of the application being served out of AWS S3 in response to a surge of increased number of GET requests is to place a CloudFront distribution in front of the S3 bucket. The reasons for this are explained below:\nA. Adding an Elasticache in front of the S3 bucket: ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. It is useful in reducing the load on databases by caching the frequently accessed data in memory, thereby improving the performance of the application. However, in this scenario, adding Elasticache in front of the S3 bucket would not help much since S3 is already a highly scalable and reliable object storage service designed for high durability, availability, and performance at scale. Elasticache can improve performance for read-heavy database workloads but won't have much of an effect on S3 access.\nB. Using DynamoDB instead of using S3: DynamoDB is a fully managed NoSQL database service that can handle millions of requests per second and automatically scales capacity to meet the demands of the workload. However, this option won't help either since DynamoDB is a database service that stores and retrieves structured data, whereas S3 is a simple storage service that stores and retrieves any type of data. The option to choose between S3 and DynamoDB depends on the type of data and the specific needs of the application.\nC. Placing a Cloudfront distribution in front of the S3 bucket: Amazon CloudFront is a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, and no minimum usage commitments. CloudFront caches content at edge locations around the world, reducing the latency of the GET requests, and improving the overall performance of the application. Cloudfront also supports SSL/TLS encryption, which helps to protect sensitive data in transit. Placing a CloudFront distribution in front of the S3 bucket will help to distribute the traffic and reduce the load on S3. It can also help to mitigate DDoS attacks.\nD. Placing an Elastic Load balancer in front of the S3 bucket: Elastic Load Balancer is a load balancing service that automatically distributes incoming traffic across multiple targets such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. It helps to improve the availability and scalability of applications by automatically detecting unhealthy targets and rerouting traffic to healthy targets. However, this option is not suitable for S3, since S3 is an object storage service, and Elastic Load Balancer is designed for distributing traffic to compute resources like EC2 instances or containers.\nTherefore, the correct answer is C. Placing a CloudFront distribution in front of the S3 bucket.\n\n"
}, {
  "id" : 300,
  "question" : "Your team is planning on creating a Lambda function which will interact with a DynamoDB stream.\nWhich of the following would need to be in place to ensure the Lambda function can interact with the DynamoDB table?\n",
  "answers" : [ {
    "id" : "8cfc8dab34c548619977f0d27e1290c9",
    "option" : "Access Keys for an IAM user embedded in the function.",
    "isCorrect" : "false"
  }, {
    "id" : "9dad758ce10f4a13b8637ed7d9ceafbd",
    "option" : "IAM Role with required permissions to access DynamoD.",
    "isCorrect" : "true"
  }, {
    "id" : "b12f3b513f0a4152a93b10385da17cc9",
    "option" : "The password for an IAM user in the environment variable for the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "080ea5c866b94d4fbcb4fe96ef55ac7b",
    "option" : "A Security group rule to allow egress traffic into DynamoD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nRegardless of what invokes a Lambda function, AWS Lambda always executes a Lambda function on your behalf.\nIf your Lambda function needs to access any AWS resources, you need to grant the relevant permissions to access those resources.\nYou also need to grant AWS Lambda permissions to poll your DynamoDB stream.\nYou grant all of these permissions to an IAM role (execution role) that AWS Lambda can assume to poll the stream and execute the Lambda function on your behalf.\nYou create this role first and then enable it at the time you create the Lambda function.\nFor more information on using Lambda with DynamoDB, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html\n\nOption B is the correct answer: IAM Role with required permissions to access DynamoDB.\nExplanation: When you create a Lambda function that interacts with a DynamoDB stream, you need to ensure that the Lambda function has the appropriate permissions to access the DynamoDB table.\nTo grant these permissions, you can create an IAM role with the necessary permissions and then assign that role to the Lambda function. This approach provides better security because the Lambda function does not need to embed any access keys or passwords.\nThe IAM role should have permissions to access the DynamoDB table, as well as any other AWS resources that the Lambda function needs to access, such as S3 buckets or other AWS services.\nAccess Keys for an IAM user embedded in the function (Option A) is not a good approach for several reasons:\nIt is less secure because access keys can be easily compromised if they are embedded in the function code. It is not scalable because if you have multiple Lambda functions, you would need to embed access keys for each of them. It is difficult to manage because you need to rotate access keys periodically to maintain security.\nThe password for an IAM user in the environment variable for the Lambda function (Option C) is also not a good approach for similar reasons. Passwords can be easily compromised and it would be difficult to manage and rotate them for multiple Lambda functions.\nSecurity group rules (Option D) are used to control inbound and outbound traffic to and from EC2 instances, RDS instances, and other AWS resources. They are not used to control access to DynamoDB tables from Lambda functions. Therefore, this option is not correct.\n\n"
}, {
  "id" : 301,
  "question" : "Your team is looking into the Serverless deployment of an AWS lambda function.\nThe function will be deployed using the Serverless application model.\nTo test this out, you first create a sample function created below. var AWS = require('aws-sdk'); exports.handler = function(event, context, callback) { var bucketName = \"Demobucket\"; callback(null, bucketName); } What should be the next steps in the serverless deployment? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "484925270cd642b1bf6052d98c5554af",
    "option" : "Create a YAML file with the deployment specific’s and package that along with the function file.",
    "isCorrect" : "true"
  }, {
    "id" : "ec851f7d60794625abe755d3872a2e9d",
    "option" : "Upload the application function file onto an S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "065d2814478f486a8359e946f34868f7",
    "option" : "Upload the function on AWS Lambda.",
    "isCorrect" : "false"
  }, {
    "id" : "e52fc8131df34a278f8896758116a119",
    "option" : "Upload the complete package onto an S3 bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThis is given in the AWS Documentation.\nOption B is incorrect since you need to package both the application function and the YAML file.\nOption C is incorrect since you have the requirement to deploy this in an automated fashion.\nPlease refer to the below link, Step 3 and Step 4, for further details.\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-quick-start.html\n\n\nTo deploy a serverless AWS Lambda function using the Serverless Application Model (SAM), you need to follow the following steps:\nWrite the function code: The first step is to write the code for your Lambda function. In this case, a sample function is given in the question, which can be used to test the deployment process. Create a SAM Template: Create a YAML file that defines the AWS resources that make up your serverless application, including the function, event sources, permissions, and any other resources you need. The SAM Template is the deployment specification that is used to deploy your serverless application. Package the code and dependencies: Use the SAM CLI to package your Lambda function code and any dependencies into an Amazon S3 bucket. The package command will create a ZIP file of your code and dependencies and upload it to the S3 bucket you specify. The packaged code will be used by AWS Lambda to create a new version of your function. Deploy the SAM Template: Use the SAM CLI to deploy your serverless application. This command creates and configures the AWS resources defined in the SAM Template. The deploy command will create or update the CloudFormation stack that represents your serverless application. Test the function: Use the AWS Lambda console or the AWS CLI to test your Lambda function. You can pass in test events to see how your function responds.\nBased on the above steps, the correct answers to the question are A and D.\nA. Create a YAML file with the deployment specific's and package that along with the function file: This step is required to create the SAM Template, which defines the AWS resources that make up your serverless application, including the function, event sources, permissions, and any other resources you need. The packaged code and the YAML file are required to deploy the serverless application.\nD. Upload the complete package onto an S3 bucket: This step is required to package the code and dependencies using the SAM CLI and upload the ZIP file to an S3 bucket. The packaged code will be used by AWS Lambda to create a new version of your function.\n\n"
}, {
  "id" : 302,
  "question" : "You have created a Lambda Function generating the error “ServiceException”\nWhich of the following is the best practice to handle this exception under AWS Step Functions?\n",
  "answers" : [ {
    "id" : "c14e4b3a4262460cb2ea8290537fe69e",
    "option" : "Use Lambda Catch code with only “ErrorEquals” string.",
    "isCorrect" : "false"
  }, {
    "id" : "e605092ac1ce4e88956a347e24329e92",
    "option" : "Use Lambda Retry code with only “BackoffRate” string",
    "isCorrect" : "false"
  }, {
    "id" : "3909bf9577464d828f0fa66139d619b7",
    "option" : "Use Lambda Retry code with only “ErrorEquals” string.",
    "isCorrect" : "true"
  }, {
    "id" : "8a12517636e14addbba706fb8a721a6b",
    "option" : "Use Lambda Catch code with only “ResultPath” string.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAny state under step functions can encounter runtime errors.\nErrors can happen for various reasons:\nTask failures (for example, an exception in a Lambda function)\nAWS Lambda can occasionally experience transient service errors.\nIn this case, invoking Lambda results in a 500 error, such as\nServiceException.\n,\nAWSLambdaException.\n, or\nSdkClientException.\n.\nBy default, when a state reports an error, AWS Step Functions causes the execution to fail entirely.\nFor errors such as “ServiceException”, the best practice is to Retry invoking the Lambda function.\nWithin a Retry Code, the “ErrorEquals” field is the required string that matches error names &amp; all other fields are optional.\nOption A is incorrect as the Lambda Catch code is only used after the function performs several retries and ErrorEquals &amp; Next are required strings.\nOption B is incorrect as the BackoffRate field is optional in Lambda Retry code &amp; if not specified, the default value of 2.0 is considered.\nOption D is incorrect as the Lambda Catch code is only used after the function performs several retries.\nResultPath is an optional field in a Catch Code, ErrorEquals &amp; Next are required strings.\nFor more information on troubleshooting Lambda Function errors, refer to the following URL-\nhttps://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html\nhttps://docs.aws.amazon.com/step-functions/latest/dg/bp-lambda-serviceexception.html\n\n\nWhen a Lambda function generates an error, you can use AWS Step Functions to handle it by specifying the appropriate response to the error. In this case, the error is a \"ServiceException,\" and the best practice for handling it under AWS Step Functions is to use a Lambda Catch code block with only \"ErrorEquals\" string.\nHere's an explanation of each answer choice:\nA. Use Lambda Catch code with only “ErrorEquals” string. This is the correct answer. With this approach, you can catch any error that matches the specified \"ErrorEquals\" string. In this case, the string would be \"ServiceException,\" which would allow you to handle this specific error.\nB. Use Lambda Retry code with only “BackoffRate” string This answer choice is incorrect because it doesn't address the specific error that the Lambda function is generating. Additionally, \"BackoffRate\" is a parameter used to specify the amount of time between retries, not the type of error to retry on.\nC. Use Lambda Retry code with only “ErrorEquals” string. This answer choice is incorrect because it doesn't provide a fallback mechanism in case the retries fail. It's better to use a Lambda Catch code block to handle the error and provide a fallback response.\nD. Use Lambda Catch code with only “ResultPath” string. This answer choice is incorrect because \"ResultPath\" is used to specify the location where the output of the Lambda function is stored, not the type of error to handle.\nIn summary, the best practice for handling a \"ServiceException\" error generated by a Lambda function under AWS Step Functions is to use a Lambda Catch code block with only \"ErrorEquals\" string.\n\n"
}, {
  "id" : 303,
  "question" : "Your team is using the AWS CodeBuild service for an application build.\nAs part of Integration testing during the build phase, the application needs to access an RDS instance in a private subnet.\nHow can you ensure that this is possible?\n",
  "answers" : [ {
    "id" : "48c9f01a8eae489686c018827260f17d",
    "option" : "Create a NAT Endpoint and ensure the codebuild project configuration is modified with the endpoint information.",
    "isCorrect" : "false"
  }, {
    "id" : "607c8cbd1db04317b1ce2b382ef274a2",
    "option" : "Provide additional VPC-specific configuration information as part of your AWS CodeBuild project.",
    "isCorrect" : "true"
  }, {
    "id" : "4109b5f1114e407184f4e38b90ac5400",
    "option" : "Mark the subnet as a public subnet during the time of the Integration tests.",
    "isCorrect" : "false"
  }, {
    "id" : "1ce5eb48c3e348a78de0ba7b7d546801",
    "option" : "Use a NAT gateway to relay the requests from AWS CodeBuild to the RDS Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is given in the AWS Documentation.\nTypically, resources in a VPC are not accessible by AWS CodeBuild.\nTo enable access, you must provide additional VPC-specific configuration information as part of your AWS CodeBuild project configuration.\nThis includes the VPC ID, the VPC subnet IDs, and the VPC security group IDs.\nVPC-enabled builds are then able to access resources inside your VPC.\nVPC connectivity from AWS CodeBuild builds makes it possible to:\nRun integration tests from your build against data in an Amazon RDS database that's isolated on a private subnet.\nQuery data in an Amazon ElastiCache cluster directly from tests.\nInteract with internal web services hosted on Amazon EC2, Amazon ECS, or services that use internal Elastic Load Balancing.\nSince the requirements are clearly mentioned in the documentation, all other options are incorrect.\nFor more information on VPC support for AWS CodeBuild, please refer to the below URL-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/vpc-support.html\n\nWhen an application needs to access resources in a private subnet, there are several options for making the resources accessible, but the most common approach is to use a NAT gateway. Therefore, the answer to this question is D. Use a NAT gateway to relay the requests from AWS CodeBuild to the RDS Instance.\nHere's a more detailed explanation:\nAWS CodeBuild is a fully managed continuous integration and continuous delivery (CI/CD) service that compiles source code, runs tests, and produces software packages that are ready to deploy. During the build phase, it is common for applications to access resources in different parts of your infrastructure, including RDS instances in private subnets.\nBy default, resources in a private subnet cannot be accessed from outside the VPC. To enable CodeBuild to access an RDS instance in a private subnet, you need to provide a way for the traffic to leave the VPC and reach the internet. One way to do this is to use a NAT gateway, which is a managed service that enables outbound internet traffic from resources in a private subnet.\nTo set up a NAT gateway for CodeBuild to access an RDS instance in a private subnet, follow these steps:\nCreate a NAT gateway in a public subnet: The NAT gateway must be in a public subnet with a route to an internet gateway. Create a new route table: Create a new route table for the private subnet where the RDS instance is located. This route table should have a default route to the NAT gateway created in step 1. Modify the subnet association: Associate the private subnet with the new route table created in step 2. Update CodeBuild project configuration: In the CodeBuild project configuration, update the VPC settings to use the private subnet and the new route table created in steps 2 and 3.\nWith these steps, CodeBuild will be able to access the RDS instance in the private subnet through the NAT gateway.\n\n"
}, {
  "id" : 304,
  "question" : "You have created a web application that saves all data in Amazon S3 buckets.\nIf you have created 3 prefixes in an S3 bucket, what will be the PUT &amp; GET read performance you will get per second?\n",
  "answers" : [ {
    "id" : "8396fb6177b340d5b61f620b70e26f30",
    "option" : "3,500 PUT and 5,500 GET requests per second.",
    "isCorrect" : "false"
  }, {
    "id" : "c73386959d564b3e969567c7a5523d04",
    "option" : "7,000 PUT and 11,000 GET requests per second.",
    "isCorrect" : "false"
  }, {
    "id" : "3289fbe34dcb42bcb21595c28c0ec921",
    "option" : "10,500 GET and 16,500 PUT requests per second.",
    "isCorrect" : "false"
  }, {
    "id" : "e8760ca2be8345ad9ac420e7d338cda4",
    "option" : "10,500 PUT and 16,500 GET requests per second.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAmazon S3 provides 3,500 PUT requests per second per prefix in a bucket &amp; 5,500 GET requests per second per prefix in a bucket.\nSo, for 3 prefixes, the S3 bucket will support 10,500 PUT, and 16,500 GET requests per second.\nOption A is incorrect as 3,500 PUT and 5,500 GET requests per second are provided for each prefix.\nOption B is incorrect as 7,000 PUT and 11,000 GET requests per second will be provided for 2 prefixes &amp; not for 3 prefixes.\nOption C is incorrect as for 3 prefixes, performance which you will get is 10,500 PUT, and 16,500 GET requests per second&amp; not 10,500 GET and 16,500 PUT requests per second.\nFor more information on PUT &amp; GET performance with Amazon S3 bucket, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/PerformanceOptimization.html\n\nTo understand the answer to this question, we need to consider the S3 performance characteristics related to PUT and GET requests.\nAmazon S3 provides an eventually consistent model for both PUT and GET requests. This means that while data is replicated across multiple storage nodes, it may take some time for all nodes to receive the latest updates. As a result, there may be some inconsistencies between the data stored on different nodes, which can impact performance.\nThe performance of S3 depends on several factors, including the object size, the number of objects in a bucket, the request rate, and the distribution of objects across prefixes.\nA prefix is a string that defines a virtual subdirectory within a bucket. For example, if you have a bucket named \"my-bucket\" and create a prefix called \"images/\", objects with the key \"images/image1.jpg\" and \"images/image2.jpg\" would be stored in the \"images\" virtual subdirectory.\nWhen you have multiple prefixes in a bucket, the request rate is distributed across the prefixes. This means that the more prefixes you have, the higher the overall request rate that S3 can handle.\nNow, let's look at the answer choices:\nA. 3,500 PUT and 5,500 GET requests per second. B. 7,000 PUT and 11,000 GET requests per second. C. 10,500 GET and 16,500 PUT requests per second. D. 10,500 PUT and 16,500 GET requests per second.\nAnswer A assumes that the overall request rate for the bucket is 9,000 requests per second (3,000 PUTs and 6,000 GETs). This means that each prefix would receive an equal share of the request rate (3,000/3 prefixes = 1,000 requests per second per prefix for PUT requests and 2,000 requests per second per prefix for GET requests). However, this assumption is incorrect since the question doesn't specify how many PUT and GET requests are made for each prefix.\nAnswer B assumes that the overall request rate for the bucket is 18,000 requests per second (6,000 PUTs and 12,000 GETs). This means that each prefix would receive an equal share of the request rate (6,000/3 prefixes = 2,000 requests per second per prefix for PUT requests and 4,000 requests per second per prefix for GET requests). This is a plausible answer, but we need to consider the other options.\nAnswer C assumes that the overall request rate for the bucket is 27,000 requests per second (16,500 PUTs and 10,500 GETs). This means that the bucket has a higher request rate for PUT requests than GET requests. However, this is also incorrect since the question states that all data is saved in S3 buckets, and there is no reason to assume that there will be a higher request rate for PUTs than GETs.\nAnswer D assumes that the overall request rate for the bucket is 27,000 requests per second (10,500 PUTs and 16,500 GETs). This means that the bucket has a higher request rate for GET requests than PUT requests, which is a plausible assumption. Each prefix would receive an equal share of the request rate (10,500/3 prefixes = 3,500 requests per second per prefix for PUT requests and 5,500 requests per second per prefix for GET requests). Therefore, the correct answer is D.\nIn summary, the answer to the question is D, which assumes an overall request rate of 27,000 requests per second, with 10,500 PUTs and 16,500 GETs per second, resulting in a performance of 3,500 PUTs and 5,500 GET\n\n"
}, {
  "id" : 305,
  "question" : "Your team has developed an application that will be launched on EC2 Instances that are part of an Autoscaling Group.\nIt needs to be ensured that the application can get the IP address of the Instance.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "2b941be44f3e47f99c33a140184d7ae0",
    "option" : "Make the application query the Instance Metadata.",
    "isCorrect" : "true"
  }, {
    "id" : "553ba5345f064308ac582a640e3e610d",
    "option" : "Make the application query the Instance Userdata.",
    "isCorrect" : "false"
  }, {
    "id" : "d0740d9d09ca47d28be9146131d4d097",
    "option" : "Make the application query the Autoscaling Group.",
    "isCorrect" : "false"
  }, {
    "id" : "3a84e6575b9f44bc96c765430838d0fc",
    "option" : "Make the application query the Launch configuration.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is given in the AWS Documentation.\nInstance metadata is data about your instance that you can use to configure or manage the running instance.\nBecause your instance metadata is available from your running instance, you do not need to use the Amazon EC2 console or the AWS CLI.\nThis can be helpful when you're writing scripts to run from your instance.\nFor example, you can access the local IP address of your instance from instance metadata to manage a connection to an external application.\nSince the data can only be retrieved from the Instance Metadata, all other options are invalid.\nFor more information on Instance Metadata, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html\n\nThe correct answer is A. Make the application query the Instance Metadata.\nWhen an EC2 instance launches, it automatically receives metadata that can be accessed using a well-known URL. EC2 instance metadata is a simple way to get information about an instance, such as the instance ID, public IP address, private IP address, and more.\nTo get the IP address of the EC2 instance, the application can make a query to the instance metadata URL: http://169.254.169.254/latest/meta-data/local-ipv4.\nThis URL will return the private IP address of the instance. If the instance has a public IP address, you can use http://169.254.169.254/latest/meta-data/public-ipv4 to get it.\nOption B, making the application query the Instance Userdata, is not the correct answer because user data is a piece of information that can be passed to the EC2 instance when it launches. It is typically used to run scripts or set up the instance's environment. It does not contain information about the instance itself.\nOption C, making the application query the Autoscaling Group, is also not the correct answer. The Autoscaling Group is a logical grouping of EC2 instances that can be automatically scaled up or down based on demand. While it does contain information about the instances in the group, it does not provide their IP addresses.\nOption D, making the application query the Launch configuration, is also not the correct answer. The Launch configuration is a template that is used to launch EC2 instances in an Autoscaling Group. It contains information such as the AMI ID, instance type, security groups, and other launch parameters. It does not contain information about the IP addresses of the instances that are launched.\n\n"
}, {
  "id" : 306,
  "question" : "You're in charge of creating a cloud formation template.\nThis template needs to create resources for multiple types of environments.\nThe template needs to be flexible so that it can create resources based on the type of environment.\nHow can you achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "a554b9c8323a4b71929e3f2507cfbcbd",
    "option" : "Create an Input Parameter to take in the type of environment.",
    "isCorrect" : "true"
  }, {
    "id" : "459aea38d6d741fcae5be82484627245",
    "option" : "Use the Outputs section to define the type of environment.",
    "isCorrect" : "false"
  }, {
    "id" : "e7355b700e7440ef9be74346dfce8ba9",
    "option" : "Use the Custom Resources feature to create resources based on the type of environment.",
    "isCorrect" : "false"
  }, {
    "id" : "771ecd4b9ffd4629824f7bc83d6fef77",
    "option" : "Use the Conditions section to create resources based on the type of environment.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThis is given in the AWS Documentation.\nThe optional Conditions section contains statements that define the circumstances under which entities are created or configured.\nFor example, you can create a condition and then associate it with a resource or output so that AWS CloudFormation only creates the resource or output if the condition is true.\nSimilarly, you can associate the condition with a property so that AWS CloudFormation only sets the property to a specific value if the condition is true.\nIf the condition is false, AWS CloudFormation sets the property to a different value that you specify.\nYou might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment.\nYou can add an EnvironmentType input parameter in your template, which accepts either prod or test as inputs.\nSince this is clearly given in the documentation, all other options are incorrect.\nFor more information on conditions in a Cloudformation template, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html\n\nTo create a CloudFormation template that can create resources for multiple types of environments, the following approaches can be taken:\nA. Create an Input Parameter to take in the type of environment: This approach involves creating an Input Parameter in the template that can take in the type of environment as input during the stack creation process. The parameter can be defined in the Parameters section of the template, and its value can be used to conditionally create resources based on the type of environment. For example, if the input parameter is set to 'development', the template can create resources suitable for a development environment, and if it is set to 'production', resources suitable for a production environment can be created.\nD. Use the Conditions section to create resources based on the type of environment: This approach involves defining conditions in the Conditions section of the CloudFormation template based on the type of environment. These conditions can then be used to conditionally create resources in the Resources section of the template. For example, if the environment is 'development', a certain resource can be created, and if it is 'production', a different resource can be created. The conditions can be defined based on input parameters, environment variables, or any other criteria.\nB. Use the Outputs section to define the type of environment: The Outputs section in a CloudFormation template can be used to define the type of environment based on the resources created by the template. This approach involves defining output values that can indicate the type of environment, such as a URL for a web application deployed in a certain environment. These output values can then be used by other CloudFormation stacks or AWS services to conditionally create resources based on the type of environment.\nC. Use the Custom Resources feature to create resources based on the type of environment: The Custom Resources feature in CloudFormation can be used to create custom resources based on the type of environment. This approach involves writing custom code, such as Lambda functions, that can be used to conditionally create resources based on the input parameters or other criteria. The custom resources can then be defined in the Resources section of the CloudFormation template.\nIn summary, options A and D are the correct answers, as they involve creating input parameters and defining conditions in the CloudFormation template to create resources based on the type of environment. Option B is not the best approach, as it involves defining the type of environment based on output values, which may not be the most flexible solution. Option C is also a valid approach, but it involves writing custom code, which may be more complex than necessary for this use case.\n\n"
}, {
  "id" : 307,
  "question" : "Your team is working on an API definition that will be deployed using the API gateway service.\nYou then need to ensure that control is established on who can access the various resources within the API gateway.\nWhich of the following can help ensure this security requirement is met? Choose 3 Options.\n",
  "answers" : [ {
    "id" : "35f9d3faf4fb498db49b31831db57d7c",
    "option" : "Key Policies",
    "isCorrect" : "false"
  }, {
    "id" : "69d3c20d60c94352b592c293d18d5ba5",
    "option" : "IAM Policies",
    "isCorrect" : "true"
  }, {
    "id" : "f5e72396ee394fc3bebe9e792387f1f0",
    "option" : "Resource Policies",
    "isCorrect" : "true"
  }, {
    "id" : "d35294d846a4491aa1c8d46ad24d2c42",
    "option" : "IAM Roles.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B, C and D.\nThis is given in the AWS Documentation.\nFor more information on using IAM Policies for controlling access, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-control-access-using-iam-policies-to-invoke-api.html\nResource policies, Iam roles and policies can control access to an API.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\n\n\nSure, here's a detailed explanation for each option:\nA. Key Policies: AWS Key Policies are used to grant or deny access to AWS KMS keys. They are JSON documents that allow the key owner to specify who can use the key, and for what operations. However, Key Policies cannot be used to control access to API Gateway resources.\nB. IAM Policies: AWS Identity and Access Management (IAM) Policies are JSON documents that define permissions for users, groups, and roles in AWS. IAM policies can be attached to IAM users, groups, and roles. IAM policies can be used to grant or deny access to API Gateway resources based on the permissions defined in the policy.\nC. Resource Policies: AWS Resource Policies are JSON documents that allow you to define permissions for AWS resources. Resource policies can be used to control who can access the resources within API Gateway, such as APIs, stages, and methods. Resource policies can be attached to API Gateway resources, and can be used to grant or deny access to those resources.\nD. IAM Roles: AWS IAM Roles allow you to delegate access to AWS resources to an AWS service, or to an IAM user or role in another AWS account. IAM Roles can be used to allow an AWS service, such as API Gateway, to assume a role that grants it access to other AWS resources. However, IAM Roles alone cannot be used to control access to API Gateway resources.\nTherefore, the correct answers are B. IAM Policies, C. Resource Policies, and D. IAM Roles. IAM policies are used to grant or deny access to API Gateway resources based on the permissions defined in the policy. Resource policies can be used to control who can access the resources within API Gateway. IAM Roles can be used to allow an AWS service, such as API Gateway, to assume a role that grants it access to other AWS resources. Together, these three options can help ensure that control is established on who can access the various resources within the API gateway.\n\n"
}, {
  "id" : 308,
  "question" : "You are a developer for a company.\nYou plan to use the X-Ray service to trace all incoming HTTP requests for an application being developed.\nIn the X-Ray SDK which of the following feature would you use to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "e6fa37e440a946668930bf1b92e24a93",
    "option" : "Interceptors",
    "isCorrect" : "true"
  }, {
    "id" : "fcca1fae80c24f5eb154c2deab2347cd",
    "option" : "Client handlers",
    "isCorrect" : "false"
  }, {
    "id" : "761d9e83152146de8d57ee964164503a",
    "option" : "Server handlers",
    "isCorrect" : "false"
  }, {
    "id" : "7321ecf8428f43b28ab7edea11e44380",
    "option" : "Daemon service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is given in the AWS Documentation.\nThe X-Ray SDK provides:\nInterceptors to add to your code to trace incoming HTTP requests.\nClient handlers to instrument AWS SDK clients that your application uses to call other AWS services.\nAn HTTP client to use to instrument calls to other internal and external HTTP web services.\nSince this is clearly given in the documentation, all other options are incorrect.\nFor more information on using AWS X-Ray, please refer to the below URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/aws-xray.html\n\nTo trace incoming HTTP requests for an application being developed, you can use the X-Ray SDK feature called \"Interceptors.\"\nInterceptors are a mechanism in the X-Ray SDK that allow you to automatically capture and instrument incoming HTTP requests to your application.\nWhen an incoming HTTP request is received by your application, the interceptor intercepts the request and automatically adds tracing information to it, including a unique trace ID and segment information. This allows you to track the request as it passes through your application, including any downstream services it calls.\nThe other options listed in the question, client handlers, server handlers, and daemon service, are not directly related to tracing incoming HTTP requests.\nClient handlers are used to instrument outgoing HTTP requests from your application. Server handlers are used to instrument outgoing HTTP requests from downstream services called by your application. Daemon service is a background service that runs on an EC2 instance or ECS task and collects trace data from the X-Ray SDK.\nTherefore, the correct answer to the question is A. Interceptors.\n\n"
}, {
  "id" : 309,
  "question" : "You're planning on using the data pipeline service to transfer data from Amazon S3 to Redshift.\nYou need to define the source and destination locations.\nWhich of the following part of the data pipeline service allows you to define these locations?\n",
  "answers" : [ {
    "id" : "fc0c61978e7543448fb815c7122af1a3",
    "option" : "Data Nodes",
    "isCorrect" : "true"
  }, {
    "id" : "05f8ebef68af4f2da8c83825e72c6b0e",
    "option" : "Task Runner",
    "isCorrect" : "false"
  }, {
    "id" : "871bde1801494cfa87b1c7bbcfa07d66",
    "option" : "Activities",
    "isCorrect" : "false"
  }, {
    "id" : "d03d4f5066a2452ba0934f5bf20c74f2",
    "option" : "Resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is given in the AWS Documentation.\nOption B is incorrect since the Task Runner is an application that polls AWS Data Pipeline for tasks and then performs those tasks.\nOption C is incorrect since an activity is a pipeline component that defines the work to perform.\nOption D is incorrect since a resource is the computational resource that performs the work that a pipeline activity specifies.\nFor more information on Data Nodes, please refer to the below URL-\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-concepts-datanodes.html\n\n\nThe correct answer is D. Resources.\nAWS Data Pipeline is a web service that allows you to automate the movement and transformation of data within AWS services. It is used to move data from one location to another and perform data processing activities such as running SQL queries, executing scripts, and performing data validations.\nTo create a data pipeline, you must define the source and destination locations, as well as any intermediate processing steps. These components are known as pipeline resources.\nPipeline resources include the following:\nData nodes: Represent data sources such as Amazon S3 buckets or DynamoDB tables. Activities: Represent processing steps such as running SQL queries, executing scripts, or performing data validations. Task runner: A software application that is installed on an Amazon EC2 instance, which runs the pipeline activities. Resources: Represent the data pipeline itself, and include information about the scheduling, dependencies, and pipeline objects such as the data nodes and activities.\nIn this scenario, the source location is Amazon S3 and the destination location is Redshift. Therefore, you would use the Resources component of the AWS Data Pipeline service to define these locations. You would create a data node for the Amazon S3 bucket, and an activity for loading the data into Redshift. You would also define the scheduling and dependencies for the pipeline.\n\n"
}, {
  "id" : 310,
  "question" : "As a database developer, you have started working with Redshift.\nYour IT administrator has provisioned a Redshift cluster.\nYou now need to load data into the Redshift cluster from S3\nWhich of the following command should you use for this activity?\n",
  "answers" : [ {
    "id" : "4e2daeb7c71f4c9f8f4ec59a60bd0944",
    "option" : "GET",
    "isCorrect" : "false"
  }, {
    "id" : "51456bb8ee254d0ca075e95078fb1884",
    "option" : "COPY",
    "isCorrect" : "true"
  }, {
    "id" : "99b9d71a410749cdad360c6d5cf986a5",
    "option" : "MERGE",
    "isCorrect" : "false"
  }, {
    "id" : "3a2dffa3638d46a091eb6fac779ac792",
    "option" : "PUT.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - B.\nThis is given in the AWS Documentation.\nThe ideal command to be used is given in the documentation.\nHence all other options are incorrect.\nFor more information on working with a sample Redshift cluster, please refer to the below URLs-\nhttps://docs.aws.amazon.com/redshift/latest/gsg/rs-gsg-create-sample-db.html\nhttps://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html\nhttps://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-s3redshift.html\n\n\nThe command you should use to load data into the Redshift cluster from S3 is the COPY command, which is option B.\nThe COPY command is a powerful command in Redshift that allows you to load large amounts of data into the database from a variety of sources, including S3. Here are the basic steps to use the COPY command to load data into Redshift:\nCreate a Redshift table to hold the data you want to load. Grant permissions to the Redshift user to access the S3 bucket that contains the data. Use the COPY command to load the data into the Redshift table.\nHere is an example of the basic syntax for the COPY command:\nrust Copy code COPY table_name FROM 's3://bucket_name/file_name' IAM_ROLE 'arn:aws:iam::123456789012:role/RedshiftRole' FORMAT AS CSV;\nIn this example, table_name is the name of the Redshift table you want to load the data into. bucket_name is the name of the S3 bucket that contains the data, and file_name is the name of the file you want to load. IAM_ROLE is the ARN of the AWS Identity and Access Management (IAM) role that you want to use to access the S3 bucket. Finally, FORMAT AS CSV indicates the format of the data you're loading.\nBy using the COPY command, you can quickly and easily load large amounts of data into Redshift from S3, making it an essential tool for any Redshift developer or administrator.\n\n"
}, {
  "id" : 311,
  "question" : "Your company is developing an application that needs to interact with an S3 bucket.\nThere is a requirement that all data in the bucket is encrypted at rest.\nYou also need to ensure that you manage the keys.\nWhich of the following options can you use for this purpose? (Select TWO)\n",
  "answers" : [ {
    "id" : "04e3edb76dd342f3a7430d6a57bd55a6",
    "option" : "Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)",
    "isCorrect" : "false"
  }, {
    "id" : "11181c0849054cac866638b662109a9a",
    "option" : "Server-Side Encryption with AWS KMS Keys stored in AWS Key Management Service (KMS)",
    "isCorrect" : "false"
  }, {
    "id" : "2d9725c8102f430480f68b1effdcc880",
    "option" : "Server-Side Encryption with with Customer-Provided Keys (SSE-C)",
    "isCorrect" : "true"
  }, {
    "id" : "0c37f7ad28d6471a8f34547d876deedd",
    "option" : "Client-Side Encryption.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nThis is given in the AWS Documentation.\nUse Server-Side Encryption with Customer-Provided Keys (SSE-C)\nYou manage the encryption keys, and Amazon S3 manages the encryption, as it writes to disks and decryption, when you access your objects.\nYou can encrypt data client-side and upload the encrypted data to Amazon S3\nIn this case, you manage the encryption process, the encryption keys, and related tools.\nOptions A and B are incorrect since these keys are managed by AWS.\nFor more information on Server-side encryption for S3, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\n\nSure, I'll explain the options for you:\nA. Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3): This option uses encryption keys managed by Amazon S3, which means that Amazon S3 handles the entire encryption process, including key management. In this option, data is encrypted before being stored in S3 and decrypted when accessed. This option is the simplest to use, as you don't need to worry about key management, but it is also the least secure since Amazon S3 manages the keys.\nB. Server-Side Encryption with AWS KMS Keys stored in AWS Key Management Service (KMS): This option uses AWS Key Management Service (KMS) to manage encryption keys for the data stored in S3. KMS provides you with more control over the keys than SSE-S3, and you can manage and audit the keys used for encryption. You can also use KMS to create and manage customer master keys (CMKs) for your data in S3, which gives you more control over the encryption process.\nC. Server-Side Encryption with with Customer-Provided Keys (SSE-C): This option allows you to provide your own encryption keys, which means that you can manage the keys yourself. In this option, you provide the encryption key to Amazon S3 when you upload the object, and Amazon S3 uses the key to encrypt the object data before storing it. You also need to provide the key when you access the object to decrypt the data.\nD. Client-Side Encryption: This option involves encrypting the data before it is uploaded to S3. This option requires you to manage the encryption keys yourself, and you can use any encryption library that supports the Advanced Encryption Standard (AES) algorithm.\nSo, in summary, options A and B are the correct answers as they both allow you to ensure that all data in the S3 bucket is encrypted at rest and manage the keys. Option C allows you to manage the keys, but you need to provide the encryption key yourself. Option D involves client-side encryption and is not a server-side encryption option.\n\n"
}, {
  "id" : 312,
  "question" : "You are developing an application that is going to make use of AWS Cognito.\nThe default sign-in and sign-up features of the AWS Cognito service will be used.\nThere is a security requirement to ensure that if the user's credentials are compromised, then they would need to use a new password.\nWhich of the following needs to be in place for this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "709ce1623bc349f4aaed31cbf6ef39c8",
    "option" : "Ensure to create a user pool in AWS Cognito.",
    "isCorrect" : "true"
  }, {
    "id" : "28157373462e4e3c82ebc0d2f4508947",
    "option" : "Ensure to “Block use” for compromised credentials in the Advanced Security section.",
    "isCorrect" : "true"
  }, {
    "id" : "82c3084c67104cb1bef33cd4e5d5ce87",
    "option" : "Ensure to “Block use” for compromised credentials in the Basic Security section.",
    "isCorrect" : "false"
  }, {
    "id" : "53de4b9ccd58468f9bc42aa87d9e5486",
    "option" : "Verify sign-in operation on Cognito using Secure Remote Password.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThis is given in the AWS Documentation.\nOption C is incorrect since this configuration needs to be done in the Advanced Security section.\nOption D is incorrect.\nCurrently, Amazon Cognito doesn't check for compromised credentials for sign-in operations with Secure Remote Password (SRP) flow, which doesn't send the password during sign-in.\nFor more information on Cognito User pools, please refer to the below URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-compromised-credentials.html\n\n\nTo meet the security requirement of forcing users with compromised credentials to create new passwords, the following two actions need to be taken:\nA. Create a User Pool in AWS Cognito: A User Pool is a user directory in AWS Cognito where users can sign up, sign in, and manage their profiles. When a user signs up or signs in, the user pool authenticates the user's credentials and issues JSON web tokens (JWTs) to authorized users. By creating a User Pool, you can enable sign-up and sign-in for users and apply policies such as password strength and expiry rules.\nB. Enable \"Block use\" for compromised credentials in the Advanced Security section: The Advanced Security section in AWS Cognito provides an option to block user accounts in case of any unusual sign-in activity or when a user's credentials are compromised. By enabling this feature, AWS Cognito will monitor user accounts for any unusual activity and automatically block the account if it detects any suspicious behavior.\nC. (Incorrect) There is no option for \"Block use\" for compromised credentials in the Basic Security section. The Basic Security section in AWS Cognito provides a basic level of security for user accounts. It does not provide the option to block user accounts for any unusual activity or when a user's credentials are compromised. Therefore, option C is incorrect.\nD. (Incorrect) Verify sign-in operation on Cognito using Secure Remote Password (SRP): Secure Remote Password (SRP) is a cryptographic protocol used for password-based authentication. It is a secure way to protect user credentials from being intercepted during the authentication process. However, this option does not address the requirement of forcing users with compromised credentials to create new passwords. Therefore, option D is incorrect.\nIn summary, options A and B are correct. To ensure that users with compromised credentials need to use new passwords, create a User Pool in AWS Cognito and enable \"Block use\" for compromised credentials in the Advanced Security section.\n\n"
}, {
  "id" : 313,
  "question" : "You are currently a Business Intelligence developer for a company.\nMany data sources that are defined for the Logistics team in your company are hosted in AWS.\nThey are now looking for a quick solution to build visualization screens around the data hosted in AWS.\nWhich of the following can be used to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "eb4b918b46a44591a50873cb8d2dd714",
    "option" : "AWS Redshift",
    "isCorrect" : "false"
  }, {
    "id" : "9485e03789d042adaabfa3ae30cd01a9",
    "option" : "AWS Quicksight",
    "isCorrect" : "true"
  }, {
    "id" : "a555d71c1ef84731ae59c2122c62a4a1",
    "option" : "AWS Glue",
    "isCorrect" : "false"
  }, {
    "id" : "f546b80a58fd4984bff769a59e545508",
    "option" : "AWS DynamoDB.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis is given in the AWS Documentation.\nAmazon QuickSight is a business analytics service you can use to build visualizations, perform ad hoc analysis, and get business insights from your data.\nIt can automatically discover AWS data sources and also works with your data sources.\nAmazon QuickSight enables organizations to scale to hundreds of thousands of users and deliver responsive performance using a robust in-memory engine (SPICE).\nOption A is invalid because this is a data warehousing solution.\nOption C is invalid because this service is a fully managed ETL (extract, transform, and load) service.\nOption D is invalid because this is a fully managed NoSQL database.\nFor more information on AWS Quicksight, please refer to the below URL-\nhttps://docs.aws.amazon.com/quicksight/latest/user/welcome.html\n\nOption B, AWS Quicksight can be used to fulfill the requirement of building visualization screens around the data hosted in AWS.\nAWS Quicksight is a Business Intelligence (BI) tool provided by Amazon Web Services (AWS) that allows users to visualize and gain insights from their data. It provides users with an easy-to-use interface to create charts, graphs, and other visualizations based on their data, making it an ideal tool for data analysis and visualization.\nAWS Redshift is a cloud-based data warehousing service provided by AWS, used to store and analyze large amounts of data in a scalable manner. While it can be used for data analysis and visualization, it is more suitable for storing large datasets and performing complex queries.\nAWS Glue is a fully-managed Extract, Transform, and Load (ETL) service that makes it easy to move data between data stores, and prepare and transform data for analysis. It can be used to prepare data for visualization, but it is not a visualization tool itself.\nAWS DynamoDB is a fully-managed NoSQL database service provided by AWS, used to store and retrieve data in a flexible and scalable manner. While it can be used to store data for analysis, it does not provide any visualization capabilities.\nTherefore, AWS Quicksight is the most appropriate option for fulfilling the requirement of building visualization screens around the data hosted in AWS.\n\n"
}, {
  "id" : 314,
  "question" : "Your team is currently publishing items to an S3 bucket.\nYou need to record the size of the objects in a separate DynamoDB table.\nHow can you ensure that each uploaded object triggers a record in the DynamoDB table in an ideal manner? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "eab0dc33e98543098dd99cf5c4bd3291",
    "option" : "Create a new SQS queue.",
    "isCorrect" : "false"
  }, {
    "id" : "e86a7c9e2c10436db8c791f5c9a541d2",
    "option" : "Create a new Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "b84b5ee686f7428094d0fa81a5e06919",
    "option" : "Add the SQS queue to the source event for the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "7f98abfdc50c4668ad5ee75c6e286918",
    "option" : "Add the Lambda function to the source event for the S3 bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThis is given in the AWS Documentation.\nAmazon S3 can publish events (for example, when an object is created in a bucket) to AWS Lambda and invoke your Lambda function by passing the event data as a parameter.\nThis integration enables you to write Lambda functions that process Amazon S3 events.\nIn Amazon S3, you add a bucket notification configuration that identifies the type of event that you want Amazon S3 to publish and the Lambda function that you want to invoke.\nOptions A and C are incorrect since the ideal option would be to create a Lambda function that could be used to automatically record the data size and then place a record in the DynamoDB table.\nFor more information on S3 with Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3.html\n\nTo record the size of the objects in a separate DynamoDB table each time an object is uploaded to the S3 bucket, we need to create a Lambda function that will be triggered by S3 bucket events.\nTherefore, the correct answers are B and D.\nHere is how it works:\nB. Create a new Lambda function To create the Lambda function, we will need to follow these steps:\nLog in to the AWS Management Console and navigate to the AWS Lambda console. Click on the \"Create function\" button. Choose \"Author from scratch\". Give your function a name, choose your preferred runtime, and select \"Create a new role with basic Lambda permissions\" in the \"Permissions\" section. Click on \"Create function\".\nD. Add the Lambda function to the source event for the S3 bucket After creating the Lambda function, we need to configure it to be triggered by S3 bucket events. Here is how to do it:\nIn the AWS Lambda console, select the function you just created. Click on the \"Add trigger\" button. Choose \"S3\" from the list of triggers. Select the S3 bucket from the dropdown menu. Choose \"All object create events\" for the Event type. Click on \"Add\".\nNow every time an object is uploaded to the S3 bucket, the Lambda function will be triggered and will record the size of the object in the DynamoDB table.\n\n"
}, {
  "id" : 315,
  "question" : "You are working on a system that will use AWS Kinesis, and it is getting data from various log sources.\nYou are looking at creating an initial number of shards for the Kinesis stream.\nWhich of the following can be used to calculate the initial number of shards for the Kinesis stream? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "60c17286d29348f998bfa3a6b3e99fdd",
    "option" : "Incoming write bandwidth",
    "isCorrect" : "true"
  }, {
    "id" : "7023f468789d4e2c81b024b62ad701e0",
    "option" : "Outgoing write bandwidth",
    "isCorrect" : "false"
  }, {
    "id" : "a276dc0b42b6424c9be51d59a6aeb2ad",
    "option" : "Incoming read bandwidth",
    "isCorrect" : "false"
  }, {
    "id" : "069f5f6a8e034b75a1362241fd8b7633",
    "option" : "Outgoing read bandwidth.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThis is given in the AWS Documentation.\nYou can calculate the initial number of shards (\nnumber_of_shards.\n) that your stream needs by using the input values in the following formula:\nnumber_of_shards = max(incoming_write_bandwidth_in_KiB/1024, outgoing_read_bandwidth_in_KiB/2048)\nFor more information on Amazon Kinesis streams, please refer to the below URL-\nhttps://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html\n\n\nWhen designing an Amazon Kinesis data stream, one of the most important factors to consider is the number of shards. A shard represents a unit of data capacity in a Kinesis stream, and it can process up to 1MB of data per second or 1000 write transactions per second. Therefore, the number of shards you create for a Kinesis stream determines the maximum amount of data that the stream can handle.\nWhen deciding on the number of shards to create for a Kinesis stream, there are several factors to consider, such as the amount of incoming write bandwidth, the expected outgoing write bandwidth, and the incoming and outgoing read bandwidth. Based on these factors, you can use the following approaches to determine the initial number of shards for your Kinesis stream:\nIncoming write bandwidth: One way to determine the initial number of shards for a Kinesis stream is to consider the expected incoming write bandwidth. To do this, you can estimate the total amount of data that the stream is expected to receive per second and divide it by the maximum amount of data that a single shard can handle (1 MB/s). For example, if you expect the stream to receive 5 MB/s of data, you would need to create five shards to handle the incoming write bandwidth. Incoming and outgoing read bandwidth: Another approach to determine the initial number of shards for a Kinesis stream is to consider the expected incoming and outgoing read bandwidth. To do this, you can estimate the maximum number of read transactions per second that the stream is expected to receive and divide it by the maximum number of read transactions that a single shard can handle (5 read transactions/s). For example, if you expect the stream to receive 10,000 read transactions per second, you would need to create 2,000 shards to handle the incoming and outgoing read bandwidth.\nOutgoing write bandwidth and outgoing read bandwidth are not used to calculate the initial number of shards for a Kinesis stream, as these factors are determined by the number of consumers and the amount of data they are consuming from the stream.\nIn summary, when determining the initial number of shards for a Kinesis stream, you should consider the expected incoming write bandwidth and the expected incoming and outgoing read bandwidth.\n\n"
}, {
  "id" : 316,
  "question" : "Your development team has developed a series of Docker containers that will be part of an application.\nThe deployment team is looking at using the Elastic Container service for hosting these containers.\nWhich of the following are 2 possible data sources for storing the Docker-based images?\n",
  "answers" : [ {
    "id" : "4dc2c5a171324fe8885c2164c699ddcf",
    "option" : "On the EC2 Instances",
    "isCorrect" : "false"
  }, {
    "id" : "ce372b3f9daa4e0b8b19b23f2fdcbabc",
    "option" : "In Docker Hub",
    "isCorrect" : "true"
  }, {
    "id" : "eeb6b0a0839e449089929d70e7fb71c3",
    "option" : "In the Elastic Container Registry",
    "isCorrect" : "true"
  }, {
    "id" : "52d10cddf4de4595b90cbd543cf374a5",
    "option" : "Store them as Amazon Machine Images.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe AWS Documentation architecture diagram of the Elastic Container service shows the 2 sources from where you can download the Docker containers.\nBecause the AWS Documentation clearly mentions this, all other options are invalid.\nFor more information on the Amazon Container Service, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n\n\nElastic Container Service (ECS) is a container management service that allows you to run, stop, and manage Docker containers on a cluster of EC2 instances. In order to deploy your Docker containers on ECS, you need to store your Docker images in a container registry.\nHere are the possible data sources for storing Docker-based images that you can use with ECS:\nA. On the EC2 Instances: You can store your Docker images on the EC2 instances that will run your ECS tasks. However, this is not a recommended approach as it can make managing the images difficult and may cause scaling and availability issues.\nB. In Docker Hub: Docker Hub is a cloud-based repository for Docker images. You can push your Docker images to Docker Hub and then use them in your ECS tasks. However, you should be aware that Docker Hub is a public registry and anyone can access your images if you do not configure them properly.\nC. In the Elastic Container Registry (ECR): ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. You can push your Docker images to ECR and then use them in your ECS tasks. ECR provides several benefits, including security, scalability, availability, and integration with other AWS services.\nD. Store them as Amazon Machine Images (AMIs): AMIs are pre-configured virtual machine images that can be used to launch EC2 instances. You can create an AMI from a Docker container and then use it to launch EC2 instances that will run your ECS tasks. However, this approach is not recommended as it can make managing the images difficult and may cause scaling and availability issues.\nTherefore, the two recommended options for storing Docker-based images when using ECS are in the Elastic Container Registry (ECR) or in Docker Hub.\n\n"
}, {
  "id" : 317,
  "question" : "You are setting out IAM policies for allowing users access to objects in an S3 bucket.\nYou have configured an IAM policy for testing which currently works as intended.\nYou try to create a more restrictive policy but find out that the changes are not working as intended.\nWhat can you do to resolve the issue in the EASIEST way possible?\n",
  "answers" : [ {
    "id" : "e8050100a68249a8950d1ce693e15284",
    "option" : "Delete the current version of the IAM policy and recreate the older one.",
    "isCorrect" : "false"
  }, {
    "id" : "7e65e5a6961f438199865046a6c3a314",
    "option" : "Revert back to the previous version of the IAM policy.",
    "isCorrect" : "true"
  }, {
    "id" : "3903ef3b48c84f7f9c6af5769b9bfe7f",
    "option" : "Recreate the IAM users again.",
    "isCorrect" : "false"
  }, {
    "id" : "dbbc92f8594c4d348bdc9a74a7cd436b",
    "option" : "Use the recycle bin to get the deleted IAM policies back.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nYou create a customer-managed policy that allows users to administer a particular Amazon S3 bucket using the AWS Management Console.\nUpon creation, your customer-managed policy has only one version, identified as v1, so that version is automatically set as the default.\nThe policy works as intended.\nLater, you update the policy to add permission to administer a second Amazon S3 bucket.\nIAM creates a new version of the policy, identified as v2, that contains your changes.\nYou set version v2 as the default, and a short time later, your users report that they lack permission to use the Amazon S3 console.\nIn this case, you can roll back to version v1 of the policy, which you know works as intended.\nTo do this, you set version v1 as the default version.\nYour users are now able to use the Amazon S3 console to administer the original bucket.\nBecause the AWS Documentation clearly mentions this, all other options are invalid.\nFor more information on the Amazon Container Service, please refer to the below URL-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-versioning.html\n\nThe easiest way to resolve the issue of a more restrictive IAM policy not working as intended is to revert back to the previous version of the IAM policy (option B).\nWhen you create a new version of an IAM policy, the previous version is automatically saved, which allows you to easily revert back to the previous version if needed.\nTo revert back to the previous version of an IAM policy:\nNavigate to the IAM console. Click on \"Policies\" in the left-hand menu. Select the policy in question. Click on the \"Versions\" tab. Find the previous version you want to revert to and click \"Make default version.\" Verify that the previous version is now the default version by checking the \"Version ID\" column.\nBy reverting back to the previous version, you can ensure that the previous, working policy is in place while you troubleshoot the issue with the more restrictive policy. Once you have identified and fixed the issue, you can create a new version of the policy with the necessary changes.\n\n"
}, {
  "id" : 318,
  "question" : "Your developers have been given access to a CodeCommit Repository.\nYou need to ensure that if any changes are made to a repository, notifications are sent accordingly.\nWhich of the below 2 destinations can be used for the notifications.\n",
  "answers" : [ {
    "id" : "96961a8f77014963b39e836f9cf63d3f",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "5c92bbfa08b54bfa8e299ee355b52c75",
    "option" : "AWS SNS",
    "isCorrect" : "true"
  }, {
    "id" : "65a21d9527ce42e990139eed51897412",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "be46d44041fe4a22be3cd0ec40f08452",
    "option" : "AWS IAM.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS Documentation mentions the following.\nYou can configure an AWS CodeCommit repository so that code pushes or other events trigger actions, such as sending a notification from Amazon Simple Notification Service (Amazon SNS) or invoking a function in AWS Lambda.\nYou can create up to ten triggers for each AWS CodeCommit repository.\nOption C is incorrect since this is used to monitor configuration changes.\nOption D is incorrect since this is used for managing IAM users.\nFor more information on the notification, please refer to the below URL-\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/how-to-notify.html\n\nThe correct answer is B. AWS SNS.\nAWS SNS (Simple Notification Service) is a fully managed messaging service that enables the sending of messages to various recipients or endpoints such as email, text messaging, mobile push notifications, and HTTP endpoints. SNS can be used to send notifications whenever a change occurs in the CodeCommit repository.\nHere's how it works:\nCreate an SNS topic: First, you need to create an SNS topic, which is a logical access point for receiving and publishing messages. The topic can be created via the AWS Management Console, AWS CLI, or AWS SDKs. Subscribe to the SNS topic: Next, you need to subscribe to the SNS topic. A subscription is a way to receive messages from a topic. You can subscribe to an SNS topic using email, SMS, or an HTTP/S endpoint. Configure CodeCommit repository: Once the SNS topic is created and subscribed, you need to configure the CodeCommit repository to publish messages to the SNS topic whenever a change is made to the repository. Create a notification rule: Finally, you need to create a notification rule in CodeCommit that specifies the SNS topic to publish messages to whenever a change is made to the repository.\nAWS Lambda, on the other hand, is a serverless compute service that allows running code without provisioning or managing servers. It can be used to process events generated by CodeCommit, but it is not suitable for sending notifications.\nAWS Config is a service that enables you to assess, audit, and evaluate the configuration of AWS resources. It does not support sending notifications for CodeCommit changes.\nAWS IAM (Identity and Access Management) is a service that enables controlling access to AWS resources. It is not used for sending notifications.\nTherefore, the correct answer is B. AWS SNS.\n\n"
}, {
  "id" : 319,
  "question" : "Your development team is planning on using the AWS Batch service to process a high number of intensive performance computing jobs.\nWhich of the following integration services with AWS Batch will allow you to monitor the jobs' progress?\n",
  "answers" : [ {
    "id" : "ac8a8c230b5e4854b8567ba1f5eea285",
    "option" : "AWS Cloudtrail",
    "isCorrect" : "false"
  }, {
    "id" : "be63b26983044ad8b6170def8e7ee609",
    "option" : "AWS Cloudwatch Events",
    "isCorrect" : "true"
  }, {
    "id" : "27fbb42e6f1e4d859aded262aed0351f",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "e5137028db8b4c9cbcd2b0332250e772",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nUsing CloudWatch Events, you can monitor jobs' progress, build AWS Batch custom workflows with complex dependencies, generate usage reports or metrics around job execution, or build your own custom dashboards.\nWith AWS Batch and CloudWatch Events, you can eliminate scheduling and monitoring code that continuously polls AWS Batch for job status changes.\nInstead, handle AWS Batch job state changes asynchronously using any CloudWatch Events target, such as AWS Lambda, Amazon Simple Queue Service, Amazon Simple Notification Service, or Amazon Kinesis Data Streams.\nOption A is incorrect since this is an API Monitoring service.\nOption C is incorrect since this is used to monitor configuration changes.\nOption D is incorrect since this is used as a messaging service.\nFor more information on the Cloudwatch event stream, please refer to the below URL-\nhttps://docs.aws.amazon.com/batch/latest/userguide/cloudwatch_event_stream.htm\n\nThe correct answer is B. AWS Cloudwatch Events.\nAWS Batch is a managed service that enables you to run batch computing workloads on the AWS Cloud. It allows you to efficiently run hundreds of thousands of batch computing jobs on AWS, and dynamically provisions the optimal quantity and type of compute resources based on the volume and specific resource requirements of the batch jobs submitted.\nTo monitor the jobs' progress, AWS Batch integrates with several AWS services, including AWS Cloudwatch Events, AWS Cloudwatch Logs, and AWS CloudTrail.\nAWS Cloudwatch Events is a service that allows you to monitor events in AWS resources and services, as well as respond to them. It can help you track and respond to changes in your AWS Batch jobs' status, such as when they start or stop running, and it can also trigger actions in response to those events, such as sending a notification or invoking a Lambda function.\nAWS CloudTrail is a service that provides a record of API calls made in your AWS account. While it can be useful for auditing purposes, it does not provide real-time monitoring of your AWS Batch jobs' progress.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources for compliance purposes. It does not provide real-time monitoring of your AWS Batch jobs' progress.\nAWS SQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. While it can be used in conjunction with AWS Batch to manage the job queue, it does not provide real-time monitoring of your AWS Batch jobs' progress.\n\n"
}, {
  "id" : 320,
  "question" : "Your team is working on an application that is going to work with a DynamoDB table.\nAt the design stage, you are trying to find out the optimum way to define partition keys and secondary indexes.\nWhich of the following are recommendations for defining secondary indexes? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "3c4b7033a2d74f2d978b7bb5aeb7e48b",
    "option" : "Keep the number of indexes to a minimum.",
    "isCorrect" : "true"
  }, {
    "id" : "374bf629a9884a19bf177394f1023e5d",
    "option" : "Define as many indexes as possible to maximize performance.",
    "isCorrect" : "false"
  }, {
    "id" : "e9c8a2a3250e4177b61eb298e9d84078",
    "option" : "Avoid indexing tables that experience heavy write activity.",
    "isCorrect" : "true"
  }, {
    "id" : "71130169b0ea4d30b422acd33bac4525",
    "option" : "Add indexes to tables that experience heavy write activity.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS Documentation mentions the following.\nKeep the number of indexes to a minimum.\nDon't create secondary indexes on attributes that you don't query often.\nIndexes that are seldom used contribute to increased storage and I/O costs without improving application performance.\nAvoid indexing tables that experience heavy write activity.\nFor example, in a data capture application, the cost of I/O operations required to maintain an index on a table with a very high write load can be significant.\nIf you need to index data in such a table, it may be more effective to copy the data to another table with the necessary indexes and queries there.\nSince the documentation mentions this clearly, all other options are invalid.\nFor more information on working with indexes, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html\n\nWhen working with DynamoDB tables, it's important to carefully define partition keys and secondary indexes to optimize performance. Secondary indexes are used to enable more efficient querying of data in DynamoDB.\nRegarding the recommendations for defining secondary indexes, option A and C are the correct answers:\nA. Keep the number of indexes to a minimum: This is a best practice recommendation because the more indexes you have, the more storage and throughput capacity is consumed. Each index consumes storage and adds write capacity units (WCUs) and read capacity units (RCUs) to your DynamoDB table. So, it's recommended to create only the necessary indexes that are required to support the application's query patterns.\nC. Avoid indexing tables that experience heavy write activity: This is also a best practice recommendation because secondary indexes incur additional write costs. Whenever a write operation is performed on a DynamoDB table, the same operation needs to be performed on all secondary indexes associated with the table. Therefore, adding too many secondary indexes to a table with high write activity can cause performance degradation and consume more resources than necessary.\nOption B and D are incorrect:\nB. Define as many indexes as possible to maximize performance: This is not a best practice recommendation because adding too many secondary indexes to a DynamoDB table can negatively impact performance and consume more resources than necessary. It's important to create only the necessary indexes that are required to support the application's query patterns.\nD. Add indexes to tables that experience heavy write activity: This is not a best practice recommendation because adding too many secondary indexes to a table with high write activity can cause performance degradation and consume more resources than necessary. It's recommended to avoid indexing tables that experience heavy write activity unless it's absolutely necessary to support the application's query patterns.\n\n"
}, {
  "id" : 321,
  "question" : "Your team is working on an application that will connect to a MySQL RDS Instance.The security mandate is that the connection to the database from the application should be encrypted.\nHow can you accomplish this?\n",
  "answers" : [ {
    "id" : "0d1d57ff8b5a41b0ab890444be8501c5",
    "option" : "By using Access Keys assigned to an IAM user",
    "isCorrect" : "false"
  }, {
    "id" : "b6d6559b8417475bae0dfcd235b08e00",
    "option" : "By using Private Key pairs",
    "isCorrect" : "false"
  }, {
    "id" : "d24bdb11230e4894a7609d467801de8c",
    "option" : "By using SSL",
    "isCorrect" : "true"
  }, {
    "id" : "b7dd0e71957646afbaeaf22f77caf02b",
    "option" : "By using KMS Keys.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nOption A is incorrect since this is used for programmatic access for a user.\nOption B is incorrect since this is used for connection to an EC2 Instance.\nOption D is incorrect since it is normally used for encrypting data at rest or before data is sent in transit.\nFor more information on using RDS with SSL, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL.html\n\n\nThe correct answer to the question is C. By using SSL.\nSSL (Secure Sockets Layer) is a protocol used to encrypt data in transit between a client and a server. When using SSL, the connection between the client and server is secured by encrypting the data that is being transmitted. This helps to protect against unauthorized access and data breaches.\nTo enable SSL for a MySQL RDS instance, you can configure the instance to use SSL by following these steps:\nCreate an SSL certificate - Amazon RDS provides an SSL certificate that you can use to enable SSL for your MySQL RDS instance. Alternatively, you can use your own SSL certificate. Configure the MySQL RDS instance - You can configure the MySQL RDS instance to use SSL by modifying the instance's parameter group. You will need to set the value of the 'ssl' parameter to '1', and then provide the SSL certificate details. Configure the client - Once the MySQL RDS instance has been configured to use SSL, you will need to configure the client to use SSL as well. This will typically involve updating the client's connection string to specify the SSL parameters.\nIt's worth noting that SSL can provide strong encryption for data in transit, but it does not protect against other types of attacks, such as SQL injection attacks. Therefore, it's important to take additional measures to protect against these types of attacks, such as implementing input validation and using parameterized queries.\nTo summarize, to accomplish encryption of the connection between the application and MySQL RDS instance, SSL protocol can be used to secure the data in transit. Therefore, option C is the correct answer.\n\n"
}, {
  "id" : 322,
  "question" : "You are planning to develop and deploy a Node.js Lambda function.\nThe code has a dependency on a lot of third-party libraries.\nWhich of the following needs to be done to ensure that the code can be executed in the AWS Lambda service?\n",
  "answers" : [ {
    "id" : "9b4f01183c2a4a63bec187d0ff2dc611",
    "option" : "Install the third-party libraries in the Lambda service.",
    "isCorrect" : "false"
  }, {
    "id" : "c9339c7b9caf4d1794d8a31c07a8076a",
    "option" : "Create a deployment package with your code and the third-party libraries.",
    "isCorrect" : "true"
  }, {
    "id" : "61447a1848cf41c2969bf7218640843d",
    "option" : "Use Cloudformation templates to deploy the third-party libraries.",
    "isCorrect" : "false"
  }, {
    "id" : "2291b348fafb452a8c904d7326ff851f",
    "option" : "Use an IAM Role with the required permissions on those libraries.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nIf you are writing code that uses other resources, such as a graphics library for image processing, or you want to use the AWS CLI instead of the console, you need to first create the Lambda function deployment package, and then use the console or the CLI to upload the package.\nBecause of what is mentioned in the AWS Documentation, all other options are invalid.\nFor more information on creating the deployment package, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/nodejs-create-deployment-pkg.html\n\nWhen developing and deploying a Node.js Lambda function that depends on third-party libraries, you need to create a deployment package that includes your code and the required third-party libraries. Therefore, the correct answer is B.\nAWS Lambda is a serverless compute service that allows you to run your code without provisioning or managing servers. When you create a Lambda function, you need to provide the code that will be executed by the service. If your code has dependencies on third-party libraries, you need to include them in the deployment package.\nA) Installing third-party libraries in the Lambda service is not possible. AWS Lambda provides a runtime environment with specific versions of programming languages and libraries installed. You cannot install additional libraries at runtime.\nC) Using CloudFormation templates to deploy third-party libraries is not the best approach, as CloudFormation is a service for defining and deploying AWS infrastructure resources. While you can use CloudFormation to create Lambda functions, it is not necessary to use it to deploy libraries.\nD) Using an IAM role with the required permissions on those libraries is not applicable in this case. IAM roles are used to manage permissions for AWS resources and services, not for third-party libraries.\nIn summary, the correct answer is B: Create a deployment package with your code and the third-party libraries. You can create the deployment package using a tool like npm, which allows you to package your code and dependencies together. Once you have the deployment package, you can upload it to AWS Lambda to create your function.\n\n"
}, {
  "id" : 323,
  "question" : "You have to create a DynamoDB table called Customers which will have two attributes.\nOne is the ID which will be the partition key, and the other is the Name which will be the sort key.\nWhich of the following is the right definition for the CLI command that would be used to create the table?\n",
  "answers" : [ {
    "id" : "57f69f4f71f74b51863ed05ad98febe8",
    "option" : "aws dynamodb create-table \\ --table-name Customers \\ --attribute-definitions \\ AttributeName=ID,AttributeType=N \\ AttributeName=Name,AttributeType=S \\ --key-schema \\ AttributeName=ID,KeyType=HASH \\ AttributeName=Name,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5",
    "isCorrect" : "true"
  }, {
    "id" : "7d19b0d5ac8342c9a303244826828de3",
    "option" : "aws dynamodb create-table \\ --table-name Customers \\ --attribute-definitions \\ AttributeName=ID,AttributeType=N \\ AttributeName=Name,AttributeType=S \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5",
    "isCorrect" : "false"
  }, {
    "id" : "fcceff3d89a543e1bd4cada0dda5f66a",
    "option" : "aws dynamodb create-table \\ --table-name Customers \\ --attribute-definitions \\ AttributeName=ID,AttributeType=N \\ AttributeName=Name,AttributeType=S \\ --key-schema \\ AttributeName=Name,KeyType=HASH \\ AttributeName=ID,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5",
    "isCorrect" : "false"
  }, {
    "id" : "c59323ac483c444bbad6bef617c32d08",
    "option" : "aws dynamodb set-table \\ --table-name Customers \\ --attribute-definitions \\ AttributeName=ID,AttributeType=N \\ AttributeName=Name,AttributeType=S \\ --key-schema \\ AttributeName=ID,KeyType=HASH \\ AttributeName=Name,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - A.\nAn example of this is given in the AWS Documentation.\nOption B is incorrect since the keys are not being defined here.\nOption C is incorrect since the Name key should be RANGE and the ID should be HASH.\nOption D is incorrect since the right CLI command is create-table.\nFor more information on working with tables, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithTables.Basics.html\n\n\nThe correct CLI command to create the DynamoDB table with the specified attributes is:\nsql Copy code aws dynamodb create-table \\ --table-name Customers \\ --attribute-definitions \\ AttributeName=ID,AttributeType=N \\ AttributeName=Name,AttributeType=S \\ --key-schema \\ AttributeName=ID,KeyType=HASH \\ AttributeName=Name,KeyType=RANGE \\ --provisioned-throughput \\ ReadCapacityUnits=10,WriteCapacityUnits=5\nExplanation:\nThe aws dynamodb create-table command is used to create a new DynamoDB table. The command takes several parameters, which are explained below:\n--table-name : This specifies the name of the table to be created. In this case, the table name is \"Customers\".\n--attribute-definitions : This parameter defines the attributes of the table. In this case, the table has two attributes: \"ID\" and \"Name\". The ID attribute is of type \"N\" (number), and the Name attribute is of type \"S\" (string).\n--key-schema : This parameter specifies the primary key schema of the table. The primary key consists of two attributes: the partition key and the sort key. In this case, the partition key is \"ID\", and the sort key is \"Name\". The partition key is designated as the hash key using the KeyType=HASH parameter, and the sort key is designated as the range key using the KeyType=RANGE parameter.\n--provisioned-throughput : This parameter specifies the initial provisioned throughput capacity for the table. The provisioned throughput consists of two values: the read capacity units (RCUs) and the write capacity units (WCUs). In this case, the table is provisioned with 10 RCUs and 5 WCUs.\nTherefore, option C is the correct answer. Option A is incorrect because it specifies the key schema in reverse order, with the Name attribute designated as the partition key and the ID attribute designated as the sort key. Option B is incorrect because it does not specify the key schema. Option D is incorrect because it uses the set-table command, which is not used to create a new table.\n\n"
}, {
  "id" : 324,
  "question" : "You have to develop an Orders processing system.\nThe system needs to store the Product Information which contains the image for every product.\nWhich of the following implementation steps should be used when storing the Product related data? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9c848519fd8744eb9353e34a4a84bb47",
    "option" : "Store the Product ID, Name, Price and the path of image stored in the S3 bucket in a DynamoDb table.",
    "isCorrect" : "true"
  }, {
    "id" : "7536be9547a7481593555521e44952ed",
    "option" : "Store the Product Image as an attribute in the same table.",
    "isCorrect" : "false"
  }, {
    "id" : "9c69ff9254774254b401db5e159625b6",
    "option" : "Store the Product ID, Name and price in an S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "2739da23ade14dd8be3680213817a083",
    "option" : "Store the Product Image in an S3 bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThis is also mentioned in the AWS Documentation.\nAs mentioned above, you can also take advantage of Amazon Simple Storage Service (Amazon S3) to store large attribute values that cannot fit in a DynamoDB item.\nYou can store them as objects in Amazon S3 and then store the object identifier in your DynamoDB item.\nYou can also use the object metadata support in Amazon S3 to provide a link back to the parent item in DynamoDB.\nStore the primary key value of the item as Amazon S3 metadata of the object in Amazon S3\nDoing this often helps with the maintenance of the Amazon S3 objects.\nOption B is incorrect since the Images should ideally be stored as an object in S3.\nOption C is incorrect since the ID, and Name data should ideally be stored in a table.\nFor more information on best developer practices for DynamoDB, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/best-practices.html\n\nThe correct answers are A and D.\nA. Store the Product ID, Name, Price and the path of image stored in the S3 bucket in a DynamoDb table: This is a recommended approach. It involves creating a DynamoDB table with columns for Product ID, Name, Price, and the path of the image stored in the S3 bucket. By storing the image path in the DynamoDB table, it makes it easier to retrieve the image from the S3 bucket when needed.\nB. Store the Product Image as an attribute in the same table: This is not a recommended approach. While it is possible to store images as attributes in DynamoDB, it is not recommended as it can cause issues with the size of the table and slow down queries.\nC. Store the Product ID, Name and price in an S3 bucket: This is not a recommended approach. S3 is not a database, and while it is possible to store data in it, it is not designed for that purpose. It's better to use a database such as DynamoDB for storing structured data.\nD. Store the Product Image in an S3 bucket: This is a recommended approach. S3 is designed for storing large objects such as images, videos, and other media files. By storing the image in an S3 bucket, it ensures that the images are easily accessible and can be easily retrieved when needed.\n\n"
}, {
  "id" : 325,
  "question" : "Your team plans to deliver content to users by using the CloudFront service and an S3 bucket as the source.\nYou need to ensure that a custom value is placed for the amount of time the object is stored in the CloudFront cache.\nWhich 2 of the following options can be used to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "fd1aeac0dae0450babf5c6f3728c8e8b",
    "option" : "Configure the origin to add an Expires header field to the object.",
    "isCorrect" : "true"
  }, {
    "id" : "3d0baac71fd94c8584d0ef23fefbc03d",
    "option" : "Configure the Cloudfront distribution to add an Expires header field to the object.",
    "isCorrect" : "false"
  }, {
    "id" : "83241121b92040fdb188d2ef29e8ad0f",
    "option" : "Specify a value for Minimum TTL in CloudFront cache behaviors.",
    "isCorrect" : "true"
  }, {
    "id" : "0e7d293897264d21a1ae0f855d903afd",
    "option" : "Specify a value for Minimum TTL in the origin object.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThis is also mentioned in the AWS Documentation.\nFor web distributions, to control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin, you can:\nConfigure your origin to add a Cache-Control or an Expires header field to each object.\nSpecify a value for Minimum TTL in CloudFront cache behaviors.\nUse the default value of 24 hours.\nSince this is clearly mentioned in the AWS Documentation, the other options are invalid.\nFor more information on request and response behavior for Cloudfront with S3, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RequestAndResponseBehaviorS3Origin.html\n\nTo ensure that a custom value is placed for the amount of time an object is stored in the CloudFront cache when using an S3 bucket as the source, the following options can be used:\nOption B: Configure the CloudFront distribution to add an Expires header field to the object.\nBy configuring the CloudFront distribution, you can specify the amount of time that an object remains in the cache. This can be achieved by adding an Expires header field to the object. The Expires header field specifies the time after which the object is considered stale and should no longer be served from the cache.\nOption C: Specify a value for Minimum TTL in CloudFront cache behaviors.\nAnother option to set the custom value for the amount of time an object is stored in the CloudFront cache is by setting the Minimum Time To Live (TTL) in the CloudFront cache behaviors. The Minimum TTL specifies the minimum amount of time that an object remains in the cache before CloudFront forwards another request to the origin to determine if the object has been updated.\nTherefore, options B and C can be used to fulfill the requirement of placing a custom value for the amount of time an object is stored in the CloudFront cache when using an S3 bucket as the source.\nOption A: Configuring the origin to add an Expires header field to the object is not the correct option as the origin server is not used in this case. The content is served from the S3 bucket, which is the source for CloudFront.\nOption D: Specifying a value for Minimum TTL in the origin object is also not correct as the origin server is not used in this case. The content is served from the S3 bucket, which is the source for CloudFront.\n\n"
}, {
  "id" : 326,
  "question" : "You are using a custom tool known as POSTMAN to make API requests to resources in AWS.\nPart of the job of sending requests is to sign the request.\nWhich of the following would you use to sign the API requests made to AWS?\n",
  "answers" : [ {
    "id" : "9a694c1df4ca49f3830956bf398455a9",
    "option" : "Your user name and password",
    "isCorrect" : "false"
  }, {
    "id" : "6e21e68ffe21489aa0eeccbf61cf8968",
    "option" : "A private key file",
    "isCorrect" : "false"
  }, {
    "id" : "4893a73bdc624de3a71b7471c7104488",
    "option" : "KMS keys",
    "isCorrect" : "false"
  }, {
    "id" : "df78c042ed994ea48ae08ff7a3ccf863",
    "option" : "Access Keys.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nWhen you send HTTP requests to AWS, you sign the requests so that AWS can identify who sent them.\nYou sign requests with your AWS access key, which consists of an access key ID and secret access key.\nSome requests do not need to be signed, such as anonymous requests to Amazon Simple Storage Service (Amazon S3) and some API operations in AWS Security Token Service (AWS STS) such as AssumeRoleWithWebIdentity.\nOption A is incorrect since this is used for console-based access.\nOption B is incorrect since this is used for logging onto EC2 Instances.\nOption C is incorrect since this is used for encrypting data.\nFor more information on signing API requests, please refer to the below URL-\nhttps://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html\n\nWhen making API requests to AWS, the request must be signed with a valid AWS signature. This is necessary to authenticate the request and to ensure that the request is authorized to access the requested resources. There are several options for signing API requests made to AWS, but one of the most commonly used is to use Access Keys.\nAccess Keys are a set of security credentials that are used to authenticate API requests to AWS. Access Keys consist of an Access Key ID and a Secret Access Key, which are used together to sign requests. When making a request, the Access Key ID is included in the request as part of the authentication process, and the Secret Access Key is used to sign the request.\nTo sign the request using Access Keys, you must calculate an HMAC-SHA256 signature using the Secret Access Key and a canonical string representation of the request. The resulting signature is then included in the Authorization header of the request.\nIn summary, to sign API requests made to AWS using POSTMAN, you would use Access Keys as the authentication mechanism. You would then calculate the HMAC-SHA256 signature using the Secret Access Key and a canonical string representation of the request, and include the resulting signature in the Authorization header of the request.\n\n"
}, {
  "id" : 327,
  "question" : "A company uses the Simple Notification Service to send notifications to various subscribers for their service.\nThere is a user requirement for the subscriber only to receive certain types of messages and not all messages published to the topic.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "ab40ee65352f4911951bb14c13a6d508",
    "option" : "By adding a filter policy to the topic subscription",
    "isCorrect" : "true"
  }, {
    "id" : "e6e5f4dceab04d9e949667004de35385",
    "option" : "By adding an IAM policy to the topic",
    "isCorrect" : "false"
  }, {
    "id" : "00d5fab722cd4ba2a00009034898ff51",
    "option" : "Publish the messages to an SQS queue",
    "isCorrect" : "false"
  }, {
    "id" : "0b7c37b4fe944eb9835c38fa026e6ff4",
    "option" : "Publish the messages to a Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - A.\nThe AWS Documentation mentions the following.\nBy default, a subscriber of an Amazon SNS topic receives every message published to the topic.\nA subscriber assigns a filter policy to the topic subscription to receive only a subset of the messages.\nA filter policy is a simple JSON object.\nThe policy contains attributes that define which messages the subscriber receives.\nWhen you publish a message to a topic, Amazon SNS compares the message attributes to the attributes in the filter policy for each of the topic's subscriptions.\nIf there is a match between the attributes, Amazon SNS sends the message to the subscriber.\nOtherwise, Amazon SNS skips the subscriber without sending the message to it.\nIf a subscription lacks a filter policy, the subscription receives every message published to its topic.\nSince the documentation clearly mentions this, all other options are incorrect.\nFor more information on message filtering, please refer to the below URL-\nhttps://docs.aws.amazon.com/sns/latest/dg/message-filtering.html\n\nThe correct answer is A. By adding a filter policy to the topic subscription.\nThe Simple Notification Service (SNS) is a fully managed messaging service that enables the publishing and delivery of messages to multiple subscribers or endpoints. It allows messages to be sent as text messages, emails, or to other AWS services such as Lambda functions or SQS queues.\nWhen a topic has multiple subscribers, each subscriber receives a copy of every message published to the topic by default. However, if you want certain subscribers to receive only specific types of messages, you can add a filter policy to the subscription.\nA filter policy is a JSON document that specifies the attributes and values of the messages that a subscriber is interested in receiving. You can use filter policies to select a subset of messages that match certain criteria. When a message is published to the topic, SNS evaluates the message attributes against the filter policy of each subscription. If the message attributes match the filter policy of a subscription, SNS sends the message to that subscription.\nHere is an example of a filter policy:\njson Copy code {   \"type\": [\"order_placed\", \"order_shipped\"],   \"price\": [{\"numeric\": [\">=\", 50]}, {\"numeric\": [\">=\", 100], \"anything-but\": true}] }\nIn this example, the filter policy selects messages with a type attribute that is either order_placed or order_shipped, and a price attribute that is either greater than or equal to 50 or greater than or equal to 100 (but not both). The anything-but operator negates the condition.\nTo add a filter policy to a topic subscription, follow these steps:\nOpen the Amazon SNS console. In the left navigation pane, choose \"Subscriptions\". Select the subscription that you want to modify. Choose \"Edit subscription\". In the \"Filter policy\" field, enter the filter policy as a JSON document. Choose \"Update subscription\".\nIn summary, by adding a filter policy to a topic subscription, you can specify the types of messages that a subscriber is interested in receiving and filter out unwanted messages. This is a powerful feature of SNS that allows you to customize the delivery of messages to your subscribers.\n\n"
}, {
  "id" : 328,
  "question" : "Your company has an existing Redshift cluster.\nThe sales team currently stores historical data in the cluster.\nThere is now a requirement to ensure that all data is encrypted at rest.\nWhat do you need to do on your end (as of October 2018)?\n",
  "answers" : [ {
    "id" : "1e022b29cffb4197b33604df171ce5cd",
    "option" : "Enable the encryption feature for the cluster.",
    "isCorrect" : "true"
  }, {
    "id" : "573f68e4eeb94dc5b94582042d7ed00e",
    "option" : "Enable encryption for the underlying EBS volumes.",
    "isCorrect" : "false"
  }, {
    "id" : "a15923af5f844e8e9a3f2c7bb18cb9b6",
    "option" : "Use SSL certificates to encrypt the data at rest.",
    "isCorrect" : "false"
  }, {
    "id" : "42d87d1aa6614c51bc2a24a77f367076",
    "option" : "Create a new cluster with encryption enabled and then migrate the data over to the new cluster.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nIn Amazon Redshift, you can enable database encryption for your clusters to help protect data at rest.\nWhen you enable encryption for a cluster, the data blocks and system metadata are encrypted for the cluster and its snapshots.\nEncryption is an optional, immutable setting of a cluster.\nIf you want encryption, you enable it during the cluster launch process.\nAs of October 2018, you can enable encryption on an un-encrypted cluster.\nAWS will handle migrating the data over to a new, encrypted cluster behind-the-scenes.\nOption A is CORRECT because you can now enable encryption for an existing Redshift cluster.\nPlease refer to the below link-\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/changing-cluster-encryption.html\nOption B is invalid since the encryption needs to be enabled at the cluster level.\nOption C is invalid since SSL certificates are used for the encryption of data in transit.\nOption D is incorrect because you can now enable encryption for an existing Redshift cluster and therefore creating a new Redshift cluster to enable encryption is unnecessary.\n\nThe correct answer to ensure that all data is encrypted at rest in an existing Redshift cluster as of October 2018 is:\nA. Enable the encryption feature for the cluster.\nExplanation:\nRedshift is a data warehouse service provided by Amazon Web Services (AWS). It is used to store and analyze large amounts of data in a highly scalable and cost-effective manner. Data security is crucial when working with sensitive information such as financial data, personally identifiable information (PII), and intellectual property.\nAWS provides encryption options to secure data in transit and at rest. In this case, the requirement is to encrypt data at rest. AWS Redshift provides a feature to encrypt data at rest called \"Encryption at Rest.\"\nBy enabling this feature, all data stored in the Redshift cluster will be encrypted with AES-256 encryption algorithm, including backups and snapshots. The encryption key used for this feature is managed by AWS Key Management Service (KMS), and it is automatically rotated on a regular basis.\nTherefore, to fulfill the requirement of ensuring that all data is encrypted at rest in an existing Redshift cluster, you need to enable the encryption feature for the cluster. Options B and C are not applicable in this case as they relate to different types of encryption (encryption for EBS volumes and SSL encryption respectively) and are not the appropriate solution for this requirement. Option D is also not necessary, as creating a new cluster is not required to enable encryption at rest in an existing cluster.\n\n"
}, {
  "id" : 329,
  "question" : "Your team has developed a web application that will run on an EC2 Instance.\nThere is a deployment requirement wherein if the primary application fails.\nThe requests need to be routed to a static website.\nWhich of the following can help you achieve this?\n",
  "answers" : [ {
    "id" : "7b79fc8d101c45e59e7d3a45cbda8892",
    "option" : "A Classic Load balancer placed in front of the EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "b81de143e7b7426eb301dda26d81848c",
    "option" : "An Application Load balancer placed in front of the EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "7080b741339345aab9b99cc7c2a29a75",
    "option" : "A health check in Route 53.",
    "isCorrect" : "true"
  }, {
    "id" : "cffe6991e3744519a22a79be7eac7e24",
    "option" : "The swap URL feature in Elastic Beanstalk.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAmazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources.\nEach health check that you create can monitor one of the following.\nThe health of a specified resource, such as a web server.\nThe status of other health checks.\nThe status of an Amazon CloudWatch alarm.\nOptions A and B are incorrect since the Load balancers are used to distribute traffic and not divert traffic.\nOption D is incorrect since the application is not being hosted on Elastic Beanstalk.\nFor more information on DNS failover, please refer to the below URL-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\n\nTo achieve the requirement of routing requests to a static website in case of primary application failure, we need to ensure that we have a failover mechanism in place.\nOption A: A Classic Load Balancer placed in front of EC2 Instances A Classic Load Balancer is a Layer 4 (transport layer) load balancer that distributes incoming traffic across multiple targets such as EC2 instances, containers, and IP addresses in a single availability zone or multiple availability zones. However, it does not have the ability to route traffic based on application content or provide advanced request routing features. As a result, it cannot achieve the required failover mechanism.\nOption B: An Application Load Balancer placed in front of EC2 Instances An Application Load Balancer is a Layer 7 (application layer) load balancer that is capable of distributing incoming traffic based on application content, such as HTTP requests and URL paths. It provides advanced routing features such as routing based on hostnames, paths, and query parameters. Additionally, it has the ability to route traffic to specific targets based on rules and also supports failover mechanisms by enabling health checks on the targets. Therefore, this option is a good fit for the requirement of routing requests to a static website in case of primary application failure.\nOption C: A health check in Route 53 Route 53 is a DNS web service that can be used to route traffic to various AWS resources such as EC2 instances, load balancers, and S3 buckets. It can also perform health checks on the resources and route traffic based on their health status. However, Route 53 alone cannot achieve the required failover mechanism as it cannot route traffic based on application content or provide advanced request routing features.\nOption D: The swap URL feature in Elastic Beanstalk Elastic Beanstalk is a fully managed service that makes it easy to deploy and scale applications. It provides the ability to perform blue/green deployments, which involves creating a new environment with updated code and then swapping the URLs of the old and new environments. However, this option is not a good fit for the requirement of routing requests to a static website in case of primary application failure as it does not provide failover mechanisms.\nTherefore, the best option to achieve the requirement of routing requests to a static website in case of primary application failure is to use an Application Load Balancer placed in front of the EC2 instances.\n\n"
}, {
  "id" : 330,
  "question" : "Your team has decided to host a static website using the Simple Storage Service.\nA bucket has been defined with the domain name, the objects uploaded, and the static web site hosting enabled for the bucket.\nBut you are still not able to access the web site.\nWhich of the following could be the underlying issue?\n",
  "answers" : [ {
    "id" : "7ee11aac8d044c859523b0716fb4f8cc",
    "option" : "You need to enable versioning for the bucket as well.",
    "isCorrect" : "false"
  }, {
    "id" : "2f9743cfce6c4309a7f959a01da89b89",
    "option" : "The bucket must have public read access.",
    "isCorrect" : "true"
  }, {
    "id" : "376c1f6378474eada45796439b47c42f",
    "option" : "You need to ensure the storage class is infrequent access.",
    "isCorrect" : "false"
  }, {
    "id" : "82b04215118a4c0fbe36e4df3eb44217",
    "option" : "You need to use AWS Managed Keys.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - B.\nThe AWS Documentation mentions the following.\nTo host a static website, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket.\nThis bucket must have public read access.\nIt is intentional that everyone in the world will have read access to this bucket.\nOption A is incorrect since this feature is used to avoid accidental deletion of objects.\nOption C is incorrect since the storage class should ideally be standard storage.\nOption D is incorrect since this is used for the encryption of objects at rest.\nFor more information on static web site hosting, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\n\nThe most probable issue in this scenario is that the bucket does not have public read access. By default, the objects stored in Amazon S3 buckets are private, which means that only the bucket owner has access to the data. To enable access to the static website hosted in the S3 bucket, the objects must be made publicly accessible.\nTo make the objects publicly accessible, you need to grant public read access to the bucket or the objects themselves. To grant public read access to the bucket, you can configure a bucket policy that allows access to all objects in the bucket. Here is an example bucket policy that grants public read access to all objects in the bucket:\njson Copy code {    \"Version\": \"2012-10-17\",    \"Statement\": [       {          \"Sid\": \"PublicRead\",          \"Effect\": \"Allow\",          \"Principal\": \"*\",          \"Action\": [             \"s3:GetObject\"          ],          \"Resource\": [             \"arn:aws:s3:::example-bucket/*\"          ]       }    ] }\nThis policy grants the \"s3:GetObject\" permission to all users (\"Principal\": \"\") to access all objects in the bucket with the key prefix \"/\" (\"Resource\": \"arn:aws:s3:::example-bucket/*\").\nRegarding the other options:\nA. Enabling versioning for the bucket is not required to host a static website using Amazon S3.\nC. The storage class (standard, infrequent access, etc.) does not affect the ability to access a static website hosted in an S3 bucket.\nD. Using AWS managed keys (SSE-S3) is not required to host a static website using Amazon S3.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 332,
  "question" : "You've developed an application that is going to be hosted on an EC2 Instance.\nThe company has decided to use Cloudfront to distribute the content.\nThe IT Security department has mandated that the traffic is encrypted between Cloudfront and the Viewer and Cloudfront and its origin.\nHow can you achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ba017262bc05407fbb5dc9eb89f78411",
    "option" : "Ensure that HTTP is mapped to port 443 at the origin.",
    "isCorrect" : "false"
  }, {
    "id" : "872e2885558a47879e11bbb7ebd18887",
    "option" : "Ensure that KMS keys are used to encrypt the traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "692d605923934c2387569feb8a7897f3",
    "option" : "Ensure that the Viewer Protocol policy is set to HTTPS only or Redirect HTTP to HTTPS.",
    "isCorrect" : "true"
  }, {
    "id" : "d865f964dbe44bdf97427e97c31a22c2",
    "option" : "Ensure that the Origin Protocol policy is set to HTTPS only.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nThis is given in the AWS Documentation.\nSince this is clearly given in the documentation, all other options are incorrect.\nFor more information on configuring HTTPS between the Viewer and Cloudfront and the Origin and Cloudfront, please refer to the below URLs-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html\n\n\nTo encrypt the traffic between CloudFront and the Viewer and CloudFront and its origin, you can follow these steps:\nEnsure that the Viewer Protocol policy is set to HTTPS only or Redirect HTTP to HTTPS. The Viewer Protocol Policy specifies the protocols that CloudFront will use to communicate with viewers. By setting this policy to HTTPS only or redirecting HTTP to HTTPS, you will ensure that all traffic between the viewer and CloudFront is encrypted. Ensure that the Origin Protocol policy is set to HTTPS only. The Origin Protocol Policy specifies the protocols that CloudFront will use to communicate with your origin. By setting this policy to HTTPS only, you will ensure that all traffic between CloudFront and your origin is encrypted.\nSo, by configuring the Viewer Protocol Policy to use HTTPS only or redirect HTTP to HTTPS and setting the Origin Protocol Policy to HTTPS only, you will ensure that all traffic between CloudFront and the viewer and between CloudFront and the origin is encrypted.\nOption A is incorrect because HTTP cannot be mapped to port 443. Port 443 is used for HTTPS traffic.\nOption B is also incorrect because KMS keys are not directly related to encrypting traffic between CloudFront and the viewer or CloudFront and its origin. KMS keys are used for data encryption at rest.\nTherefore, the correct answers are C and D.\n\n"
}, {
  "id" : 333,
  "question" : "Your application is currently configured to interact with an S3 bucket.\nNow you are getting errors that the bucket does not exist.\nWhich of the following is the best way to understand how the bucket was deleted?\n",
  "answers" : [ {
    "id" : "619ec9e91ddc4ecfa0a6c31b8e0eb31d",
    "option" : "Use the Cloudwatch logs to see the Bucket Deletion API request.",
    "isCorrect" : "false"
  }, {
    "id" : "5d1d21694adf4e23a17e9eb6e84f2f2f",
    "option" : "Use the Cloudtrail logs to see the Bucket Deletion API request.",
    "isCorrect" : "true"
  }, {
    "id" : "61b9b5b23994424a99a0e10ec1ba59a7",
    "option" : "Use the AWS Inspector service to see the Bucket Deletion API request.",
    "isCorrect" : "false"
  }, {
    "id" : "771d58a763c347e4a97bd6fa29a8ec9c",
    "option" : "Use the AWS Trusted Advisor service to see the Bucket Deletion API request.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nYou can use the Cloudtrail service to see when the bucket was deleted and who initiated the bucket deletion request.\nOption A is incorrect since the logs will not have the detailed information about the bucket deletion request.\nOption C is incorrect since this service is only used to check the vulnerabilities on servers.\nOption D is incorrect since this service is only used to provide recommendations.\nFor more information on Cloudtrail logging, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html\n\n\nThe best way to understand how the bucket was deleted is to use the CloudTrail logs. CloudTrail is a service that records API calls made within an AWS account. By default, it logs events for most AWS services, including S3.\nOption A, using CloudWatch logs, would not be the best choice because CloudWatch is primarily used for monitoring and alerting purposes. While it is possible to enable S3 bucket access logging to send logs to CloudWatch, it would not provide information on whether the bucket was deleted.\nOption C, using the AWS Inspector service, is a security assessment service that analyzes the behavior of applications running on EC2 instances. It is not relevant for investigating S3 bucket deletions.\nOption D, using the AWS Trusted Advisor service, provides recommendations to optimize and secure AWS resources. While it may highlight issues related to S3 bucket configurations, it would not provide information on the deletion of the bucket.\nTherefore, the correct option to choose is B, using CloudTrail logs to see the Bucket Deletion API request. By analyzing the CloudTrail logs, you can identify the user or role that made the API request to delete the bucket and investigate the cause of the deletion.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 335,
  "question" : "An application is making calls to a DynamoDB table.\nThe queries are taking on a lot of read capacity.\nThe table has a large number of attributes.\nNot all of the attributes are used in the query.\nWhich of the following can be used to minimize the read capacity being used by the queries?\n",
  "answers" : [ {
    "id" : "1cb0c7a1f3f34b47814b51247e725ffe",
    "option" : "Use global secondary indexes with projected attributes.",
    "isCorrect" : "true"
  }, {
    "id" : "28cb2a444d134ecfba27f9d70146964e",
    "option" : "Use an Application Load balancer in front of the DynamoDB table.",
    "isCorrect" : "false"
  }, {
    "id" : "5f8287ef21eb4a2ea41456edae0edb4d",
    "option" : "Consider using parallel scans on the table.",
    "isCorrect" : "false"
  }, {
    "id" : "b5d2c7526a28413e91e53bc73d70a687",
    "option" : "Use a cloudfront distribution in front of the DynamoDB table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nYou can use Global secondary indexes and use only those attributes which will be queried.\nThis can help reduce the amount of read throughput used on the table.\nOptions B and D are invalid because these are invalid architecture designs.\nOption C is incorrect since using queries is more effective.\nFor more information on general guidelines for indexes, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html\n\nThe correct answer is A. Use global secondary indexes with projected attributes.\nExplanation:\nWhen you query a DynamoDB table, it consumes read capacity units (RCUs) based on the size of the item(s) that are being read. DynamoDB charges you based on the number of RCUs that you consume.\nIf your queries are consuming a lot of read capacity, there are a few things you can do to reduce the amount of capacity that your queries are using. One option is to use global secondary indexes (GSIs) with projected attributes.\nA GSI is a separate index that you can create on a DynamoDB table. You can specify which attributes from the table should be projected (i.e., included) in the GSI. When you query the GSI, DynamoDB only needs to read the projected attributes, which can be a subset of the attributes in the table. This can reduce the amount of read capacity that your queries consume.\nFor example, if you have a table with 10 attributes, but your queries only use 2 of those attributes, you can create a GSI that only projects those 2 attributes. When you query the GSI, DynamoDB will only read those 2 attributes, which will consume less read capacity than if you were querying the entire table.\nOption B, using an Application Load Balancer (ALB) in front of the DynamoDB table, is not relevant for reducing the read capacity of queries. An ALB is used to distribute incoming traffic across multiple targets, such as EC2 instances or containers. It does not affect how much read capacity a DynamoDB query consumes.\nOption C, parallel scans on the table, is not an efficient way to minimize read capacity. Parallel scans are a way to split a large table scan into smaller segments that can be scanned concurrently. However, if your table is very large and your queries are only using a small subset of attributes, parallel scans will still consume a lot of read capacity.\nOption D, using a CloudFront distribution in front of the DynamoDB table, is not relevant for reducing the read capacity of queries. CloudFront is a content delivery network (CDN) that can cache and deliver content from multiple origins, such as S3 buckets or EC2 instances. It does not affect how much read capacity a DynamoDB query consumes.\n\n"
}, {
  "id" : 336,
  "question" : "Your team is developing a set of Lambda functions.\nYou need to ensure the team uses the best practices for working with AWS Lambda.\nChoose one of the options given below.\n",
  "answers" : [ {
    "id" : "c62ccf5af66c47268f3410a5fa924379",
    "option" : "The Lambda function should not have any external dependencies.",
    "isCorrect" : "false"
  }, {
    "id" : "a99e0f9b041d4d9fafc715fe156df8ff",
    "option" : "Avoid using environment variables.",
    "isCorrect" : "false"
  }, {
    "id" : "14526743363c437b93ac405cc1ad288d",
    "option" : "Separate the Lambda handler from your core logic.",
    "isCorrect" : "true"
  }, {
    "id" : "998b67f4a52e466d9e9666a888765e9f",
    "option" : "Use recursive code to make the code clean.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOption A is incorrect because Lambda functions can have external dependencies.\nOption B is incorrect because it is recommended to use environment variables in Lambda functions.\nOption C is CORRECT because the method allows you to make the function clean and unit-testable.\nOption D is incorrect because recursive code should be avoided in Lambda functions.\nFor more information on best practices, please refer to the below URLs-\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/running-lambda-code.html\n\nOut of the options given, the best practice for working with AWS Lambda is to separate the Lambda handler from your core logic (Option C).\nHere's a more detailed explanation:\nAWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. With Lambda, you can write code in several programming languages and execute it in response to events, such as changes to data in an Amazon S3 bucket, updates to an Amazon DynamoDB table, or custom events generated by your applications or services.\nWhen working with AWS Lambda, it's important to follow best practices to ensure that your functions are efficient, scalable, and maintainable. Some of the best practices include:\nSeparating the Lambda handler from your core logic: The Lambda handler is the entry point for your function, and it's responsible for receiving the event data and passing it to your core logic. By separating the handler from your core logic, you can make your code more modular and easier to test and maintain. You can also reuse the same core logic across multiple Lambda functions. Keeping your Lambda functions small and focused: Each Lambda function should have a single responsibility and do one thing well. This makes it easier to reason about your code, test it, and deploy it. It also helps to reduce the size and complexity of your functions, which can improve their performance and reduce their cold start time. Minimizing external dependencies: Your Lambda functions should have as few external dependencies as possible. This makes them more self-contained and easier to deploy and manage. If you need to use external libraries or modules, consider packaging them with your function or using AWS Lambda Layers. Using environment variables sparingly: Environment variables can be used to pass configuration information to your Lambda functions. However, you should use them sparingly and avoid hard-coding sensitive information in your code. Instead, consider using AWS Secrets Manager or AWS Systems Manager Parameter Store to securely store and retrieve your configuration data. Avoiding recursive code: Recursive code can be difficult to understand, debug, and optimize. Instead, consider using iterative algorithms or functional programming techniques to solve your problems.\nIn summary, when working with AWS Lambda, you should separate the Lambda handler from your core logic, keep your functions small and focused, minimize external dependencies, use environment variables sparingly, and avoid recursive code.\n\n"
}, {
  "id" : 337,
  "question" : "Your team is transitioning a stateful-based web application to AWS.\nYou need to decide the AWS services based on the functionalities that will help to design the required architecture.\nSelect TWO from the options given below.\n",
  "answers" : [ {
    "id" : "95907c62895c4e0aac77f50b0f117df5",
    "option" : "DynamoDB to store the session data",
    "isCorrect" : "true"
  }, {
    "id" : "38d325dbd0ab458ca183fe6dc4b2cc6b",
    "option" : "AWS Lambda to store the session data",
    "isCorrect" : "false"
  }, {
    "id" : "70c3da25b43e400497f0cc1938e5d660",
    "option" : "An Application Load balancer to distribute traffic",
    "isCorrect" : "true"
  }, {
    "id" : "7938a0a658ee4f9f8155e472129f3ec1",
    "option" : "An API gateway to distribute traffic.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe below example from the AWS Documentation shows how DynamoDB can be used to store session data.\nAnd the application load balancer can be used to distribute traffic for the application.\nOption B is incorrect since this service is a Compute Service.\nOption D is incorrect since this is an API management service.\nFor more information on AWS Cloud Best practices, please refer to the below URL-\nhttps://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf\n\n\nWhen transitioning a stateful-based web application to AWS, it's crucial to choose the right AWS services that can cater to the application's needs.\nOption A: DynamoDB to store the session data. DynamoDB is a fully managed NoSQL database service that can provide a scalable and high-performance database solution. It can be an excellent choice to store session data since it can support high-traffic workloads and handle large amounts of data. With DynamoDB, you can easily manage and scale the session data store, providing seamless user experiences.\nOption B: AWS Lambda to store the session data. AWS Lambda is a serverless compute service that allows you to run code without managing servers. While it's possible to store session data using Lambda, it's not the most efficient solution. Since Lambda functions have a maximum execution time limit, they may not be suitable for long-lived session data.\nOption C: An Application Load balancer to distribute traffic. An Application Load Balancer (ALB) is a fully managed service that can efficiently distribute traffic across multiple instances, containers, and microservices. When combined with Auto Scaling, it can provide a scalable and highly available architecture. By distributing traffic, you can ensure that the application can handle high-traffic loads without downtime.\nOption D: An API Gateway to distribute traffic. An API Gateway is a fully managed service that can provide secure and reliable access to backend services, including HTTP-based APIs and non-HTTP backend services. By distributing traffic using an API Gateway, you can ensure that the backend services can handle high-traffic loads, providing a seamless user experience.\nTherefore, the two best options to design the required architecture would be Option A: DynamoDB to store the session data and Option C: An Application Load balancer to distribute traffic.\n\n"
}, {
  "id" : 338,
  "question" : "An application includes an Autoscaling Group.\nIt has been determined that the best way to scale the group is based on the number of concurrent users.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "fa27c0ce9979477cbeeb449ea1d614d5",
    "option" : "Create a tag for the Group to contain the number of concurrent users.",
    "isCorrect" : "false"
  }, {
    "id" : "5196819ae5434cd0ac2d0beb0a960e47",
    "option" : "Create a custom metric for the number of concurrent users.",
    "isCorrect" : "true"
  }, {
    "id" : "72836238bbeb46e49a4c10a4198c94af",
    "option" : "Since concurrent user metrics are not available, base the scaling of the group on CPU percentage.",
    "isCorrect" : "false"
  }, {
    "id" : "fb9d0751569a49c18a0c7f47b4890333",
    "option" : "Since concurrent user metrics are not available, base the scaling of the group on Memory percentage.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nWith target tracking scaling policies, you select a predefined metric or configure a customized metric and set a target value.\nApplication Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling policy and calculates the scaling adjustment based on the metric and the target value.\nThe scaling policy adds or removes capacity as required to keep the metric at, or close to, the specified target value.\nIn addition to keeping the metric close to the target value, a target tracking scaling policy also adjusts to changes in the metric due to a changing load pattern.\nIt minimizes changes to the capacity of the scalable target.\nSince you can define a custom metric based on concurrent usage, you can scale the Autoscaling group based on that.\nBased on this, all other options are invalid.\nFor more information on auto-scaling based on target tracking, please refer to the below URL-\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html\n\nThe best way to scale an Autoscaling Group is to base the scaling on the metric that is most closely related to the performance of your application. In this case, since it has been determined that the best way to scale the group is based on the number of concurrent users, the Autoscaling Group needs to be scaled based on this metric.\nHowever, concurrent user metrics are not natively available in AWS CloudWatch, so you cannot use it directly to scale your Autoscaling Group. Therefore, you will need to create a custom metric for the number of concurrent users using Amazon CloudWatch.\nOption A, creating a tag for the group to contain the number of concurrent users, is not the best way to scale the group since it does not provide a mechanism to measure the actual number of concurrent users.\nOption B, creating a custom metric for the number of concurrent users, is the correct answer. This will require adding the necessary instrumentation to the application code to measure the number of concurrent users and push that metric to CloudWatch. Once the metric is available in CloudWatch, it can be used as a scaling metric for the Autoscaling Group.\nOptions C and D, basing the scaling of the group on CPU or Memory percentage, respectively, are not appropriate since these metrics are not closely related to the performance of the application in terms of concurrent users. Basing the scaling on these metrics may lead to over-provisioning or under-provisioning of resources, depending on the workload characteristics of the application.\n\n"
}, {
  "id" : 339,
  "question" : "You have an application storing objects in an Amazon S3 bucket.\nDue to massive popularity, these objects are frequently accessed from the S3 bucket.\nWhich of the following can be used to reduce the latency of these objects?\n",
  "answers" : [ {
    "id" : "c3ef7f4d12354778b300f71d493e525d",
    "option" : "Save objects in separate buckets in each region.",
    "isCorrect" : "false"
  }, {
    "id" : "8a24c5e67f4d49539680fd7bd46efb6a",
    "option" : "Use Amazon CloudFront in front of the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "f86478700d1c4f08b9f8ced3b893f723",
    "option" : "Increase the number of prefixes in the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "f217f57159084238be32672a71e14987",
    "option" : "Add random suffix to key names.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nFor objects stored in the Amazon S3 bucket, Amazon CloudFront can be used to achieve a very high GET performance.\nUsing Amazon CloudFront, users can access data locally with low latency &amp; high throughput from the nearest POP.\nAlso, this results in a decrease in the number of read requests directly to Amazon S3 buckets.\nOption A is incorrect as although this will lower latency for users accessing from buckets within the same region.\nBut this is not a cost-effective solution.\nOption C is incorrect as although this will increase GET performance, this will not result in low latency for users.\nOption D is incorrect as we need to add a prefix and not a suffix to the key names.\nFor more information on Performance Optimisation with Amazon S3, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/PerformanceOptimization.html\n\nThe best solution for reducing the latency of frequently accessed objects in an Amazon S3 bucket is to use Amazon CloudFront in front of the S3 bucket. This option is provided by option B in the given answers.\nAmazon CloudFront is a content delivery network (CDN) that caches content at edge locations worldwide, which can significantly reduce the latency of accessing objects. CloudFront retrieves the object from the nearest edge location to the user's request, which reduces the latency of the request.\nOption A, saving objects in separate buckets in each region, would provide some reduction in latency but would add complexity to the application and result in higher costs. In addition, it would not necessarily improve performance for all users globally.\nOption C, increasing the number of prefixes in the S3 bucket, would not reduce latency but would make it more difficult to find and access specific objects in the bucket.\nOption D, adding random suffixes to key names, would not reduce latency and could potentially make it more difficult to access objects in the bucket if the naming convention is not consistent.\nTherefore, the best option for reducing the latency of frequently accessed objects in an S3 bucket is to use Amazon CloudFront in front of the S3 bucket.\n\n"
}, {
  "id" : 340,
  "question" : "Your company has a SOAP service that receives requests in XML.\nThe service is going to be placed behind the API gateway service.\nWhich of the following must be done to ensure that the SOAP service can consume requests made to the API gateway service?\n",
  "answers" : [ {
    "id" : "f2d54c18222641348a843a41057ccbed",
    "option" : "Create a mapping template.",
    "isCorrect" : "true"
  }, {
    "id" : "3827d8d27f7b4e2eae4e4260ba684a2f",
    "option" : "Setup a gateway response.",
    "isCorrect" : "false"
  }, {
    "id" : "1899a154bb7e4780900fc231ea74fd3a",
    "option" : "Enable support for binary workloads.",
    "isCorrect" : "false"
  }, {
    "id" : "001b1773c0bf4e29b270497528be9318",
    "option" : "Setup Private integrations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nIn API Gateway, an API's method request can take a payload in a different format from the corresponding integration request payload, as required in the backend.\nSimilarly, the backend may return an integration response payload different from the method response payload, as expected by the frontend.\nAPI Gateway lets you use mapping templates to map the payload from a method request to the corresponding integration request and from an integration response to the corresponding method response.\nA mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions.\nThe payload can have a data model according to the JSON schema draft 4\nYou must define the model to have API Gateway to generate an SDK or to enable basic request validation for your API.\nYou do not have to define any model to create a mapping template.\nHowever, a model can help you create a template because API Gateway will generate a template blueprint based on a provided model.\nOption B is incorrect since this is used to customize gateway responses.\nOption C is incorrect since this is used specifically for binary workloads.\nOption D is incorrect since this is used for use within a VPC.For more information on models and mappings, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/models-mappings.html\n\nThe correct answer is A. Create a mapping template.\nExplanation:\nThe API Gateway service is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It supports many different types of web service protocols, including RESTful services and SOAP-based services.\nSOAP is an XML-based protocol that is often used for creating web services. If you want to place a SOAP service behind an API Gateway service, you need to create a mapping template. A mapping template is used to transform the incoming request from the API Gateway service into the format that the SOAP service can understand.\nTo create a mapping template, you need to specify the input and output format. The input format is the format of the incoming request, which is typically in JSON or XML format. The output format is the format that the SOAP service expects, which is typically in XML format.\nOnce you have created the mapping template, you can configure the API Gateway service to use it to transform incoming requests before forwarding them to the SOAP service.\nOption B, Setup a gateway response, is not relevant in this scenario. Gateway responses are used to customize the response that the API Gateway service sends back to the client. They are not used to transform the request that the API Gateway service sends to the backend service.\nOption C, Enable support for binary workloads, is also not relevant in this scenario. Enabling support for binary workloads is useful when you need to send binary data, such as images or videos, in your API requests. It is not necessary for handling XML-based requests.\nOption D, Setup Private integrations, is not relevant either. Private integrations are used to integrate APIs that are hosted in a VPC. They are not used to integrate SOAP services.\n\n"
}, {
  "id" : 341,
  "question" : "An application needs to make use of the SQS service for sending and receiving messages.\nThe application takes 60 seconds to process a message.\nAssuming that a queue has been created with the default settings, which one of the following must be implemented?\n",
  "answers" : [ {
    "id" : "f63b4a525370467c99b93fc88cc31bf1",
    "option" : "Call the ChangeMessageVisibility API and increase the timeout.",
    "isCorrect" : "true"
  }, {
    "id" : "77ce8082d6704b9ebea27b2cb0d3470c",
    "option" : "Call the DeleteMessage API to increase the timeout.",
    "isCorrect" : "false"
  }, {
    "id" : "2b8cb888b631463da01158159588e911",
    "option" : "Call the ChangeMessageVisibility API and decrease the timeout. Call the DeleteMessage API to delete the message.",
    "isCorrect" : "false"
  }, {
    "id" : "78d983923c6e44fbb3f95efe78e2281d",
    "option" : "Call the DeleteMessage API to delete the message from the queue first. Call the ChangeMessageVisibility API and increase the timeout.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nChanges the visibility timeout of a specified message in a queue to a new value.\nThe default visibility timeout for a message is 30 seconds.\nThe minimum is 0 seconds.\nThe maximum is 12 hours.\nFor example, you have a message with a visibility timeout of 5 minutes.\nAfter 3 minutes, you call ChangeMessageVisibility with a timeout of 10 minutes.\nYou can continue to call ChangeMessageVisibility to extend the visibility timeout to the maximum allowed time.\nIf you try to extend the visibility timeout beyond the maximum, your request is rejected.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dg.pdf\nOptions B and D are incorrect since you first need to call ChangeMessageVisibility API.\nOption C is incorrect since the ChangeMessageVisibility API should be used to increase the timeout.\nFor more information on the visibility timeout, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ChangeMessageVisibility.html\n\nThe correct answer is A. Call the ChangeMessageVisibility API and increase the timeout.\nExplanation: Amazon SQS (Simple Queue Service) is a fully managed message queuing service that enables decoupling and scalability of microservices, distributed systems, and serverless applications. In SQS, messages are stored in a queue until they are processed by a consumer.\nWhen a message is retrieved by a consumer, SQS sets an invisible timeout period, called the visibility timeout, during which the message is hidden from other consumers. This is done to prevent multiple consumers from processing the same message at the same time. If the message is processed within this timeout period, it is deleted from the queue. Otherwise, it becomes visible again and can be retrieved by another consumer.\nIn this scenario, the application takes 60 seconds to process a message, which means that the visibility timeout needs to be increased to more than 60 seconds to ensure that the message is not visible to other consumers before it is fully processed.\nOption A is the correct answer because it suggests calling the ChangeMessageVisibility API and increasing the timeout. This API allows the visibility timeout of a message to be changed, so the message remains hidden from other consumers until the application completes processing the message. Once processing is complete, the application can call the DeleteMessage API to remove the message from the queue.\nOption B is incorrect because calling the DeleteMessage API does not affect the visibility timeout of a message.\nOption C is incorrect because decreasing the timeout using the ChangeMessageVisibility API would make the message visible to other consumers before it is fully processed. The message should only be deleted from the queue using the DeleteMessage API once processing is complete.\nOption D is incorrect because deleting the message from the queue first using the DeleteMessage API would not allow the application enough time to process the message. The visibility timeout needs to be increased first to ensure that the message remains hidden from other consumers until processing is complete.\n\n"
}, {
  "id" : 342,
  "question" : "As a programmer, you have been hired to develop an application for a company.\nThe application needs to first encrypt the data at the client side before sending it to a destination location.\nHow can you achieve this? The size of the data is generally around 1 - 4 MB.\nEach object needs to have its own key to encrypt the data.\n",
  "answers" : [ {
    "id" : "cdec99330c1845448721951a4d33aabc",
    "option" : "Upload the data to KMS and use the CMK key to encrypt the data.",
    "isCorrect" : "false"
  }, {
    "id" : "711a010ca7354ab98b372466768d0914",
    "option" : "Use the CMK key ARN to get the key and encrypt the data.",
    "isCorrect" : "false"
  }, {
    "id" : "cc24c59cc232428999071d67b78f4142",
    "option" : "Use the GenerateDataKey API to get the key from a CMK.",
    "isCorrect" : "true"
  }, {
    "id" : "fb9ad99c3aca404898871f26375678ab",
    "option" : "Upload the data to an S3 bucket with encryption enabled.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nWe recommend that you use the following pattern to encrypt data locally in your application.\nUse this operation (GenerateDataKey) to get a data encryption key.\nUse the plaintext data encryption key (returned in the Plaintext field of the response) to encrypt data locally, then erase the plaintext data key from memory.\nStore the encrypted data key (returned in the CiphertextBlob field of the response) alongside the locally encrypted data.\nOption A is incorrect since you don't upload the data to KMS to encrypt it.\nOption B is incorrect since you don't use the CMK key to encrypt large data objects.\nOption D is incorrect since the data would only be encrypted at rest, and the question states that the object needs to be encrypted at the client side.\nFor more information on the Generate Data Key API, please refer to the below URL-\nhttps://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html\n\nThe correct answer for this scenario is option C - Use the GenerateDataKey API to get the key from a CMK.\nWhen we need to encrypt data at the client side, we need a way to generate a key that can be used for encryption. This key should be unique for each object that needs to be encrypted. Using a single key to encrypt multiple objects can lead to security vulnerabilities as the compromise of one key can lead to the compromise of all the data encrypted with that key.\nAWS Key Management Service (KMS) provides us with the GenerateDataKey API that can be used to generate a unique data encryption key for each object that needs to be encrypted. The API generates a plaintext and an encrypted version of the data key. The plaintext version can be used by the client-side application to encrypt the data and the encrypted version can be stored with the encrypted data. This way, when the encrypted data is received at the destination, the encrypted data key can be used to decrypt the data.\nHere are the reasons why the other options are incorrect:\nA. Upload the data to KMS and use the CMK key to encrypt the data - This is not a viable option because KMS keys are used for encrypting other keys and not data.\nB. Use the CMK key ARN to get the key and encrypt the data - This option is not correct because a CMK key cannot be used to encrypt data directly. It can only be used to encrypt/decrypt data encryption keys.\nD. Upload the data to an S3 bucket with encryption enabled - This option is not viable because S3 server-side encryption does not allow for client-side encryption. It can only encrypt data at rest when it is stored in S3.\n\n"
}, {
  "id" : 343,
  "question" : "As a developer, you have the requirement to access resources in another account.\nWhich of the following is the best way to achieve this?\n",
  "answers" : [ {
    "id" : "2613436ff19f480db8b0c1debfc5df43",
    "option" : "Create a cross-account role and call the AssumeRole API.",
    "isCorrect" : "true"
  }, {
    "id" : "0bc1059723c2498fb512e3360f2bd218",
    "option" : "Create a user in the destination account and share the password.",
    "isCorrect" : "false"
  }, {
    "id" : "75296c5a1ae84c2a8809012af7603c2c",
    "option" : "Create a user in the destination account and share the Access Keys.",
    "isCorrect" : "false"
  }, {
    "id" : "99cc029f2b6446f3a6518754a84d4775",
    "option" : "Create an IAM Role and attach it to an EC2 Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nFor cross-account access, imagine that you own multiple accounts and need to access resources in each account.\nYou could create long-term credentials in each account to access those resources.\nHowever, managing all those credentials and remembering which one can access which account can be time consuming.\nInstead, you can create one set of long-term credentials in one account and then use temporary security credentials to access all the other accounts by assuming roles in those accounts.\nAll other options are incorrect since the right option is to use cross-account roles.\nFor more information on Assuming a Role, please refer to the below URL-\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\n\nThe best way to access resources in another AWS account as a developer is to create a cross-account role and call the AssumeRole API. This approach enables you to delegate access to resources in the destination account without having to share long-term security credentials such as passwords or access keys.\nWhen you create a cross-account role, you define a trust policy that specifies the accounts and entities that can assume the role. The trust policy must be created in the destination account and must allow the source account to assume the role. Once the trust policy is in place, you can call the AssumeRole API to obtain temporary security credentials that enable you to access the resources in the destination account.\nUsing this method, you can grant granular permissions to the role, allowing developers to access only the specific resources they need to perform their tasks. Additionally, the temporary security credentials obtained via AssumeRole have a limited lifespan, so you can ensure that access to the resources is granted only when it is needed.\nThe other options provided are not recommended for several reasons:\nCreating a user in the destination account and sharing the password is not recommended because it violates the AWS security best practices of never sharing long-term security credentials. It also poses a risk of unauthorized access if the password is compromised. Creating a user in the destination account and sharing the access keys also violates the best practices of never sharing long-term security credentials. Additionally, it is difficult to revoke access when using shared access keys, as you would need to rotate the keys across all systems that use them. Creating an IAM role and attaching it to an EC2 instance is not recommended as it only allows access to the resources from that specific EC2 instance. It is also not an ideal solution if you need to provide access to resources to multiple developers from different locations or systems.\n\n"
}, {
  "id" : 344,
  "question" : "You are a developer for an application.\nThe application needs to make use of AWS for managing authentication.\nThe users should be able to authenticate using identity providers such as Facebook and Google.\nAt the same time, you also need to enable guest user access to limited resources.\nHow can you achieve this in the best possible way?\n",
  "answers" : [ {
    "id" : "953fd1a5f2884c1fa40bf60407c0e555",
    "option" : "Use IAM users and groups.",
    "isCorrect" : "false"
  }, {
    "id" : "096958e554aa4234afee690c42b06b77",
    "option" : "Use AWS Cognito and identity pools with both authenticated and unauthenticated identities.",
    "isCorrect" : "true"
  }, {
    "id" : "4b86d844c61943b18e4b348896104f7c",
    "option" : "Use IAM policy.",
    "isCorrect" : "false"
  }, {
    "id" : "53e7f2d7cc2f4f479d47bb6924db9a4a",
    "option" : "Use AWS Cognito App Sync.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon Cognito identity pools support both authenticated and unauthenticated identities.\nAuthenticated identities belong to users who are authenticated by any supported identity provider.\nUnauthenticated identities typically belong to guest users.\nOption A is incorrect since this would be too much of a maintenance overhead to maintain the users.\nOption C is incorrect since we don't need an IAM policy access over here.\nOption D is incorrect since we don't need Sync capabilities here.\nFor more information on Identity pools, please refer to the below URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\n\nThe best possible way to manage authentication for an application with identity providers such as Facebook and Google while also enabling guest user access to limited resources would be to use AWS Cognito and identity pools with both authenticated and unauthenticated identities.\nOption B is the correct answer because it provides a solution that is designed to handle authentication and user management, with a high degree of flexibility, scalability, and security.\nAWS Cognito is a fully managed identity service that allows developers to add user sign-up, sign-in, and access control to mobile and web applications quickly and easily. Identity pools in Cognito are used to grant users access to AWS resources, and they can be used to manage both authenticated and unauthenticated identities.\nBy using an identity pool with both authenticated and unauthenticated identities, you can allow users to sign in with their Facebook or Google accounts while also allowing guest access to certain resources. Guest users can be granted limited access to the application's resources without requiring them to sign in or create an account.\nOption A (Use IAM users and groups) is not the best possible solution because it is designed for managing access to AWS resources and does not provide authentication services for external identity providers.\nOption C (Use IAM policy) is also not the best possible solution because it is designed to manage access to AWS resources and does not provide authentication services for external identity providers.\nOption D (Use AWS Cognito App Sync) is not the best possible solution because it is designed for offline data synchronization between devices and does not provide authentication services for external identity providers.\nTherefore, the best possible way to achieve the given requirements is to use AWS Cognito and identity pools with both authenticated and unauthenticated identities.\n\n"
}, {
  "id" : 345,
  "question" : "You are currently working on a function for AWS Lambda.\nWhen uploading the package for your AWS Lambda function, you are receiving the following error. CodeStorageExceededException What can you do to resolve this error? Choose 2 options.\n",
  "answers" : [ {
    "id" : "668cdf5d8d0d4dfe8d00b05a4bd2d4b0",
    "option" : "Raise a call with AWS Support to raise the limit on storage.",
    "isCorrect" : "true"
  }, {
    "id" : "d5910eee6aeb41d8974e2f7aefc8111f",
    "option" : "Change the code to Node.js for taking up less storage.",
    "isCorrect" : "false"
  }, {
    "id" : "b4d22490f2b94f28888a117b48b06c50",
    "option" : "Reduce the size for your code.",
    "isCorrect" : "true"
  }, {
    "id" : "6fa0953642364fab98277f5a2c2c761b",
    "option" : "Change the memory limit.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer -A and C.\nThe AWS Documentation mentions the following.\nSince the documentation clearly mentions this, all other options are invalid.\nFor more information on limits for AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/limits.html\n\n\nWhen you upload a package for an AWS Lambda function, it can sometimes result in a CodeStorageExceededException error. This error occurs when the size of the compressed code, including all its layers, exceeds the maximum size limit allowed by AWS Lambda.\nTo resolve this error, you can take the following steps:\nReduce the size of your code: You can start by reducing the size of your code by removing any unnecessary dependencies or compressing any large files. This can help you stay within the size limit allowed by AWS Lambda. Change the memory limit: You can also try to increase the memory allocated to the function as this also increases the size limit for code storage. Change the code to Node.js: Node.js is a lightweight runtime that can help reduce the overall size of your code. You can try to rewrite your code in Node.js to make it more efficient and reduce its size. Raise a call with AWS Support: If the above methods do not work, you can contact AWS Support to raise the limit on storage for your account. However, this should be considered as a last resort and should only be done after all other options have been exhausted.\nIn summary, the best options to resolve the CodeStorageExceededException error when uploading a package for an AWS Lambda function are to reduce the size of your code and change the memory limit allocated to the function. If these options are not feasible, you can consider changing the code to Node.js or contacting AWS Support for assistance.\n\n"
}, {
  "id" : 346,
  "question" : "Your team is developing a series of Lambda functions.\nYou need to ensure that you analyze the invocations of the functions during the testing phase.\nWhich of the following tools can help you achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "ee24ce4b06f7407a890ea35562342648",
    "option" : "Amazon Cloudwatch",
    "isCorrect" : "true"
  }, {
    "id" : "697e65b912f44ebe8fed281a206f9bc9",
    "option" : "Amazon Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "e09e78c1a0be4c19b003c8f5ae9d9951",
    "option" : "Amazon X-Ray",
    "isCorrect" : "true"
  }, {
    "id" : "5f45309742974851b2abb3fa0d348e89",
    "option" : "Amazon Cloudtrail.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS Documentation mentions the following.\nOption B is incorrect since this is used to check EC2 Instances for vulnerabilities.\nOption D is incorrect since this is used to monitor API activity.\nFor more information on troubleshooting AWS Lambda, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/troubleshooting.html\n\n\nThe two tools that can help analyze the invocations of Lambda functions during the testing phase are Amazon CloudWatch and Amazon X-Ray.\nAmazon CloudWatch is a monitoring service that can be used to collect and track metrics, collect and monitor log files, and set alarms. It provides a set of metrics and logs for AWS resources, including Lambda functions. By using CloudWatch Logs, developers can collect and monitor logs from Lambda functions, and use CloudWatch Metrics to visualize and analyze the performance and usage of the functions during testing. Developers can also set alarms to alert them when certain thresholds are reached.\nAmazon X-Ray is a service that can be used to trace requests made to distributed applications, including AWS resources such as Lambda functions. X-Ray provides a detailed view of the entire application, and can help identify performance bottlenecks, errors, and exceptions. By using X-Ray with Lambda functions, developers can trace the path of requests through the function, and analyze the time taken by each component of the application.\nAmazon Inspector and Amazon CloudTrail are not the correct answers for this question. Amazon Inspector is a security assessment service that can be used to identify security vulnerabilities in applications running on AWS. Amazon CloudTrail is a service that can be used to track API calls made to AWS resources, including Lambda functions. While CloudTrail can provide insight into who accessed the Lambda function, it does not provide performance metrics or trace requests through the function.\n\n"
}, {
  "id" : 347,
  "question" : "Your company needs to develop an application that needs to have a caching feature.\nThe application cannot afford many cache failures and should be highly available.\nWhich of the following would you choose for this purpose?\n",
  "answers" : [ {
    "id" : "a7f47f7f753e443084a8e1e7b09a8182",
    "option" : "Use Memcached on an EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "33eca38acf414919bbe8c96ded7b6587",
    "option" : "Use ElastiCache – Memcached.",
    "isCorrect" : "false"
  }, {
    "id" : "57a7981bb08745bc8f91a0c4b95e66d3",
    "option" : "Use ElastiCache – Redis with Cluster-Mode Enabled.",
    "isCorrect" : "true"
  }, {
    "id" : "6d73e471184e4fc4b5190bf3ee69ff8c",
    "option" : "Use Redis on an EC2 Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nCluster Mode comes with the primary benefit of horizontal scaling up and down of your Redis cluster, with almost zero impact on the performance of the cluster, as I will demonstrate later.\nElastiCache for Redis with Cluster Mode Enabled works by spreading the cache key space across multiple shards.\nThis means that your data and read/write access to that data is spread across multiple Redis nodes.\nBy spreading the load over a greater number of nodes, we can both enhance availability and reduce bottlenecks during periods of peak demand, while providing more memory space than a single node could offer.\nOptions A and D are incorrect since using a single EC2 Instance would be a single point of failure, not suitable for high availability.\nOption B is incorrect since Redis would be better for high availability.\nFor more information on Redis ElastiCache, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.html\nhttps://aws.amazon.com/blogs/database/work-with-cluster-mode-on-amazon-elasticache-for-redis/\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis-RedisCluster.html\n\nFor an application that requires caching with high availability and minimal cache failures, using Amazon ElastiCache would be the best option. ElastiCache is a fully managed, in-memory data store and caching service that supports two popular open-source in-memory caching engines, Redis and Memcached.\nOption A suggests using Memcached on an EC2 instance, which means setting up and managing a standalone cache node on an EC2 instance. While this option may work, it requires more effort in terms of maintenance, monitoring, and scaling, and is therefore not an ideal solution for an application that needs high availability and minimal cache failures.\nOption B suggests using ElastiCache with Memcached. This option is a better choice than using Memcached on an EC2 instance because it provides a fully managed caching solution with high availability, scalability, and fault tolerance. ElastiCache manages the Memcached nodes for you and provides automatic failover, backup and restore, and monitoring.\nOption C suggests using ElastiCache with Redis in Cluster-Mode Enabled. Redis is an in-memory data store that supports key-value, document, graph, and other data structures. Cluster mode enables Redis to distribute data across multiple nodes in a cluster, providing better scalability and fault tolerance than standalone Redis nodes. Using ElastiCache with Redis in Cluster-Mode enabled provides a highly available and scalable caching solution.\nOption D suggests using Redis on an EC2 instance, which is similar to option A but with Redis instead of Memcached. While Redis can be a good choice for caching, using it on an EC2 instance means managing the Redis nodes yourself, which requires more effort and can lead to higher cache failures and lower availability than using a fully managed caching service like ElastiCache.\nTherefore, the best option for an application that needs caching with high availability and minimal cache failures is to use ElastiCache with either Memcached or Redis in Cluster-Mode enabled, depending on the specific requirements of the application.\n\n"
}, {
  "id" : 348,
  "question" : "Your team is developing a set of Lambda functions.\nThey need to debug the Lambda functions using the X-Ray service.\nWhich of the following are environment variables which AWS Lambda uses to communicate with the X-Ray service? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "21d55688ea3c4dbdabcc0de0676f1989",
    "option" : "_X_AMZN_TRACE_ID",
    "isCorrect" : "true"
  }, {
    "id" : "f5f8b657eec94454a57b3b46db50d26f",
    "option" : "AWS_XRAY_CONTEXT_MISSING",
    "isCorrect" : "true"
  }, {
    "id" : "5ecba07a87a54facb69172e8d30baba4",
    "option" : "AWS_XRAY_DAEMON_ADDRESS",
    "isCorrect" : "true"
  }, {
    "id" : "6655c5acd4a048969748e034be53ef97",
    "option" : "AWS_LAMBDA_XRAY.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A, B and C.\nThe AWS Documentation mentions the following.\nAWS Lambda uses environment variables to facilitate communication with the X-Ray daemon and configure the X-Ray SDK.\n_X_AMZN_TRACE_ID: Contains the tracing header, which includes the sampling decision, trace ID, and parent segment ID.\nIf Lambda receives a tracing header when your function is invoked, that header will be used to populate the _X_AMZN_TRACE_ID environment variable.\nIf a tracing header was not received, Lambda will generate one for you.\nAWS_XRAY_CONTEXT_MISSING: The X-Ray SDK uses this variable to determine its behavior in the event that your function tries to record X-Ray data, but a tracing header is not available.\nLambda sets this value to LOG_ERROR by default.\nAWS_XRAY_DAEMON_ADDRESS: This environment variable exposes the X-Ray daemon's address in the following format: IP_ADDRESS:PORT.\nYou can use the X-Ray daemon's address to send trace data to the X-Ray daemon directly, without using the X-Ray SDK.\nFor more information on using Lambda with X-Ray, please refer to the below URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html\n\nSure, I'd be happy to provide a detailed explanation!\nWhen you enable AWS X-Ray for your Lambda functions, you need to make sure that the functions are properly instrumented with the X-Ray SDK. Once instrumented, your functions can automatically record tracing data and send it to the X-Ray service for analysis.\nIn addition to instrumenting your functions, you also need to set a few environment variables so that AWS Lambda can communicate with the X-Ray service. Here are the three environment variables that you need to set:\n_X_AMZN_TRACE_ID: This environment variable contains the tracing header that is used to propagate tracing information between services. When a Lambda function is invoked, the _X_AMZN_TRACE_ID environment variable is automatically set to the tracing header that was sent with the incoming request. AWS_XRAY_CONTEXT_MISSING: This environment variable specifies how the X-Ray SDK should behave if tracing information is not available. The default value is \"LOG_ERROR\", which means that the SDK will log an error message if tracing information is missing. You can also set this variable to \"RUNTIME_ERROR\" to make the SDK throw a runtime error if tracing information is missing. AWS_XRAY_DAEMON_ADDRESS: This environment variable specifies the address and port number of the X-Ray daemon that is running on your EC2 instances or containers. The X-Ray daemon is responsible for receiving tracing data from your Lambda functions and sending it to the X-Ray service. If you don't set this variable, the SDK will use the default address and port (127.0.0.1:2000).\nNote that the answer \"AWS_LAMBDA_XRAY\" is not a valid environment variable. However, it is worth noting that you can also enable X-Ray tracing for your Lambda functions by setting the \"TracingConfig\" property of your Lambda function's configuration to \"Active\". This will automatically set the environment variables described above and enable tracing for your function.\n\n"
}, {
  "id" : 349,
  "question" : "Your team is planning to deploy an application that processes periodic tasks using the worker environment on AWS Elastic Beanstalk.\nWhich of the following is an additional requirement for a worker environment in AWS Elastic Beanstalk?\n",
  "answers" : [ {
    "id" : "d49bc24223f6472481b3b0cfd39bacb7",
    "option" : "Ensure that the application is uploaded as a zip file.",
    "isCorrect" : "false"
  }, {
    "id" : "43a22abc6a494de1a2cc4dd08454dbbe",
    "option" : "Ensure that the application size does not exceed 512 M.",
    "isCorrect" : "false"
  }, {
    "id" : "e2317d25c4c648c9b27bc2b848e69387",
    "option" : "Ensure that the application does not include a parent level folder.",
    "isCorrect" : "false"
  }, {
    "id" : "126c13d761b24daba2f7d77ee0daddfd",
    "option" : "Ensure that the application contains a file called cron.yaml.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nWhen you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle.\nYour source bundle must meet the following requirements.\nConsist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file).\nNot exceed 512 MB.Not include a parent folder or top-level directory (subdirectories are fine).\nIf you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron.yaml file.\nSince this is clearly mentioned in the documentation, all other options are incorrect.\nFor more information on application source bundles in Elastic beanstalk, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html\n\nSure, I'd be happy to explain!\nAWS Elastic Beanstalk provides a platform for deploying and scaling web applications and services. It supports several types of environments, including web server environments and worker environments. Worker environments are used to process background tasks or jobs, often using a message queue like Amazon Simple Queue Service (SQS).\nWhen deploying an application to a worker environment on Elastic Beanstalk, there are a few requirements to keep in mind. Let's go through each of the answer options and see which one is the correct additional requirement for a worker environment:\nA. Ensure that the application is uploaded as a zip file. This is not an additional requirement specific to worker environments on Elastic Beanstalk. All applications deployed to Elastic Beanstalk, regardless of the environment type, must be uploaded as a zip file.\nB. Ensure that the application size does not exceed 512 MB. This is also not an additional requirement specific to worker environments on Elastic Beanstalk. All applications deployed to Elastic Beanstalk, regardless of the environment type, must comply with the platform's limits for storage size and other resources.\nC. Ensure that the application does not include a parent level folder. This is not an additional requirement specific to worker environments on Elastic Beanstalk either. All applications deployed to Elastic Beanstalk, regardless of the environment type, must follow certain guidelines for application packaging and directory structure. One of these guidelines is that the root of the application package should not contain a parent level folder.\nD. Ensure that the application contains a file called cron.yaml. This is the correct answer! A worker environment on Elastic Beanstalk requires an additional configuration file called cron.yaml. This file defines scheduled tasks (also known as cron jobs) that the worker environment should run at specific intervals. These tasks can be used to perform periodic or recurring tasks, such as database backups, log rotation, or other maintenance tasks.\nIn summary, when deploying an application to a worker environment on Elastic Beanstalk, you need to ensure that it includes a file called cron.yaml, which defines scheduled tasks that the environment should run at specific intervals. The other answer options (A, B, and C) are not additional requirements specific to worker environments, but rather general requirements for all applications deployed to Elastic Beanstalk.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 351,
  "question" : "Your team has just started using the API gateway service.\nSeveral AWS Lambda functions are used as a backend for the gateway service.\nYou have deployed the API and made it available for test users.\nYou have made some changes to the method responsible for the API gateway.\nWhat should be the next step, so that the changes are available for the test users?\n",
  "answers" : [ {
    "id" : "e0d96034f9b84d8d9552a0e5842c7d29",
    "option" : "Recreate the gateway service.",
    "isCorrect" : "false"
  }, {
    "id" : "30c384037ffa43fa8d2e8e8baf59b2a8",
    "option" : "Redeploy the API.",
    "isCorrect" : "true"
  }, {
    "id" : "a36f95ef4b9a42cea32fa4f4646edf44",
    "option" : "Redeploy the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "27cd183cc51c4c1693a2c3cf0b3edb3f",
    "option" : "Make a copy of the gateway API.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nTo deploy an API, you create an API deployment and associate it with a stage.\nEach stage is a snapshot of the API and is made available for the client to call.\nStages enable robust version control of your API.\nFor each stage, you can optimize API performance by adjusting the default account-level request throttling limits and enabling API caching.\nIn addition, you can override stage-level settings for individual methods and define stage variables to pass stage-specific environment contexts to the API integration at runtime.\nEvery time you update an API, which includes modification of methods, integrations, authorizers, and anything else other than stage settings, you must redeploy the API to an existing stage or to a new stage.\nAll other options are incorrect since the right way is to Redeploy the API.\nFor more information on how to deploy an API, please refer to the below URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html\n\nThe correct answer is B. Redeploy the API.\nExplanation: API Gateway is a fully managed service provided by AWS that makes it easy to create, deploy, and manage APIs. It acts as a \"front door\" for your backend services, and you can use it to define HTTP endpoints, map them to AWS Lambda functions, and configure the API Gateway to handle the request/response flow, including authentication and authorization, caching, and rate limiting.\nWhen you make changes to the method responsible for the API gateway, you need to redeploy the API to make those changes available to the test users. The process of deploying an API involves creating a new deployment version of the API, which includes the updated configuration and code changes. After creating a new deployment, you can promote it to a stage, such as a test or production environment, and make it available to your users.\nRecreating the gateway service (answer A) would involve deleting the existing API Gateway and creating a new one, which is not necessary or desirable if you just need to make changes to the method. Redeploying the Lambda function (answer C) would update the function's code, but it would not update the API Gateway configuration or deployment. Making a copy of the gateway API (answer D) would not achieve the desired result either, as you would still need to redeploy the copied API to make the changes available to the users.\nTherefore, the correct answer is B. Redeploy the API.\n\n"
}, {
  "id" : 352,
  "question" : "You are working on a REST API where you require to pass client submitted method requests as it is to a Lambda Function.\nWhich of the following can be set as an Integration type for this requirement?\n",
  "answers" : [ {
    "id" : "bd56982a25e14c5daa4c49e6510f55ce",
    "option" : "\"type\": \"aws_proxy\"",
    "isCorrect" : "true"
  }, {
    "id" : "3bd0540e772241debfebdf3007426177",
    "option" : "\"type\": \"aws\"",
    "isCorrect" : "false"
  }, {
    "id" : "0e25a05b082c461b8bf5f31e3ce23daa",
    "option" : "\"type\": \"http_proxy\"",
    "isCorrect" : "false"
  }, {
    "id" : "3a98028a398745c3a241dd7c89f4d42c",
    "option" : "\"type\": \"mock\"",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nIntegration Type “Aws_Proxy” can be used for an API method to be integrated with the Lambda Function where incoming requests from the clients are passed as input to Lambda Function.\nOption B is incorrect with AWS integration.\nThere is a mapping between method request &amp; integration request along with method response &amp; integration response.\nThis is not a suitable integration type for passing client requests to the Lambda function directly.\nOption C is incorrect as the \"http_proxy\" integration allows clients to access backend HTTP endpoints.\nThis is not a suitable integration type for passing client requests to the Lambda function directly.\nOption D is incorrect as mock integration type is used to test integration type with API Gateway sending response without sending request to backend.\nFor more information on choosing API Gateway API Integration type, refer to the following URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html\n\nThe correct answer for this question is A. \"type\": \"aws_proxy\".\nWhen creating a REST API in Amazon API Gateway, you have several integration types to choose from, which determine how requests are routed from the API Gateway to your backend service.\n\"aws_proxy\": This integration type is used to integrate with Lambda functions. When you select this option, the API Gateway will pass the entire request to the backend Lambda function as is, without any modifications. This means that the Lambda function will receive the same method request that was submitted by the client. This integration type is useful if you want to offload your API's logic to a Lambda function and don't want to perform any processing on the request before it reaches the Lambda function. \"aws\": This integration type is also used to integrate with Lambda functions, but it requires you to define the mapping between the incoming request and the format expected by the backend Lambda function. This can be useful if you want to transform the request before it reaches the Lambda function, for example, to extract parameters or headers from the request and pass them as input to the Lambda function. \"http_proxy\": This integration type is used to integrate with HTTP or HTTPS endpoints. When you select this option, the API Gateway will forward the request to the backend endpoint using the same method and headers as the incoming request. This integration type is useful if you have an existing HTTP endpoint that you want to expose through an API Gateway. \"mock\": This integration type is used for testing purposes only. It allows you to create a mock backend for your API, which will return predefined responses to requests without actually invoking any backend service.\nIn this case, the requirement is to pass the client submitted method requests as is to a Lambda function, which means that we don't want to modify the request in any way before it reaches the backend Lambda function. The \"aws_proxy\" integration type is the best fit for this requirement, as it will pass the entire request to the Lambda function without any modifications.\n\n"
}, {
  "id" : 353,
  "question" : "A company has an application that is making use of a DynamoDB table.\nThere is now a requirement to ensure that all changes to the items in the table are recorded and stored in a MySQL database.\nWhich of the following would ideally be one of the implementation steps?\n",
  "answers" : [ {
    "id" : "ab54f99cfdcd48bbab149ee82bf21c01",
    "option" : "Enable DynamoDB Accelerator.",
    "isCorrect" : "false"
  }, {
    "id" : "6c0c226d7790472187486b05c71068c7",
    "option" : "Enable DynamoDB global tables.",
    "isCorrect" : "false"
  }, {
    "id" : "0b233b9166f5439e9f193afc0b58f7dc",
    "option" : "Enable DynamoDB streams.",
    "isCorrect" : "true"
  }, {
    "id" : "6fdb11067fe644bfbba0f4857b5e8689",
    "option" : "Enable DynamoDB triggers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nDynamoDB Streams enables solutions such as these and many others.\nDynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours.\nApplications can access this log and view the data items as they appeared before and after they were modified, in near real-time.\nOption A is invalid since this is used to provide a DynamoDB-compatible caching service that enables you to benefit from fast in-memory performance for demanding applications.\nOption B is invalid since this is used to provide a fully managed solution for deploying a multi-region, multi-master database, without having to build and maintain your own replication solution.\nOption D is invalid since there are no inbuilt triggers in DynamoDB.For more information on DynamoDB streams, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\n\nThe ideal implementation step for recording and storing changes made to items in a DynamoDB table in a MySQL database is to enable DynamoDB streams. Therefore, the correct answer is option C.\nDynamoDB streams provide an ordered, time-sequenced stream of updates that occur to items in a DynamoDB table. Each stream record contains the details of a specific modification to an item in the table, including the nature of the change, the item's primary key, and a timestamp. DynamoDB streams can be used to capture data changes in near real-time and can be used to trigger actions, such as replicating data to other systems.\nBy enabling DynamoDB streams for the DynamoDB table in question, the stream records can be sent to an AWS Lambda function that can process and transform the data as needed. In this case, the Lambda function can be configured to store the stream records in a MySQL database.\nOptions A, B, and D are incorrect because they are not directly related to recording and storing changes made to items in a DynamoDB table in a MySQL database.\nOption A: Enable DynamoDB Accelerator - is a service that provides fast read access to a DynamoDB table's data by caching frequently accessed items in memory.\nOption B: Enable DynamoDB global tables - is a feature that allows for automatic, cross-region replication of DynamoDB tables, which can improve the availability and durability of data.\nOption D: Enable DynamoDB triggers - is a feature that allows for invoking an AWS Lambda function or an Amazon SNS topic in response to data changes made to a DynamoDB table.\n\n"
}, {
  "id" : 354,
  "question" : "You have developed a Lambda function.\nThis function needs to run on a scheduled basis.\nWhich of the following can be done to accomplish this requirement in an ideal manner?\n",
  "answers" : [ {
    "id" : "e2d53d36ea19410bbd2690a622012129",
    "option" : "Use the schedule service in AWS Lambda.",
    "isCorrect" : "false"
  }, {
    "id" : "8d1c2dbb00014bd2905ba07f142343b0",
    "option" : "Use an EC2 Instance to schedule the Lambda invocation.",
    "isCorrect" : "false"
  }, {
    "id" : "375200b972e246f0bfa73b2ffa7c6c04",
    "option" : "Use Cloudwatch events to schedule the Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "45286026923b4dc390346d74d8f403b5",
    "option" : "Use Cloudtrail to schedule the Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nOption A is incorrect since there is no inbuilt scheduler in AWS Lambda.\nOption B is incorrect since this would add more maintenance overhead.\nOption D is incorrect since this service is an API monitoring service.\nFor more information on running Lambda functions on schedules, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html\n\n\nThe correct answer is C. Use Cloudwatch events to schedule the Lambda function.\nExplanation: AWS Lambda is a serverless compute service that allows developers to run their code without managing servers. Developers can create Lambda functions and trigger them using various events such as API Gateway, S3, and Cloudwatch events.\nTo run a Lambda function on a scheduled basis, we need to configure it to trigger at specific times. AWS provides various ways to schedule a Lambda function, such as:\nA. Use the schedule service in AWS Lambda: AWS Lambda provides a built-in scheduler that allows developers to schedule a function to run at specific times. This option is ideal if the Lambda function needs to run at a fixed interval, for example, every 5 minutes, every hour, or every day. However, this option is limited to fixed schedules and cannot trigger a Lambda function based on more complex events.\nB. Use an EC2 Instance to schedule the Lambda invocation: This option involves creating an EC2 instance and configuring it to run a script that invokes the Lambda function. This option is not ideal because it involves managing an EC2 instance and increases the infrastructure overhead.\nC. Use Cloudwatch events to schedule the Lambda function: Cloudwatch events is a service that allows developers to create rules that match events and route them to targets such as Lambda functions. This option is ideal because Cloudwatch events can trigger Lambda functions based on complex event patterns, such as an S3 bucket upload or a Cloudtrail event.\nD. Use Cloudtrail to schedule the Lambda function: Cloudtrail is a service that records API calls and events in AWS. This option is not ideal because Cloudtrail does not provide a mechanism to trigger Lambda functions directly. However, it can be used in conjunction with Cloudwatch events to trigger a Lambda function based on Cloudtrail events.\nIn summary, the ideal option to schedule a Lambda function is to use Cloudwatch events.\n\n"
}, {
  "id" : 355,
  "question" : "A company has a set of APIs and a web application.\nThey want to deploy it to AWS.\nThey don't want to manage the underlying infrastructure.\nWhich of the following services can help to accomplish this?\n",
  "answers" : [ {
    "id" : "2fc1acb8a24c4798a773c6c18806c827",
    "option" : "AWS Lambda and API Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "bcd3366a8b164ed3953fe96aacd99c6c",
    "option" : "AWS EC2 and Cloudfront",
    "isCorrect" : "false"
  }, {
    "id" : "02942818281b494ebc1819edab2f175c",
    "option" : "AWS Lambda and Cloudfront",
    "isCorrect" : "false"
  }, {
    "id" : "3d6c0bbe19584db8a93d3cc0daa0fe38",
    "option" : "AWS Lambda and EC2",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers.\nAWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running.\nWith AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration.\nAmazon API Gateway is an AWS service that enables developers to create, publish, maintain, monitor, and secure APIs at any scale.\nYou can create APIs that access AWS or other web services, as well as data stored in the AWS Cloud.\nOptions B and D are incorrect since for EC2 you would need to manage the compute layer.\nOption C is incorrect since Cloudfront is not used along with AWS Lambda.\nFor more information on AWS Lambda and the API gateway service, please refer to the below URLs-\nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\n\nThe correct answer is A. AWS Lambda and API Gateway.\nExplanation:\nAWS Lambda is a serverless compute service that allows developers to run their code without the need to manage servers or infrastructure. With Lambda, developers can write their code in a supported language (such as Python, Node.js, or Java) and upload it to AWS, which will then execute it on their behalf. This allows developers to focus on writing their code rather than managing infrastructure.\nAPI Gateway is a fully managed service that makes it easy for developers to create, publish, and manage APIs at any scale. API Gateway provides a simple way for developers to create RESTful APIs, WebSocket APIs, and HTTP APIs, which can be used to expose Lambda functions as APIs. API Gateway also provides features such as throttling, caching, and authentication to help developers secure and manage their APIs.\nTogether, AWS Lambda and API Gateway provide a powerful way to deploy APIs and web applications to AWS without the need to manage infrastructure. Developers can write their code in a supported language, upload it to Lambda, and then use API Gateway to create and manage their APIs. This allows developers to focus on writing their code and building their applications, while AWS takes care of the underlying infrastructure.\nOption B, AWS EC2 and Cloudfront, would require managing the underlying infrastructure and scaling the servers as necessary.\nOption C, AWS Lambda and Cloudfront, is a possible combination, but it would not provide the ability to create and manage APIs.\nOption D, AWS Lambda and EC2, would require managing the underlying infrastructure and would not provide the ability to create and manage APIs as easily as with API Gateway.\n\n"
}, {
  "id" : 356,
  "question" : "You have exceeded your maximum allowed provisioned throughput for a table in DynamoDB.\nWhich of the following approaches are correct to fix this problem? (Select Three)\n",
  "answers" : [ {
    "id" : "b1bb7b4be21c45ef91b77a20f8e9af8a",
    "option" : "In your DynamoDB table distribute all the operations read and written across more distinct partition key values.",
    "isCorrect" : "true"
  }, {
    "id" : "7797ee1a2fc74cf9a6453739a18530a8",
    "option" : "Enable DynamoDB Streams.",
    "isCorrect" : "false"
  }, {
    "id" : "9af0f2865df04e54886b3fdc9ed8d4f2",
    "option" : "Enable DynamoDB Time to Live.",
    "isCorrect" : "false"
  }, {
    "id" : "7750d18370514779b86614136fbc278b",
    "option" : "Implement DAX (DynamoDB Accelerator) as a cache solution to improve the performance of the tables.",
    "isCorrect" : "true"
  }, {
    "id" : "bb7605b36eb349ec96882cba55f09d44",
    "option" : "Implement error retries and exponential backoff in your application code.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: A, D &amp; E.\nOption A is CORRECT because each partition is limited to 100 Reads per second.\nIf a partition receives more than 100 reads per second it will become a Hot Partition and the performance of the table will be reduced.\nIf you distribute your queries to more partition keys, you can prevent them from becoming Hot partitions.\nMore details: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html.\nOption B is incorrect because DynamoDB streams are used for stored item level modifications, it does not affect the performance or the throughput.\nMore details: https://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/\nOption C is incorrect because DynamoDB TTL is designed to reduce the data stored volume, it does not affect the performance or the throughput.\nMore details: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html.\nOption D is CORRECT because DAX increase the performance and throughput of repeated read queries.\nMore details: https://docs.amazonaws.cn/en_us/amazondynamodb/latest/developerguide/DAX.html.\nOption E is CORRECT because you can manage the quantity of error retries or increase timeouts between retries.\nMore details: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html.\n\nWhen you exceed your maximum allowed provisioned throughput for a table in DynamoDB, it means that you have reached the limit of read and write capacity units that you have provisioned for that table. Provisioned throughput is the amount of data that DynamoDB can read or write in a second, and it is determined by the amount of read and write capacity units that you have provisioned for the table.\nTo fix this problem, you can take the following approaches:\nA. In your DynamoDB table distribute all the operations read and written across more distinct partition key values. DynamoDB partitions your data across multiple partitions to achieve scalability and high availability. Each partition has a limited amount of throughput capacity, and if you are overloading a single partition, you can distribute the load across more distinct partition key values to improve performance. By using a different partition key, you can distribute the data evenly across multiple partitions, and this will increase the overall throughput capacity of your table.\nB. Enable DynamoDB Streams. DynamoDB Streams is a feature that allows you to capture changes to your DynamoDB table and then process them in real-time. By enabling DynamoDB Streams, you can create triggers to perform certain actions when specific events happen in your table. For example, you can create a trigger to notify you when a certain number of read or write capacity units are consumed, and then take appropriate actions to fix the problem.\nC. Enable DynamoDB Time to Live. DynamoDB Time to Live (TTL) is a feature that allows you to automatically delete expired items from your table. By enabling DynamoDB TTL, you can reduce the amount of data that you need to store in your table, and this will free up capacity units that you can use for other operations. For example, if you have a table that stores log data, you can set a TTL to automatically delete logs that are older than a certain time period.\nD. Implement DAX (DynamoDB Accelerator) as a cache solution to improve the performance of the tables. DynamoDB Accelerator (DAX) is a caching service that provides a high-performance, in-memory cache for DynamoDB tables. By implementing DAX, you can reduce the number of requests that you make to DynamoDB and improve the response time of your applications. DAX can also help you reduce the amount of provisioned throughput that you need to provision for your DynamoDB tables.\nE. Implement error retries and exponential backoff in your application code. When you exceed your maximum allowed provisioned throughput for a table in DynamoDB, you may receive ProvisionedThroughputExceededException errors in your application code. To handle these errors, you can implement error retries and exponential backoff in your application code. By retrying failed requests with increasing delays between retries, you can reduce the number of failed requests and improve the overall throughput capacity of your table.\nIn conclusion, to fix the problem of exceeding your maximum allowed provisioned throughput for a table in DynamoDB, you can use a combination of the above approaches. You can distribute the load across more distinct partition key values, enable DynamoDB Streams and TTL, implement DAX as a cache solution, and implement error retries and exponential backoff in your application code.\n\n"
}, {
  "id" : 357,
  "question" : "You are developing a Lambda in your Dev1 account, the purpose of this Lambda is to save QR codes in two S3 buckets, one bucket (BucketDev1) is in the same account as the Lambda and the second bucket (BucketDev2) is in other AWS account called Dev2\nA requirement is to only allow this specified Lambda from Dev1 to create objects in BucketDev2. What do you need to grant access to this Lambda function for both S3 buckets?\n",
  "answers" : [ {
    "id" : "346f3a6ae1494bed85a6b0e75b6ea694",
    "option" : "Create a role called S3-Lambda with a policy that allows to create objects in any S3 bucket and attach it to the Lambda, create in BucketDev1 a bucket policy that only allows resources with the S3-Lambda role to write in BucketDev1.",
    "isCorrect" : "false"
  }, {
    "id" : "bbd59a42ca694f23bca330f0332eb427",
    "option" : "Create a role called S3-Lambda with a policy that allows to create objects in any S3 bucket and attach it to the Lambda, create in BucketDev2 a bucket policy that allows resources with the S3-Lambda role from Dev1 account to write in BucketDev2.",
    "isCorrect" : "true"
  }, {
    "id" : "e4357e5923224385b7c3205640ce287a",
    "option" : "Create a role called S3-Lambda with a policy that allows you to write in BucketDev1 and BucketDev2 and attach it to the Lambda. Create in BucketDev2 a bucket policy with two statements. The first statement denies all resources with the S3-Lambda role of Dev1 account to write in any S3 bucket, the second statement allows the specified Lambda from Dev1 account to write in the specified bucket in dev2 account.",
    "isCorrect" : "false"
  }, {
    "id" : "2c28a716d54645cd8390fd7d2ee23e4d",
    "option" : "Create two policies, the first one allows Lambda to create objects in BucketDev1 and the second, allows Lambda to create objects in BucketDev2. Create an IAM role with these two policies and attach it to the Lambda.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer: B.\nOption A is incorrect because when you need to access a S3 bucket from an external account, you need to specify in the bucket policy an allowed statement with the ARN of the AWS Account as principal.\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"S3bucketLambda\",\n\"Action\": [\n\"s3:GetObject\"\n],\n\"Effect\": \"Allow\",\n\"Resource\": [\n\"arn:aws:s3:::YOURBUCKETHERE/*\"\n],\n\"Principal\": {\n\"AWS\": [\n\"arn:aws:iam::123456789012:role/LambdaRole123\"\n]\n}\n}\n]\n}\nOption B is CORRECT because, you don't need to write a bucket policy to access S3 bucket if the resource is in the same account and it has the permission to access the S3 bucket in the attached role or policy.\nWith the bucket policy in the second account the Lambda from the first account can access BucketDev2\nMore details: https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/\nOption C is incorrect because of the AWS IAM Logic and Hierarchy an explicit denial in any policy overrides any allowed statement.\nSo with this approach in the end, the bucket has a denial for Lambda to access the bucket BucketDev2\nMore details: https://aws.amazon.com/premiumsupport/knowledge-center/iam-policy-tags-deny/\nOption D is incorrect because if the S3 bucket is in a different account it will need a bucket policy to allow resources from other accounts to access it.\n\nThe correct answer is B.\nTo grant access to the Lambda function to write objects in both S3 buckets, you need to create an IAM role that allows the Lambda function to write to S3 buckets and then create bucket policies that allow access to the S3 buckets.\nOption A is incorrect because it allows the S3-Lambda role to write to any S3 bucket. This policy does not restrict access to the specific S3 buckets that need to be accessed. Also, BucketDev1 should not be restricted to only resources with the S3-Lambda role as other resources may need to write to it.\nOption C is incorrect because it denies access to all resources with the S3-Lambda role in the Dev1 account to write to any S3 bucket. This policy effectively blocks access to the specified bucket, which is not what is required. The second statement also allows the specified Lambda function from Dev1 account to write in the specified bucket in Dev2 account, but it does not restrict access to other buckets.\nOption D is incorrect because it creates two policies instead of creating a single role with a policy that allows access to both S3 buckets.\nTherefore, the correct approach is to:\nCreate an IAM role called S3-Lambda. Attach a policy to the S3-Lambda role that allows the role to write objects to any S3 bucket. Attach the S3-Lambda role to the Lambda function. Create a bucket policy on BucketDev2 that allows resources with the S3-Lambda role from Dev1 account to write to the bucket.\nThe policy for BucketDev2 should look like this:\njson Copy code {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Principal\": {                 \"AWS\": \"arn:aws:iam::Dev1-account-ID:role/S3-Lambda\"             },             \"Action\": [                 \"s3:PutObject\",                 \"s3:PutObjectAcl\",                 \"s3:GetObject\",                 \"s3:GetObjectAcl\"             ],             \"Resource\": [                 \"arn:aws:s3:::BucketDev2/*\",                 \"arn:aws:s3:::BucketDev2\"             ]         }     ] }\nThis policy allows the S3-Lambda role from Dev1 account to write objects to the specified bucket, BucketDev2. The policy also grants access to the specified bucket and all objects within it.\nNote that the ARN of the IAM role needs to be updated with the Dev1 account ID.\n\n"
}, {
  "id" : 358,
  "question" : "You have a serverless application with AWS Lambda.\nThis application has Lambda as a backend and has its data stored in DynamoDB.\nThis application is very latency-sensitive.\nHow could you alert your team if the application gets high latency in any of its components?\n",
  "answers" : [ {
    "id" : "7cd6435946404253a3eaf8a072707463",
    "option" : "Import the time library in the Lambda code. Calculate how much time it takes the Lambda to finish operations in DynamoDB, deploy these logs to Cloudwatch and create an alarm if the time in the logs is high.",
    "isCorrect" : "false"
  }, {
    "id" : "212c35d687a844d588878a20164a8738",
    "option" : "Enable X-Ray tracing on Lambda, use the AWS SDK inside Lambda code to monitor DynamoDB API calls in X-Ray, send this information to CloudWatch, and create a metric that triggers an SNS alert if the response times get too high.",
    "isCorrect" : "true"
  }, {
    "id" : "44a8b24c45c74677b4e285b8380623ea",
    "option" : "Enable X-Ray tracing on Lambda, use the AWS SDK inside Lambda code to monitor DynamoDB API calls in X-Ray, send this information to CloudTrail, and create a metric that triggers an SNS alert if the response times get too high.",
    "isCorrect" : "false"
  }, {
    "id" : "c68e0a9bfb5646ea9698723ce06b7c5b",
    "option" : "Enable X-Ray in Lambda and DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "c561ab46125540cfba19ce1bd3d39fd3",
    "option" : "Create a Lambda that will retrieve the info from X-Ray every minute. Send this information to CloudWatch, create a metric that triggers an SNS alert if the response times get too high.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect because this option considers how long it takes Lambda to establish a connection, perform operations in DynamoDB and return a response, in this question we only want to know how long it takes to connect with DynamoDB.\nAdditionally, more factors can affect the performance of the Lambda, for instance, cold starts, code optimization, or CPU memory.\nOption B is CORRECT because AWS X-Ray allows you to record every call to the other AWS service like DynamoDB and trace the latency for AWS services.\nMixing X-Ray with CloudWatch is a very good choice to get alarms if the latency becomes high.\nOption C is incorrect because with AWS CloudTrail you can track the API requests made by IAM users or services, Cloudtrail is not used to send alerts or to process logs.\nOption D is incorrect because until now, it is not possible to enable AWS X-Ray in DynamoDB.\nYou can only track the traces generated by calls to DynamoDB in Lambda with the AWS SDK.\n\nThe correct answer is B: Enable X-Ray tracing on Lambda, use the AWS SDK inside Lambda code to monitor DynamoDB API calls in X-Ray, send this information to CloudWatch, and create a metric that triggers an SNS alert if the response times get too high.\nExplanation: AWS Lambda is a serverless computing service that lets you run code without provisioning or managing servers. DynamoDB is a NoSQL database service that provides fast and predictable performance with seamless scalability. To monitor the latency of a serverless application, you can use AWS X-Ray, which is a service that helps you debug and analyze distributed applications.\nOption A suggests importing the time library in Lambda code and calculating how much time it takes the Lambda to finish operations in DynamoDB, deploying these logs to CloudWatch, and creating an alarm if the time in the logs is high. However, this approach requires manual intervention, and it does not provide a scalable solution.\nOption B suggests enabling X-Ray tracing on Lambda, using the AWS SDK inside Lambda code to monitor DynamoDB API calls in X-Ray, sending this information to CloudWatch, and creating a metric that triggers an SNS alert if the response times get too high. This approach provides an automated and scalable solution that leverages X-Ray's tracing capabilities.\nOption C suggests enabling X-Ray tracing on Lambda, using the AWS SDK inside Lambda code to monitor DynamoDB API calls in X-Ray, sending this information to CloudTrail, and creating a metric that triggers an SNS alert if the response times get too high. However, CloudTrail is not designed for real-time monitoring and alerting, so this approach is not suitable for latency-sensitive applications.\nOption D suggests enabling X-Ray in Lambda and DynamoDB. However, it does not provide a solution for monitoring and alerting on high latency.\nOption E suggests creating a Lambda that will retrieve the info from X-Ray every minute, sending this information to CloudWatch, creating a metric that triggers an SNS alert if the response times get too high. This approach requires manual intervention, and it does not provide real-time monitoring and alerting.\nIn conclusion, option B is the best solution for monitoring and alerting on high latency in a serverless application with Lambda and DynamoDB.\n\n"
}, {
  "id" : 359,
  "question" : "You have a data stream of 4.5 MB of throughput and 4600 PUT records per second, how many shards will you need in Amazon Kinesis to process this data stream?\n",
  "answers" : [ {
    "id" : "a73d989717e4466dbc2315e248cef3ad",
    "option" : "4",
    "isCorrect" : "false"
  }, {
    "id" : "a2d3a2bb86514b6fa6aa2a016cd2a306",
    "option" : "4.5",
    "isCorrect" : "false"
  }, {
    "id" : "a94a8bd1a01f4f7897f87d644b609c8a",
    "option" : "5",
    "isCorrect" : "true"
  }, {
    "id" : "6387ab87e9424c62a0109b1eb984abac",
    "option" : "1",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct answer: C.\nOption A is incorrect because the maximum capabilities of a shard is a throughput capacity of 1MB data input and 1000 PUT records per second.\nMore information: https://aws.amazon.com/kinesis/data-streams/faqs/#:~:text=Shard%20is%20the%20base%20throughput,you%20create%20a%20data%20stream.\nOption B is incorrect because you can only have complete units of shards.\nOption C is CORRECT because with 5 shards you will have 5MB MB/sec of data input and 5000 PUT records per second.\nWith this you can cover the requirements.\nOption D is incorrect because one shard is not enough to cover these requirements.\n\nTo calculate the number of shards required for a Kinesis stream, we need to consider the total throughput and the number of records per second.\nEach shard in a Kinesis stream can handle up to a maximum of 1 MB of data per second or 1000 records per second (whichever limit is reached first).\nSo, the first step is to determine which limit is reached first for our data stream:\nThroughput limit: 4.5 MB/sec ÷ 1 MB/sec per shard = 4.5 shards (rounded up to 5 shards)\nRecords per second limit: 4600 records/sec ÷ 1000 records/sec per shard = 4.6 shards (rounded up to 5 shards)\nSince we need to round up to the nearest integer, we can see that we need a minimum of 5 shards to handle this data stream.\nTherefore, the answer is C. 5.\n\n"
}, {
  "id" : 360,
  "question" : "Which of these options is NOT a true advantage of Kinesis over SQS?\n",
  "answers" : [ {
    "id" : "f0295bf8729540ed9d38cf41cb4738b9",
    "option" : "Kinesis is able to route data records to selected shards using a partition key.",
    "isCorrect" : "false"
  }, {
    "id" : "2f2e82cd67d94821903b0eb7243a976e",
    "option" : "Kinesis is a queue that can be replayed.",
    "isCorrect" : "false"
  }, {
    "id" : "48b2b1cd37b44f5c99785f03a98ef0c9",
    "option" : "Kinesis is able to maintain the order of log statements.",
    "isCorrect" : "false"
  }, {
    "id" : "6d29aff4260948b9bfe058478d3c2f0f",
    "option" : "Kinesis can track the successful completion of each item independently.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct answer: D.\nOption A is incorrect because this statement is correct.\nOption B is incorrect because this statement is correct.\nOption C is incorrect because this statement is correct.\nOption D is CORRECT because as the question says, this was the only false statement.\nKinesis does not have this capability.SQS tracks the successful and failed messages, SQS will delete successful messages and redeliver failed messages after a visibility timeout.\nMore details:\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\n\nAmazon Kinesis and Amazon Simple Queue Service (SQS) are both messaging services provided by AWS that enable decoupling of application components, allowing for greater scalability and reliability. However, there are several key differences between the two services.\nOption B is NOT a true advantage of Kinesis over SQS.\nAmazon Kinesis is a real-time data streaming service that is designed to handle high-volume, high-speed data streams. It can capture and process data in real-time, making it well-suited for use cases such as real-time analytics, machine learning, and Internet of Things (IoT) data processing.\nOn the other hand, Amazon SQS is a message queuing service that enables decoupling and asynchronous communication between application components. SQS is designed to handle messages in a distributed environment, making it well-suited for use cases where reliability and fault-tolerance are important.\nHere are some of the advantages of Kinesis over SQS:\nA. Partition key-based routing: With Kinesis, data records can be routed to specific shards using a partition key. This enables you to control the ordering of messages and to ensure that related messages are processed by the same consumer application. In contrast, with SQS, messages are sent to a queue and consumed in a first-in, first-out (FIFO) order. You cannot control the order in which messages are consumed.\nC. Maintaining order of log statements: Kinesis is designed to maintain the order of log statements, making it well-suited for use cases where maintaining the order of events is critical. In contrast, SQS does not guarantee message order.\nD. Tracking successful completion: Kinesis can track the successful completion of each data item independently, which makes it easier to build fault-tolerant applications. With SQS, messages are deleted from the queue as soon as they are consumed, so it can be difficult to determine which messages were processed successfully and which were not.\nIn summary, the advantages of Kinesis over SQS include the ability to route data records using a partition key, maintain the order of log statements, and track the successful completion of each data item. Option B is NOT a true advantage of Kinesis over SQS.\n\n"
}, {
  "id" : 361,
  "question" : "Which of the next options could you use as MFA in Cognito? (Select two)\n",
  "answers" : [ {
    "id" : "1f1cb0e07af7459c89cf8bfdf2b6b500",
    "option" : "Google Authenticator",
    "isCorrect" : "true"
  }, {
    "id" : "29783c6911864437b1412c2ec0990afe",
    "option" : "Cognito Identity Pool",
    "isCorrect" : "false"
  }, {
    "id" : "22dd13bcf06c44309b5d2fba319cde42",
    "option" : "SMS text message with MFA code.",
    "isCorrect" : "true"
  }, {
    "id" : "c432bbb160644c778047b04a1e7e93f9",
    "option" : "SES.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct answers A &amp; C.\nOption A is CORRECT because Google authenticator is a time-based one-time (TOTP) password service.\nMore details: https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa-totp.html.\nOption B is incorrect because Cognito identity pool is used to provide credentials to users in a user pool in Cognito.\nMore details: https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html.\nOption C is CORRECT because you can set up Cognito to send an authentication code via SMS after you submit your correct credentials.\nMore details: https://docs.aws.amazon.com/cognito/latest/developerguide/user-pool-settings-mfa-sms-text-message.html.\nOption D is incorrect because SES is a service used to send emails.\nMore details: https://aws.amazon.com/ses/\n\nThe answer is A. Google Authenticator and C. SMS text message with MFA code.\nAmazon Cognito supports multiple factors of authentication (MFA) to help improve the security of your user sign-in process. Here's a brief explanation of each of the options:\nA. Google Authenticator: It is a time-based one-time password (TOTP) authentication method that uses a secret key and a TOTP algorithm to generate unique codes that can be used as an additional factor for authentication. The Google Authenticator app can be installed on a user's mobile device, and it generates the TOTP codes without requiring a network connection.\nB. Cognito Identity Pool: It is a secure and scalable authentication and authorization service that allows users to sign in with an identity provider (such as Amazon, Facebook, or Google) or with their own user credentials. However, it is not a MFA option.\nC. SMS text message with MFA code: It sends a one-time password (OTP) to the user's mobile phone via SMS. The user enters the OTP into the sign-in page to complete the authentication process. This method is widely used but is considered less secure than other MFA methods because it is susceptible to SIM swapping attacks.\nD. SES: Amazon Simple Email Service (SES) is a scalable and cost-effective email service that can be used to send transactional emails, marketing messages, or other types of content. It is not a MFA option.\nIn conclusion, options A and C are the correct answers for this question, as they are both supported MFA methods in Amazon Cognito.\n\n"
}, {
  "id" : 362,
  "question" : "You are developing an application that is using AWS Lambda.\nA Lambda function is placed in a private subnet which does not have access to internet.\nThis Lambda function will receive bank account information as a JSON object.\nThis JSON is about 7 Kb of size.\nYou need to encrypt this information and then save it to DynamoDB.\nWhich approach could satisfy this requirement?\n",
  "answers" : [ {
    "id" : "7a1862701ed34e9fa2805de010cbb28b",
    "option" : "Create a VPC Endpoint in the Lambda VPC for the KMS Service. Generate a new Customer Master Key. Encrypt the JSON data using KMS Envelope Encryption. Upload the encrypted data to DynamoD.",
    "isCorrect" : "true"
  }, {
    "id" : "f76f24b63eb1433c8f0343702f3cb327",
    "option" : "Generate a new Customer Master Key. Encrypt the JSON data using KMS Envelope Encryption. Upload the encrypted data to DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "46c5df787ff241a79c15588132f6bbc3",
    "option" : "Create a VPC Endpoint in the Lambda VPC for the KMS Service. Generate a new Customer Master Key. Encrypt the JSON data with the encrypt() KMS method using the CMS previously generated. Upload the encrypted data to DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "86506263f28d4a81837ddfefb72eb071",
    "option" : "Generate a new Customer Master Key. Encrypt the JSON data with the encrypt() KMS method using the CMS previously generated. Upload the encrypted data to DynamoD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT because the Lambda function is placed in a subnet with no internet access, so it will need the KMS VPC endpoint in order to work with KMS.\nBecause the JSON object is bigger than 4kb you will need to encrypt using Envelope Encryption.\nMore details: https://lobster1234.github.io/2017/09/29/aws-kms-envelope-encryption/\nOptionB is incorrect because the Lambda Function needs internet access in order to communicate with KMS.\nMore details: https://aws.amazon.com/blogs/security/how-to-connect-directly-to-aws-key-management-service-from-amazon-vpc-by-using-an-aws-privatelink-endpoint/\nOption C is incorrect because the JSON object is 7Kb of size and the maximum size allowed with the KMS encryption() method is 4kb.\nMore details: https://docs.aws.amazon.com/cli/latest/reference/kms/encrypt.html.\nOption D is incorrect because the Lambda Function needs internet access in order to communicate with KMS.\nMore details: https://aws.amazon.com/blogs/security/how-to-connect-directly-to-aws-key-management-service-from-amazon-vpc-by-using-an-aws-privatelink-endpoint/\n\nThe correct answer is A: Create a VPC Endpoint in the Lambda VPC for the KMS Service. Generate a new Customer Master Key. Encrypt the JSON data using KMS Envelope Encryption. Upload the encrypted data to DynamoDB.\nHere is a detailed explanation of why this is the best approach:\nCreate a VPC Endpoint in the Lambda VPC for the KMS Service: The Lambda function is in a private subnet that does not have internet access, so it cannot directly access the KMS service, which is a managed service that provides encryption keys. To enable the Lambda function to use KMS, we need to create a VPC endpoint in the Lambda VPC for the KMS service. A VPC endpoint is a private connection between the VPC and the KMS service, which does not require internet access. Generate a new Customer Master Key: A Customer Master Key (CMK) is a logical representation of a master key in KMS. It is used to encrypt and decrypt data. In this case, we need to generate a new CMK to encrypt the bank account information. Encrypt the JSON data using KMS Envelope Encryption: KMS Envelope Encryption is a technique used to encrypt large amounts of data with KMS. It uses a data encryption key (DEK) to encrypt the data, and then encrypts the DEK with the CMK. This allows us to encrypt the large JSON object without hitting the maximum size limit of KMS encryption. Upload the encrypted data to DynamoDB: After encrypting the JSON data, we can upload it to DynamoDB, a fully managed NoSQL database service provided by AWS. DynamoDB allows us to store and retrieve data with high availability and durability.\nOption B is incorrect because it does not specify how the Lambda function can access KMS in a private subnet. Option C is incorrect because it mentions the use of the encrypt() method, which is not a valid method for encrypting data with KMS. Option D is incorrect because it does not specify how the Lambda function can access KMS in a private subnet.\n\n"
}, {
  "id" : 363,
  "question" : "You are the security leader in a firm that has its workloads on AWS.\nYou have been asked to implement a way for developers to pass their roles to EC2\nThere are two conditions: Do not let the developers pass the Administrator role to EC2. The credentials for the role must last a maximum of 45 minutes. How could you achieve this?\n",
  "answers" : [ {
    "id" : "66c374cd8424479fb61392bdddf7c6bc",
    "option" : "Create a role named “EC2NotAdmin”. Create a new policy and attach it to the EC2NotAmin Role. This policy will have two statements. The first policy will have a statement to deny PassRole with the Admin Role as a resource. The second policy will allow a statement to pass the role to all resources.",
    "isCorrect" : "false"
  }, {
    "id" : "de1b7728355c479daf764cc4f7a86be5",
    "option" : "Create a role named “EC2NotAdmin”.Create a new policy and attach it to the IAM role. This policy will have two statements. The first policy will have a statement to allow PassRole with the Admin Role as a resource. The second policy will deny the statement to pass the role to all resources.",
    "isCorrect" : "false"
  }, {
    "id" : "f30c23f7c6f241c68c3295d0e4e16fe2",
    "option" : "Create a role named “EC2NotAdmin”. Attach a trust policy to the role that allows the service EC2 to assume a role and set the condition sts:DurationSeconds to 45 minutes. Create a new policy and attach it to the IAM role. This policy will have two statements. The first policy will have a statement to deny PassRole with the Admin Role as a resource. The second policy will allow the statement to pass the role to all resources.",
    "isCorrect" : "true"
  }, {
    "id" : "c72df54cad214c26aec7950e98be378b",
    "option" : "Create a role named “EC2NotAdmin”. Attach a trust policy to the role that allows the service EC2 to assume a role and set the condition sts:DurationSeconds to 45 minutes. Create a new policy and attach it to the IAM role. This policy will have two statements. The first policy will have a statement to allow PassRole with the Admin Role as a resource. The second policy will deny the statement to pass the role to all resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect because there is no trust policy for EC2, therefore EC2 won't be able to assume any role.\nMore details: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html.\nOption B is incorrect because there is no trust policy for EC2, therefore EC2 won't be able to assume any role.\nAlso the explicit deny in the policy will overwrite the allow policy.\nOption C is CORRECT because with the trust policy the EC2 could assume a role.\nIn the policy there is an explicit deny to pass the AdminRole.\nTherefore the Developer could pass any role except the Admin one.\nMore details: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html.\nOption D is incorrect because the explicit deny in the policy will overwrite the allow policy.\nMore details: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html.\n\nThe correct answer to this question is A. Create a role named \"EC2NotAdmin\". Create a new policy and attach it to the EC2NotAdmin Role. This policy will have two statements. The first policy will have a statement to deny PassRole with the Admin Role as a resource. The second policy will allow a statement to pass the role to all resources.\nExplanation: The scenario presented in the question requires the implementation of a solution that allows developers to pass their roles to EC2 instances, but only under specific conditions. In this case, the solution must comply with two requirements: do not allow the developers to pass the Administrator role to EC2, and make sure the credentials for the role last no more than 45 minutes.\nTo achieve this, the first step is to create a new IAM role named \"EC2NotAdmin\". Then, a new policy must be created and attached to this role. This policy should have two statements, which are:\nDeny PassRole with the Admin Role as a resource: This statement ensures that developers cannot pass the Administrator role to EC2 instances. This is achieved by specifying the \"NotResource\" field in the statement, which allows the role to be passed to any resource except the Administrator role. Allow a statement to pass the role to all resources: This statement grants the necessary permissions for developers to pass their roles to EC2 instances, but only for roles other than the Administrator role. Additionally, this statement sets the maximum duration for the credentials to 45 minutes, which meets the second requirement.\nTherefore, the correct solution is A, which implements the required restrictions and permissions to allow developers to pass their roles to EC2 instances while complying with the specified conditions.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 365,
  "question" : "Your security Chief Officer has asked you to implement a solution to send an email alert every time someone logged in the AWS console or AWS Cli with the Administrator Role.\nHow could you implement this?\n",
  "answers" : [ {
    "id" : "941fe2d323d646239b07b1032119ea0e",
    "option" : "Create a new Trail in Cloudtrail and send all the events to a new Cloudwatch Log Group. Create a rule in Cloudwatch that triggers an SNS Alert if the event record of CloudTrail contains the ARN of the AdminRole.",
    "isCorrect" : "true"
  }, {
    "id" : "08268157f075445aad39690921516ede",
    "option" : "Create a Cloudtrail trigger to send an SNS alert if the event record of CloudTrail contains the ARN of the AdminRole.",
    "isCorrect" : "false"
  }, {
    "id" : "857e520015404d42bac138214217b38a",
    "option" : "Create a new trail and configure SNS notifications when new logs of Cloudtrail are published.",
    "isCorrect" : "false"
  }, {
    "id" : "98c4b6a81f2c4210a7d12baee7cd70ab",
    "option" : "Create a new Trail in Cloudtrail and send all the events to a new Cloudwatch Log Group. Create a rule in Cloudwatch that triggers an SNS Alert if the event record of CloudTrail contains the word “Admin”.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT the event record of Cloudtraiils has the ARN of the role that the user has asked to STS.\nWhen Cloudwatch detects this it will trigger an alarm in SNS.\nMore details: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html.\nOption B Is incorrect the SNS integration with Cloudtrail does not have these parameters to match specific information in the records.\nMore details: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/configure-sns-notifications-for-cloudtrail.html#configure-cloudtrail-to-send-notifications.\nOption C Is incorrect with this approach SNS will send an alarm of all the events that Cloudtrails log.\nOption D Is incorrect we are looking to log only the events with the ARN of the role.\nThis approach could log more events.\n\nThe correct answer is A: Create a new Trail in Cloudtrail and send all the events to a new Cloudwatch Log Group. Create a rule in Cloudwatch that triggers an SNS Alert if the event record of CloudTrail contains the ARN of the AdminRole.\nHere's an explanation of the solution:\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It provides a record of actions taken by a user, role, or an AWS service in your account. CloudTrail captures API calls made in the AWS Management Console, AWS CLI, AWS SDKs, and other AWS services.\nTo implement the solution, follow these steps:\nCreate a new CloudTrail Trail: In the AWS Management Console, go to the CloudTrail service and create a new trail. Select the S3 bucket where the logs will be stored and choose to log all management events. Configure CloudTrail to send events to CloudWatch Logs: Once the trail is created, configure CloudTrail to send events to a new CloudWatch Logs group. This can be done from the CloudTrail service console. Create a CloudWatch Logs subscription filter: Once the CloudTrail logs are streaming to the CloudWatch Logs group, create a new subscription filter for the CloudWatch Logs group. This filter will search for events in the log group that match a specific pattern. Configure SNS to send an email alert: Create a new SNS topic and configure an email subscription. Then, create a CloudWatch rule that triggers an SNS notification when a CloudTrail event matches the pattern specified in the subscription filter. The pattern should match the ARN of the Administrator Role.\nWith this solution in place, you will receive an email alert every time someone logs into the AWS console or CLI with the Administrator Role. This solution provides a way to monitor and track all actions taken by users with elevated permissions in your AWS account.\n\n"
}, {
  "id" : 366,
  "question" : "You want to read 420 items of 5KB each from a DynamoDB table every minute using strong consistency read.\nHow many RCUs will you need to provision your DynamoDB table with?\n",
  "answers" : [ {
    "id" : "8fb7b5400c9243e7ad6a31880f553e49",
    "option" : "10",
    "isCorrect" : "false"
  }, {
    "id" : "42a43ed58d5d416ea986aa121b093a4a",
    "option" : "14",
    "isCorrect" : "true"
  }, {
    "id" : "3754ac806d8b467194f2c232d3e3e6a0",
    "option" : "28",
    "isCorrect" : "false"
  }, {
    "id" : "0c8b22588e2b4420b077c5f97e94bd1f",
    "option" : "12",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\n1 RCU provides 4KB read per second for a strong consistency model.\nItems to read every second: 420/60 = 7\nTo read one item, we need: 5/4 = 1.25, rounding UP to the nearest whole number will be 2\nHence, we require 2 RCUs to read one item.\nTotal RCUs required: 2*7 = 14\nOption A, C and D are incorrect since they are not the required RCUs to perform the above operation.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nTo calculate the provisioned read capacity units (RCUs) needed to read 420 items of 5KB each from a DynamoDB table every minute using strong consistency read, we can use the following formula:\nRCUs = (item_size * number_of_items * number_of_reads_per_second) / 4KB\nwhere item_size is the size of each item in KB, number_of_items is the number of items to be read, and number_of_reads_per_second is the frequency of reads per second.\nIn this case, item_size is 5KB, number_of_items is 420, and number_of_reads_per_second is 420/60 = 7 (since we want to read 420 items every minute).\nTherefore, the provisioned RCUs required would be:\nRCUs = (5KB * 420 * 7) / 4KB = 735\nSo, the answer is B. 14 (since the smallest unit of RCUs that can be provisioned is 1 and we need at least 735 RCUs).\nNote that this assumes that there are no other reads or writes happening on the table, and that the provisioned RCUs are sufficient to handle the peak load. In practice, you may want to monitor the table's performance and adjust the provisioned RCUs accordingly.\n\n"
}, {
  "id" : 367,
  "question" : "You are developing a gaming application that needs to write 100 items every second.\nEach item is of 2KB containing session data and game data.\nHow many WCUs do you need to provision for your table?\n",
  "answers" : [ {
    "id" : "8082f95dcf8e4bf6931b9aeed12cf445",
    "option" : "100",
    "isCorrect" : "false"
  }, {
    "id" : "5bff8784bced4e42b5e4b46b2635509d",
    "option" : "50",
    "isCorrect" : "false"
  }, {
    "id" : "fc73da8dddda47d99bbf8e21dcf3a15c",
    "option" : "400",
    "isCorrect" : "false"
  }, {
    "id" : "42078758fcfb453a81ac687bdbf458b1",
    "option" : "200",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\n1 WCU provides 1KB write per second.\nItems to write every second: 100\nTo write one item, we need: 1*2=2\nHence, we require 2 WCUs to write one item.\nTotal WCUs required: 2*100 = 200\nOption A, B and C are incorrect since they are not the required WCUs to perform the above operation.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nTo determine the WCU (Write Capacity Unit) required for the given scenario, we need to first calculate the total data size that will be written per second.\n100 items per second, each item containing 2KB of data, therefore: Total data size per second = 100 * 2KB = 200KB/s\nNext, we need to calculate the WCU required to handle this amount of data.\nOne WCU can handle up to 1 KB of data per second, so we need to divide the total data size per second by 1KB to get the required number of WCUs.\nWCU required = (200KB/s) / (1KB/WCU) = 200 WCUs\nTherefore, the answer is D. 200.\nWe need to provision 200 WCUs to handle the write capacity for the gaming application to write 100 items every second, each item containing session data and game data of 2KB size.\n\n"
}, {
  "id" : 368,
  "question" : "You are working on a client application that uses DynamoDB as its database to store IoT sensor data.\nThe application is receiving ProvisionedThroughputExceededException too frequently for read requests.\nThe client has asked you to overcome this issue by adjusting RCUs.\nThe application receives 1200 eventual read requests every minute where each request is of 2.5KB.\nHow many RCUs does the application need to fulfill all the requests?\n",
  "answers" : [ {
    "id" : "7b460231baf44baa9ee360d2679b1327",
    "option" : "10",
    "isCorrect" : "true"
  }, {
    "id" : "46fca80e86e14f0eb93843cb66399b25",
    "option" : "11",
    "isCorrect" : "false"
  }, {
    "id" : "fbf22098652342caae16936d56c6a035",
    "option" : "20",
    "isCorrect" : "false"
  }, {
    "id" : "2e4bcf81b79f45bf9fc2fbd67361825c",
    "option" : "9",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is correct: 1 RCU provides two 4KB read per second for the eventual consistency model.\nItems to read every second: 1200/60 = 20\nTo read one item, we need: 2.5/4 = 0.63, rounding off to the nearest whole number will be 1\nHence, we require 1 RCUs to read one item.\nTotal RCUs required: 1*20 = 20; 20/2 = 10, because eventual consistency model provides us 2 4KB read every second.\nOption B, C and D are incorrect since they are not the required RCUs to overcome ProvisionedThroughputExceededException error.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html\n\nDynamoDB is a highly scalable NoSQL database service offered by AWS. It provides a Provisioned Throughput capacity that allows you to specify the number of reads and writes per second that you need for your application.\nIn this scenario, the application is receiving a large number of read requests, and the application is experiencing a ProvisionedThroughputExceededException error. This error is thrown when the application is trying to read more data than the provisioned throughput capacity that is assigned to the table.\nTo overcome this issue, the client has asked you to adjust the Read Capacity Units (RCUs). An RCU is a unit of measurement for the amount of capacity that is assigned to read operations in DynamoDB. One RCU can read up to 4KB of data per second.\nTo calculate the RCUs required for your application, you need to calculate the amount of data that is being read per second. In this scenario, the application is receiving 1200 read requests every minute, and each request is of 2.5KB. Therefore, the total amount of data that is being read per second can be calculated as:\n1200 requests/minute * 2.5KB/request * (1 minute/60 seconds) = 50 KB/second\nTo convert the amount of data into RCUs, you need to divide the amount of data by the maximum amount of data that one RCU can read per second, which is 4KB. Therefore, the RCUs required can be calculated as:\n50 KB/second / 4KB/second = 12.5 RCUs\nAs DynamoDB only supports integers, you need to round up the RCUs to the nearest integer, which is 13 RCUs. However, none of the answer options provided match with 13, so the closest option is 11 (option B). Therefore, the correct answer is option B.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 371,
  "question" : "You have been hired as a Database administrator by a start-up.\nYour first task is to modify a DynamoDB table so that its data expires automatically after a certain period of time.\nUpon looking at the documentation, you figured out that DynamoDB supports a concept of TTL using which you can achieve the same.\nWhat are the steps to use the feature?\n",
  "answers" : [ {
    "id" : "fcc0ba3401e044c096475b16e6e6d9f6",
    "option" : "Enable TTL and use the dataExpiry keyword as a key attribute to store the expiry timestamp.",
    "isCorrect" : "false"
  }, {
    "id" : "9741ee881c9e453897ce5f8354ccbdbc",
    "option" : "Enable TTL and use any name of your choice as a key attribute to store the expiry timestamp.",
    "isCorrect" : "true"
  }, {
    "id" : "50afd296b41c4b7bb326277e3bb44e49",
    "option" : "Enable TTL and use the keyword expiryTTL as a key attribute to store the expiry timestamp.",
    "isCorrect" : "false"
  }, {
    "id" : "5730fc248e184981ba9125a413b9559a",
    "option" : "It is by default enabled and will automatically pick a key attribute with timestamp value.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nWhile enabling TTL, you can provide any name of your choice to use as a key attribute name to be used for storing timestamp value.\nOption A is incorrect: dataExpiry is not a reserved keyword for TTL.\nOption C is incorrect: expiryTTL is also not a reserved keyword for TTL.\nOption D is incorrect: TTL is not enabled by default, and it does not even pick the TTL key attribute automatically.\nReference:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html\n\nAs a Database Administrator, you can use the Time to Live (TTL) feature in DynamoDB to remove data that is no longer needed. When you enable TTL for a table, you can set a timestamp for each item in the table. When the timestamp for an item expires, the item is automatically deleted from the table.\nTo enable TTL for a table in DynamoDB, you need to perform the following steps:\nEnable TTL: You need to enable TTL for the DynamoDB table. You can do this by selecting the table in the DynamoDB console, clicking on the 'Manage TTL' button and then setting the TTL attribute. Choose Key Attribute: You need to choose a key attribute in the table that will store the expiry timestamp. The attribute that you choose should be of the Number data type and should be in Unix epoch time format. Set Expiry Timestamp: Once you have chosen the key attribute, you need to set the expiry timestamp for each item in the table. You can do this by setting the value of the key attribute to the Unix epoch time when the item should expire. Wait for TTL to take effect: Once you have enabled TTL and set the expiry timestamp for the key attribute, you need to wait for TTL to take effect. DynamoDB checks the TTL attribute of each item in the table approximately once every hour. When DynamoDB finds an expired item, it deletes the item from the table.\nOption D is incorrect because TTL is not enabled by default in DynamoDB. You need to enable it manually for each table. Option A and Option C are also incorrect because 'dataExpiry' and 'expiryTTL' are not valid keywords to store the expiry timestamp. You can use any attribute name of your choice to store the expiry timestamp, as long as it is of the Number data type and in Unix epoch time format. Therefore, the correct answer is Option B - Enable TTL and use any name of your choice as a key attribute to store the expiry timestamp.\n\n"
}, {
  "id" : 372,
  "question" : "You have been asked to troubleshoot a serverless application that is occasionally throwing errors while storing payment transaction details.\nThe application is using DynamoDB as its database.\nUpon actively looking at the application logs, you see ProvisionedThroughputExceededException as an error against each timestamp application unable to serve the request successfully.\nWhich of the following combinations of steps would work to overcome this problem? (Select TWO)\n",
  "answers" : [ {
    "id" : "fc7ab39f75ff41ffaa26489bfb2aed23",
    "option" : "Create one more table and split the data between both the tables.",
    "isCorrect" : "false"
  }, {
    "id" : "2a72219ae22941ea9e2a3901f1a0840b",
    "option" : "Use Exponential Backoff algorithm.",
    "isCorrect" : "true"
  }, {
    "id" : "cbc94de64af34510b7237b7bc5dbb7bb",
    "option" : "Increase RCUs and WCUs to get rid of ProvisionedThroughputExceededException.",
    "isCorrect" : "false"
  }, {
    "id" : "427290615ff448d08a5b46cb31402ef5",
    "option" : "Use AWS SDK.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: B and D.\nNumerous components on a network, such as DNS servers, switches, load balancers, and others, can generate errors anywhere in the life of a given request.\nThe usual technique for dealing with these error responses in a networked environment is to implement retries in the client application.\nEach AWS SDK implements retry logic automatically.\nYou can modify the retry parameters to your needs.\nIn addition to simple retries, each AWS SDK implements an exponential backoff algorithm for better flow control.\nThe concept behind exponential backoff is to use progressively longer waits between retries for consecutive error responses.\nFor example, up to 50 milliseconds before the first retry, up to 100 milliseconds before the second, up to 200 milliseconds before the third, and so on.\nHowever, after a minute, if the request has not succeeded, the problem might be the request size exceeding your provisioned throughput, and not the request rate.\nSet the maximum number of retries to stop around one minute.\nIf the request is not successful, investigate your provisioned throughput options.\nOption A is incorrect: This is not practically feasible and involves overhead in implementation.\nHence, this is not the best practice.\nOption C is incorrect: Mere increasing RCUs and WCUs every time you receive this error is not an optimal solution.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.RetryAndBackoff\n\nThe ProvisionedThroughputExceededException error in DynamoDB indicates that the number of read or write requests to a table exceeds its provisioned throughput capacity. This error may occur if the application is making too many requests at once or if the table's provisioned capacity is too low.\nTo overcome this problem, we can take the following steps:\nUse Exponential Backoff Algorithm: When the application receives a ProvisionedThroughputExceededException error, it should wait for a random amount of time before retrying the request. This wait time should increase exponentially with each retry attempt, using a backoff algorithm. Exponential backoff helps prevent the application from flooding the table with requests and potentially exhausting its provisioned capacity. Increase RCUs and WCUs: If the application is regularly exceeding the table's provisioned capacity, it may be necessary to increase the number of read and write capacity units (RCUs and WCUs) assigned to the table. This will allow the table to handle more requests per second. The number of RCUs and WCUs should be increased gradually to avoid sudden spikes in traffic.\nOptions A and D are not relevant to the problem at hand. Creating a new table would not address the issue of the table's provisioned capacity being exceeded, and using the AWS SDK is not a solution in itself.\nTherefore, the correct answers to the question are B and C.\n\n"
}, {
  "id" : 373,
  "question" : "You have been asked to design a serverless web application with DynamoDB as its database.\nThe client wants to record each and every change in data in DynamoDB and store it individually as a file on S3\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "d164046f2f9c4137b6a82a76da0a28f1",
    "option" : "Write a script that will record each and every create or update request and store it on S3.",
    "isCorrect" : "false"
  }, {
    "id" : "dc6069dc277e45ab9cffbfc9a4e99e63",
    "option" : "Enable DynamoDB streams and attach them to Lambda which will read the data and store it on S3.",
    "isCorrect" : "true"
  }, {
    "id" : "4efb44330ec444799a35c4da0856f55b",
    "option" : "Enable DynamoDB streams and set the destination to S3. This will automatically store each change on S3.",
    "isCorrect" : "false"
  }, {
    "id" : "59e0e6aeb8524dd691aa130551a03dbf",
    "option" : "Use DynamoDB binary logging feature to record each and every change on S3.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nYou can enable DynamoDB streams to record both old and new image and send it to a Lambda function which can store the changes on S3.\nOption A is incorrect: This is not an ideal choice because the same can be implemented using DynamoDB streams.\nOption C is incorrect: DynamoDB stream only supports Lambda as a target as of writing this question.\nOption D is incorrect: DynamoDB does not support binary logging.\nThis feature is only supported for SQL databases.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\n\nThe correct answer for this scenario is option B, which is to enable DynamoDB streams and attach them to Lambda, which will read the data and store it on S3.\nDynamoDB Streams is a feature of Amazon DynamoDB that captures a time-ordered sequence of item-level modifications in a DynamoDB table. When an item in a table is modified, DynamoDB Streams writes a stream record containing the modification data to a DynamoDB stream. The stream record contains information such as the modified item, the type of modification, and the time of the modification.\nLambda is a serverless compute service that lets you run code without provisioning or managing servers. You can use AWS Lambda to process stream records from DynamoDB Streams and perform any actions you want, such as writing to an Amazon S3 bucket. When you attach a Lambda function to a DynamoDB stream, the function is triggered whenever a new stream record is written.\nTherefore, to achieve the client's requirement of recording each and every change in data in DynamoDB and storing it individually as a file on S3, you need to enable DynamoDB streams on the table and configure a Lambda function to process the stream records and write them to S3.\nOption A, which is to write a script that will record each and every create or update request and store it on S3, is not a scalable solution, and it would require you to manage the infrastructure that runs the script.\nOption C, which is to enable DynamoDB streams and set the destination to S3, is not valid since there is no direct integration of DynamoDB streams to store records on S3.\nOption D, which is to use DynamoDB binary logging feature to record each and every change on S3, is not applicable to the requirement of the client since the binary logs are not in a file format that can be easily accessed or managed. Additionally, this feature is primarily designed for auditing and compliance purposes rather than real-time data analysis or processing.\n\n"
}, {
  "id" : 374,
  "question" : "You are working on a web application whose database was created by a DB admin who is no longer working with your company.\nThe manager has asked you to handle the database administration.\nYou are working on a new reporting feature for which you need to query a DynamoDB table, and the attributes you need to query upon are not set as Partition Key and Sort Key.\nWhat is the best way to solve this problem?\n",
  "answers" : [ {
    "id" : "b241bd40f7cf4d03824188d3e68bede1",
    "option" : "Clone the existing table and change the Partition and Sort Key during the table creation process.",
    "isCorrect" : "false"
  }, {
    "id" : "3004dddfe5ce4aae83be289b5493637b",
    "option" : "Create a Local Secondary Index with required attributes as Partition and Sort Key.",
    "isCorrect" : "false"
  }, {
    "id" : "77fb8755699545a3b8a3075c01032811",
    "option" : "Create Global Secondary Index and set required attributes as Partition and Sort Key.",
    "isCorrect" : "true"
  }, {
    "id" : "2a4fc9bc958f4103bb0129d47d57b282",
    "option" : "Edit the table and change the existing Partition and Sort Key to the required ones.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nUsing Global Secondary Index, you can create a different pair of Partition and Sort key even after table creation for better search performance.\nOption A is incorrect: This is not the best practice as you are duplicating the data unnecessarily.\nOption B is incorrect: Local Secondary Index only supports additional Sort Key and cannot be created after the table has been created.\nOption D is incorrect: DynamoDB does not support this feature as of writing this question.\nReference:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\n\nThe correct answer is B. Create a Local Secondary Index with required attributes as Partition and Sort Key.\nDynamoDB is a NoSQL database, and it uses a different data model than relational databases. Tables in DynamoDB are made up of partitions, and each partition is made up of items. Each item has a primary key, which consists of a partition key and an optional sort key. Partition key is used to identify the partition where the item is stored, and sort key is used to sort items within the partition.\nIn DynamoDB, queries can only be performed on partition keys and sort keys. Therefore, to perform a query on attributes that are not set as Partition and Sort Key, we need to create an index.\nOption A, cloning the table and changing the Partition and Sort Key during table creation, is not a feasible solution because it requires copying all the data from the original table to the new table, which is not practical for large tables. Additionally, it does not solve the problem of querying on non-Partition Key and Sort Key attributes.\nOption C, creating a Global Secondary Index, is a possible solution, but it is an overkill. Global Secondary Indexes are used to query across partitions and allow for faster and more efficient querying, but they come at the cost of increased storage and write capacity requirements.\nOption D, editing the table and changing the existing Partition and Sort Key to the required ones, is not possible. Once a table is created, its Partition Key and Sort Key cannot be changed.\nTherefore, the best solution is to create a Local Secondary Index (LSI) with the required attributes as Partition and Sort Key. Local Secondary Indexes are indexes that are created within the same partition as the original table, and they can only be created at the time of table creation. LSI allows you to query on attributes that are not set as the table's Sort Key. However, you can only have up to five LSI per table, and they share the same provisioned throughput as the table.\nIn summary, Option B is the best solution to query upon attributes not set as Partition and Sort Key in DynamoDB.\n\n"
}, {
  "id" : 375,
  "question" : "You are working as a Senior Software Developer for a large pharmaceutical company.\nYour lead has asked you to work on a new module in which you need to query a DynamoDB table with multiple Partition Key values at once and store the result in CSV format on S3\nWhich operation will you use to achieve the same?\n",
  "answers" : [ {
    "id" : "2d59787caba245518afd0eb94de0602d",
    "option" : "Scan",
    "isCorrect" : "false"
  }, {
    "id" : "4e968ec00e41435cb4211fe5b97506f1",
    "option" : "Query",
    "isCorrect" : "false"
  }, {
    "id" : "ff71dbb53c43417fb9e4b12f2bae2985",
    "option" : "GetItem",
    "isCorrect" : "false"
  }, {
    "id" : "e1f7e4bf3d544a199106126b2b6bfb39",
    "option" : "BatchGetItem.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nBatchGetItem API allows you to pass multiple Partition Key values in a single request.\nOption A is incorrect: Scan API does not query table using Partition or Sort Key in request.\nIt fetches all records from the table.\nOption B is incorrect: Query API does not support querying table using a multiple Partition Key values.\nOption C is incorrect: GetItem API only accepts a single Partition or Sort key for querying data.\nUnlike Query operation, GetItem API requires passing of both Partition and Sort Key in the request.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html\n\nThe operation that you would use to query a DynamoDB table with multiple Partition Key values at once and store the result in CSV format on S3 is BatchGetItem.\nBatchGetItem is a DynamoDB API operation that allows you to retrieve multiple items from one or more tables in a single request. It is designed to be efficient for retrieving items that have a known primary key. In this case, you are querying the table with multiple Partition Key values at once.\nTo use BatchGetItem, you would specify the table name and a list of keys for each item you want to retrieve. The keys can be either a single Partition Key value or a composite Primary Key value. If any of the requested items are not found, the operation returns only the items that were found.\nOnce you have retrieved the items using BatchGetItem, you can then convert them to CSV format and store them on S3 using the AWS SDK or a third-party library. The exact implementation would depend on the programming language and framework you are using.\nThe other options provided in the question are not suitable for this use case:\nScan retrieves all items in a table and is not efficient for querying specific items. Query can only retrieve items with a specific Partition Key value or a combination of Partition Key and Sort Key values. GetItem retrieves a single item based on its primary key value, so it cannot be used to retrieve multiple items at once.\n\n"
}, {
  "id" : 376,
  "question" : "You and your colleague are working on a serverless application that involves the registration of 500 users at a time.\nYou have written two functions to support this where one function handles the storing of information to DynamoDB, and the other sends a confirmation email using SES.One of the functions that sends an email when a new user's information is saved to DynamoDB doesn't scale.\nOn querying DynamoDB, you realize all 500 user's information is saved successfully.\nBut CloudWatch shows failed invocations for the second function.\nHow will you address the bottleneck in the least possible time?\n",
  "answers" : [ {
    "id" : "2124655408a34770a61d2e6a46394543",
    "option" : "Use AWS ALB with Lambda function to manage traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "7dc86d83242a4db5b7e18eb8b50325a8",
    "option" : "Refactor code to decouple and create multiple functions to send emails.",
    "isCorrect" : "false"
  }, {
    "id" : "6b9c11d152a14b19b301c006935fcfd7",
    "option" : "Reserve concurrency for your second Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "3949313f657e493c9d0545e883f39174",
    "option" : "Get your second function to poll DynamoDB for new records.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect because setting up ALB and routing traffic through ALB can take time.\nOption B is incorrect because refactoring code may consume a long time and may address the bottleneck.\nOption C is CORRECT given that 500 users are registering simultaneously, concurrency would be extremely important, and reserving it would guarantee invocations without having to use the unreserved pool of concurrency.\nEach account has a default concurrency limit of 900 as concurrent execution for all the available lambda functions, and 100 must be left for unreserved lambdas.\nTherefore setting concurrency per function to 500 will meet this requirement.\nAs a developer, it is extremely important to know the constraints of the operating environment.\nOption D is incorrect because polling DynamoDB may result in unnecessary invocations that can overload downstream resources such as SES.\nReference:\nhttps://docs.aws.amazon.com/lambda/latest/dg/per-function-concurrency.html\nAs mentioned here, Concurrency can be defined per function which reserves capacity for the function.\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/set-concurrency-limits-on-individual-aws-lambda-functions/\n\nIn this scenario, we have two serverless functions working together to register 500 users at a time. The first function stores user information to DynamoDB, and the second function sends a confirmation email using SES. However, the second function is not scaling properly, and CloudWatch is showing failed invocations for this function.\nTo address this bottleneck in the least possible time, we have four potential solutions to choose from:\nA. Use AWS ALB with Lambda function to manage traffic: This solution involves using an Application Load Balancer (ALB) to distribute traffic across multiple Lambda functions. While this may help with scaling, it's not clear if this will address the underlying issue with the email-sending function. It's possible that there are still problems with the function's code or configuration that need to be addressed.\nB. Refactor code to decouple and create multiple functions to send emails: This solution involves breaking up the email-sending function into smaller, more manageable functions. This can help with scaling and also make it easier to troubleshoot any issues that arise. However, this may require significant changes to the codebase and could take some time to implement.\nC. Reserve concurrency for your second Lambda function: This solution involves allocating a fixed number of concurrent executions for the email-sending function. This can help ensure that there are always enough resources available to handle incoming requests, which can help with scaling. However, this may not be the most efficient solution, as it could potentially waste resources when the function is not being used.\nD. Get your second function to poll DynamoDB for new records: This solution involves having the email-sending function regularly check DynamoDB for new records to process. This can help ensure that all user information is properly processed and can also help with scaling. However, this may not be the most efficient solution, as it could lead to unnecessary processing if there are no new records to process.\nOverall, the most efficient and effective solution will depend on the specific details of the problem and the constraints of the application. However, if we had to choose one solution from these four options, we would likely go with option B: refactoring the code to decouple and create multiple functions to send emails. This solution addresses the underlying issue with the email-sending function and can also help with scaling and troubleshooting in the future.\n\n"
}, {
  "id" : 377,
  "question" : "You have a SAM template used to deploy a Lambda function, and you are now working to create a new version.\nYour manager has asked you to instantly switch traffic once you have built and tested your version.\nWhat is the most efficient, effortless and simple way to achieve this?\n",
  "answers" : [ {
    "id" : "6d0f76b819ed469aac8b2496e02a28dd",
    "option" : "Use Qualifiers in the Management Console to select the version which you built and tested.",
    "isCorrect" : "false"
  }, {
    "id" : "6761c6feeddf4ef7ac4edbcab2786d61",
    "option" : "Point an alias, PROD to the version which you built and tested.",
    "isCorrect" : "false"
  }, {
    "id" : "2f62b6cfe6ce4e23bbe02572a4f3270c",
    "option" : "Set DeploymentPreference property of the function resource.",
    "isCorrect" : "false"
  }, {
    "id" : "18bbb56459a04a19bd3fdbc2c183377d",
    "option" : "Set AutoPublishAlias property of the function resource.",
    "isCorrect" : "true"
  } ],
  "explanations" : "\n\nAnswer: D.\nOption A is incorrect because using the Management Console might tempt a one-off deployment inefficient when it needs to be done several times.\nOption B is incorrect because pointing an alias will not instantly switch traffic as it doesn't state that the PROD alias is actually deployed.\nOption C is incorrect.Although it is the next closest answer; setting DeploymentPreference will immediately switch traffic but is used for a more complex deployment.\nWith this property, you can mention alarms that need to be monitored, hooks that can be run pre and post traffic shifting, and also a traffic switching type needs to be defined.\nOption D is CORRECT as the AutoPublishAlias property will create a new alias, create and publish a new version of the Lambda code, point alias to this version and point all event sources to this alias.\nThe question states find the most effortless and efficient way to achieve this, and the above actions are performed by setting the AutoPublishAlias property with the alias name.\nYou are also asked to switch traffic to the new version instantly.\nSo finding an automated way to this will always be the most efficient.\nReference:\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\n\nThe most efficient, effortless and simple way to switch traffic from an existing version of an AWS Lambda function to a newly built and tested version is to point an alias to the new version. Therefore, option B is the correct answer.\nHere's a more detailed explanation:\nAWS Lambda allows you to create aliases, which are pointers to a specific version of a function. Aliases enable you to decouple your function code from the function ARN, so you can update your function without updating the ARN used by other services that depend on it.\nWhen you deploy a new version of a function, you can create an alias that points to the new version. You can then update other services to use the alias instead of the specific function version. This allows you to easily switch traffic to the new version by updating the alias.\nIn the context of a SAM (Serverless Application Model) template, you can create an alias using the AWS::Lambda::Alias resource. Here's an example:\nyaml Copy code Resources:   MyFunction:     Type: 'AWS::Serverless::Function'     Properties:       ...   MyAlias:     Type: 'AWS::Lambda::Alias'     Properties:       FunctionName: !Ref MyFunction       Name: PROD       FunctionVersion: '1'\nIn this example, the MyAlias resource creates an alias named PROD that points to version 1 of the MyFunction function. To switch traffic to a new version, you can simply update the FunctionVersion property of the alias to point to the new version.\nFor example, if you deploy a new version of the function and it gets assigned version 2, you can update the FunctionVersion property of the alias to '2':\nyaml Copy code Resources:   MyFunction:     Type: 'AWS::Serverless::Function'     Properties:       ...   MyAlias:     Type: 'AWS::Lambda::Alias'     Properties:       FunctionName: !Ref MyFunction       Name: PROD       FunctionVersion: '2'\nOnce you've updated the alias, traffic will be routed to the new version of the function.\nOption A is incorrect because Qualifiers are used to differentiate between different versions of a Lambda function when you invoke it, not to switch traffic between them.\nOption C is incorrect because the DeploymentPreference property only controls the way in which Lambda deploys new versions of your function. It does not control how traffic is routed between different versions.\nOption D is also incorrect because the AutoPublishAlias property is used to automatically create an alias for new versions of a function. It does not control how traffic is routed between different versions.\n\n"
}, {
  "id" : 378,
  "question" : "You have been asked to enable a gradual deployment of your Lambda function in two increments such that 10 percent of your traffic is shifted immediately.\nAfter 10 minutes, all traffic is shifted to your deployment.\nWhat deployment preference type will meet this requirement?\n",
  "answers" : [ {
    "id" : "9388de92f3cc44a68b908e3b0ee16c04",
    "option" : "Linear10PercentEvery10Minutes",
    "isCorrect" : "false"
  }, {
    "id" : "1acca2d68f514b1eb0d8b7dd4feb9e10",
    "option" : "Canary10Percent10Minutes",
    "isCorrect" : "true"
  }, {
    "id" : "e3e5bea628d84a8b8bbfceac43c97755",
    "option" : "AllAtOnce",
    "isCorrect" : "false"
  }, {
    "id" : "3271441345714cff84875c5586201e84",
    "option" : "Lambda function in the PreTraffic hook to move 10 % traffic and all traffic after 10 minutes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect as Linear10PercentEvery10Minutes will shift traffic in equal increments with an equal number of minutes between each increment.\nEvery 10 minutes 10% of the traffic is shifted.\nOption B is CORRECT as Canary10Percent10Minutes will ensure 10% traffic shifted first and then all traffic is shifted, thereby meeting the requirement also, this ensures traffic is shifted in two increments.\nOption C is incorrect as all traffic will be shifted from the old Lambda function to the updated Lambda function and will not meet the requirement.\nOption D is incorrect as pre-hook functions are used to validate and run sanity checks before traffic begins to shift.\nIf this fails, CodeDeploy will abort and rollback the shift.\nReference:\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html\n\nThe correct answer is B. Canary10Percent10Minutes.\nA canary release deployment strategy is a technique that enables a gradual deployment of new code changes to a small subset of users before rolling it out to the entire production environment. This approach is useful for detecting issues and bugs in the new code without impacting the entire user base.\nThe AWS Lambda service supports canary deployments through the use of aliases and weighted routing. When a canary deployment is configured, a small percentage of incoming traffic is directed to the new code version, while the remaining traffic is directed to the previous version.\nIn this scenario, the requirement is to deploy the Lambda function in two increments, with 10 percent of the traffic shifted immediately, and the rest shifted after 10 minutes. The Canary10Percent10Minutes deployment preference type satisfies this requirement as it shifts 10% of\n\n"
}, {
  "id" : 379,
  "question" : "You are a developer at a company that has built a serverless application that allows users to make payments online.\nThe applications consist of several Lambda functions and a DynamoDB table.\nThis is implemented using a SAM template.\nHowever, when users want to see their transactions and update their payment information, they cannot do so.\nAfter debugging, you discover that the Lambda functions don't have permissions to access records from the DynamoDB table.\nHow will you resolve this issue using more tighter and secure AWS Managed Policy?\n",
  "answers" : [ {
    "id" : "72eeb3d9e75f48958a65c0d72c1ecbee",
    "option" : "Create an IAM role using AWS Managed Policy AmazonDynamoDBFullAccess and attach to Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "33e8468fded34fdc9962a90e9f90ede1",
    "option" : "Use AmazonDynamoDBFullAccess policy template in the SAM template.",
    "isCorrect" : "false"
  }, {
    "id" : "407c1ea6c0bc434684aa189a53568124",
    "option" : "Use DynamoDBCrudPolicy policy template in the SAM template.",
    "isCorrect" : "true"
  }, {
    "id" : "e3ede53335da4a5aa9e58884b00e3009",
    "option" : "Create an IAM role using AWS Managed Policy AmazonDynamoDBReadOnlyAccess and attach to Lambda functions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect as providing full access to DynamoDB defeats AWS best practice of least privilege.\nThere is no need to give access to all DynamoDB table APIs across all regions.\nThe requirement is to read and update a table.\nFurther setting up an IAM role will require a special acknowledgment to deploy the application which is not very efficient for maintainability of the application.\nOption B is incorrect as providing full access to DynamoDB defeats AWS best practice of least privilege.\nThere is no need to give access to all DynamoDB table APIs across all regions.\nThe requirement is to read and update a table.\nOption C is CORRECT as DynamoDBCrudPolicy will give, create, read, update and delete permissions to a DynamoDB table which is tighter and more secure inline with the best practice of least privilege.\nIt is also managed by AWS which would make it AWS's responsibility to maintain the policy.\nOption D is incorrect as providing read-only access to DynamoDB doesn't meet the requirement is to read and update a table.\nFurther setting up an IAM role will require a special acknowledgment to deploy the application which is not very efficient for maintainability of the application.\nReference:\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-policy-templates.html\n\nIn this scenario, users of the serverless application are not able to see their transactions and update their payment information. This is because the Lambda functions don't have permissions to access the DynamoDB table that stores the payment information. To resolve this issue, we need to grant appropriate permissions to the Lambda functions so that they can access the DynamoDB table.\nOption A suggests creating an IAM role with the AWS Managed Policy AmazonDynamoDBFullAccess and attaching it to the Lambda functions. This policy provides full access to DynamoDB, including read and write access to all tables and streams. While this would solve the problem, it is not the best solution from a security perspective. Giving full access to DynamoDB could result in unintended modifications or deletions of data, which could lead to data loss or security breaches.\nOption B suggests using the AmazonDynamoDBFullAccess policy template in the SAM template. This would apply the same policy as option A, giving the Lambda functions full access to DynamoDB. Again, this is not the best solution from a security perspective.\nOption C suggests using the DynamoDBCrudPolicy policy template in the SAM template. This policy grants access to perform Create, Read, Update, and Delete operations on a specific DynamoDB table. This is a more secure option compared to options A and B, as it limits the access to only the necessary operations and only on the specified table. However, it does not specify any further restrictions such as limiting access to specific attributes or conditions.\nOption D suggests creating an IAM role with the AWS Managed Policy AmazonDynamoDBReadOnlyAccess and attaching it to the Lambda functions. This policy grants read-only access to all DynamoDB tables and streams. While this option would not allow the Lambda functions to update the payment information, it would provide read access to the users' transaction data. This could be a good option if the Lambda functions only need to read data from DynamoDB and not modify it. However, if the Lambda functions need to update data as well, then this option would not be sufficient.\nIn conclusion, option C (using the DynamoDBCrudPolicy policy template in the SAM template) is the best option to resolve the issue of Lambda functions not having permissions to access the DynamoDB table. This option provides the necessary permissions for the Lambda functions to perform Create, Read, Update, and Delete operations on the specific DynamoDB table, while also limiting the access to only the necessary operations and only on the specified table.\n\n"
}, {
  "id" : 380,
  "question" : "You have many Lambda functions that read data from the AWS Kinesis stream.\nYour colleague informs you that our application has too many Lambda function invocations, increasing latency for your application.\nHow can you minimize latency and increase the read throughput of your function efficiently?\n",
  "answers" : [ {
    "id" : "174707330a7d4ec89e5bb538e772bbf8",
    "option" : "Create a data stream consumer.",
    "isCorrect" : "true"
  }, {
    "id" : "273eaf009d304d9cb805ebcede9e0a58",
    "option" : "Reduce the number of functions and let each function do the work of 2 functions.",
    "isCorrect" : "false"
  }, {
    "id" : "eed1bee79b654271b8753f264a1c8efc",
    "option" : "Configure CloudWatch logs to check the latency and rebuild the Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "ba26131a5a81495a9c7c93781a7a12f5",
    "option" : "Configure a dead letter queue.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT as creating a data stream consumer assures a dedicated connection to each shard.\nThe connection will help maximize throughput and minimize latency.\nBy default, Amazon states that the Lambda function polls each shard at a base rate of 1 per second.\nOption B is incorrect as this option will increase the function execution time that will require longer timeouts and maybe more memory.\nThis will not be an efficient solution.\nOption C is incorrect because the question mentions that there are many Lambda functions.\nSo it is not feasible to rebuild the Lambda functions.\nOption D is incorrect as a dead letter queue will help when your Lambda functions are overwhelmed and are missing processing of the data coming from the stream which is not the case in this scenario.\nReference:\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\n\nTo minimize latency and increase the read throughput of your Lambda functions that read data from AWS Kinesis streams, you should consider the following:\nA. Create a data stream consumer: AWS provides a Kinesis Client Library (KCL) that enables you to build Amazon Kinesis-enabled applications using AWS Lambda. You can create a data stream consumer using the KCL library to read data from the Kinesis stream and process it in parallel by running multiple instances of the Lambda function. The KCL library manages the state of the stream and the distribution of work among the instances, which can help to reduce latency and increase throughput.\nB. Reduce the number of functions and let each function do the work of 2 functions: Reducing the number of Lambda functions can decrease the number of function invocations and thus reduce latency. However, you should ensure that the workload of each function is not too high. One approach is to let each function process multiple records at a time by increasing the batch size configuration, but this may increase the memory requirements of the function.\nC. Configure CloudWatch logs to check the latency and rebuild the Lambda functions: CloudWatch can be used to monitor the latency of the Lambda function invocations and identify the causes of latency. By identifying the bottlenecks, you can optimize the code of the Lambda function or adjust the AWS configuration to reduce the latency.\nD. Configure a dead letter queue: A dead letter queue can be used to handle the records that the Lambda function is unable to process. By configuring a dead letter queue, you can ensure that these records are not lost and can be retried by the Kinesis stream. However, this option does not directly address the issue of latency.\nIn summary, options A and B are the most effective ways to minimize latency and increase the read throughput of your Lambda functions that read data from AWS Kinesis streams. Option C and D can be used as complementary measures to monitor and handle the errors in the Lambda function.\n\n"
}, {
  "id" : 381,
  "question" : "You have a stock market trading application.\nIt sends real-time data to AWS Kinesis which is then connected to a Lambda function to process and sort the information before it saves to a DynamoDB table.\nCustomers then consume this table via a dashboard.\nAs soon as the market opens, your customers complain that not all data is delivered to them.\nWhich Lambda CloudWatch metric should you look at first to resolve this problem?\n",
  "answers" : [ {
    "id" : "494d482b36834977910052b6fe152e71",
    "option" : "Throttles",
    "isCorrect" : "false"
  }, {
    "id" : "629c4b3a1b024095932af8b493a19d7f",
    "option" : "Dwell time",
    "isCorrect" : "false"
  }, {
    "id" : "c574107348f242df9f990398ce801b02",
    "option" : "ConcurrentExecutions",
    "isCorrect" : "false"
  }, {
    "id" : "d34d0c93ff4a4c42a23a9a72841aabf6",
    "option" : "IteratorAge.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect as throttles will measure the number of Lambda function invocation attempts that were throttled when the invocation rates are more than concurrency limits.\nIn this case, this is not the first metric to look at, as failed invocations will often trigger retry attempts.\nOption B is incorrect as this option is not a CloudWatch metric but a metric that can be observed via X-Ray.\nOption C is incorrect as this metric is an aggregated value for all Lambda functions making it difficult to find data per function.\nOption D is CORRECT as iterator age will determine the age of stream records processed by functions.\nAmazon says, “Measures the age of the last record for each batch of records processed.\nAge is the difference between the time Lambda received the batch and when the last record in the batch was written to the stream.\n“This then leads to exceeding retention period for Kinesis.\nThis should be the first to look at as data streams are not processed as fast as the stream records are getting generated.\nWhen the stock market opens, it's generally one of the busiest times of the day and could cause a spike in records.\nReference:\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-metrics.html\nhttps://aws.amazon.com/premiumsupport/knowledge-center/lambda-iterator-age/\n\nThe most appropriate Lambda CloudWatch metric to look at first in order to address the problem of customers not receiving all data from the stock market trading application is IteratorAge.\nIteratorAge is a metric that tracks the age of the last record that was processed by a Lambda function. In other words, it measures the time between when a record was placed into the Kinesis stream and when it was processed by the Lambda function. This metric can help identify if there is a delay in processing records, which may be causing some records to be skipped or dropped.\nIf IteratorAge is high, it indicates that the Lambda function is taking longer to process records than the rate at which they are being added to the Kinesis stream. This could be due to a variety of factors such as insufficient processing power, poor network connectivity, or inefficient code. In this case, it is recommended to increase the processing power of the Lambda function or optimize the code to reduce processing time.\nOn the other hand, if IteratorAge is consistently low, it could indicate that the Kinesis stream is not receiving data at the expected rate or that there are other issues with data delivery. In this case, it is recommended to investigate the source of the data and ensure that it is being generated and sent to the Kinesis stream correctly.\nWhile Throttles, Dwell time, and ConcurrentExecutions are also useful Lambda CloudWatch metrics to monitor, they are less relevant in this scenario. Throttles track the number of times a Lambda function is throttled due to reaching resource limits, but this is unlikely to be the cause of missing data. Dwell time measures the duration of the interval between a function being invoked and its execution being completed, which is not directly related to the delivery of data from Kinesis. ConcurrentExecutions track the number of function instances that are executing at a given time, which may be useful to identify if the function is overloaded, but it is not as directly related to the delivery of data from Kinesis as IteratorAge.\n\n"
}, {
  "id" : 382,
  "question" : "You are deploying a new Lambda function that needs to use a credit card number to make payments for purchases for internal company employees.\nThis credit card information is used by default when an employee doesn't provide a purchase order number or another credit card number.\nWhat is the most secure and less-costly way to store this information?\n",
  "answers" : [ {
    "id" : "317d0802e7014742bcea4c4d4096312f",
    "option" : "Use Environment variable with AWS managed key.",
    "isCorrect" : "true"
  }, {
    "id" : "eb8b4ea9fb5f43c1a4ecb39f1f1bb1a4",
    "option" : "Use Environment variable with custom KMS key.",
    "isCorrect" : "false"
  }, {
    "id" : "6bc78979ea7f40d8965e72d198c01496",
    "option" : "Use Environment variable with Transport Layer Security (TLS).",
    "isCorrect" : "false"
  }, {
    "id" : "62295eea91724c7aa3d984b3e26b5286",
    "option" : "Assign a variable with credit card information.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nLambda always provides server-side encryption at rest with an AWS KMS key.\nBy default, Lambda uses an AWS-managed key.\nIf this default behavior suits your workflow, you don't need to set anything else up.\nLambda creates the AWS managed key in your account and manages permissions to it for you.\nAWS doesn't charge you to use this key.\nIf you prefer, you can provide an AWS KMS customer-managed key instead.\nYou might do this to have control over the rotation of the KMS key or to meet the requirements of your organization for managing KMS keys.\nWhen you use a customer-managed key, only users in your account with access to the KMS key can view or manage environment variables on the function.\nOptions B is incorrect because it is the distractor, there is nothing like a custom KMS key.\nThe real component is Customer managed keys.\nOption C is incorrect because Transport Layer Security (TLS) is used for encryption in transit and When you manage Lambda resources all communication is encrypted in transit with Transport Layer Security (TLS).\nOption D is incorrect because this will not secure the credit card information.\nReference:\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html#configuration-envvars-encryption\nhttps://aws.amazon.com/kms/pricing/\n\nWhen storing sensitive data such as credit card numbers in a serverless architecture, it is important to ensure that the data is stored securely and is only accessible to authorized users.\nOption A - Use Environment variable with AWS managed key: AWS Key Management Service (KMS) is a service that allows you to create and manage encryption keys used to encrypt your data. With this option, you would store the credit card number as an environment variable in your Lambda function and use an AWS managed key to encrypt and decrypt the data. This option is a secure way of storing sensitive data, and the cost is dependent on the usage of the AWS KMS service. However, it is important to note that using environment variables to store sensitive data may not be the best option, as they can be easily accessed by anyone with access to the AWS Management Console or API.\nOption B - Use Environment variable with custom KMS key: This option is similar to option A, but instead of using an AWS managed key, you would use a custom KMS key to encrypt and decrypt the credit card number. This provides an additional layer of security, as the custom key can only be accessed by authorized users. However, the cost of using a custom key can be higher than using an AWS managed key.\nOption C - Use Environment variable with Transport Layer Security (TLS): TLS is a cryptographic protocol that provides secure communication over the internet. With this option, you would store the credit card number as an environment variable in your Lambda function and use TLS to encrypt and decrypt the data. This option is a secure way of storing sensitive data, but it may not be the best option if you need to access the credit card number frequently, as the cost of using TLS can be higher than using encryption with AWS KMS.\nOption D - Assign a variable with credit card information: This option is not recommended, as storing sensitive data in plaintext is not secure and could lead to a data breach. Additionally, it is not a scalable solution, as you would need to manually update the variable every time the credit card information changes.\nOverall, Option B (Use Environment variable with custom KMS key) is the most secure and cost-effective way to store the credit card information in a Lambda function.\n\n"
}, {
  "id" : 383,
  "question" : "You are deploying a mobile application for an eCommerce company.\nThey require you to change themes based on the device type.\nAndroid devices see a green theme and iOS devices see blue.\nYou are advised to use AWS Lambda for your mobile backend deployment.\nHow could this be achieved efficiently?\n",
  "answers" : [ {
    "id" : "c96243271d0c4e2b86f067f2fa213666",
    "option" : "Use a REST client to pass device info to API Gateway that invokes AWS Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "0a65e9d6262c4c4bacb5dbbea8d1e0e2",
    "option" : "Different environment variables for Android and iOS.",
    "isCorrect" : "false"
  }, {
    "id" : "8d63f4d7b7c04c31b542126ca645cc79",
    "option" : "Use AWS Mobile SDK and use the “context” object.",
    "isCorrect" : "true"
  }, {
    "id" : "f0f202b8236d4794ab75e2ab1f79baea",
    "option" : "Use the “event” object to determine which device invoked the function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect as this is not very efficient as it requires writing code at the application level to send data to the Lambda function to process.\nOption B is incorrect as environment variables are used for more constant information and can be used during runtime.\nOption C is CORRECT as per AWS Lambda FAQ's the context object, when called through AWS Mobile SDK, will give the Lambda function automatic access to device and application information to them use to change themes.\nOption D is incorrect as the event object will not have the device information required.\nReference:\nhttps://aws.amazon.com/lambda/faqs/\n\nThe most efficient way to achieve the theme change based on device type for a mobile application deployed on AWS Lambda backend is to use option C: Use AWS Mobile SDK and use the “context” object.\nOption A: Use a REST client to pass device info to API Gateway that invokes AWS Lambda function is not the most efficient way because passing device information through the API Gateway will require additional network requests and could add latency to the application.\nOption B: Different environment variables for Android and iOS may work, but it would require updating the environment variables for every new device type, which is not scalable.\nOption C: Use AWS Mobile SDK and use the “context” object is the most efficient way to achieve the theme change based on device type. The AWS Mobile SDK provides a “context” object that can be used to determine the platform of the device calling the Lambda function. This information can be used to return the appropriate theme to the device.\nOption D: Use the “event” object to determine which device invoked the function is not the best option because the “event” object does not contain information about the device calling the function. The “event” object contains information about the event that triggered the Lambda function.\n\n"
}, {
  "id" : 384,
  "question" : "3 developers are working with you on a Lambda function that calls AWS SES to send email notifications to clients who have successfully registered with your application.\nYou are now publishing a version of the function.\nBut while doing so, you get a 412 error for PreconditionFailedException.\nWhat can you do to debug and resolve the error?\n",
  "answers" : [ {
    "id" : "3407a173a804495a8758fc75f7c727e8",
    "option" : "Reduce the size of your code and try again.",
    "isCorrect" : "false"
  }, {
    "id" : "efa40309ac4f46dd85ac6d1962554eee",
    "option" : "Call the GetPolicy API to get the revisionId and publish a version again.",
    "isCorrect" : "false"
  }, {
    "id" : "4cc3149097ac40fda9787520651f2a51",
    "option" : "Call the GetAlias API to get the revisionId and publish a version again.",
    "isCorrect" : "true"
  }, {
    "id" : "fca29922171d47abb8a7b80d4963175d",
    "option" : "Use the UpdateAlias API to publish a version by updating Lambda function Alias.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect as large size code will not trigger a precondition error.\nIt will trigger a code storage exception error.\nOption B is incorrect as the GetPolicy API will return the revissionId of the policy instead of the function needed to correct.\nOption C is CORRECT as per the API documentation.\nA GetFuncton or GetAlias call will return the right revisionId, and then with that, the new version can be published.\nLambda will trigger an error when the revisionId in the request doesn't align with the current version revisionId.\nThis is often used to protect against a racing condition when multiple people are working on the same function.\nOption D is incorrect.\nThe update API will not actually work as it will throw an exception for the same reasons.\nReference:\nhttps://docs.aws.amazon.com/lambda/latest/dg/API_PublishVersion.html\n\nThe PreconditionFailedException error with a 412 HTTP status code indicates that one or more of the specified precondition headers in the request failed to match the corresponding values on the server. When publishing a version of an AWS Lambda function, AWS verifies that the specified preconditions, such as function version or alias, match the current state of the function before proceeding.\nTo debug and resolve the error, we need to determine which precondition failed and update it accordingly.\nOption A, reducing the size of the code, is unlikely to solve the issue as it does not address the underlying cause of the error.\nOption B, calling the GetPolicy API to get the revisionId and publishing a version again, is not relevant to the current error. GetPolicy is used to retrieve the resource-based policy associated with a Lambda function version or alias.\nOption C, calling the GetAlias API to get the revisionId and publishing a version again, is also not relevant to the error. GetAlias is used to retrieve the specified alias resource policy and configuration information.\nOption D, using the UpdateAlias API to publish a version by updating the Lambda function Alias, is a possible solution. UpdateAlias can be used to update an alias to point to a different version of the Lambda function, which may resolve the precondition failure.\nAnother possible solution is to check if any of the developers made changes to the function's permissions, roles, or policies, which may have caused the error. In this case, updating the function's IAM policies or roles could resolve the issue.\nIn summary, the best solution for debugging and resolving the PreconditionFailedException error while publishing a version of an AWS Lambda function would be to investigate the cause of the error, possibly using the AWS CloudWatch logs, and update the relevant preconditions or IAM policies as needed. Option D may be a viable solution in some cases.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 386,
  "question" : "You work for a travel company that books accommodation for customers.\nThe company has decided to release a new feature that will allow customers to book accommodation in real-time through their API.\nAs a developer, you have planned to deploy this new feature.\nHow will you test the new API feature with minimal impact to customers?\n",
  "answers" : [ {
    "id" : "004da8b9bf9040ae8fbeacca19da1f7b",
    "option" : "Create a stage and inform your pilot customers to change their endpoint.",
    "isCorrect" : "false"
  }, {
    "id" : "18335873cb8140b7a0fa8d5e10b30694",
    "option" : "Create a stage and inform your pilot customers to change their endpoint and attach a usage plan.",
    "isCorrect" : "false"
  }, {
    "id" : "faed44da8afc42c885d8a7901af8669a",
    "option" : "Create a stage and enable canary release.",
    "isCorrect" : "true"
  }, {
    "id" : "ce79b4e6e5ce426c8e7b568825c2df8f",
    "option" : "Create a stage and promote a canary release.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect because setting up the stage and informing your customers to change their endpoint will affect them by adding downtime on their side.\nOption B is incorrect as the same as above but adding the usage plan only helps throttle requests.\nIt will still affect the customers.\nOption C is CORRECT as enabling canary release will allow the developer to route a % of the traffic to the new API in a random order ensuring no one customer is affected too long.\nCanary release is a part of a stage in the API Gateway console.\nOption D is incorrect because promoting a canary release will make the deployment release, thereby having no delta between the canary and base stage.\nIf this option is selected, all customers will be affected.\nReference:\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/amazon-api-gateway-supports-canary-release-deployments/\nFurther, read.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html\n\nThe correct answer for this question is C. Create a stage and enable canary release.\nExplanation: When deploying a new feature, it is crucial to ensure minimal impact to customers. A canary release is an effective way to do this. It involves releasing the new feature to a small subset of users first, and gradually increasing the percentage of users who receive the update over time.\nOption A is incorrect because simply creating a new stage and informing customers to change their endpoint will not provide any protection against unexpected problems or errors that may occur during the deployment.\nOption B is partially correct because attaching a usage plan can help manage access and usage of the new API. However, it still does not address the main concern of ensuring minimal impact to customers during the deployment.\nOption D is incorrect because promoting a canary release means making the new feature available to all customers, which goes against the objective of minimizing impact.\nTherefore, the best answer is C. By creating a new stage and enabling canary release, the new feature will be released gradually to a subset of users, allowing for monitoring and troubleshooting before a wider release. This approach ensures minimal impact to customers while allowing for the testing of the new feature in real-time.\n\n"
}, {
  "id" : 387,
  "question" : "You are a lead developer for an application that uses WebSockets through API Gateway to push payloads between the clients and server.\nYour API has a proxy integration with Lambda.\nWhen the client connects for the first time, it receives a preflight error message.\nWhich steps will you take to resolve this issue?\n",
  "answers" : [ {
    "id" : "688cd8054b4c4149873a48ea432a6365",
    "option" : "Enable CORS using the API Gateway console.",
    "isCorrect" : "false"
  }, {
    "id" : "1c04c377e13f417183114fb1cf4652ca",
    "option" : "Setup the OPTIONS method and set up the required OPTIONS response headers in API Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "228d570e95b44d59950bf60756aab017",
    "option" : "Make changes to your backend to return “Access-Control-Allow-Headers” and “Access-Control-Allow-Origin” headers.",
    "isCorrect" : "false"
  }, {
    "id" : "a7aea68a9006402c85c308fbdad00d13",
    "option" : "A and",
    "isCorrect" : "false"
  }, {
    "id" : "54a235cc3ee3415ba1c0548f42aa5846",
    "option" : "A, B and",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: E.\nOption A is incorrect because using the Management Console to enable CORS will by itself not resolve the issue.\nOption B is incorrect because when using a proxy integration with Lambda, it is necessary to add “Access-Control-Allow-Headers” and “Access-Control-Allow-Origin” headers to the response in the Lambda function as a proxy integration will not return an integration response.\nOption C is partially correct as it on its own it will not resolve the issue as laid in the AWS documentation.\nOption D is partially correct as it on its own it will not resolve the issue as laid in the AWS documentation.\nOption E is CORRECT.\nWhen using a proxy integration with Lambda, it is necessary to add “Access-Control-Allow-Headers” and “Access-Control-Allow-Origin” headers the response in the Lambda function as a proxy integration will not return an integration response.\nAll three steps need to be used when enabling CORS for an API with a Lambda proxy integration.\nReference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\n\nWebSockets provide a persistent connection between a client and a server, allowing real-time data exchange. AWS API Gateway offers WebSockets support and proxy integration with AWS Lambda.\nWhen a client connects to a WebSocket API through API Gateway for the first time, it sends an HTTP request to initiate a WebSocket handshake. This request is an HTTP upgrade request, which the WebSocket API Gateway endpoint receives as an HTTP request.\nAs the WebSocket request is an HTTP request, the browser will send a preflight request. This preflight request is sent to verify if the server supports WebSocket connections and what security measures are in place.\nThe preflight request includes an OPTIONS method call that expects an HTTP response with specific headers to indicate if CORS is enabled or not.\nTo resolve the preflight error message, we need to take the following steps:\nOption A: Enable CORS using the API Gateway console CORS (Cross-Origin Resource Sharing) is a mechanism that enables sharing resources between different origins (domains). Enabling CORS in the API Gateway console is the first step to allow access to the WebSocket API endpoint from a different domain.\nOption B: Set up the OPTIONS method and required OPTIONS response headers in API Gateway To handle the preflight request, API Gateway must have an OPTIONS method configured with the appropriate response headers. These headers include \"Access-Control-Allow-Origin,\" \"Access-Control-Allow-Headers,\" \"Access-Control-Allow-Methods,\" and \"Access-Control-Allow-Credentials.\"\nOption C: Make changes to the backend to return \"Access-Control-Allow-Headers\" and \"Access-Control-Allow-Origin\" headers If you're using Lambda with API Gateway, the Lambda function must return the required headers to complete the preflight request successfully.\nTherefore, to resolve the preflight error message, we need to implement options B and C.\nOption D and E are incorrect because Option A alone is not sufficient to resolve the preflight error, and Option B and C must be implemented together to resolve the preflight error.\n\n"
}, {
  "id" : 388,
  "question" : "You are an API developer for a large corporation and have been asked to investigate a public API's latency problem in API Gateway.\nUpon investigation, you realize that all clients making requests to the API are invalidating the cache using the Cache-Control header,which has increased the latency of the API.\nHow will you invalidate the API cache by reducing the latency of the API?\n",
  "answers" : [ {
    "id" : "ea618c68d5bc49a3921bfba412abab08",
    "option" : "Flush entire cache.",
    "isCorrect" : "false"
  }, {
    "id" : "4419896c9e1144ca86862edec6d791b1",
    "option" : "Disable API caching.",
    "isCorrect" : "false"
  }, {
    "id" : "dd7aca267ad0420aaa4638aa8d0fbf97",
    "option" : "Set TTL value to 0 (zero)",
    "isCorrect" : "false"
  }, {
    "id" : "dc8549a4b1f14e939b9aab568ecfbf87",
    "option" : "Check Require Authorization box.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nA client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests.\nThe client must send a request that contains the\nCache-Control: max-age=0\nheader.\nThe client receives the response directly from the integration endpoint instead of the cache, provided that the client is authorized to do so.\nThis replaces the existing cache entry with the new response, which is fetched from the integration endpoint.\nOption A is incorrect as flushing the entire cache will not help solve latency problems.\nAll requests will be forwarded to the integration backend until the cache has been refilled.\nOption B is incorrect as disabling caching all together will not reduce latency as all requests will be forwarded to the integration backend.\nOption C is incorrect because setting a value of TTL=0 actually means that you are disabling the cache.\nOption D is CORRECT because if authorized users invalidate the cache, the latency of the API can be reduced.\nIf you don't impose an\nInvalidateCache.\npolicy (or choose the Require authorization check box in the console), any client can invalidate the API cache.\nIf most or all of the clients invalidate the API cache, this could significantly increase the latency of your API.\nReference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\n\nWhen clients make requests to an API, the API Gateway caches the response data for subsequent requests. This caching mechanism can significantly reduce the API's latency and improve its overall performance. However, in this scenario, clients are invalidating the cache by using the Cache-Control header, which is causing an increase in latency.\nTo reduce the API's latency, you need to invalidate the cache without requiring clients to invalidate it using the Cache-Control header. There are a few options for doing this:\nA. Flush entire cache: This option will clear the entire cache of the API, forcing clients to retrieve fresh data on subsequent requests. While this option will certainly solve the latency issue, it may not be ideal in all situations. For example, if the API has a large amount of frequently accessed data, flushing the entire cache could significantly increase the API's response time.\nB. Disable API caching: This option will turn off caching entirely, ensuring that clients always receive fresh data on each request. While this option will certainly solve the latency issue, it may not be ideal in all situations. For example, if the API has a large amount of frequently accessed data, disabling caching could significantly increase the API's response time.\nC. Set TTL value to 0 (zero): This option will set the \"time-to-live\" (TTL) value of the cache to zero, causing cached data to immediately expire. This option will invalidate the cache without requiring clients to use the Cache-Control header, while still allowing the API to cache data for subsequent requests.\nD. Check Require Authorization box: This option will require clients to provide valid authorization credentials in order to retrieve cached data. This option may not be ideal in all situations, as it adds an additional layer of security and complexity to the API.\nIn conclusion, setting the TTL value to zero is likely the best option for this scenario. It will invalidate the cache without requiring clients to use the Cache-Control header, while still allowing the API to cache data for subsequent requests.\n\n"
}, {
  "id" : 389,
  "question" : "You are a developer at a company that has built a serverless application that allows users to get NBA stats.\nThe application consists of three different levels of subscription Free, Premium and Enterprise, implemented using stages in API Gateway.\nThe Free level allows developers to access stats up to 5 games per player, and premium and enterprise get full access to the database.\nYour manager has asked you to limit the free level to 100 concurrent requests at any given point in time.\nHow can this be achieved?\n",
  "answers" : [ {
    "id" : "638931295169403eb74ce3515ffecf4c",
    "option" : "Under usage plan for the stage change Burst to 50 and Rate to 50.",
    "isCorrect" : "false"
  }, {
    "id" : "04faacc3f29546f68769e8f223fb9213",
    "option" : "Under usage plan for the stage change Burst to 100 and Rate to 100.",
    "isCorrect" : "true"
  }, {
    "id" : "c2b84acd40e24d33970bde5b87b5c7bc",
    "option" : "Under usage plan for the stage changeBurst to 50 and Rate to 100.",
    "isCorrect" : "false"
  }, {
    "id" : "272880cfe514482086b7fb76796d7e3d",
    "option" : "All of the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect as changing Burst to 50 and Rate to 50 will mean that 50 concurrent requests can be made and processed at any given point in time.\nOption B is CORRECT as changing Burst to 100 and Rate to 100 will allow 100 concurrent requests to be made and processed at any given point in time.\nOption C is incorrect as changing Burst to 50 and Rate to 100 will allow 50 concurrent requests to be made but 100 processed at any given point in time.\nThe remaining 50 requests will be a 429 error and will have to retry after an interval.\nOption D is incorrect as not all options are correct.\nReference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\nhttps://aws.amazon.com/blogs/aws/new-usage-plans-for-amazon-api-gateway/\n\nThe correct answer to the question is B. Under usage plan for the stage change Burst to 100 and Rate to 100.\nAPI Gateway provides the ability to create usage plans to limit and meter usage of API resources for different levels of subscription. Usage plans can define different limits on requests, such as the number of requests per second or the number of concurrent requests.\nTo limit the free level to 100 concurrent requests at any given point in time, we need to create a usage plan for the Free level and set the limit for concurrent requests to 100. We can achieve this by following these steps:\nIn the API Gateway console, select the API for which you want to create a usage plan. Click on the \"Usage Plans\" option from the left-hand menu and click \"Create\". Enter a name for the usage plan and a description if necessary. Select the \"Create a new API key\" option if you want to require API keys for usage of this plan. Under \"Throttling\", set the \"Max concurrent requests\" to 100. Under \"Quotas\", set the desired limits for the number of requests per second or per day, depending on the needs of the application. Save the usage plan.\nOnce the usage plan is created, we need to associate it with the Free stage. We can do this by following these steps:\nIn the API Gateway console, select the API and then select the \"Stages\" option from the left-hand menu. Select the Free stage and click on the \"Associated Usage Plan\" tab. Click on \"Add usage plan\" and select the usage plan that was just created. Save the changes.\nNow, any requests made to the Free stage will be subject to the limits defined in the usage plan, including the limit of 100 concurrent requests at any given point in time.\nOption A (Burst to 50 and Rate to 50) and Option C (Burst to 50 and Rate to 100) are incorrect because they do not meet the requirement of limiting the free level to 100 concurrent requests. Only Option B (Burst to 100 and Rate to 100) meets the requirement. Therefore, the correct answer is option B.\n\n"
}, {
  "id" : 390,
  "question" : "You are an API developer for a large manufacturing company.\nYou have developed an API resource that adds new products to the distributor's inventory using a POST HTTP request.\nIt includes an Origin header and accepts application/x-www-form-encoded as request content type.\nWhich response header will allow access to this resource from another origin?\n",
  "answers" : [ {
    "id" : "3a164cade0014b849be267eff89ac33f",
    "option" : "Access-Control-Allow-Origin",
    "isCorrect" : "true"
  }, {
    "id" : "48eee175ce994dc594956bafa7328d70",
    "option" : "Access-Control-Request-Method",
    "isCorrect" : "false"
  }, {
    "id" : "9ec61f76945e4718887b0164f8b351e2",
    "option" : "Access-Control-Request-Headers",
    "isCorrect" : "false"
  }, {
    "id" : "59a6309301a447c9b189549125ac2095",
    "option" : "All of the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nFor simple cross-origin\nPOST.\nmethod requests, the response from your resource needs to include the header\nAccess-Control-Allow-Origin.\n, where the value of the header key is set to\n'*'\n(any origin) or is set to the origins allowed to access that resource.\nWhen a browser receives a non-simple HTTP request, the CORS protocol requires the browser to send a preflight request to the server and wait for approval (or a request for credentials) from the server before sending the actual request.\nThe preflight request appears to your API as an HTTP request that:\nIncludes an\nOrigin.\nheader.\nUses the\nOPTIONS.\nmethod.\nIncludes the following headers:\nAccess-Control-Request-Method.\nAccess-Control-Request-Headers.\nOption A is CORRECT as the POST request satisfies the condition for a simple cross-origin request.\nSo allowing the Access-Control-Allow-Origin header will make it so that it can be accessed from other origins.\nOption B is incorrect as this option will not allow the resource to be cross-origin.\nThis header is a part of enabling CORS support for a complex HTTP request.\nOption C is incorrect as this option will not allow the resource to be cross-origin.\nThis header is a part of enabling CORS support for a complex HTTP request.\nOption D is incorrect but is the next closest answer.\nThe question reads which header and not headers.\nReference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\n\nThe correct answer is A. Access-Control-Allow-Origin.\nExplanation: Cross-Origin Resource Sharing (CORS) is a security feature implemented by web browsers that restricts web pages from making requests to a different domain than the one that served the web page. CORS is a security feature that prevents malicious scripts from exploiting a website's data and APIs.\nWhen a web page makes a request to an API resource on a different domain, the web browser sends a preflight request to the API resource before sending the actual request. The preflight request is an HTTP OPTIONS request that includes the Origin header and other headers that the client intends to send with the actual request.\nIn response to the preflight request, the server should send a response header that indicates which origins are allowed to access the API resource. The Access-Control-Allow-Origin response header specifies the origins that are allowed to access the resource. If the server sends a wildcard (*) value, then any origin can access the resource.\nThe Access-Control-Request-Method header is sent in the preflight request and specifies the HTTP method (e.g., GET, POST, PUT, DELETE) that the client intends to use for the actual request. The Access-Control-Allow-Methods header in the response specifies which HTTP methods are allowed.\nThe Access-Control-Request-Headers header is sent in the preflight request and specifies which custom headers the client intends to send with the actual request. The Access-Control-Allow-Headers header in the response specifies which custom headers are allowed.\nTherefore, the correct answer is A. Access-Control-Allow-Origin, as it is the response header that allows access to the API resource from another origin.\n\n"
}, {
  "id" : 391,
  "question" : "You are using AWS S3 to encrypt and store large documents for your application.\nYou have been asked to use the AWS Lambda function for this purpose by your Technical Architect.\nYou have determined the use of AWS KMS for encryption as your data is stored and managed in the AWS platform.\nWhich CMK will you use for this purpose?\n",
  "answers" : [ {
    "id" : "62103a0e26af4a21bd7279e7f63c8dbe",
    "option" : "Asymmetric CMK",
    "isCorrect" : "false"
  }, {
    "id" : "892cc89fb0a047fd95da328ff4f8bbb1",
    "option" : "Symmetric CMK",
    "isCorrect" : "true"
  }, {
    "id" : "33326d01cd584b8c9f8c3ab062141df0",
    "option" : "RSA CMK",
    "isCorrect" : "false"
  }, {
    "id" : "41d85a661c4644bf8961a7aa2d2527fe",
    "option" : "ECC CMK.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect as AWS services that integrate with AWS KMS do not support the use of asymmetric keys.\nOption B is CORRECT as mentioned in the documentation.\nSymmetric keys never leave KMS unencrypted and hence are used to integrate with other AWS services.\nOption C is incorrect as RSA CMK is a type of asymmetric key which cannot be used with other AWS services for integration.\nOption D is incorrect as ECC CMK is a type of asymmetric key which cannot be used with other AWS services for integration.\nReference:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/symm-asymm-concepts.html#symmetric-cmks\n\nWhen encrypting data with AWS Key Management Service (KMS), you have the option of using either an asymmetric customer master key (CMK) or a symmetric CMK.\nAsymmetric CMKs are designed for encrypting small amounts of data, such as passwords or keys. They use public and private keys, where the public key is used for encryption and the private key is used for decryption.\nSymmetric CMKs, on the other hand, are designed for encrypting large amounts of data, such as documents or files. They use a single key for both encryption and decryption.\nIn this scenario, the requirement is to encrypt and store large documents using AWS S3 and AWS Lambda functions. Therefore, a symmetric CMK would be the appropriate choice.\nAs for the specific type of symmetric CMK to use, AWS KMS supports both RSA and Elliptic Curve Cryptography (ECC) CMKs. The choice between RSA and ECC depends on the performance and security requirements of the application.\nRSA CMKs are widely used and offer good performance and security. ECC CMKs, on the other hand, are a newer technology that offer better performance and security compared to RSA for the same key length.\nIn summary, the appropriate choice of CMK in this scenario would be a symmetric CMK, and the specific type of CMK would depend on the performance and security requirements of the application, which could be either RSA or ECC.\n\n"
}, {
  "id" : 392,
  "question" : "Your company is standing up a new DynamoDB table for an application and will store the customers' credit card information that have made purchases on your eCommerce store.\nYou have been asked to track the AWS API calls that DynamoDB sends to AWS KMS for data encryption.\nHow will you achieve this?\n",
  "answers" : [ {
    "id" : "50fcfbe1d56e41b681da16699f842646",
    "option" : "Use CloudWatch logs.",
    "isCorrect" : "false"
  }, {
    "id" : "90358aeba286461a93fef02e467c976d",
    "option" : "Use DynamoDb streams.",
    "isCorrect" : "false"
  }, {
    "id" : "52ae0b2f1fa34cda88a6ae33720999b3",
    "option" : "Use CloudTrail logs.",
    "isCorrect" : "true"
  }, {
    "id" : "053f2b09d96b4d3c8597cbd4e22f7503",
    "option" : "Use CloudWatch metrics.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect as cloudwatch logs show logs created by DynamoDb but can't confirm if KMS was used.\nOption B is incorrect streams record the values added to the DynamoDb table.\nOption C is CORRECT as per AWS documentation, “you can use AWS CloudTrail logs to track the requests that DynamoDB sends to AWS KMS on your behalf.\n“ CloudTrail is used to audit API calls between AWS services.\nOption D is incorrect as no such metrics exist in CloudWatch.\nReference:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-dynamodb.html\n\nThe correct answer is C. Use CloudTrail logs.\nCloudTrail is a service provided by AWS that tracks and logs all API calls made within an AWS account. It captures detailed information about the identity of the caller, the time of the call, the source IP address, the request parameters, and the response elements returned by the AWS service. CloudTrail logs can be used for security analysis, compliance auditing, and troubleshooting.\nIn this scenario, we need to track the AWS API calls that DynamoDB sends to AWS KMS for data encryption. Since CloudTrail logs all API calls made within an AWS account, we can use it to track the calls made by DynamoDB to KMS for data encryption. By analyzing the CloudTrail logs, we can determine the frequency of the calls, the number of requests, and the types of requests made.\nOption A, Use CloudWatch logs, is incorrect because CloudWatch is a monitoring service that provides metrics, logs, and alarms for AWS resources and applications. While CloudWatch logs can be used to monitor and troubleshoot applications, it does not track AWS API calls.\nOption B, Use DynamoDB streams, is incorrect because DynamoDB streams are used to capture and process item-level changes made to a DynamoDB table. While DynamoDB streams can be used for real-time data processing and analysis, it does not track AWS API calls.\nOption D, Use CloudWatch metrics, is incorrect because CloudWatch metrics are used to monitor the performance and health of AWS resources and applications. While CloudWatch metrics can be used to monitor and troubleshoot applications, it does not track AWS API calls.\n\n"
}, {
  "id" : 393,
  "question" : "You have used a CMK to create a data key using the GenerateDataKey operation to encrypt your application's data using envelope encryption.\nYou have been asked to provide temporary secured access to external auditors so that they can audit the data stored.\nThese auditors should be able to gain access to your data immediately.\nWhat is the most effective and efficient way of achieving this?\n",
  "answers" : [ {
    "id" : "dda8a85601524e75b8b1cdacc7f304a3",
    "option" : "Download all data and send data via a secure courier.",
    "isCorrect" : "false"
  }, {
    "id" : "77c3dd5401f948f38508001d8e1fc339",
    "option" : "Use key policies to assign decrypt access to auditors.",
    "isCorrect" : "false"
  }, {
    "id" : "4c1840978efd4beebf0fca3b8b2fa51b",
    "option" : "Use grants to assign decrypt access to auditors.",
    "isCorrect" : "false"
  }, {
    "id" : "abc6e84596954d7ea4e2f60e61246695",
    "option" : "Use grant tokens after using grants with the decrypt and re-encrypt operation.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect as this is not very efficient as it requires downloading all the data and then [physically transporting the data.\nThis could cause exposure to theft and foul play, leading to tampered data not good for auditors.\nOption B is not ideal as key policies are used for providing static permissions to data.\nOption C is the next closest answer as it would be apt to provide temporary access.\nBut since it's eventually consistent, this access might not be immediate.\nOption D is a CORRECT answer as grant tokens received from the CreateGrant request will mitigate potential delay and grant immediate access.\nDecrypt operation is needed for the auditors to decrypt and re-encrypt this data.\nReference:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#enveloping\n\nThe most effective and efficient way to provide temporary secured access to external auditors to audit the data stored is by using AWS Key Management Service (KMS) grants to assign decrypt access to auditors.\nAWS KMS grants allow you to delegate permissions to AWS principals (e.g., IAM users or roles) without having to modify the key policy or update the permissions of the principal itself. This means you can grant access to the auditors without having to modify your existing key policy or create new IAM users or roles.\nUsing key policies to assign decrypt access to auditors (option B) is not recommended because it would require modifying the key policy, which could impact the security of the key and the data it protects. It could also grant decrypt access to more users than intended.\nDownloading all data and sending it via a secure courier (option A) is not efficient because it would take time to download all the data and could potentially compromise the security of the data during transportation.\nUsing grant tokens after using grants with the decrypt and re-encrypt operation (option D) is not necessary in this case because the auditors only need temporary access to the data and do not need to perform any encryption or re-encryption operations.\nTherefore, the correct answer is C: use grants to assign decrypt access to auditors. With grants, you can specify the duration of the access and limit the scope of the permissions to specific actions, resources, and conditions. Once the grant expires, the permissions are revoked automatically. This ensures that the auditors can only access the data for a limited time and only perform the actions they are authorized to do.\n\n"
}, {
  "id" : 394,
  "question" : "You work for a large bank and are tasked to build an application that allows 30 large customers to perform more than 1000 online transactions per second swiftly and collectively in the us-east-1 region.\nThe size of each transaction is around 5 KB.\nYour manager has told you to ensure data is encrypted end-to-end, you decide to use AWS KMS to meet your requirements.\nWhile using the SDK and testing you see ThrottlingException error.\nHow will you deliver the application with optimum performance metrics?\n",
  "answers" : [ {
    "id" : "193ab1fedcaa40c3bb9d67668014f674",
    "option" : "Send data directly to AWS KMS for encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "e5a3b6ef20c046279349804f7422fb39",
    "option" : "Use LocalCryptoMaterialsCache operation.",
    "isCorrect" : "true"
  }, {
    "id" : "c20d2c0494524c4ca8994495356a0930",
    "option" : "Use RequestServiceQuotaIncrease operation.",
    "isCorrect" : "false"
  }, {
    "id" : "d0d9109e09074884962943f86aed1e18",
    "option" : "Use AWS SQS to queue all requests made to AWS KMS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect as sending the data to KMS for encryption doesn't meet the encryption requirement in transit and so can't be used.\nOption B is CORRECT as using LocalCryptoMaterialsCache is an in-memory cache that can be used to save data keys.\nIt is a configurable cache that can configured to be made more secure.\nOption C is incorrect as increasing the quota will change the performance metrics of the application.\nOption D is incorrect as it will not improve the process of encryption and decryption and will not change the performance metrics of the application.\nReference:\nhttps://aws.amazon.com/blogs/security/aws-encryption-sdk-how-to-decide-if-data-key-caching-is-right-for-your-application/\n\nThe ThrottlingException error occurs when you exceed the maximum rate or limit of a particular AWS service. In this case, it means that you are sending too many requests to AWS KMS and the service is throttling your requests to prevent overload.\nTo deliver the application with optimum performance metrics, you can consider the following options:\nA. Send data directly to AWS KMS for encryption.\nThis option is not ideal because it means that you will be sending a large number of requests directly to AWS KMS, which may lead to the ThrottlingException error. Additionally, sending data directly to AWS KMS may not be the best option from a security perspective, as it means that you will need to manage encryption keys and potentially expose them to the client.\nB. Use LocalCryptoMaterialsCache operation.\nLocalCryptoMaterialsCache is a feature of the AWS SDK that allows you to cache encryption materials locally on the client. This can help reduce the number of requests made to AWS KMS, which may help reduce the likelihood of the ThrottlingException error. However, this option may not be suitable for all use cases, particularly those where encryption materials need to be rotated frequently.\nC. Use RequestServiceQuotaIncrease operation.\nRequestServiceQuotaIncrease is an operation that allows you to request an increase in the maximum rate or limit of a particular AWS service. If you are consistently hitting the ThrottlingException error, this option may be suitable for you. However, it's important to note that AWS may not always grant a request for a quota increase, so this option may not be a reliable solution.\nD. Use AWS SQS to queue all requests made to AWS KMS.\nUsing AWS SQS to queue requests to AWS KMS can help reduce the likelihood of the ThrottlingException error by allowing you to control the rate at which requests are sent to AWS KMS. Additionally, using a message queue can help decouple the client from AWS KMS, which can help improve the scalability and reliability of the application.\nIn conclusion, option D - using AWS SQS to queue all requests made to AWS KMS - is the best option for delivering the application with optimum performance metrics.\n\n"
}, {
  "id" : 395,
  "question" : "You're the lead developer for a company that uses AWS KMS to decrypt passwords from an AWS RDS MySQL database using an asymmetric CMK.\nWhile decrypting the data, you receive an InvalidCiphertextException error which causes the application to fail.\nYou have made sure that the CMK ID used is accurate.\nWhat could have caused this error?\n",
  "answers" : [ {
    "id" : "1de9f27189c240e081b08b9c98cf9315",
    "option" : "EncryptionAlgorithm set to default value.",
    "isCorrect" : "true"
  }, {
    "id" : "3dfe5a1310ab4326a1e89ba227656244",
    "option" : "EncryptionContext is empty.",
    "isCorrect" : "false"
  }, {
    "id" : "51c24e51fe734f769a3a4239fa659ff9",
    "option" : "GrantTokens is an empty array.",
    "isCorrect" : "false"
  }, {
    "id" : "f031b1b294ff4b198d0a75f4c8d0c698",
    "option" : "KeyId is empty.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT as asymmetric CMKs cannot use the default algorithm as the default one is used for symmetric only.\nIt is required to record the algorithm during encryption and provide the exact one during decryption.\nOn the contrary, for symmetric CMKs, the default value would work.\nOption B is incorrect.\nThis is not a required parameter and so leaving it empty will not cause this error.\nOption C is incorrect as GrantTokens is not a required parameter but takes values in an array, and so it will not cause the error.\nOption D is incorrect as it is mentioned that the CMK ID is verified and accurate.\nReference:\nhttps://docs.aws.amazon.com/kms/latest/APIReference/API_Decrypt.html\n\nThe InvalidCiphertextException error occurs when the ciphertext that is being decrypted is not in a valid format or cannot be decrypted. This error can be caused by several factors, including:\nA. EncryptionAlgorithm set to default value: This option is unlikely to cause the InvalidCiphertextException error, as the default value for EncryptionAlgorithm is AES_CBC_PKCS5Padding, which is a valid encryption algorithm that AWS KMS supports.\nB. EncryptionContext is empty: EncryptionContext is a set of key-value pairs that you can pass to AWS KMS when encrypting data, and it must be the same when decrypting data. If EncryptionContext is not provided during encryption or is different during decryption, AWS KMS will return an InvalidCiphertextException error. Therefore, if EncryptionContext is empty during decryption, it is likely to cause the error.\nC. GrantTokens is an empty array: GrantTokens are additional authentication data that you can pass to AWS KMS when decrypting data. If the GrantTokens parameter is provided during encryption, it must be the same when decrypting data. If GrantTokens is not provided during encryption or is different during decryption, AWS KMS will return an InvalidCiphertextException error. Therefore, if GrantTokens is an empty array during decryption, it is likely to cause the error.\nD. KeyId is empty: If KeyId is empty, AWS KMS cannot determine which key to use for decryption and will return an error. However, since the question states that the developer has made sure that the CMK ID used is accurate, it is unlikely that this option is the cause of the error.\nIn summary, the most likely cause of the InvalidCiphertextException error is that EncryptionContext or GrantTokens is not provided or is different during decryption than during encryption.\n\n"
}, {
  "id" : 396,
  "question" : "You are planning to deploy the serverless application using AWS CodeDeploy.\nThis application will be using AWS Lambda compute platform &amp; an AppSpec file will be created using YAML.\nTo specify a Lambda function's additional properties, which of the following properties needs to be specified in the resources section with an AppSpec file? (Select Three).\n",
  "answers" : [ {
    "id" : "5b51e976bee144ec8e611dfa1e04569a",
    "option" : "Name",
    "isCorrect" : "true"
  }, {
    "id" : "2cfb6f2627fe462d94cc010e52c7d39d",
    "option" : "TaskDefinition",
    "isCorrect" : "false"
  }, {
    "id" : "7694528f0778428b8b5ccbaa8d456b71",
    "option" : "Port",
    "isCorrect" : "false"
  }, {
    "id" : "daa3b146537e4e079201338b70a4b8f1",
    "option" : "Targetversion",
    "isCorrect" : "true"
  }, {
    "id" : "42025b16ef0f4b198ffb0b2c8fd8086c",
    "option" : "Currentversion.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - A, D, E.\nAppSec Resources Section for a Lambda deployment consists of the following properties,\nname.\nalias.\ncurrent version.\ntarget version.\nOption B &amp; C are incorrect as TaskDefinition &amp; Port property needs to be specified for ECS Deployment &amp; not for Lambda Deployment.\nFor more information on AppSpec resources for ECS/Lambda deployment with AWS CodeDeploy, refer to the following URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-resources.html\n\nAWS CodeDeploy is a deployment service that automates software deployments to a variety of compute services including Amazon EC2, AWS Lambda, and on-premises servers. It simplifies the process of rolling out new features or updating an application in production, with minimal downtime.\nWhen deploying a serverless application using AWS CodeDeploy with AWS Lambda compute platform, an AppSpec file is created using YAML. The AppSpec file is used to specify the deployment instructions, such as where to copy files, which functions to execute, and how to route traffic during deployment.\nTo specify a Lambda function's additional properties, we need to specify the following properties in the resources section with an AppSpec file:\nName: The name of the Lambda function. This property is required and must match the name of the function in the AWS Lambda service. TaskDefinition: The task definition that contains the configuration information for the container that runs the Lambda function. This property is optional and is used when the Lambda function is deployed in a container. Targetversion: The version of the Lambda function to deploy. This property is required and must match the version number of the function in the AWS Lambda service.\nTherefore, the correct options from the given choices are A. Name, B. TaskDefinition, and D. Targetversion.\nC. Port and E. Currentversion are not required to be specified in the AppSpec file when deploying a serverless application using AWS CodeDeploy with AWS Lambda compute platform.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 398,
  "question" : "Your Company is using an AWS CodeDeploy for deployment of applications to EC2 instance.\nFor a financial application project to be deployed in the us-east region, log files are saved in the EC2 instance launched in this region.\nA new joinee mistakenly deleted the deployment log file for AWS CodeDeploy.\nWhich of the following can be done to create a new log file?\n",
  "answers" : [ {
    "id" : "174c4d17baeb414c97c12b56fee1589a",
    "option" : "AWS CodeDeploy will automatically create a replacement file.",
    "isCorrect" : "false"
  }, {
    "id" : "e2f1f274a17049df8e7af30d2bfa9aec",
    "option" : "Restore deployment log file from backup file.",
    "isCorrect" : "false"
  }, {
    "id" : "d317395c8a3349cf8b61331a926e69ff",
    "option" : "Restart CodeDeployAgent Service.",
    "isCorrect" : "true"
  }, {
    "id" : "c473dd7adee741ec881bf75b0d537986",
    "option" : "Restart EC2 instance with AWS CodeDeploy agent.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nIf a deployment log file is deleted, a new log file can be created by restarting the CodeDeployAgent service using the following commands,\nFor Windows,\no powershell.exe -Command Restart-Service -Name codedeployagent.\nFor Linux.\no sudo service codedeploy-agent stop.\no sudo service codedeploy-agent start.\nOption A is incorrect as AWS CodeDeploy does not create a replacement file automatically.\nOption B is incorrect as this is not a valid option for creating a new log file for AWS CodeDeploy.\nOption D is incorrect as Restarting the EC2 instance will impact all services running on that instance.\nFor more information on AWS CodeDeploy Error codes, refer to the following URL-\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-ec2-instances.html\n\nOption A is not correct. AWS CodeDeploy does not automatically create a new log file if the existing one is deleted.\nOption B is a valid solution if backups were taken regularly and a backup of the log file is available. However, if no backup was taken, this option cannot be used.\nOption C is also not correct. Restarting the CodeDeployAgent service will not create a new log file. It will only restart the service and it may start logging new data, but the previously deleted log file will not be recovered.\nOption D is the correct solution. Restarting the EC2 instance with the AWS CodeDeploy agent will create a new log file. This is because the AWS CodeDeploy agent logs to a file located on the EC2 instance where the agent is running. When the instance is restarted, the agent will start logging again to a new file.\nTherefore, the correct answer is D - Restart EC2 instance with AWS CodeDeploy agent.\n\n"
}, {
  "id" : 399,
  "question" : "You are deploying an AWS CodeDeploy agent on an EC2 &amp; required to store all logs at a custom location to meet security guidelines.\nWhich of the following can be used to capture wire logs from the Amazon S3 bucket &amp; stored at a custom location within the EC2 instance?\n",
  "answers" : [ {
    "id" : "331366e16504467398a51b2030041afc",
    "option" : "Set “:log_aws_wire:” as False &amp; specify location for logs using “:log_dir:”",
    "isCorrect" : "false"
  }, {
    "id" : "4b5cdc6531684df583b59d8b919fddcc",
    "option" : "Set “:log_dir:” as True &amp; specify location for logs using “:log_aws_wire:”",
    "isCorrect" : "false"
  }, {
    "id" : "15ca774d9021448380a4fa093983f092",
    "option" : "Set “:log_aws_wire:” as True &amp; specify location for logs using “:log_dir:”",
    "isCorrect" : "true"
  }, {
    "id" : "18593a279b214eb19639821e4ea8a747",
    "option" : "Set “:log_dir:” as False &amp; specify location for logs using “:log_aws_wire:”",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS CodeDeploy Agent captures wire logs from Amazon S3 when :log_aws_wire: is set to True &amp; is stored at the location as specified at :log_dir: .\nOptions A &amp; D are incorrect as log_aws_wire needs to be True to capture logs.\nOption B is incorrect as log_aws_wire is set to True to capture logs &amp; stored at the location specified in log_dir.\nFor more information on.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-agent-configuration.html\n\nThe AWS CodeDeploy agent is used to automate code deployments to any instance, including Amazon EC2 instances. The agent sends logs to Amazon S3 buckets by default. However, if you want to store the logs at a custom location within the EC2 instance, you can configure the CodeDeploy agent by modifying the configuration file.\nIn the context of the given question, the objective is to capture wire logs from the Amazon S3 bucket and store them at a custom location within the EC2 instance. Wire logs are the logs generated by the CodeDeploy agent that detail the interactions between the agent and the AWS service.\nOption A: Set “:log_aws_wire:” as False & specify the location for logs using “:log_dir:” This option is incorrect because it sets the value of \"log_aws_wire\" to false, which means that the wire logs will not be captured. Instead, the logs will be written to the location specified in \"log_dir.\"\nOption B: Set “:log_dir:” as True & specify the location for logs using “:log_aws_wire:” This option is incorrect because it sets the value of \"log_dir\" to true, which is not a valid configuration. Additionally, the value of \"log_aws_wire\" should be set to true to capture wire logs.\nOption C: Set “:log_aws_wire:” as True & specify the location for logs using “:log_dir:” This option is correct because it sets the value of \"log_aws_wire\" to true, which captures wire logs. Additionally, the logs will be written to the location specified in \"log_dir.\"\nOption D: Set “:log_dir:” as False & specify the location for logs using “:log_aws_wire:” This option is incorrect because it sets the value of \"log_dir\" to false, which is not a valid configuration. Additionally, the value of \"log_aws_wire\" should be set to true to capture wire logs.\nIn summary, the correct answer is option C: Set “:log_aws_wire:” as True & specify the location for logs using “:log_dir:”\n\n"
}, {
  "id" : 400,
  "question" : "The deployment team is working on a project for a new serverless application using AWS Lambda, CodeBuild &amp; AWS CloudFormation.\nSource code will be accessed by users working on this project from various on-premise locations.\nWhich of the following are supported locations for storing source code used for build? (Select Three)\n",
  "answers" : [ {
    "id" : "9127391b707247f5b48a9aa8ca845ffc",
    "option" : "EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "4900b9d98e3246088cb999134901ad6b",
    "option" : "Amazon S3 Bucket",
    "isCorrect" : "true"
  }, {
    "id" : "796439d3cbd84714ab35259070ba9f49",
    "option" : "On-Premise Local machine",
    "isCorrect" : "false"
  }, {
    "id" : "315916569d4d4585a670fdfa308b161d",
    "option" : "AWS CodeCommit",
    "isCorrect" : "true"
  }, {
    "id" : "4f7c4b35f19c4978b83d81fbe47c3511",
    "option" : "Bitbucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - B, D, E.\nAWS CodeBuild supports building a build from a source code located at the following locations.\nAWS CodeCommit.\nAmazon S3\nGitHub.\nBitbucket.\nOption A &amp; C are incorrect as EC2 instance &amp; On-Premise local machines are not valid locations that can store source code for creating a build with AWS CodeBuild.\nFor more information on Source Code location for AWS CodeBuild refer to the following URL-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/planning.html\n\nSure, I'd be happy to help!\nAWS Lambda, CodeBuild, and CloudFormation are all part of the AWS serverless ecosystem and are designed to work together to enable rapid and efficient development and deployment of serverless applications. As part of this process, source code needs to be stored somewhere where it can be accessed by the development team.\nThe options for storing source code used for build are as follows:\nA. EC2 Instance: This option is not recommended for storing source code used for build as EC2 instances are not typically used for this purpose. EC2 instances are typically used for running applications and other compute-intensive workloads.\nB. Amazon S3 Bucket: This is a recommended option for storing source code used for build. S3 buckets are highly scalable, durable, and secure. They are also highly available, meaning that developers can access the source code from anywhere with an internet connection.\nC. On-Premise Local machine: This is another option for storing source code used for build. However, it is not recommended because it can lead to issues with version control and synchronization. In addition, it can be difficult to access the source code from different locations.\nD. AWS CodeCommit: This is another recommended option for storing source code used for build. CodeCommit is a fully-managed source control service that is highly secure and scalable. It is also integrated with other AWS services, such as CodeBuild and CloudFormation.\nE. Bitbucket: This is an option for storing source code used for build, but it is not an AWS service. Bitbucket is a web-based hosting service that is owned by Atlassian. While it is possible to integrate Bitbucket with AWS services such as CodeBuild and CloudFormation, it may not provide the same level of integration and security as AWS CodeCommit.\nIn summary, the recommended options for storing source code used for build with AWS Lambda, CodeBuild, and CloudFormation are Amazon S3 buckets and AWS CodeCommit. On-premise local machines and Bitbucket are also options, but they may not provide the same level of integration and security as the AWS services. EC2 instances are not typically used for this purpose.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 403,
  "question" : "You are planning to build a serverless application with microservices using AWS Code Build.\nThis application will be using AWS Lambda.\nHow will you specify AWS SAM CLI &amp; Application package within the build spec file of an AWS CodeBuild?\n",
  "answers" : [ {
    "id" : "5a83ce456cd0403da3c3ce96bb4a819f",
    "option" : "Use aws-sam-cli in the install phase &amp; sam package in the build phase of the BuildSpec file.",
    "isCorrect" : "true"
  }, {
    "id" : "12f02477b9464747b8fca4974d18ea01",
    "option" : "Use aws-sam-cli in the pre-build phase &amp; sam package in the post_build phase of the BuildSpec file.",
    "isCorrect" : "false"
  }, {
    "id" : "5b1522cef655461e9378e21ea16b9c27",
    "option" : "Use aws-sam-cli in the install phase &amp; sam package in the pre_build phase of the BuildSpec file.",
    "isCorrect" : "false"
  }, {
    "id" : "269461b8e3754e28a28d9bc0705c679f",
    "option" : "Use aws-sam-cli in the pre-build phase &amp; sam package in the install phase of the BuildSpec file.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS SAM CLI needs to be installed during the install phase of AWS CodeBuild, while AWS SAM package needs to be specified in the Build section, creating a zip file of code &amp; upload to Amazon S3.\nRefer page 149 on the below link:\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/codepipeline-user.pdf\nOption B is incorrect as AWS-sam-cli needs to be installed in the install phase &amp; not in the pre-build phase.\nOption C is incorrect as the sam package needs to be specified in the post-build section &amp; not in the pre-build phase.\nOption D is incorrect as AWS-sam-cli needs to be installed in the install phase &amp; not in the pre-build phase.\nsam package needs to be specified in the post-build section &amp; not in the install phase.\nFor more information on using AWS SAM with AWS CodeBuild, refer to the following URLs-\nhttps://aws.amazon.com/blogs/devops/building-and-testing-polyglot-applications-using-aws-codebuild/\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-build.html\n\nAWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeBuild can be used to build serverless applications with microservices using AWS SAM CLI and Application package.\nWhen building a serverless application with microservices using AWS CodeBuild, the AWS SAM CLI and Application package should be specified in the build spec file. The build spec file is a YAML-formatted file that contains the instructions for AWS CodeBuild to follow when building the application.\nTo specify the AWS SAM CLI and Application package within the build spec file of an AWS CodeBuild, the following steps can be followed:\nChoose the appropriate phase in the build spec file where the AWS SAM CLI and Application package will be used. The options are install, pre_build, build, post_build. Use the AWS SAM CLI command 'aws-sam-cli' to install it in the install phase or pre_build phase. The AWS SAM CLI is a tool that enables local development and testing of serverless applications defined with AWS SAM. Use the 'sam package' command to package the AWS Lambda functions in the pre_build phase or build phase. This command creates an AWS CloudFormation template and packages the code and dependencies for deployment to Lambda. Finally, ensure that the build spec file is saved and committed to version control for future reference.\nTherefore, the correct answer to the question is option C: Use aws-sam-cli in the install phase & sam package in the pre_build phase of the BuildSpec file.\n\n"
}, {
  "id" : 404,
  "question" : "For security reasons, the Development Team is looking for an alternate secure location for saving BuildSpec files for a New Build Project in the US-west region.\nWhich of the following options can be used for storing BuildSpec file?\n",
  "answers" : [ {
    "id" : "c91bb3e4afc14d3c880bde1a03796229",
    "option" : "Buildspec file can only be saved in the root of the source directory.",
    "isCorrect" : "false"
  }, {
    "id" : "c816c75d56824ae9a79da8e9df37c0e6",
    "option" : "Store Buildspec file in Amazon S3 bucket in same region as that of Build Project.",
    "isCorrect" : "true"
  }, {
    "id" : "7c40664499b448d9881c99ec8c0cf483",
    "option" : "Store Buildspec file in Amazon S3 bucket in any other region from the Build Project.",
    "isCorrect" : "false"
  }, {
    "id" : "6ebef14866cc436b93379ab270e0708d",
    "option" : "Buildspec file can be stored in Buildspec folder of source directory.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nBy Default, the Buildspec file is stored in the root of the source directory.\nAlternatively, this file can be saved in an Amazon S3 bucket in the same region as that of the Build Project.\nOption A is incorrect as the Buildspec file can be saved in other locations apart from the source directory's default root.\nOption C is incorrect as the Buildspec file needs to be saved only in the Amazon S3 bucket in the same region as that of the Build Project.\nOption D is incorrect as Buildspec Folder is not required for the Buildspec file to be saved.\nFor more information on the Buildspec file, refer to the following URL-\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html\n\nThe BuildSpec file is a YAML-formatted file that is used by AWS CodeBuild to run a build project. It is typically stored in the root of the source directory, but it can also be stored in the Buildspec folder of the source directory.\nIf the Development Team wants to store the BuildSpec file in a secure location outside of the root of the source directory, then option B or C would be the best choice.\nOption B suggests storing the BuildSpec file in an Amazon S3 bucket in the same region as the Build Project. This option is a good choice for a secure location as Amazon S3 is designed to provide high durability, availability, and scalability of data.\nOption C suggests storing the BuildSpec file in an Amazon S3 bucket in any other region from the Build Project. While this option can be used, it may introduce additional latency and network costs when accessing the BuildSpec file during the build process.\nOption A is incorrect, as the BuildSpec file can be stored in the Buildspec folder of the source directory.\nTherefore, the correct answer would be either option B or C, depending on the specific needs and requirements of the Development Team.\n\n"
}, {
  "id" : 405,
  "question" : "Development Team uses AWS CodePipeline to deploy a new application from the Amazon S3 bucket to a fleet of Amazon EC2 instances across multiple AZ's.\nThe team needs to have a test before the Deploy Stage to ensure no bugs in the source code.\nWhich of the following is an additional requirement for Custom Action in AWS CodePipeline?\n",
  "answers" : [ {
    "id" : "7c05514df5ac498bbd3de15e8393fdc8",
    "option" : "Create a Custom Worker with access to the public endpoint for AWS CodePipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "4364004a97fa4f679ba42f5fdbdb9a3d",
    "option" : "Create a Custom Worker with access to the private endpoint for AWS CodePipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "84c194b45d9d42478daa53f985b2f133",
    "option" : "Create a Job Worker with access to the private endpoint for AWS CodePipeline.",
    "isCorrect" : "false"
  }, {
    "id" : "2d95a4a869c64a95bf3d9e97601f3cdf",
    "option" : "Create a Job Worker with access to the public endpoint for AWS CodePipeline.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nWhile creating Custom actions with AWS CodePipeline, a Job worker needs to be created on any device which has access to the Public endpoint for AWS CodePipeline.\nOption A &amp; B are incorrect as a Custom Worker is not a valid option.\nOption C is incorrect as Job worker needs access to the public endpoint for AWS CodePipeline &amp; not to the private endpoint.\nFor more information on creating a Job Worker with AWS CodePipeline, refer to the following URL-\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-custom-action.html#actions-create-custom-action-job-worker\n\nTo add a custom action in AWS CodePipeline, the development team needs to create a custom worker or a job worker with access to the appropriate endpoint for AWS CodePipeline. A custom action in AWS CodePipeline allows users to perform additional actions beyond the built-in actions, such as running custom tests or deploying code to a specific environment.\nIn this scenario, the development team needs to create a test before the Deploy Stage to ensure that there are no bugs in the source code. This means that they need to add a custom action in AWS CodePipeline to run the test.\nTo create a custom action in AWS CodePipeline, the team needs to create a custom worker or a job worker with access to the appropriate endpoint for AWS CodePipeline. The endpoint they need access to depends on whether they want to create a custom action for the source stage or the deploy stage.\nIn this case, the team needs to create a custom action before the deploy stage. Therefore, they need to create a job worker with access to the private endpoint for AWS CodePipeline (Option C).\nOption A (creating a custom worker with access to the public endpoint for AWS CodePipeline) is not a valid option because the public endpoint for AWS CodePipeline is not secure and should not be used for accessing sensitive resources.\nOption B (creating a custom worker with access to the private endpoint for AWS CodePipeline) is also not a valid option because a custom worker is not needed for this scenario. A job worker is sufficient to create a custom action in AWS CodePipeline.\nOption D (creating a job worker with access to the public endpoint for AWS CodePipeline) is not a valid option because the public endpoint for AWS CodePipeline is not secure and should not be used for accessing sensitive resources.\nTherefore, the correct answer is Option C (create a job worker with access to the private endpoint for AWS CodePipeline).\n\n"
}, {
  "id" : 406,
  "question" : "A start-up firm has created a new distributed application using microservice architecture.\nA large number of services across multiple AWS regions are involved in this application.\nThe Project Director wants to have an aggregated view of all connections between these services along with latency to benchmark application performance.\nWhich of the following can be used to meet this requirement?\n",
  "answers" : [ {
    "id" : "14aa63c4a7524f06960c2a136450137d",
    "option" : "Enable AWS X-Ray &amp; use Service Maps to view data for all regions.",
    "isCorrect" : "true"
  }, {
    "id" : "38a8160db9ca4f6790b5fd30b9292451",
    "option" : "Enable AWS X-Ray &amp; use Service Maps which will capture data in each region &amp; then correlate latency between AWS regions.",
    "isCorrect" : "false"
  }, {
    "id" : "bb85a7fb745145c5a3db3fb110beafe5",
    "option" : "Enable AWS X-Ray &amp; use Sampling to view data for all regions.",
    "isCorrect" : "false"
  }, {
    "id" : "acaf801b59a142da9e8bf0772b741d9d",
    "option" : "Enable AWS X-Ray &amp; use Sampling to view data in each region &amp; then correlate latency between AWS regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS X-Ray Service maps can be used to create dependency trees between multiple services across multiple AWS regions.\nThey also detect latency between these services which can help resolve performance issues.\nOption B is incorrect as Service Maps will display data for services across regions &amp; not for individual regions.\nOptions C &amp; D is incorrect as Sampling is the number of requests between application services, which AWS X-Ray gathers to create Service maps.\nFor more information on AWS X-Ray use cases, refer to the following URL-\nhttps://aws.amazon.com/xray/features/\n\nThe best solution to meet the project director's requirements is to enable AWS X-Ray and use Service Maps to view data for all regions. Therefore, the correct answer is A.\nAWS X-Ray is a service that helps developers analyze and debug production, distributed applications, such as those built with a microservices architecture. It allows developers to trace requests made to the application and provides insights into how the application is performing.\nService Maps is a feature of AWS X-Ray that provides a visual representation of the architecture of a distributed application, including all the services involved in the application, along with connections between those services. With Service Maps, you can see a high-level view of your application's architecture and how services are interconnected.\nBy enabling AWS X-Ray and using Service Maps, the project director can view data for all regions and have an aggregated view of all connections between services along with latency to benchmark application performance.\nOption B is not a valid solution because Service Maps capture data in each region and correlate latency between AWS regions, but this option does not allow viewing data for all regions.\nOption C is also not a valid solution because Sampling only captures a subset of data, not all data.\nOption D is not the best solution because it only allows viewing data in each region and correlating latency between AWS regions, not viewing data for all regions.\nTherefore, the correct solution is to enable AWS X-Ray and use Service Maps to view data for all regions.\n\n"
}, {
  "id" : 407,
  "question" : "A new application is launched into production by your software firm using AWS Cloud infrastructure.\nYou have integrated it with AWS X-Ray to perform further optimization.\nWhich of the following error code will provide insights into server faults in an application?\n",
  "answers" : [ {
    "id" : "ebcf3229029946e1bead716ba3613d9e",
    "option" : "400",
    "isCorrect" : "false"
  }, {
    "id" : "4529b2d51368430690d6144048ab6392",
    "option" : "500",
    "isCorrect" : "true"
  }, {
    "id" : "0ae96df2d3bd46cf8f378c7c2d328b12",
    "option" : "603",
    "isCorrect" : "false"
  }, {
    "id" : "4d5cdba963f447c0a9f633016aa51869",
    "option" : "429",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nFor server related faults, AWS X-Ray will generate error code 500.\nOption A is incorrect as Error code 400 is generated for client errors.\nOption C is incorrect as Error code 603 is an invalid error code.\nOption D is incorrect as Error code 429 is generated for throttling errors.\nFor more information on Error codes with AWS X-Ray, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html\n\nThe correct answer is B. 500.\nWhen an application is integrated with AWS X-Ray, it allows you to trace and analyze user requests as they travel through the application. X-Ray helps you identify the root cause of performance issues and errors within your application. AWS X-Ray groups fault data into categories, such as 4xx (client errors) and 5xx (server errors).\nIn this scenario, you are looking for an error code that will provide insights into server faults in the application. Therefore, the answer would be an HTTP status code in the 5xx range. These codes indicate that the server encountered an error while trying to fulfill the request.\nAmong the options provided, only option B (500) falls into this range, making it the correct answer. The other options are as follows:\nA. 400: This error code indicates a bad request or a client-side error. It indicates that the server could not understand or process the request due to malformed syntax or invalid parameters.\nB. 500: This error code indicates an internal server error. It indicates that the server encountered an error while trying to fulfill the request.\nC. 603: This error code is not a standard HTTP status code and does not provide any insight into server faults. Therefore, it is an incorrect answer.\nD. 429: This error code indicates too many requests or rate limiting. It indicates that the client has sent too many requests in a given amount of time, and the server is unable to process them all.\nIn conclusion, when integrating an application with AWS X-Ray, error code 500 is the most relevant code to provide insights into server faults within an application.\n\n"
}, {
  "id" : 408,
  "question" : "A microservice application is developed using AWS Lambda functions which are invoked by non-instrumented service.\nFor performance optimization, AWS X-Ray needs to be integrated with this AWS Lambda function to get traces with multiple services.\nWhat should a developer do when a non-instrumented service invokes an AWS Lambda function?\n",
  "answers" : [ {
    "id" : "fde0f4117c0346309d8985b389e43f68",
    "option" : "For non-instrumented services, AWS Lambda records trace without any additional configuration.",
    "isCorrect" : "false"
  }, {
    "id" : "5e5c4c04bbfc4a218864a357ea8eb436",
    "option" : "Enable Start tracing under AWS Lambda Function configuration.",
    "isCorrect" : "false"
  }, {
    "id" : "b5b1203a68764ab9a7fae48080caa46c",
    "option" : "Enable Active tracing under AWS Lambda Function configuration.",
    "isCorrect" : "true"
  }, {
    "id" : "788bddb039c94939b78b63243bb02419",
    "option" : "AWS Lambda functions invoked by non-instrumented services do not support tracing.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nFor an AWS Lambda function invoked by a non-instrumented service, the tracing can be enabled from the AWS console or by the following AWS CLI.\n$ aws lambda update-function-configuration --tracing-config '{\"Mode\": \"Active\"}'\nOption A is incorrect as AWS Lambda trace requests without any additional configuration if AWS Lambda function is called by another instrumented service &amp; not for non-instrumented service.\nOption B is incorrect as Start Tracing is an invalid option to enable Tracing for Lambda Function invoked by a non-instrumented service.\nOption D is incorrect as the AWS Lambda function invoked by non-instrumented services supports tracing by enabling active tracing within a Lambda function.\nFor more information on AWS X-Ray with Lambda, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-services-lambda.html\n\nThe correct answer is C. Enable Active tracing under AWS Lambda Function configuration.\nAWS X-Ray is a service that helps developers analyze and debug distributed applications, including those built using microservices. It provides end-to-end tracing of requests as they travel through the different components of the application, giving developers visibility into performance issues and errors.\nTo integrate AWS X-Ray with AWS Lambda functions, developers need to enable tracing for the function. There are two types of tracing available: Active tracing and Passive tracing.\nActive tracing means that AWS Lambda will automatically create trace segments for incoming requests and send them to X-Ray. This is the recommended approach when integrating with non-instrumented services.\nPassive tracing requires the caller to send a trace header along with the request, which AWS Lambda will then use to create trace segments. This approach is suitable when integrating with instrumented services that already generate trace headers.\nOption A is incorrect because AWS Lambda does not automatically record traces for non-instrumented services without additional configuration.\nOption B is incorrect because starting a trace is not enough to ensure that the trace will be sent to X-Ray. Additional configuration is needed to enable AWS Lambda to send the trace segments to X-Ray.\nOption D is incorrect because AWS Lambda functions invoked by non-instrumented services do support tracing, but they require additional configuration to enable tracing.\n\n"
}, {
  "id" : 409,
  "question" : "For a new HTTP based application deployed on Amazon EC2 &amp; DynamoDB, you are using AWS API to retrieve service graph data.\nTeam Lead has suggested using GetTraceSummaries API to get trace summaries.\nWhich of the following flags are available while retrieving traces using GetTraceSummaries API? (Select Two).\n",
  "answers" : [ {
    "id" : "517a162fda8e4e3191a0d7e4ea7626e8",
    "option" : "UserId",
    "isCorrect" : "false"
  }, {
    "id" : "3fa521a283114ac498c62f884fa2dddf",
    "option" : "Annotations",
    "isCorrect" : "false"
  }, {
    "id" : "bc7a37030d7345e7a6ab309330e367bb",
    "option" : "HTTPMethod",
    "isCorrect" : "false"
  }, {
    "id" : "5a9b6039875840f887c7df46115766fa",
    "option" : "TraceID",
    "isCorrect" : "true"
  }, {
    "id" : "f558e1404bd942c39053aec9e5fc0c81",
    "option" : "Event Time.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D, E.\nWhen GetTraceSummaries API is called, the following time range flags are available.\n1)TraceID: Search uses TraceId time &amp; returns traces between computed start &amp; end time range.\n2)Event Time: Searches based upon the event occur &amp; returns traces between which are active during start &amp; end time range.\nOption A, B &amp; C are incorrect as these flags are not used in retrieving traces with GetTraceSummaries API.\nFor more information on AWS X-Ray API, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-gettingdata.html\n\nThe GetTraceSummaries API is used to retrieve a list of trace summaries for a specified time range from AWS X-Ray. Each trace summary includes general information about a trace, including its ID, duration, and start time. The flags available while retrieving traces using the GetTraceSummaries API are as follows:\nD. TraceID: The TraceID flag is used to retrieve a specific trace by its ID. This flag is useful if you already know the ID of the trace you want to retrieve.\nA. UserId: The UserId flag is used to retrieve traces that contain a specific user ID. This flag is useful if you want to find traces for a particular user or group of users.\nB. Annotations: The Annotations flag is not available while retrieving traces using the GetTraceSummaries API. Annotations are key-value pairs that you can add to traces to provide additional information.\nC. HTTPMethod: The HTTPMethod flag is not available while retrieving traces using the GetTraceSummaries API. HTTP method is used to specify the type of request being made to the server.\nE. Event Time: The Event Time flag is not available while retrieving traces using the GetTraceSummaries API. Event Time is used to specify the time when an event occurred.\nTherefore, the correct answer is D. TraceID and A. UserId.\n\n"
}, {
  "id" : 410,
  "question" : "A distributed application written in Node.js is being deployed using AWS Elastic Beanstalk.\nThere is an additional requirement to have AWS X-Ray integration.\nWhich of the following options can be used to integrate AWS X-Ray daemon with AWS Elastic Beanstalk in this scenario?\n",
  "answers" : [ {
    "id" : "ae1fd247fdef41e4aeac1f4d5c1602b8",
    "option" : "Download AWS X-Ray daemon from Amazon S3 &amp; run with configuration file.",
    "isCorrect" : "false"
  }, {
    "id" : "c0cabeab1e0b41c8a8a035ee4af5f9c2",
    "option" : "Install AWS X-Ray agent in AWS Elastic beanstalk.",
    "isCorrect" : "false"
  }, {
    "id" : "c115e0981fac4387977cdb9382198c51",
    "option" : "Use AWS Elastic Beanstalk console to enable AWS X-Ray integration.",
    "isCorrect" : "true"
  }, {
    "id" : "72103a6c83234fa297a22fe5112c1e6a",
    "option" : "Install AWS X-Ray daemon on EC2 and run the configuration file.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nYou can use the X-Ray SDK with the following Elastic Beanstalk platforms:\nGo - version 2.9.1 and later.\nJava 8 - version 2.3.0 and later.\nJava 8 with Tomcat 8 - version 2.4.0 and later.\nNode.js - version 3.2.0 and later.\nWindows Server - all platform versions released on or after December 18th, 2016\nPython - version 2.5.0 and later.\nOn supported platforms, you can use a configuration option to run the X-Ray daemon on the instances in your environment.\nYou can enable the daemon in the Elastic Beanstalk console or by using a configuration file.\nIf the X-Ray daemon isn't available for your platform configuration, you can download it from Amazon S3 and run it with a configuration file.\nOption A is incorrect as this process is used for non-supported Platforms.\nAWS X-Ray Daemon can be installed with AWS Elastic Beanstalk by downloading from the Amazon S3 bucket.\nOption B is incorrect as AWS X-Ray agents are used for EC2/ECS instances &amp; not with AWS Elastic Beanstalk.\nOption C is correct as the AWS Elastic Beanstalk console can be used to integrate AWS X-Ray for supported platforms.\nOption D is incorrect as there is no need to install anything on EC2\nElastic Beanstalk can be integrated with AWS X-Ray from the console.\nFor more information on AWS X-Ray, refer to the following URLs-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-configuration.html\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-beanstalk.html\n\nTo integrate AWS X-Ray with Elastic Beanstalk, you can use the AWS Elastic Beanstalk console or download and run the AWS X-Ray daemon on EC2 instances.\nOption A: Download AWS X-Ray daemon from Amazon S3 & run with configuration file. This option is possible but requires additional configuration and management. The AWS X-Ray daemon needs to be downloaded from Amazon S3 and configured to run with the Node.js application. This option is suitable for advanced users who prefer more control over the integration process.\nOption B: Install AWS X-Ray agent in AWS Elastic Beanstalk. This option is not possible as there is no AWS X-Ray agent available to be installed in Elastic Beanstalk. The agent is used to instrument AWS resources such as AWS Lambda functions, AWS Elastic Container Service (ECS), and Amazon EC2 instances. Elastic Beanstalk is already managed and deployed by AWS, and it provides built-in integration with AWS X-Ray.\nOption C: Use AWS Elastic Beanstalk console to enable AWS X-Ray integration. This option is the simplest and recommended way to integrate AWS X-Ray with Elastic Beanstalk. You can enable X-Ray integration in the Elastic Beanstalk console with just a few clicks. Once enabled, Elastic Beanstalk automatically instruments the application and sends tracing data to AWS X-Ray. This option is suitable for users who prefer a simple and easy integration process.\nOption D: Install AWS X-Ray daemon on EC2 and run the configuration file. This option is possible but requires additional configuration and management. The AWS X-Ray daemon needs to be installed on EC2 instances and configured to run with the Node.js application. This option is suitable for advanced users who prefer more control over the integration process.\nIn summary, the best option to integrate AWS X-Ray with Elastic Beanstalk in this scenario is to use the AWS Elastic Beanstalk console to enable AWS X-Ray integration.\n\n"
}, {
  "id" : 411,
  "question" : "An online educational institute uses a three-tier web application &amp; is using AWS X-Ray to trace data between various services.\nUser A is experiencing latency issues using this application &amp; the Operations team has asked you to gather all traces for User A.\nWhich of the following needs to be enabled to get Filtered output for User A from all other traces?\n",
  "answers" : [ {
    "id" : "ef1c7ae5ed38444eac134d3879863702",
    "option" : "Trace ID",
    "isCorrect" : "false"
  }, {
    "id" : "1340c302ad644f1299744a82d9b66730",
    "option" : "Annotations",
    "isCorrect" : "true"
  }, {
    "id" : "99004e20cb7941c98b97aa3654707d18",
    "option" : "Segment ID",
    "isCorrect" : "false"
  }, {
    "id" : "386d8f85c1a341d19235b18cc4db0ea1",
    "option" : "Tracing header.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAnnotations are key-value pairs indexed to use with filter expressions.\nIn the above case, traces for a user A needs to be tracked, for which Annotations can be used along with a Filter expression to find all traces related to that user.\nOption A is incorrect as Trace ID will track a request's path through the application &amp; will not be used in filtering messages.\nOption C is incorrect as Segment will provide details of resource name, request &amp; work done.\nIt will not help in filtering messages.\nOption D is incorrect as the Tracing Header consists of Root trace ID, Segment ID &amp; sampling decision.\nIt is not useful for filtering messages.\nFor more information on Annotations &amp; Filter expressions on AWS X-Ray, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html\n\nTo gather all traces for a specific user in a three-tier web application using AWS X-Ray, you need to enable the tracing header. The tracing header is a unique identifier that allows X-Ray to associate all traces that belong to a specific user or request.\nAWS X-Ray is a service that allows you to trace and analyze requests as they travel through your application. It provides end-to-end visibility into requests and helps you identify issues such as latency, errors, and other performance problems.\nIn AWS X-Ray, a trace is a collection of data about a single request as it moves through your application. A trace consists of one or more segments, which are individual components of your application, such as a web server, a database, or an API.\nAnnotations are metadata that you can attach to segments to provide additional information about them. Annotations are useful for adding context to traces, but they are not required to trace requests.\nThe segment ID is a unique identifier for each segment in a trace. The segment ID can be used to identify a specific segment within a trace, but it is not useful for filtering traces by user.\nThe trace ID is a unique identifier for each trace. The trace ID can be used to group all segments in a trace, but it is not useful for filtering traces by user.\nIn summary, to get filtered output for a specific user in a three-tier web application using AWS X-Ray, you need to enable the tracing header. The tracing header is a unique identifier that allows X-Ray to associate all traces that belong to a specific user or request. Annotations and segment IDs are useful for adding context to traces, but they are not required for tracing or filtering traces by user.\n\n"
}, {
  "id" : 412,
  "question" : "You are planning to use AWS X-Ray for a multiservice application for which operations teams are getting lots of complaints regarding application performance.\nBefore integrating AWS X-Ray, you are looking into the core details of AWS X-Ray.\nWhich of the following is the correct statement of AWS X-Ray?\n",
  "answers" : [ {
    "id" : "ea00b6a8b8e547828ffb4e2c28a7137a",
    "option" : "Sub-Segments can only be embedded in a full segment document.",
    "isCorrect" : "false"
  }, {
    "id" : "7ab5616aa0b8471d96b412c0d4db2365",
    "option" : "X-Ray Trace is a set of data points sharing the same Segment I.",
    "isCorrect" : "false"
  }, {
    "id" : "3f56320eaed94d6ca393f94ab8668d4d",
    "option" : "Annotations consist of only system defined data.",
    "isCorrect" : "false"
  }, {
    "id" : "b6cd1f2134474e4f8f225aba79a6b097",
    "option" : "Segment consists of multiple annotations.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nSegment consists of tracing records for a request which a distributed application makes.\nSegment consists of multiple system defined &amp; user-defined annotations and sub-segments consisting of remote calls made from this application.\nOption A is incorrect as Sub-Segments can also be sent independently apart from embedding in segment documents.\nOption B is incorrect as X-Trace is a set of data points sharing the same Trace ID &amp; not segment ID.Option C is incorrect as Annotations consists of system-defined &amp; user-defined data.\nFor more information on AWS X-Ray, refer to the following URL-\nhttps://aws.amazon.com/xray/faqs/\n\nAWS X-Ray is a service that helps in analyzing, monitoring, and debugging distributed applications, which are composed of several microservices. With AWS X-Ray, you can understand how your application and its underlying services are performing and pinpoint the root cause of performance issues.\nNow let's look at the options provided in the question:\nA. Sub-Segments can only be embedded in a full segment document.\nThis statement is incorrect. Sub-segments can be embedded in both the full segment document and other sub-segments. A segment can contain multiple sub-segments, and each sub-segment can contain additional sub-segments.\nB. X-Ray Trace is a set of data points sharing the same Segment I.\nThis statement is incorrect. An X-Ray Trace is a collection of data points gathered from a single request as it travels through a distributed system. Each trace is identified by a unique trace ID, which is used to group the data points together.\nC. Annotations consist of only system-defined data.\nThis statement is incorrect. Annotations can contain both system-defined and user-defined data. Annotations are key-value pairs that provide additional information about a trace or a segment.\nD. Segment consists of multiple annotations.\nThis statement is correct. A segment represents a single component within a distributed application, such as a microservice or a database. A segment can contain multiple annotations, which are used to provide additional context about the segment.\nIn conclusion, the correct statement about AWS X-Ray is that a segment consists of multiple annotations.\n\n"
}, {
  "id" : 413,
  "question" : "You have integrated the application with the X-Ray SDK which generates segment documents recording the applications' tracing information.\nNew recruit is enquiring about fields which are required with a Segment Document.\nWhich of the following is a required field in a trace Segment sent by an application?\n",
  "answers" : [ {
    "id" : "bdce7641ad444f4184f74bb03a2de23e",
    "option" : "service",
    "isCorrect" : "false"
  }, {
    "id" : "095dc36153b14568bc9db8191fba6eda",
    "option" : "http",
    "isCorrect" : "false"
  }, {
    "id" : "cbce5e6eb241448b8e504ebda2c3bb5f",
    "option" : "start_time",
    "isCorrect" : "true"
  }, {
    "id" : "f5f3096fb09b494485c3ec54a264037a",
    "option" : "parent_id.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nFollowing are the required segment fields in a Segment document.\n1)Name.\n2)Id.\n3)Trace_id.\n4)Start_time.\n5)End_time.\n6)In_progress.\nFollowing are the optional segment fields in a Segment Document.\n1)Service.\n2)User.\n3)Origin.\n4)Parent_id.\n5)http.\n6)aws.\n7)error, throttle, fault, and cause.\n8)annotations.\n9)metadata.\n10) subsegments.\nOption A is incorrect as service is an optional field in a Segment.\nOption B is incorrect as HTTP is an optional field in a Segment.\nOption D is incorrect as the parent_id field is required in the Subsegment field &amp; optional in the Segment field.\nFor more information on Segments in an AWS X-Ray, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html\n\nSure, I'd be happy to provide a detailed explanation!\nWhen an application is integrated with the AWS X-Ray SDK, it generates trace data in the form of segment documents. These documents record information about the various components and resources that the application interacts with, allowing developers and operators to gain insight into the application's performance and behavior.\nEach segment document contains a variety of fields that provide information about the segment itself and its relationship to other segments in the trace. Of the four fields listed in the question, one is required for every segment document: the \"service\" field.\nThe \"service\" field is used to identify the name of the AWS resource or component that the segment represents. For example, if a segment represents a request to an Amazon S3 bucket, the \"service\" field would be set to \"s3\". This field is important because it allows X-Ray to group related segments together in the trace, making it easier to understand the flow of requests through the system.\nThe other fields listed in the question are also important, but they are not strictly required for every segment document. The \"http\" field, for example, is used to record information about HTTP requests and responses, such as status codes and headers. If a segment represents an HTTP request or response, the \"http\" field should be included, but it is not necessary for other types of segments.\nThe \"start_time\" field is used to record the start time of the segment, and the \"parent_id\" field is used to identify the ID of the parent segment in the trace. These fields are important for understanding the timing and relationships between segments, but they are not strictly required.\nIn summary, the \"service\" field is the only required field in a trace segment sent by an application. The other fields listed in the question are important for providing additional context and information, but they are not strictly required for every segment.\n\n"
}, {
  "id" : 414,
  "question" : "A Junior Engineer is configuring AWS X-Ray daemon which would be running locally on a multi-vendor OS environment.\nHe is concerned about the listening port to be configured for this scenario.\nWhich is the correct statement for the AWS X-Ray Daemon listening port?\n",
  "answers" : [ {
    "id" : "80e07723b96a443f814f3dff0b6ac62a",
    "option" : "AWS X-Ray Daemon Listening Port can be changed only for the Linux environment.",
    "isCorrect" : "false"
  }, {
    "id" : "611591d7343d4eca81a844b229387c6a",
    "option" : "AWS X-Ray Daemon Listening Port cannot be changed while running the daemon locally.",
    "isCorrect" : "false"
  }, {
    "id" : "5a9939326c09494e9e2d18106da5afe2",
    "option" : "AWS X-Ray Daemon Listening Port can be changed using --bind command with CLI.",
    "isCorrect" : "true"
  }, {
    "id" : "366b96417feb46baba9c0aba08a1e18a",
    "option" : "AWS X-Ray Daemon Listening Port is default as 2000 &amp; cannot be changed.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nBy default, AWS X-Ray Daemon listens for traffic on UDP port 2000\nThis port can be changed while configuring daemon with CLI option as - bind “Different port number”.\nOption A is incorrect as the AWS X-Ray Daemon Listening port can be changed for all environments.\nOption B is incorrect as Listening Ports can be changed for running Daemons Locally as well.\nOption D is incorrect as Daemons Listening port is UDP 2000, but it can be changed using - bind command.\nFor more information on AWS X-Ray Daemon Port configuration, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html\n\nThe correct statement for the AWS X-Ray Daemon listening port is:\nC. AWS X-Ray Daemon Listening Port can be changed using --bind command with CLI.\nThe AWS X-Ray daemon is a software application that is responsible for sending trace data from applications to the X-Ray service. The daemon listens on a specific port for incoming trace data.\nHere is an explanation of the other answer options:\nA. AWS X-Ray Daemon Listening Port can be changed only for the Linux environment. This statement is incorrect because the AWS X-Ray daemon can be run on multiple operating systems, not just Linux.\nB. AWS X-Ray Daemon Listening Port cannot be changed while running the daemon locally. This statement is incorrect because the AWS X-Ray daemon can be configured while it is running. However, some changes may require the daemon to be restarted for the new configuration to take effect.\nD. AWS X-Ray Daemon Listening Port is default as 2000 & cannot be changed. This statement is incorrect because the default listening port for the AWS X-Ray daemon is 2000, but it can be changed using the \"--bind\" command with the CLI.\nIn summary, the correct statement is that the AWS X-Ray Daemon Listening Port can be changed using the \"--bind\" command with the CLI.\n\n"
}, {
  "id" : 415,
  "question" : "A loose coupled distributed application is built using Amazon SQS along with Amazon EC2 container service and on-premise hosts.\nAWS X-Ray is enabled for all messages passing through Amazon SQS.\nWhat will be the effect of AWS X-Ray Trace headers on the Amazon SQS message?\n",
  "answers" : [ {
    "id" : "201dd04d64514b5fa294d816c79018f2",
    "option" : "1 byte of Trace header is added to AWS SQS message size.",
    "isCorrect" : "false"
  }, {
    "id" : "df0be1cee54943fd88aca51e788f3b5d",
    "option" : "Trace Header will be included as a message attribute in an AWS SQS message.",
    "isCorrect" : "false"
  }, {
    "id" : "865fe716826b4e50911938a31287f7df",
    "option" : "Trace Header will be included as message content in an AWS SQS message.",
    "isCorrect" : "false"
  }, {
    "id" : "3dd719601ca741d19dc330d1cfdc83f1",
    "option" : "Trace header is excluded from AWS SQS message size.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nWhen AWS X-Ray is integrated with AWS SQS, there is no impact of trace Header on AWS SQS message size or message attributes quota.\nOption A is incorrect as 1 byte of trace header is not added to AWS SQS message size.\nOption B is incorrect as Trace Header will be excluded from message attribute quota in AWS SQS message.\nOption C is incorrect as Trace Header will be carried as Default HTTP Header or AWSTraceHeader System Attribute but not as a message content in an AWS SQS message.\nFor more information on AWS X-Ray with Amazon SQS, refer to the following URL-\nhttps://docs.aws.amazon.com/xray/latest/devguide/xray-services-sqs.html\n\nWhen AWS X-Ray is enabled for an application, it generates trace data for each request that passes through the application. This trace data contains information about the request, such as its origin, destination, and any other metadata that may be relevant.\nTo propagate this trace data across multiple services, X-Ray injects a trace header into each request as it passes through the application. This header contains a unique identifier for the request, as well as other information such as the sampling decision and parent ID.\nIn the context of an application that uses Amazon SQS and Amazon EC2 Container Service, the trace header is added to the messages that are sent through SQS. This allows X-Ray to trace the entire path of the request as it moves through the different components of the application.\nNow, coming to the given options:\nA. 1 byte of Trace header is added to AWS SQS message size.\nThis option is incorrect because the trace header is not a fixed size. The size of the header depends on the length of the trace ID, sampling decision, and parent ID, which can vary based on the configuration of the application.\nB. Trace Header will be included as a message attribute in an AWS SQS message.\nThis option is correct. When X-Ray injects the trace header into an SQS message, it does so by adding the header as a message attribute. This attribute can then be used by the receiving service to continue the trace.\nC. Trace Header will be included as message content in an AWS SQS message.\nThis option is incorrect because the trace header is not added as message content. Message content refers to the actual data being transmitted, whereas the trace header is metadata about the request.\nD. Trace header is excluded from AWS SQS message size.\nThis option is also incorrect because the trace header is included in the message size. The size of the message will increase based on the size of the trace header that is added.\nIn conclusion, option B is the correct answer. When AWS X-Ray is enabled for an application that uses Amazon SQS, the trace header is added as a message attribute in each SQS message.\n\n"
}, {
  "id" : 416,
  "question" : "A start-up organization is using FIFO AWS SQS queues for their distributed application.\nThe developer team is observing messages delivered out of order.\nWhich of the following can ensure that the orderly delivery of messages?\n",
  "answers" : [ {
    "id" : "61c37f9917c54403b137991bd65e8a80",
    "option" : "Associate a same Batch ID with all messages.",
    "isCorrect" : "false"
  }, {
    "id" : "70facc1ca4dc4c98a176481a1d7e1c64",
    "option" : "Associate the same message group ID with all messages.",
    "isCorrect" : "true"
  }, {
    "id" : "af410ca64fde4fff856e1c79d57ccf35",
    "option" : "Associate a same Message deduplication ID with all messages.",
    "isCorrect" : "false"
  }, {
    "id" : "80032f1f1476430ebae4f8665faeaf9c",
    "option" : "Associate a Sequence number with all messages.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon SQS FIFO queues use message group ID for orderly delivery of messages.\nOption A is incorrect as Batch ID is an invalid option.\nOption C is incorrect as this is used for the deduplication of messages &amp; not for the orderly processing of messages.\nOption D is incorrect as the Sequence number is a unique number assigned by AWS SQS &amp; this will not impact the order of messages.\nFor more information on Amazon SQS, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues.html\n\nThe correct answer is B. Associate the same message group ID with all messages.\nExplanation:\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. There are two types of SQS queues - standard and FIFO. FIFO (first-in, first-out) queues provide exactly-once processing, and messages are processed in the order that they are sent.\nIn a FIFO queue, messages are grouped together based on a message group ID. Each message group contains one or more messages, and Amazon SQS ensures that messages within the same message group are delivered to consumers in the order in which they were sent. However, messages in different message groups might be delivered out of order.\nTo ensure the orderly delivery of messages, the developer team can associate the same message group ID with all messages. This will ensure that all messages are part of the same message group, and Amazon SQS will deliver them in the order in which they were sent.\nThe other options are incorrect:\nA. Associate the same batch ID with all messages: This is not a valid solution as Amazon SQS doesn't provide any such feature to group messages using batch ID.\nC. Associate the same message deduplication ID with all messages: This is not a valid solution as message deduplication ID is used to prevent sending duplicate messages within a certain time period.\nD. Associate a sequence number with all messages: This is not a valid solution as sequence numbers are assigned by Amazon SQS and are used for tracking purposes only.\n\n"
}, {
  "id" : 417,
  "question" : "The developer team has created a distributed application using Amazon SQS which buffers messages.\nCurrently, the team faces issues when the downstream component cannot process the SQS messages in time.\nTo resolve this issue, the Developer Team wants messages in the Amazon SQS queue to wait a period of time before being processed by the downstream.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "04c174142aa74d34a9129f2808f386a9",
    "option" : "Decrease time period using visibility timeouts.",
    "isCorrect" : "false"
  }, {
    "id" : "575be0d163cc4f6ca5581eb2f56a4d98",
    "option" : "Decrease time period using delay queue.",
    "isCorrect" : "false"
  }, {
    "id" : "88254f74de27428b93fb6cbbbc1641e5",
    "option" : "Increase time period using delay queue.",
    "isCorrect" : "true"
  }, {
    "id" : "97be005555e24460879fc6cf65254133",
    "option" : "Increase time period using visibility timeouts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nDelay Queue will add delay to the message when it's added to the queue.\nIn the above case, the Application needs additional time to process backend activities before processing new messages.\nThis can be done by increasing the time period of the delay queue.\nDefault Delay is 0 sec which is the minimum value, while the maximum delay can be set is 900 sec.\nOptions A &amp; D are incorrect as visibility timeouts will be required to be set to hide the message once the subscriber consumes it.\nIn the above case, there should be a delay initially when a message is added to the queue.\nOption B is incorrect as decreasing delay timers will make messages available to subscribers quickly.\nFor more information on delay queues with Amazon SQS, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html\n\nTo resolve the issue of downstream components being unable to process messages in time, the developer team can introduce a delay before the messages are processed. This can be achieved by using a delay queue in Amazon SQS.\nA delay queue is a type of queue that holds messages for a specified amount of time before making them available for processing. Messages are not visible to consumers until the delay period has expired. When the delay period expires, the messages become visible to consumers and can be processed.\nTherefore, the correct answer is C: Increase time period using delay queue.\nOption A: Decreasing the time period using visibility timeouts is not an appropriate solution as it would decrease the time a message is hidden from other consumers once it has been picked up by a consumer, which is not relevant to the problem.\nOption B: Decreasing the time period using a delay queue would make the messages available for processing sooner, which is opposite to what the developer team wants.\nOption D: Increasing the time period using visibility timeouts is not an appropriate solution as it would increase the time a message is hidden from other consumers once it has been picked up by a consumer, which is not relevant to the problem.\nTherefore, the correct answer is C: Increase time period using delay queue.\n\n"
}, {
  "id" : 418,
  "question" : "You plan to develop a new application using AWS Lambda Function to process messages in Amazon SQS standard &amp; FIFO queue.\nYour Team lead is concerned about scaling of AWS Lambda function triggered by Amazon SQS message as event source.\nWhich of the following is used by AWS Lambda for scaling?\n",
  "answers" : [ {
    "id" : "f1daccbfc2ac4fdab5ba7153e8406a71",
    "option" : "AWS Lambda uses long-polling in case of standard queue &amp; short polling in case of FIFO queue.",
    "isCorrect" : "false"
  }, {
    "id" : "4af407b1f2af4eed9beaf10109ec26f5",
    "option" : "AWS Lambda uses short polling in case of standard queue &amp; long polling in case of FIFO queue.",
    "isCorrect" : "false"
  }, {
    "id" : "3c109f58ff7e4890a654cf0e16776cd1",
    "option" : "AWS Lambda uses short polling in case of standard queue &amp; message group ID in case of FIFO queue.",
    "isCorrect" : "false"
  }, {
    "id" : "7012872a38d34e44aba98fc284d72438",
    "option" : "AWS Lambda uses long polling in case of standard queue &amp; message group ID in case of FIFO queue.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nIn the case of Standard queue, AWS Lambda uses long polling to poll queues, &amp; once messages are available it sends them to Lambda function.\nIn the case of FIFO queues, it has a message group ID that ensures messages in the same group are delivered to AWS Lambda.\nOption A is incorrect.\nFor FIFO queues AWS Lambda uses message group ID for scaling &amp; not short polling.\nOption B is incorrect.\nFor FIFO queues AWS Lambda uses message group ID for scaling &amp; not long polling.\nAlso, for standard queues, long polling is used for scalability.\nOption C is incorrect.\nFor standard queue, AWS Lambda uses long polling for scaling.\nFor more information on AWS Lambda with Amazon SQS, refer to the following URLs-\nhttps://aws.amazon.com/blogs/compute/new-for-aws-lambda-sqs-fifo-as-an-event-source/\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\n\nWhen AWS Lambda is triggered by an event source such as Amazon SQS, it automatically scales to process the incoming events. AWS Lambda uses a combination of long polling and short polling to scale the function for both standard and FIFO queues.\nLong polling is used for Amazon SQS standard queues, which helps reduce the number of empty responses by allowing AWS Lambda to wait for a specified period of time for messages to arrive. With long polling, AWS Lambda can retrieve messages from the SQS queue as soon as they become available, which helps to reduce the number of requests to Amazon SQS.\nOn the other hand, short polling is used for Amazon SQS FIFO queues, which is a simple and efficient way to retrieve messages from the queue. AWS Lambda checks the SQS queue for messages at a regular interval and retrieves the messages that are available at that time.\nIn addition to short polling, AWS Lambda also uses message group ID in case of Amazon SQS FIFO queues. When a message is sent to an Amazon SQS FIFO queue, it is assigned a message group ID that is used to ensure that messages are processed in the order they are received within each message group. This helps ensure that the order of processing is maintained even when multiple messages are being processed in parallel.\nTherefore, the correct answer is option D. AWS Lambda uses long polling in case of standard queue and message group ID in case of FIFO queue to scale the function triggered by Amazon SQS message as an event source.\n\n"
}, {
  "id" : 419,
  "question" : "The developer team is planning to create a new distributed application.\nMost of the messages between the application components have a size larger than 256 KB.\nThe messages need to be polled periodically &amp; buffered so that other components can retrieve and process.\nWhich of the following services would you use to design the messaging system?\n",
  "answers" : [ {
    "id" : "c09d543a307f4482abe9812f6347a988",
    "option" : "Use Amazon Kinesis Streams.",
    "isCorrect" : "false"
  }, {
    "id" : "b120e4f718c041ad83d03a25d7012deb",
    "option" : "Use Amazon SNS Extended Client Library.",
    "isCorrect" : "false"
  }, {
    "id" : "4589d3b6caf944a99e39cfd2cba7c153",
    "option" : "Use Amazon SQS Extended Client Library.",
    "isCorrect" : "true"
  }, {
    "id" : "528306c6710c464a8c4dd02e238a50d5",
    "option" : "Use Amazon MQ FIFO.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS SQS can be used for distributed applications for queuing messages between applications which will decouple components.\nAWS SQS performs periodic polling of messages between components.\nFor message size larger than 256 KB, Amazon SQS Extended Client Library for Java can be used to reference message payload stored in Amazon S3.\nOption A is incorrect as Amazon Kinesis Streams will be more effective in the real-time streaming of data.\nOption B is incorrect as Amazon SNS is a push notification messaging service.\nIn this case client is looking for a polling message AWS SQS is a better option.\nOption D is incorrect as AWS MQ is too heavy for this scenario.\nSQS can meet the requirements and be more cost-efficient.\nFor more information on the use case for Amazon SQS, refer to the following URL-\nhttps://aws.amazon.com/sqs/faqs/\n\nBased on the requirement, the most appropriate service to use for designing the messaging system is Amazon Kinesis Streams. Here's why:\nA. Amazon Kinesis Streams Amazon Kinesis Streams is a real-time data streaming service that allows you to collect and process large streams of data records in real-time. It can handle high volumes of data, and messages up to 1 MB in size. Kinesis streams can be used to poll the messages periodically and buffer them until they can be retrieved and processed by other components.\nB. Amazon SNS Extended Client Library Amazon SNS (Simple Notification Service) is a fully managed messaging service that enables you to decouple and scale microservices, distributed systems, and serverless applications. While SNS supports messages up to 256 KB, the service is designed for pub/sub messaging where a message is sent to multiple subscribers. Amazon SNS Extended Client Library can be used to store messages larger than 256 KB, but it's not an ideal solution for this use case as it doesn't buffer messages for processing by other components.\nC. Amazon SQS Extended Client Library Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. While SQS supports messages up to 256 KB, the Amazon SQS Extended Client Library can be used to store messages larger than 256 KB. However, like SNS, SQS is not an ideal solution for this use case as it doesn't buffer messages for processing by other components.\nD. Amazon MQ FIFO Amazon MQ is a managed message broker service for Apache ActiveMQ that makes it easy to set up and operate message brokers in the cloud. Amazon MQ FIFO is a variation of Amazon MQ that provides message ordering and deduplication for FIFO (First-In-First-Out) queues. However, Amazon MQ FIFO is not designed to handle large messages, and the service has a limit of 256 KB message size.\nTherefore, based on the given requirements, the most suitable option is Amazon Kinesis Streams as it can handle messages larger than 256 KB, can buffer messages for processing by other components, and can scale to handle high volumes of data.\n\n"
}, {
  "id" : 420,
  "question" : "A web application uses a single thread to poll multiple queues of Amazon SQS using long-polling having a wait time set as 15 seconds.\nThis is causing a delay in the processing of messages in a queue.\nWhich of the following best practices can be used to enhance message processing performance without any cost impact?\n",
  "answers" : [ {
    "id" : "de2499a66534495ab40418b862c8c707",
    "option" : "Shorten long-poll wait time.",
    "isCorrect" : "false"
  }, {
    "id" : "4126885e92fc4bc2b7b7680f340736b8",
    "option" : "Use a single thread to process a single queue using long polling.",
    "isCorrect" : "true"
  }, {
    "id" : "69d740f2c2d349d1ba16a16004baefae",
    "option" : "Use short polling instead of long polling.",
    "isCorrect" : "false"
  }, {
    "id" : "7b8091f405c14865b315f6f64017269d",
    "option" : "Use a single thread to process a single queue using short polling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nWhen an application uses a single thread to query multiple queues, to increase message performance best practice is to have a single thread query single queue &amp; use long polling.\nOption A is incorrect as this will not increase performance as a single thread has to wait till long poll wait time on multiple queues which it is using.\nOption C is incorrect as short polling can be used in this case, but it will have an additional cost impact.\nOption D is incorrect as short polling can be used in this case, but it will have an additional cost impact.\nFor more information on polling options with Amazon SQS, refer to the following URL-\nhttps://aws.amazon.com/sqs/faqs/\n\nLong polling is a technique used in Amazon Simple Queue Service (SQS) to reduce the number of empty responses returned by a query when there are no messages available to consume from a queue. In long polling, the consumer thread requests messages from the queue and waits for a specified amount of time until a message becomes available or the timeout is reached.\nIn the given scenario, a single thread is being used to poll multiple queues of Amazon SQS using long-polling with a wait time of 15 seconds. However, this is causing a delay in message processing in a queue.\nTo enhance message processing performance without any cost impact, the best practice would be to use a single thread to process a single queue using short polling. Short polling is a technique in which the consumer thread requests messages from the queue, and if no messages are available, the request returns an empty response immediately.\nThis approach ensures that the consumer thread doesn't waste time waiting for messages when none are available, and it can move on to process messages from other queues. Additionally, processing a single queue with a single thread helps to avoid contention between threads and ensures that messages are processed in the order they are received, which is important for applications that require message ordering.\nTherefore, option D - \"Use a single thread to process a single queue using short polling\" is the correct answer in this scenario. It is worth noting that short polling is less efficient than long polling in terms of API calls, but it is suitable for situations where processing messages quickly is more important than reducing API calls.\n\n"
}, {
  "id" : 421,
  "question" : "A Web-application uses Amazon SQS to buffer messages processed by another application deployed in a fleet of Amazon EC2 instances.\nApplication deployed on Amazon EC2 instances sends message confirmation to end-users.\nDue to the heavy load on Amazon EC2 instances, it takes a long time to process messages &amp; users are receiving multiple messages.\nWhich of the following changes can be done on the Amazon SQS queue to ensure users receive only a single message?\n",
  "answers" : [ {
    "id" : "b2879bb2264f4da080687f7da763d33b",
    "option" : "Increase visibility timeout to 5 mins using ChangeMessageVisibility.",
    "isCorrect" : "true"
  }, {
    "id" : "297b5f29a7ef4626bf01b4b771e4c891",
    "option" : "Increase visibility timeout to 5 mins using UpdateMessageVisibility.",
    "isCorrect" : "false"
  }, {
    "id" : "9301f36bdc6e45ab8e665cb65fa0b184",
    "option" : "Decrease visibility timeout to 2 mins using UpdateMessageVisibility.",
    "isCorrect" : "false"
  }, {
    "id" : "ba436656dcae4c86a79d19c0391b05dd",
    "option" : "Decrease visibility timeout to 2 mins using ChangeMessageVisibility.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nVisibility Timeout is used to hide messages from other consumers when one of the consumers is processing the message.\nIn the above case, when one of the Amazon EC2 instances is processing messages from the Amazon SQS queue, it takes time to process messages due to load on the instance.\nWithin that time period, visibility timeout is expired &amp; another EC2 instance is processing the same message &amp; sending updates to users.\nTo avoid multiple processing of the same message, the Visibility timeout can be increased so that Amazon EC2 processes the message before the visibility timeout expires.\nOption B &amp; D are incorrect as UpdateMessageVisibility is an invalid option to change the visibility timeout.\nOption C is incorrect as decreasing visibility timeout will not resolve the issue.\nFor more information on visibility timeout with Amazon SQS, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\n\nThe scenario described involves a web application that uses Amazon SQS to buffer messages that are processed by an application deployed on a fleet of EC2 instances. The application deployed on EC2 instances sends message confirmation to end-users. However, due to the heavy load on the EC2 instances, it takes a long time to process messages, and end-users are receiving multiple messages.\nTo ensure that end-users receive only a single message, we need to modify the behavior of the Amazon SQS queue. Specifically, we need to adjust the visibility timeout of the queue. The visibility timeout is the amount of time that a message remains hidden from other consumers after it has been retrieved by a consumer. During this time, the message is processed by the consumer, and if the processing is successful, the message is deleted from the queue.\nIf multiple consumers retrieve the same message from the queue simultaneously, they will attempt to process it independently, resulting in multiple messages being sent to the end-users. By increasing the visibility timeout, we can prevent other consumers from retrieving the same message while it is being processed.\nOption A and B suggest increasing the visibility timeout to 5 minutes, which would allow more time for the messages to be processed before they become visible to other consumers. Both ChangeMessageVisibility and UpdateMessageVisibility APIs can be used to modify the visibility timeout. However, ChangeMessageVisibility is used to change the visibility timeout of a single message, whereas UpdateMessageVisibility is used to change the visibility timeout of multiple messages or all messages in a queue.\nOption C and D suggest decreasing the visibility timeout to 2 minutes. However, this would not necessarily prevent end-users from receiving multiple messages, as it would allow messages to become visible to other consumers sooner, potentially resulting in multiple consumers processing the same message.\nTherefore, the correct answer is either A or B, depending on the specific requirement of the scenario. If the processing time of messages is expected to be longer than 5 minutes, increasing the visibility timeout to 5 minutes using either ChangeMessageVisibility or UpdateMessageVisibility would be an appropriate solution to prevent end-users from receiving multiple messages.\n\n"
}, {
  "id" : 422,
  "question" : "Distributed web application is using multiple Amazon SQS standard &amp; FIFO queues created in various AWS regions.\nThere is an additional requirement to create a Dead-letter queue and these queues to isolate &amp; debug problematic messages.\nWhile creating a dead-letter queue, which of the following is the TRUE statement? (Select Three).\n",
  "answers" : [ {
    "id" : "88767b50139f4e33a3592198762c9077",
    "option" : "For the FIFO queue, FIFO Dead Letter queue should be created.",
    "isCorrect" : "true"
  }, {
    "id" : "2aac6872db4f45a3a6cb6e406bf2614a",
    "option" : "For Standard queue, FIFO or Standard Dead-Letter queue can be created.",
    "isCorrect" : "false"
  }, {
    "id" : "ba34da4046644cb6ac916ab86d070218",
    "option" : "Use the same AWS Account to create Dead-Letter &amp; standard queue.",
    "isCorrect" : "true"
  }, {
    "id" : "2b57bea135704c399dece6c9ee62ef5a",
    "option" : "Dead-Letter queue &amp; Standard queue should reside in the same AWS region.",
    "isCorrect" : "true"
  }, {
    "id" : "7ba5baa619774d1499f5177ad199c0a1",
    "option" : "Dead-Letter queue &amp; FIFO queue can reside in any AWS region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A, C, D.\nWhile creating the Dead-Letter queue, it should be the same as an original queue which will send a message to the Dead-Letter queue that is for standard queue, standard dead-letter queue should be created &amp; for FIFO queue, FIFO dead-letter queue.\nAlso, the same AWS account needs to create both these queues &amp; both these queues should reside in the same AWS region.\nOption B is incorrect as for the standard queue, a dead letter queue should also be a standard queue &amp; not FIFO queue.\nOption E is incorrect as FIFO queue &amp; Dead-letter queue should need to be in the same AWS region.\nFor more information on DLQ with Amazon SQS, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html\n\nWhen creating a Dead-letter queue for an Amazon Simple Queue Service (SQS), the following are the true statements:\nA. For the FIFO queue, FIFO Dead Letter queue should be created: A dead-letter queue for a FIFO queue should be a FIFO queue. This means that messages will be processed in the same order they were added to the dead-letter queue.\nB. For Standard queue, FIFO or Standard Dead-Letter queue can be created: For a standard queue, you can create a dead-letter queue that is either standard or FIFO. This is because standard queues do not enforce message ordering.\nC. Use the same AWS Account to create Dead-Letter & standard queue: When creating a dead-letter queue, it should be created in the same AWS account as the source queue. This is because SQS does not support cross-account dead-letter queues.\nD. Dead-Letter queue & Standard queue should reside in the same AWS region: The dead-letter queue must reside in the same region as the source queue. This means that you cannot create a dead-letter queue in a different region.\nE. Dead-Letter queue & FIFO queue can reside in any AWS region: This statement is false. The dead-letter queue must reside in the same region as the source FIFO queue.\nIn summary, when creating a dead-letter queue for SQS, it is important to consider the type of queue (standard or FIFO), the same AWS account must be used, and the dead-letter queue should be created in the same region as the source queue.\n\n"
}, {
  "id" : 423,
  "question" : "Developer Team is working on an event driven application that needs to process data stored in the Amazon S3 bucket &amp; notify multiple subscribers using Amazon SNS.\nThus, a single topic is created in Amazon SNS &amp; messages are pushed to multiple Amazon SQS queues subscribed to this topic.\nWhich of the following is a correct statement with regards to messages sent to the Amazon SQS queue?\n",
  "answers" : [ {
    "id" : "8566a240730e4d96a9ee75ee8a49eda3",
    "option" : "Each Queue will receive an identical message sent to that topic instantaneously.",
    "isCorrect" : "true"
  }, {
    "id" : "195f7fcba3bf426a8d695a8074969cc5",
    "option" : "Message sent to the topic will be evenly distributed among all the queues which have subscribed to this topic.",
    "isCorrect" : "false"
  }, {
    "id" : "ab7c3753118a4b2182b6a1b2d75bf3fa",
    "option" : "Each Queue will receive a message sent to that topic asynchronously with a time delay.",
    "isCorrect" : "false"
  }, {
    "id" : "385b952120064776881b3a4c23426a96",
    "option" : "Messages sent to the topic will be visible to the queue, once processing of the message is completed by the first queue.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nWhen multiple Amazon SQS queues are subscribed to a single topic within an Amazon SNS, each queue will receive an identical message.\nThis is useful for parallel independent processing of messaging.\nOption B is incorrect as All the queues subscribed to the topic will not get an evenly distributed message.\nBut all queues will have identical messages each time a message is pushed for a topic.\nOption C is incorrect as there would be any time delay for receiving messages.\nOption D is incorrect as All queue receives identical message &amp; can start processing messages parallelly independent of other queues.\nFor more information on Fanout with Amazon SNS, refer to the following URL-\nhttps://aws.amazon.com/getting-started/hands-on/send-fanout-event-notifications/\n\nThe correct answer to this question is (B) Message sent to the topic will be evenly distributed among all the queues which have subscribed to this topic.\nWhen a message is sent to an Amazon SNS topic, it is delivered to all the subscribers of the topic. In this case, the subscribers are Amazon SQS queues that have been subscribed to the topic. Each queue will receive a copy of the message.\nAmazon SNS uses a publish-subscribe model where a publisher sends a message to a topic, and all the subscribers receive a copy of the message. Amazon SQS is a message queue service, where a queue receives messages sent to it.\nWhen an Amazon SQS queue is subscribed to an SNS topic, the messages sent to the topic are evenly distributed among all the queues that have subscribed to the topic. This ensures that the workload is evenly distributed among the subscribers, and no queue is overwhelmed with too many messages.\nThe delivery of messages to Amazon SQS queues is asynchronous, which means that there may be a delay between when a message is sent to the topic and when it is received by the queues. However, this delay is typically minimal, and messages are delivered to the queues within seconds.\nIt's important to note that Amazon SQS queues operate on a pull-based model, which means that the queues themselves retrieve messages from the topic. Therefore, messages will only be visible to the queue after it has retrieved them from the topic.\nIn summary, when a message is sent to an Amazon SNS topic with multiple Amazon SQS queues subscribed to it, the message will be evenly distributed among all the queues. Delivery to the queues is asynchronous, but the delay is typically minimal, and messages are retrieved from the topic by the queues in a pull-based model.\n\n"
}, {
  "id" : 424,
  "question" : "A Start-up firm plans to implement a distributed stock trading application that requires time-critical messages to be sent to premium subscribers.\nThis developer team needs your guidance to integrate Amazon SNS with Amazon SQS.\nWhich of the following is NOT required to be done to enable Amazon SNS topic to send messages to Amazon SQS?\n",
  "answers" : [ {
    "id" : "98b225884d814efea9bf3e05c5404939",
    "option" : "Get ARN of the queue to send a message &amp; topic to be subscribed for this queue.",
    "isCorrect" : "false"
  }, {
    "id" : "70381d271b0942fb9bffa8a15626c046",
    "option" : "Give sns:Publish permission to Amazon SNS topic.",
    "isCorrect" : "true"
  }, {
    "id" : "566fd29ea73f4640a74979cacb85f7c5",
    "option" : "Give sqs:SendMessage permission to Amazon SNS topic.",
    "isCorrect" : "false"
  }, {
    "id" : "f7307dc3f77045ad8b626cd1f74274a0",
    "option" : "Subscribe queue to Amazon SNS topic.",
    "isCorrect" : "false"
  }, {
    "id" : "3018c60441b045a69a49677723b3ddad",
    "option" : "Allow IAM users to publish to SNS topics &amp; read messages from the queue.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nTo enable an Amazon SNS topic to send messages to an Amazon SQS queue, follow these steps.\n· Get the Amazon Resource Name (ARN) of the queue &amp; SNS topic.\n· Give SQS: SendMessage permission to the Amazon SNS topic.\n· Subscribe the queue to the Amazon SNS topic.\n· Give IAM users, or AWS accounts the appropriate permissions.\nOption A, C, D &amp; E are incorrect as these are required steps for sending messages to the Amazon SQS queue from Amazon SNS.\nFor more information on Fanout with Amazon SNS, refer to the following URL-\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-sqs-as-subscriber.html\n\nTo integrate Amazon SNS with Amazon SQS, we need to follow these steps:\nCreate an Amazon SNS Topic: We need to create an Amazon SNS topic to publish messages. This topic will act as a channel to send messages to Amazon SQS. Create an Amazon SQS Queue: We need to create an Amazon SQS queue to receive messages. This queue will act as a receiver of messages sent by Amazon SNS. Subscribe the Amazon SQS Queue to the Amazon SNS Topic: We need to subscribe the Amazon SQS queue to the Amazon SNS topic. This will allow the Amazon SNS topic to send messages to the Amazon SQS queue. Give sns:Publish Permission to Amazon SNS Topic: We need to give sns:Publish permission to the Amazon SNS topic so that it can publish messages to the Amazon SQS queue. Give sqs:SendMessage Permission to Amazon SNS Topic: We need to give sqs:SendMessage permission to the Amazon SNS topic so that it can send messages to the Amazon SQS queue. Allow IAM Users to Publish to SNS Topics and Read Messages from the Queue: We need to allow IAM users to publish messages to the Amazon SNS topic and read messages from the Amazon SQS queue.\nAnswer (A) is required to enable Amazon SNS topic to send messages to Amazon SQS. We need to get the ARN of the queue to send a message and the topic to be subscribed to this queue.\nAnswer (B) is required to enable Amazon SNS topic to send messages to Amazon SQS. We need to give sns:Publish permission to the Amazon SNS topic so that it can publish messages to the Amazon SQS queue.\nAnswer (C) is required to enable Amazon SNS topic to send messages to Amazon SQS. We need to give sqs:SendMessage permission to the Amazon SNS topic so that it can send messages to the Amazon SQS queue.\nAnswer (D) is required to enable Amazon SNS topic to send messages to Amazon SQS. We need to subscribe the Amazon SQS queue to the Amazon SNS topic so that it can receive messages from the Amazon SNS topic.\nAnswer (E) is not required to enable Amazon SNS topic to send messages to Amazon SQS. It is an additional step to allow IAM users to publish messages to the Amazon SNS topic and read messages from the Amazon SQS queue.\nTherefore, the answer is (E) - Allow IAM users to publish to SNS topics and read messages from the queue.\n\n"
}, {
  "id" : 425,
  "question" : "You have been assigned a task to evaluate messaging services AWS SNS &amp; AWS SES which can be effectively used to deploy new applications.\nWhich of the following is a key difference between SES &amp; SNS?\n",
  "answers" : [ {
    "id" : "8d0e87961ac848ecad491f863f4ad8d2",
    "option" : "SES is used to send communications via emails while SNS is messaging applications to push notifications with various transport protocols like HTTP, Amazon SQS &amp; email.",
    "isCorrect" : "true"
  }, {
    "id" : "bcc315401cbe444a85cfe9d56769dc06",
    "option" : "SES is used to send communications via HTTP while SNS is messaging applications to poll notifications with various transport protocols like HTTP, Amazon SQS &amp; email.",
    "isCorrect" : "false"
  }, {
    "id" : "04c18a809b9b48dfbb515532339334bf",
    "option" : "SES is used to send communications via email while SNS is messaging applications to poll notifications with various transport protocols like HTTP, Amazon SQS &amp; email.",
    "isCorrect" : "false"
  }, {
    "id" : "e88243575a8f4a1895addd93c99c8213",
    "option" : "SES is used to send communications via Amazon SQS while SNS is messaging applications to push notifications with various transport protocols like HTTP, Amazon SQS &amp; email.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS SES is a cost-effective solution for sending/receiving emails, while AWS SNS is a cost-effective messaging service to push notifications between decoupled services &amp; applications.\nOption B &amp; C are incorrect as Amazon SNS is a messaging application used to push notifications &amp; not to poll notifications.\nOption D is incorrect as SES sends communications using email &amp; not using SQS.\nFor more information on the difference between SES &amp; SNS, refer to the following URLs-\nhttps://aws.amazon.com/ses/faqs/#11\nhttps://forums.aws.amazon.com/thread.jspa?threadID=59190\n\nAWS SES (Simple Email Service) and AWS SNS (Simple Notification Service) are two popular messaging services offered by Amazon Web Services (AWS) that can be used to deploy new applications. These services differ in terms of their primary use cases and functionality.\nSES is primarily used to send emails and is designed to make it easy for developers to send bulk emails or transactional emails to their customers. It allows you to send emails using the Simple Mail Transfer Protocol (SMTP) or the AWS SDKs for various programming languages. SES can also be used to receive incoming emails and process them using AWS Lambda or Amazon S3.\nSNS, on the other hand, is a messaging service that can be used to send notifications to a large number of subscribers or endpoints. It allows you to publish messages to topics that can be subscribed to by multiple recipients or endpoints. SNS supports a variety of transport protocols such as HTTP, Amazon SQS, email, and mobile push notifications. When a message is published to a topic, it is delivered to all the subscribed endpoints that are interested in receiving that message.\nBased on the above explanation, the correct answer to the question is A. SES is used to send communications via email while SNS is messaging applications to push notifications with various transport protocols like HTTP, Amazon SQS & email. This option highlights the key difference between the two services: SES is primarily designed for sending emails, while SNS is used to send notifications to a variety of endpoints using various transport protocols.\nOption B is incorrect because SES is not used to send communications via HTTP. Option C is incorrect because SNS is not used to poll notifications, it is used to push notifications. Option D is incorrect because SES is not used to send communications via Amazon SQS.\n\n"
}, {
  "id" : 426,
  "question" : "As a part of modernizing CI/CD processes to deploy Docker containers with Amazon ECS as standard deployment, you must integrate AWS CodeBuild as a step into AWS CodePipeline to provide high availability.\nAt the same time, the source code changes for the Docker image.\nWhat are relevant statements true in this case? (Select TWO).\n",
  "answers" : [ {
    "id" : "562dd56a16584581b00960283d1849b7",
    "option" : "Use the Jenkinsfile to describe in AWS CodePipeline the steps to be deployed.",
    "isCorrect" : "false"
  }, {
    "id" : "e0acf61c1a034fe4af668d5e90edb42c",
    "option" : "Use the imagedefinitions.json file to describe in AWS CodePipeline the container application.",
    "isCorrect" : "true"
  }, {
    "id" : "1866e882e063452e863849e28625bc5b",
    "option" : "Use the Dockerfile to describe in AWS CodeBuild the phases of a Docker image build.",
    "isCorrect" : "false"
  }, {
    "id" : "c8e8e32e089b498196c7eaebdf356b9e",
    "option" : "Use buildspec.yml file to describe in AWS CodeBuild the phases of a Docker image build.",
    "isCorrect" : "true"
  }, {
    "id" : "0cc7d347c4f14814907cc3bb1d2825a6",
    "option" : "Use buildspec.yml file to describe in Amazon ECS the phases of a Docker image build.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: B, D.\nAWS CodePipeline job workers for container actions, such as an Amazon ECR source action or Amazon ECS deploy actions, use definitions files to map the image URI and container name to the task definition.\nEach definition file is a JSON-formatted file used by the action provider.\nAmazon ECS standard deployments require an imagedefinitions.json file as an input to the deploy action.\nAlso, a buildspec is a collection of build commands and related settings.\nIf you include a buildspec as part of the source code, by default, the buildspec file must be named buildspec.yml and placed in the root of your source directory.\nIncorrect Answers:\nOption A is incorrect because Jenkinsfile (a text file that contains the definition of a Jenkins Pipeline) is not part of the AWS CodePipeline steps and is also irrelevant for this question.\nOption C is incorrect because you can describe the phases of a Docker image build in AWS CodeBuild by using the buildspec.yml file instead of a Dockerfile (a text document that contains all the commands to assemble a Docker image).\nOption E is incorrect because the buildspec.yml file does not have anything to do with Amazon ECS.\nReferences:\nhttps://amzn.to/2AgWpJC\nhttps://amzn.to/3dcorob\n\nIn this scenario, you are modernizing the CI/CD processes to deploy Docker containers using Amazon ECS as the standard deployment. You also need to integrate AWS CodeBuild into AWS CodePipeline to provide high availability. Additionally, there are source code changes for the Docker image.\nHere are the relevant statements that are true in this case:\nB. Use the imagedefinitions.json file to describe in AWS CodePipeline the container application. The imagedefinitions.json file is used in AWS CodePipeline to define the container application to be deployed to Amazon ECS. This file specifies the container name, tag, and repository URL. It also defines any additional container settings, such as environment variables, memory and CPU limits, and port mappings.\nD. Use the buildspec.yml file to describe in AWS CodeBuild the phases of a Docker image build. The buildspec.yml file is used in AWS CodeBuild to define the phases of a Docker image build. This file includes information about the environment, such as the base image to use, and the steps to build and push the Docker image to the Amazon ECR repository.\nA, C, and E are incorrect options: A. Use the Jenkinsfile to describe in AWS CodePipeline the steps to be deployed. The Jenkinsfile is a configuration file used in Jenkins, not AWS CodePipeline. It is not relevant in this scenario.\nC. Use the Dockerfile to describe in AWS CodeBuild the phases of a Docker image build. The Dockerfile is used to define the image build process, but it is not relevant in this scenario because the question asks about integrating AWS CodeBuild with AWS CodePipeline.\nE. Use buildspec.yml file to describe in Amazon ECS the phases of a Docker image build. The buildspec.yml file is used in AWS CodeBuild, not Amazon ECS. It is used to define the phases of a Docker image build, but it is not relevant in this scenario because the question asks about integrating AWS CodeBuild with AWS CodePipeline.\n\n"
}, {
  "id" : 427,
  "question" : "You are planning to create a build environment using AWS CodeBuild.\nBut you want to use Docker images as your runtime environment.\nWhat statement correctly describes from where Docker images can be gotten in this context?\n",
  "answers" : [ {
    "id" : "cca343ee57e741a583374ce5c687186e",
    "option" : "Use Docker images from public Docker Hub or Amazon ECR only.",
    "isCorrect" : "false"
  }, {
    "id" : "0a4468c4440e4067bbc2cb3e342d6cf5",
    "option" : "Use Docker images from a private registry only.",
    "isCorrect" : "false"
  }, {
    "id" : "b030878549324d40963b5e4865483048",
    "option" : "Use Docker images from public Docker Hub, private registry or Amazon ECR.",
    "isCorrect" : "true"
  }, {
    "id" : "17a40577df644c218169709ad2df2f8b",
    "option" : "Use Docker images from Amazon ECR only.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nYou can use Docker images from public Docker Hub, Amazon ECR and also your own private repository.\nIncorrect Answers:\nOption A is incorrect because you can use your private registry as well.\nOption B is incorrect because you can also use public Docker Hub and Amazon ECR.\nOption D is incorrect because you can also use a public Docker Hub or a private registry.\nReferences:\nhttps://go.aws/2yCSBC4\nhttps://amzn.to/2zk25mi\n\nThe correct answer is C: Use Docker images from public Docker Hub, private registry, or Amazon ECR.\nAWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. When you use CodeBuild to create a build environment, you can specify the runtime environment for your build as a Docker image. This allows you to create a customized environment for your build that includes specific dependencies and tools required by your application.\nIn terms of where to get the Docker images, CodeBuild supports images from various sources, including public Docker Hub, private registries, and Amazon ECR.\nPublic Docker Hub is a popular registry of Docker images that are shared publicly. You can use images from Docker Hub in your build environment, but you need to be cautious about the security and reliability of these images since they are not always verified or maintained by reputable sources.\nPrivate registries are self-hosted Docker registry services that enable organizations to store and manage their own Docker images. You can use images from private registries in your build environment, but you need to configure CodeBuild to access your private registry, which requires authentication and authorization.\nAmazon Elastic Container Registry (ECR) is a fully-managed Docker container registry service that makes it easy to store, manage, and deploy Docker images. You can use images from ECR in your build environment, and because it is an AWS service, it integrates seamlessly with other AWS services, such as CodeBuild.\nIn summary, CodeBuild provides flexibility in terms of where to get your Docker images. You can use public images from Docker Hub, private images from your own registry, or images from Amazon ECR. Depending on your use case and requirements, you need to consider the security, reliability, and accessibility of the images from each source.\n\n"
}, {
  "id" : 428,
  "question" : "As a developer, you have to build Windows containers to use with AWS CodeBuild and integrate it into AWS CodePipeline and push the resulting containers to Amazon ECR.\nYou might consider custom Docker images.\nWhat assertions are true in this scenario? (Select TWO).\n",
  "answers" : [ {
    "id" : "0c3a74d28bb54f9eaf7f15a8bb4ed672",
    "option" : "The default maximum execution time for CodePipeline custom actions is one hour. If your build jobs require more than an hour, you need to request a limit increase for custom actions.",
    "isCorrect" : "true"
  }, {
    "id" : "23cb7c95f46d426a83ea1f4032f51862",
    "option" : "AWS CodeBuild caches Docker images to avoid downloading new copies each build job, which reduces the time for large Docker images.",
    "isCorrect" : "false"
  }, {
    "id" : "653c2b97f71b4037aab97550c449c962",
    "option" : "AWS CodeBuild fully supports Windows builds in all AWS Regions.",
    "isCorrect" : "false"
  }, {
    "id" : "0088db92b2374ce99f1dbcfe55d964b3",
    "option" : "AWS CodeBuild has to download a new copy of the Docker image for each build job, which may take longer time for large Docker images.",
    "isCorrect" : "true"
  }, {
    "id" : "a77e363d5c6d4610919fb86151ca8ac8",
    "option" : "Any limitations with Windows Server containers can be addressed by using Amazon EC2 instances. AWS CodeBuild and AWS CodePipeline support Amazon EC2 instances directly.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: A, D.\nThere are custom action timeouts whose default maximum execution time for AWS CodePipeline is one hour.\nIt can be requested to increase the limit through the AWS Service Quotas console.\nAWS CodeBuild downloads a new copy of the Docker image for each build job.\nIncorrect Answers:\nOption B is incorrect because AWS CodeBuild does not cache Docker images to avoid downloading new copies.\nInstead, a new copy of the Docker image for each build job is downloaded.\nOption C is incorrect because AWS CodeBuild supports Windows builds in some AWS Regions.\nOption E is incorrect because even though you can work around AWS Region limitations with Windows Servers through Amazon EC2 instances, the downside of this approach is additional management burden -neither AWS CodeBuild nor AWS CodePipeline support Amazon EC2 instances directly.\nReferences:\nhttps://go.aws/2M1RcZ1\n\nIn this scenario, as a developer, you are required to build Windows containers using AWS CodeBuild and integrate them into AWS CodePipeline. You also need to push the resulting containers to Amazon ECR. You may consider using custom Docker images for this process.\nThe following assertions are true in this scenario:\nB. AWS CodeBuild caches Docker images to avoid downloading new copies each build job, which reduces the time for large Docker images. This assertion is true. AWS CodeBuild caches Docker images to avoid downloading new copies each build job. This helps in reducing the time taken to build large Docker images.\nC. AWS CodeBuild fully supports Windows builds in all AWS Regions. This assertion is true. AWS CodeBuild supports Windows builds in all AWS Regions. This means that you can use AWS CodeBuild to build Windows containers in any AWS Region.\nThe following assertions are false in this scenario:\nA. The default maximum execution time for CodePipeline custom actions is one hour. If your build jobs require more than an hour, you need to request a limit increase for custom actions. This assertion is false. The default maximum execution time for CodePipeline custom actions is actually 4 hours, not one hour. However, if your build jobs require more than 4 hours, you will need to request a limit increase for custom actions.\nD. AWS CodeBuild has to download a new copy of the Docker image for each build job, which may take longer time for large Docker images. This assertion is false. As mentioned earlier, AWS CodeBuild caches Docker images to avoid downloading new copies each build job. Therefore, it does not have to download a new copy of the Docker image for each build job.\nE. Any limitations with Windows Server containers can be addressed by using Amazon EC2 instances. AWS CodeBuild and AWS CodePipeline support Amazon EC2 instances directly. This assertion is false. While AWS CodeBuild and AWS CodePipeline do support Amazon EC2 instances directly, using Amazon EC2 instances does not address any limitations with Windows Server containers. Instead, using Amazon EC2 instances would be an alternative to using AWS CodeBuild and AWS CodePipeline for building and deploying applications.\n\n"
}, {
  "id" : 429,
  "question" : "You plan to run a Jenkins service requiring Amazon EFS as a shared, persistent storage attached to an Amazon ECS cluster and using AWS Fargate.\nWhat two statements are true in this context? (Select TWO).\n",
  "answers" : [ {
    "id" : "f39bf7a4ae514f52961be05c175c9b66",
    "option" : "You define the host and sourcePath parameters in the task definition.",
    "isCorrect" : "false"
  }, {
    "id" : "060ce4df22c94b439703ee4eb2fb521a",
    "option" : "You mount the Amazon EFS file system before the Docker daemon starts.",
    "isCorrect" : "true"
  }, {
    "id" : "cf0d207af1254b18a32994daa6c25fa0",
    "option" : "Amazon EFS file system support relies on platform version 1.3.0 or later.",
    "isCorrect" : "false"
  }, {
    "id" : "f14846cad1c048eb8b738073631fcaaa",
    "option" : "The supervisor container is responsible for managing the Amazon EFS volume.",
    "isCorrect" : "true"
  }, {
    "id" : "35d2030e092c466087a7c49dde4dc79a",
    "option" : "The supervisor container is visible in CloudWatch Container Insights.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answers: B, D.\nYou must configure your container instance AMI to mount the Amazon EFS file system before the Docker daemon starts.\nWhen specifying Amazon EFS volumes in tasks using the Fargate launch type, Fargate creates a supervisor container responsible for managing the Amazon EFS volume.\nThe supervisor container uses a small amount of the task's memory.\nIncorrect Answers:\nOption A is incorrect because the host and sourcePath parameters are not supported for Fargate tasks.\nOption C is incorrect because for tasks using the Fargate launch type, Amazon EFS file system support was added when using platform version 1.4.0 or later.\nOption E is incorrect because the supervisor container is visible when querying the task metadata version 4 endpoint.\nBut it is not visible in CloudWatch Container Insights.\nReferences:\nhttps://amzn.to/2WYRtlF\nhttps://amzn.to/3c8cezI\n\nSure, I'd be happy to explain the two correct statements in this scenario!\nFirst, let's review some basic concepts related to the components mentioned in the question:\nJenkins: an open-source automation server used to build, test, and deploy software projects. Amazon EFS (Elastic File System): a scalable, fully managed file storage service that can be accessed from multiple Amazon EC2 instances and other AWS services. Amazon ECS (Elastic Container Service): a fully managed container orchestration service that allows you to run Docker containers on a scalable cluster of Amazon EC2 instances or AWS Fargate. AWS Fargate: a serverless compute engine for containers that allows you to run Docker containers without having to manage the underlying EC2 instances.\nNow, let's examine each answer option and determine which ones are true:\nA. You define the host and sourcePath parameters in the task definition. This statement is true. In order to mount an Amazon EFS file system as a shared, persistent storage volume in an ECS task running on Fargate, you need to include the appropriate volume configuration in the task definition. Specifically, you need to specify the host and sourcePath parameters for the volume, as shown in the following example:\njson Copy code {   \"name\": \"efs-volume\",   \"host\": {     \"sourcePath\": \"/mnt/efs\"   } }\nB. You mount the Amazon EFS file system before the Docker daemon starts. This statement is false. When using Fargate, you do not have direct access to the underlying EC2 instances and cannot mount the EFS file system before the Docker daemon starts. Instead, you must include the appropriate mount commands in the container definition of your ECS task, as shown in the following example:\njson Copy code {   \"name\": \"my-container\",   \"image\": \"my-image\",   \"mountPoints\": [     {       \"sourceVolume\": \"efs-volume\",       \"containerPath\": \"/var/www/html\"     }   ] }\nC. Amazon EFS file system support relies on platform version 1.3.0 or later. This statement is true. In order to use EFS file systems with ECS on Fargate, you must use platform version 1.3.0 or later. This platform version includes the necessary support for mounting EFS file systems in Fargate tasks.\nD. The supervisor container is responsible for managing the Amazon EFS volume. This statement is false. There is no supervisor container in Fargate tasks, as Fargate is a serverless compute engine that does not require you to manage the underlying infrastructure. Instead, you can use the appropriate task definition and container definition settings to specify how to mount and manage the EFS file system.\nE. The supervisor container is visible in CloudWatch Container Insights. This statement is false. There is no supervisor container in Fargate tasks, so there is no supervisor container to be visible in CloudWatch Container Insights. However, you can use Container Insights to monitor the performance and resource utilization of your Fargate tasks and containers, including those that use EFS file systems as shared storage.\nTherefore, the two statements that are true in this context are:\nA. You define the host and sourcePath parameters in the task definition. C. Amazon EFS file system support relies on platform version 1.3.0 or later.\n\n"
}, {
  "id" : 430,
  "question" : "As a cloud engineer, you have been granted access to an Amazon ECR image repository.\nYou have to pull images from the repository as a part of a container definition when creating an Amazon ECS task.\nWhat statement correctly describes this scenario?\n",
  "answers" : [ {
    "id" : "10887a9f957a4d9b97ebb64e31374197",
    "option" : "You can access the Amazon ECR image repository only with the Amazon EC2 launch type.",
    "isCorrect" : "false"
  }, {
    "id" : "0ba38cfe9b634f5f83bfd6761691456b",
    "option" : "You can access the Amazon ECR image repository only with the AWS Fargate launch type.",
    "isCorrect" : "false"
  }, {
    "id" : "5a37a51b22c74bafb3bdbc2253f21f55",
    "option" : "You can access the Amazon ECR image repository with Amazon EC2 or AWS Fargate launch types.",
    "isCorrect" : "true"
  }, {
    "id" : "dd7c5fde15934344b30f85e2ad448e87",
    "option" : "You must grant your Amazon ECS task execution role permission to access Amazon ECS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nYou can access the Amazon ECR image repository with Amazon EC2 or AWS Fargate launch types.\nIncorrect Answers:\nOptions A, B are incorrect because you can access the Amazon ECR image repository with Amazon EC2 or AWS Fargate launch types.\nOption D is incorrect because it does not make sense as a recursive statement.\nThe right statement is that for AWS Fargate launch types, you must grant your Amazon ECS task execution role permission to access the Amazon ECR image repository.\nReferences:\nhttps://go.aws/2WZNtBj\n\nThe correct answer is C. You can access the Amazon ECR image repository with Amazon EC2 or AWS Fargate launch types.\nExplanation: Amazon Elastic Container Registry (Amazon ECR) is a fully-managed container registry that makes it easy for developers to store, manage, and deploy Docker container images. Amazon ECR can be used to host and manage Docker images that are used in Amazon Elastic Container Service (Amazon ECS) tasks.\nAmazon ECS is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a cluster. Amazon ECS can be launched with two different launch types - Amazon EC2 and AWS Fargate. With Amazon EC2 launch type, you have to provide and manage the underlying infrastructure that runs your containerized applications. With AWS Fargate launch type, you do not have to manage the underlying infrastructure, and you can run your containers on serverless compute.\nWhen creating an Amazon ECS task definition, you specify the container image to use and the location of the image. If the image is stored in an Amazon ECR repository, you can pull the image as part of the container definition. This can be done with both the Amazon EC2 and AWS Fargate launch types.\nTherefore, the correct answer is C. You can access the Amazon ECR image repository with Amazon EC2 or AWS Fargate launch types. However, you must ensure that your task execution role has permission to access the Amazon ECR repository. This can be done by granting the necessary permissions to the task execution role, as mentioned in option D.\n\n"
}, {
  "id" : 431,
  "question" : "You are a developer who is supporting a containerized application.\nYou are told to set up dynamic port mapping for Amazon ECS and load balancing.\nWhat statement is true in this case?\n",
  "answers" : [ {
    "id" : "36e411dc3de7424fa15e5b1768e25716",
    "option" : "Classic Load Balancer allows you to run multiple copies of a task on the same instance.",
    "isCorrect" : "false"
  }, {
    "id" : "1a515f4f665946ab8f1041d04a6cb57b",
    "option" : "Application Load Balancer uses static port mapping on a container instance.",
    "isCorrect" : "false"
  }, {
    "id" : "79f8a2f5c93f460d951bc614a574da34",
    "option" : "After creating an Amazon ECS service, you add the load balancer configuration.",
    "isCorrect" : "false"
  }, {
    "id" : "9af33fcc74004c4ba048a86a1665d759",
    "option" : "If dynamic port mapping is set up correctly, then you see the registered targets and the assigned port for the task.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nThis is just the clear result of having set up dynamic port mapping correctly since you can see the registered targets in the target group used to route requests and the assigned port for the task.\nIncorrect Answers:\nOption A is incorrect because Classic Load Balancer actually doesn't allow you to run multiple copies of a task on the same instance.\nOption B is incorrect because Application Load Balancer uses dynamic port mapping.\nSo you can run multiple tasks from a single service on the same container instance.\nOption C is incorrect because you can add a load balancer only during the creation of the service.\nYou can't add, remove, or change the load balancer configuration of an existing service.\nReferences:\nhttps://go.aws/2Ztw5GW\n\nThe correct answer is C. After creating an Amazon ECS service, you add the load balancer configuration.\nExplanation:\nDynamic port mapping is a feature of Amazon ECS that allows the application to use any available port when it starts up. This feature is useful when running multiple copies of the same task on a single container instance, as it ensures that each copy of the task gets a unique port.\nLoad balancing is used to distribute traffic evenly among the tasks that are running in the service. Amazon ECS supports two types of load balancers: Classic Load Balancer and Application Load Balancer.\nOption A, Classic Load Balancer, is incorrect because it does not allow you to run multiple copies of a task on the same instance. Classic Load Balancer distributes traffic to registered instances based on the IP address of the instance, so each task would need to run on a separate instance.\nOption B, Application Load Balancer, is incorrect because it uses dynamic port mapping, not static port mapping. When you create a task definition for a service that uses an Application Load Balancer, you specify the container port and the protocol (HTTP or HTTPS) to use for the container. The Application Load Balancer then dynamically maps the container port to a port on the container instance.\nOption D is incorrect because it is not a statement about load balancing. However, if dynamic port mapping is set up correctly, then you should see the registered targets and the assigned port for the task in the Amazon ECS console.\nTherefore, the correct answer is C. After creating an Amazon ECS service, you add the load balancer configuration. After you create a service in Amazon ECS, you can add a load balancer to distribute traffic to the tasks in the service. To do this, you specify the load balancer type (Classic Load Balancer or Application Load Balancer), the target group, and the listener port.\n\n"
}, {
  "id" : 432,
  "question" : "You are working with an architecture team and several cloud engineers.\nThe project requires containerization, and you are responsible for maintaining the Amazon ECS tasks.\nWhat concepts are correct when it comes to task definitions?\n",
  "answers" : [ {
    "id" : "421a79573542427ab49ed47cf8204429",
    "option" : "Name, image, memory, and port mapping are task definition parameters required and used in most container definitions.",
    "isCorrect" : "false"
  }, {
    "id" : "edb3291c915b457e99a057389d99f98c",
    "option" : "If the network mode is a bridge, the task utilizes Docker`s built-in virtual network which runs inside each container instance.",
    "isCorrect" : "true"
  }, {
    "id" : "6d1861b0dff142579a6a26de01942aab",
    "option" : "If using the Fargate launch type, the awsvpc network mode is no longer required.",
    "isCorrect" : "false"
  }, {
    "id" : "6461ed08a7ea43dca242d716a8d8e802",
    "option" : "Docker for Windows offers host and awsvpc network modes as the highest networking performance so you can take advantage of dynamic host port mappings.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nThe Docker networking mode to use for the containers in the task.\nThe valid values are none, bridge, awsvpc, and host.\nThe default Docker network mode is bridge.\nIf the network mode is a bridge, the task utilizes Docker's built-in virtual network which runs inside each container instance.\nIncorrect Answers:\nOption A is incorrect because only name and image are required as task definition parameters.\nOption C is incorrect because If using the Fargate launch type, the awsvpc network mode is required.\nIf using the EC2 launch type, the allowable network mode depends on the underlying EC2 instance's operating system.\nOption D is incorrect because Docker for Windows uses a different network mode (known as NAT) than Docker for Linux.\nWhen you register a task definition with Windows containers, you must not specify a network mode.\nReferences:\nhttps://amzn.to/2Xv7mzc\nhttps://amzn.to/36De3n3\n\nSure, I can explain each option in detail:\nA. Name, image, memory, and port mapping are task definition parameters required and used in most container definitions.\nThis option is correct. A task definition is a blueprint that describes how a Docker container should launch in Amazon ECS. It includes information like the container image, CPU and memory requirements, network configuration, and other settings. The name, image, memory, and port mapping are required parameters in a task definition that specify the container's image to use, the amount of memory allocated to it, and how it maps to the host ports.\nB. If the network mode is a bridge, the task utilizes Docker's built-in virtual network which runs inside each container instance.\nThis option is also correct. Amazon ECS allows you to choose the network mode for your tasks, which determines how the containers communicate with each other and the host network. If you use the bridge network mode, each container gets its own IP address on a Docker virtual network, which runs inside each container instance. This mode provides basic isolation between containers and is suitable for most use cases.\nC. If using the Fargate launch type, the awsvpc network mode is no longer required.\nThis option is correct. Fargate is a serverless launch type for Amazon ECS that allows you to run containers without managing the underlying EC2 instances. When you use the Fargate launch type, you can use the awsvpc network mode, which provides each task with its own elastic network interface (ENI). This means that each task has a unique IP address and can communicate directly with other tasks on the same ENI. Because Fargate manages the networking for you, you don't need to specify the awsvpc network mode in your task definition.\nD. Docker for Windows offers host and awsvpc network modes as the highest networking performance so you can take advantage of dynamic host port mappings.\nThis option is incorrect. While Docker for Windows does offer host and awsvpc network modes, these modes are not specific to Windows and are available on other platforms as well. The host network mode allows the containers to share the host network stack, which can provide higher networking performance, but it also means that the containers can conflict with other processes running on the host. The awsvpc network mode, as mentioned earlier, is designed for Fargate and provides each task with its own ENI. Neither of these modes are required for dynamic host port mappings, which is a feature that allows you to specify a range of ports for the containers to use, rather than a specific port.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 435,
  "question" : "As a cloud engineer, you have to use containers for packaging source code and dependencies into immutable artifacts to be deployed predictably to any environment.\nWhat options are the most appropriate in this scenario to optimize the application CI/CD pipelines and reduce costs?\n",
  "answers" : [ {
    "id" : "305cab84a5ef456792f07dd9739e70b9",
    "option" : "Run Kubernetes Workloads on Amazon EC2 Spot Instances with Amazon EKS.",
    "isCorrect" : "true"
  }, {
    "id" : "ba9d4e7c0e3d423ab88d384996d80642",
    "option" : "Run Docker Workloads on Amazon EC2 On-Demand Instances with Amazon EKS.",
    "isCorrect" : "false"
  }, {
    "id" : "c0b777e59b9d4513a800342a5490676c",
    "option" : "Run Kubernetes Workloads on Amazon EC2 On-Demand Instances with Amazon ECS.",
    "isCorrect" : "false"
  }, {
    "id" : "25e9b769a9dd430f8bf0d2d70662afb5",
    "option" : "Run Docker Workloads on Amazon EC2 Spot Instances with Amazon EMR and Auto Scaling Groups.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nIt is recommended to run Kubernetes Workloads on Amazon EC2 Spot Instances with Amazon EKS as a cost optimization practice.\nIncorrect Answers:\nOption B is incorrect because EC2 Spot instances are unused.\nEC2 instances are available for less than the On-Demand price.\nOption C is incorrect because you run Kubernetes Workloads with Amazon EKS.\nOption D is incorrect because Amazon EMR is not a service for Kubernetes.\nReferences:\nhttps://go.aws/2Xs3XRW\nhttps://go.aws/2XEb8pY\n\nIn the given scenario, the most appropriate options to optimize the application CI/CD pipelines and reduce costs are A. Run Kubernetes Workloads on Amazon EC2 Spot Instances with Amazon EKS and B. Run Docker Workloads on Amazon EC2 On-Demand Instances with Amazon EKS.\nExplanation:\nContainers have become a popular way of packaging and deploying applications due to their flexibility, portability, and efficiency. They provide a lightweight and isolated environment that encapsulates an application and its dependencies. Containers make it easy to develop, test, and deploy applications across different environments, from development to production.\nTo run containers at scale, you need an orchestration system that can manage the deployment, scaling, and monitoring of containerized applications. Kubernetes is a popular open-source container orchestration system that automates the deployment, scaling, and management of containerized applications. Amazon Elastic Kubernetes Service (EKS) is a managed service that makes it easy to run Kubernetes on AWS.\nOption A: Run Kubernetes Workloads on Amazon EC2 Spot Instances with Amazon EKS\nAmazon EC2 Spot Instances are spare compute capacity in the AWS cloud that can be purchased at a discount of up to 90% compared to On-Demand pricing. Spot Instances can be interrupted with a two-minute notification when the capacity is needed back by AWS. However, if the workload is fault-tolerant and can handle interruptions, using Spot Instances can be a cost-effective option. Kubernetes makes it easy to run workloads on Spot Instances by automatically rescheduling the workload on another node when the node is terminated.\nAmazon EKS supports running Kubernetes workloads on Spot Instances, which can reduce the cost of running containerized applications by up to 90%. Using Spot Instances can help optimize the CI/CD pipelines by reducing the cost of the infrastructure required for building, testing, and deploying the applications. Spot Instances can also be used to run non-production workloads, such as development and testing environments, to further reduce costs.\nOption B: Run Docker Workloads on Amazon EC2 On-Demand Instances with Amazon EKS\nAmazon EC2 On-Demand Instances are virtual servers that are billed by the hour, with no long-term commitments or upfront payments. On-Demand Instances provide a predictable and stable pricing model, with the ability to scale up and down as needed. Amazon EKS supports running Docker workloads on On-Demand Instances, which provides a cost-effective and scalable option for running containerized applications.\nUsing On-Demand Instances can help optimize the CI/CD pipelines by providing a predictable and stable infrastructure for building, testing, and deploying the applications. On-Demand Instances can also be used to run production workloads that require high availability and reliability.\nOption C: Run Kubernetes Workloads on Amazon EC2 On-Demand Instances with Amazon ECS\nAmazon Elastic Container Service (ECS) is a managed container orchestration service that supports running Docker containers on EC2 instances. ECS provides a scalable and secure platform for running containerized applications, with features such as automatic scaling, load balancing, and service discovery.\nHowever, Option C is not the most appropriate option in this scenario, as it suggests running Kubernetes workloads on ECS. While ECS supports running Kubernetes workloads using Kubernetes on ECS (KoE), it is not as mature and feature-rich as Amazon EKS. Moreover, using ECS may require more configuration and management overhead compared to using EKS.\nOption D: Run Docker Workloads on Amazon EC2 Spot Instances with Amazon EMR and Auto Scaling Groups\nAmazon EMR (Elastic MapReduce) is a managed big data platform that makes it easy to process large amounts of data using open-source tools such as Apache Hadoop and Apache Spark. While EMR supports running Docker containers on EC2 instances, it is not the most appropriate option in this scenario, as\n\n"
}, {
  "id" : 436,
  "question" : "During the development, definition and deployment of a backend you are building, you have to decide how to use user and identity pools as a part of a serverless application.\nWhat statements are correct in this scenario?\n",
  "answers" : [ {
    "id" : "9a29c5470f8b4e1abe135fd14e4c07c1",
    "option" : "User pools support temporary, limited-privilege AWS credentials to access other AWS services while Identity pools provide sign-up and sign-in services.",
    "isCorrect" : "false"
  }, {
    "id" : "a6b5752727e14168b613184ea08e02cd",
    "option" : "User pools help you track user device, location, and IP address while Identity pools help you generate temporary AWS credentials for unauthenticated users.",
    "isCorrect" : "true"
  }, {
    "id" : "e5a646c904c14f95a6faecb8d8ba2eff",
    "option" : "User pools give your users access to AWS resources.",
    "isCorrect" : "false"
  }, {
    "id" : "8e20227b7f4441ec83566a0df4ba1cfa",
    "option" : "User pools are for authorization (access control) which is fully managed on your behalf.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nUser pools are for authentication (identify verification)\nIdentity pools are for authorization (access control)\nUser pools help you track user device, location, and IP address, and adapt to sign-in requests of different risk levels.\nIdentity pools help you generate temporary AWS credentials for unauthenticated users.\nIncorrect Answers:\nOptions A, C are incorrect because user pools do not deal with AWS credentials to access other AWS services.\nOption D is incorrect because Identity pools are for authorization (access control).\nReferences:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/cognito-user-pools-identity-pools/\n\nUser and identity pools are two essential components of AWS Cognito. AWS Cognito is a service that provides user sign-up, sign-in, and access control to AWS resources. User pools and identity pools provide different functionalities and are used in different scenarios.\nA user pool is a user directory that manages sign-up, sign-in, and user profiles for your application. User pools allow you to create and manage a set of users, and define authentication flows to control how users sign in to your application. User pools support various authentication flows, such as email and password-based authentication, social identity provider-based authentication, and multi-factor authentication.\nOption D is correct: User pools are used for authorization, also called access control. Authorization is the process of verifying that a user has the necessary permissions to access resources. User pools can be used to control access to AWS resources and other resources in your application. User pools allow you to define groups and roles, and you can assign permissions to groups and roles. User pools also allow you to define custom attribute fields to store additional user information.\nOption A is incorrect: User pools do not support temporary, limited-privilege AWS credentials to access other AWS services. Instead, AWS Identity and Access Management (IAM) provides temporary credentials that can be used by authenticated users to access other AWS services.\nOption B is incorrect: User pools do not track user devices, location, and IP address. Instead, you can use Amazon CloudFront or Amazon API Gateway to track user devices, location, and IP address. Identity pools provide temporary AWS credentials for unauthenticated users. Identity pools are used to grant access to AWS resources for unauthenticated users or users from external identity providers such as Facebook or Google. Identity pools allow you to define authentication providers and map them to IAM roles. When an unauthenticated user logs in, an identity is created for the user, and temporary AWS credentials are generated for the user. The temporary credentials can be used to access AWS resources with the permissions associated with the IAM role.\nOption C is incorrect: User pools do not give your users access to AWS resources directly. Instead, user pools are used to authenticate users and control access to AWS resources through IAM roles.\nIn summary, user pools are used for user authentication and authorization, while identity pools are used to grant temporary AWS credentials to unauthenticated users or users from external identity providers.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 439,
  "question" : "While developing an application, MFA and password recovery were included as additional requirements to increase security by adding a second authentication and recovery mechanisms.\nWhat is considered a recommended practice in this context?\n",
  "answers" : [ {
    "id" : "2ded24ed33034d0bae6da02771759259",
    "option" : "Use TOTP as a second factor and SMS as a password recovery mechanism which is disjoint from an authentication factor.",
    "isCorrect" : "false"
  }, {
    "id" : "7e68ff3bb6b048898c36829ae7546543",
    "option" : "Enable MFA as Required immediately after creating a user pool to add another layer of security.",
    "isCorrect" : "false"
  }, {
    "id" : "1225fdc968b94724bf95fafaf2ce29e9",
    "option" : "Disable adaptive authentication, so you can configure a second factor authentication in response to an increased risk level.",
    "isCorrect" : "false"
  }, {
    "id" : "86f41e2be68f4706aa7b1623a7983e23",
    "option" : "Use SMS as a second factor and TOTP along with a security key as the MFA device for your IAM and root users.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nMFA (Multi-Factor Authentication) and password recovery are important features to ensure the security of an application. MFA provides an additional layer of security by requiring users to provide two or more forms of identification before accessing their account, while password recovery allows users to regain access to their account if they forget their password.\nAmong the options listed, A and D are the only two that provide both MFA and password recovery mechanisms. Let's examine each option in more detail:\nA. Use TOTP as a second factor and SMS as a password recovery mechanism which is disjoint from an authentication factor.\nTOTP (Time-Based One-Time Password) is a common method of providing a second factor authentication. It involves generating a unique, one-time code that the user must enter along with their password to access their account. SMS (Short Message Service) is a common method of delivering a password reset code to a user's phone. This option recommends using TOTP as the second factor and SMS as the password recovery mechanism, with the two mechanisms being disjoint, meaning that they are not related to each other.\nB. Enable MFA as Required immediately after creating a user pool to add another layer of security.\nThis option recommends enabling MFA as soon as a user pool is created, which would require users to provide an additional form of authentication before accessing their account. However, it doesn't provide any guidance on how to implement password recovery.\nC. Disable adaptive authentication, so you can configure a second factor authentication in response to an increased risk level.\nAdaptive authentication is a feature that allows the authentication mechanism to adjust based on the risk level associated with the user's login attempt. This option recommends disabling adaptive authentication to configure a second factor authentication in response to an increased risk level, but it doesn't provide any guidance on how to implement password recovery.\nD. Use SMS as a second factor and TOTP along with a security key as the MFA device for your IAM and root users.\nThis option recommends using SMS as the second factor authentication and TOTP along with a security key for IAM (Identity and Access Management) and root users. This option provides both MFA and password recovery mechanisms, but it is important to note that SMS has some security concerns as it can be intercepted by attackers, making it a less secure option than TOTP.\nIn conclusion, option A is the recommended practice in this context as it provides both MFA and password recovery mechanisms and uses TOTP as the second factor, which is more secure than SMS.\n\n"
}, {
  "id" : 440,
  "question" : "A developer is designing a mobile game application relying on some AWS serverless services.\nTo access these services, requests must be signed with an AWS access key.\nAmong recommended approaches, which one is the most appropriate in this sort of scenario?\n",
  "answers" : [ {
    "id" : "223f4c31ccc6483f86e524a5381326fd",
    "option" : "Embed or distribute long-term AWS credentials that a user downloads to an encrypted store.",
    "isCorrect" : "false"
  }, {
    "id" : "b3c2c4ed8c204ce695928adf11f95f04",
    "option" : "Use Amazon Cognito which acts as an identity broker to implement web identity federation.",
    "isCorrect" : "true"
  }, {
    "id" : "d70daeb0df214c88beabc9426521cdd7",
    "option" : "Write code that interacts with a web identity provider and trades the authentication token for AWS temporary security credentials.",
    "isCorrect" : "false"
  }, {
    "id" : "fc5ddf55681c4360b2ef0fdd312ae6d3",
    "option" : "Use federation and AWS IAM to enable single sign-on (SSO) to your AWS root accounts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nIt is recommended for best results, using Amazon Cognito as your identity broker for almost all web identity federation scenarios.\nIncorrect Answers:\nOption A is incorrect because it is strongly recommended that you do not embed or distribute long-term AWS credentials with apps that users download to a device, even in an encrypted store.\nUsing a web identity provider helps you keep your AWS account secure.\nBecause you don't have to embed and distribute long-term security credentials with your application.\nOption C is incorrect because the best approach here is using Amazon Cognito.\nIf you don't use Amazon Cognito, you must write code that interacts with a web identity provider, such as Facebook, and then calls the AssumeRoleWithWebIdentity API to trade the authentication token you get from those web identity providers for AWS temporary security credentials.\nOption D is incorrect because it does not make sense, and it is referring to AWS root accounts.\nReferences:\nhttps://amzn.to/2TAAGDd\nhttps://amzn.to/3d0zQr7\n\nIn a scenario where a mobile game application relies on AWS serverless services, requests must be signed with an AWS access key. To handle this, the developer must choose a recommended approach that provides security while also allowing users to access the services seamlessly.\nLet's examine the options provided:\nA. Embed or distribute long-term AWS credentials that a user downloads to an encrypted store.\nEmbedding long-term AWS credentials into a mobile application can pose a significant security risk. If the application is compromised, the credentials can be stolen and used to gain access to AWS resources. Distributing long-term credentials to users to download into an encrypted store also poses a risk as it can be challenging to ensure that the store remains encrypted and secure.\nB. Use Amazon Cognito which acts as an identity broker to implement web identity federation.\nAmazon Cognito is a managed service that enables developers to add user sign-up, sign-in, and access control to mobile and web applications. It acts as an identity broker that authenticates users through various identity providers, including social media platforms, and provides temporary AWS credentials to access AWS services securely. This approach allows for seamless integration with mobile applications while providing strong security and user management features.\nC. Write code that interacts with a web identity provider and trades the authentication token for AWS temporary security credentials.\nThis approach involves writing custom code to interact with a web identity provider to authenticate users and trade their authentication tokens for temporary AWS security credentials. While it provides a high level of control over the authentication process, it can be challenging to implement correctly and securely.\nD. Use federation and AWS IAM to enable single sign-on (SSO) to your AWS root accounts.\nThis approach involves using a trusted identity provider, such as Active Directory, to enable single sign-on (SSO) to AWS root accounts. While it provides a high level of security and user management features, it can be complicated to implement and can be challenging to integrate with mobile applications.\nBased on the above analysis, option B is the most appropriate approach in this scenario. Amazon Cognito provides a straightforward way to authenticate users through various identity providers and issue temporary AWS credentials, allowing secure access to AWS serverless services while providing user management features.\n\n"
}, {
  "id" : 441,
  "question" : "You are managing a central AWS account in your company.\nYou need a tool to analyze access control policies in resources such as S3 and determine if resources can be accessed publicly or from other AWS accounts.\nWith this tool, you can protect the resources from unexpected access from outside.\nWhich option should be selected to achieve the requirement?\n",
  "answers" : [ {
    "id" : "766268b1ec924c0bb3c003c5aafdfec2",
    "option" : "Download the Credential Report from IAM and analyze the unexpected access.",
    "isCorrect" : "false"
  }, {
    "id" : "0d603d55be74451ba172163caea0890e",
    "option" : "Download the service access report of the AWS Organization.",
    "isCorrect" : "false"
  }, {
    "id" : "a41b8980e61a4455a6993c60c92e87af",
    "option" : "Check the Access Advisor of IAM roles or S3 bucket policies.",
    "isCorrect" : "false"
  }, {
    "id" : "6d2b56a7861f4e7783c5705cfdb53265",
    "option" : "Create an IAM Access Analyzer and review the findings.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect: Because Credential Report lists the users and their credentials.\nHowever, it cannot provide the access information.\nOption B is incorrect: Because the question does not mention AWS Organization, and the service access report does not tell if resources can be accessed from outside.\nOption C is incorrect: Because Access Advisor provides the last accessed information for the allowed services.\nBut it does not identify the services that may be accessed publicly from the outside.\nOption D is CORRECT: Because IAM Access Analyzer can identify resources that are shared with external entities.\nReference:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html\n\nThe option that should be selected to achieve the requirement is D: Create an IAM Access Analyzer and review the findings.\nIAM Access Analyzer is a tool that can be used to identify any unintended access granted to resources in your AWS account. It works by analyzing resource-based policies to identify any resources that are accessible to external principals, such as other AWS accounts or the public internet. With IAM Access Analyzer, you can easily identify potential security risks in your AWS resources and take appropriate actions to mitigate those risks.\nOption A: Download the Credential Report from IAM and analyze the unexpected access is incorrect because the Credential Report only provides information about IAM users and their access keys. It does not provide information about resource-based policies or unexpected access.\nOption B: Download the service access report of the AWS Organization is incorrect because the service access report only provides information about services that have been used in your AWS account. It does not provide information about resource-based policies or unexpected access.\nOption C: Check the Access Advisor of IAM roles or S3 bucket policies is incorrect because the Access Advisor only provides information about the permissions granted to a specific IAM role or S3 bucket policy. It does not provide information about unexpected access or permissions granted to external principals.\nTherefore, the correct option to achieve the requirement of analyzing access control policies in resources such as S3 and determine if resources can be accessed publicly or from other AWS accounts is to create an IAM Access Analyzer and review the findings.\n\n"
}, {
  "id" : 442,
  "question" : "Your team is building up an application, and an Amazon Elastic File System (EFS) is required to share data across different nodes.\nFor the EFS file system, you need to configure a policy to enforce some default security rules.\nFor example, the root access should be disabled, and connections from EFS clients must use TLS.\nWhich method is the most suitable?\n",
  "answers" : [ {
    "id" : "b88ab5ac051945579cb699f2ad0995cd",
    "option" : "Attach an IAM identity policy to each IAM entity to perform specific actions on the file system.",
    "isCorrect" : "false"
  }, {
    "id" : "4fbc1a6a36ea400285f04cc0bde80f01",
    "option" : "Configure an EFS file system policy to control NFS client access to the EFS resource.",
    "isCorrect" : "true"
  }, {
    "id" : "1d8e5648b7a245758de7bd41374965e1",
    "option" : "Create an EFS service-linked role and attach it to the file system.",
    "isCorrect" : "false"
  }, {
    "id" : "82fddde1346f4bfaa01924e8c619a153",
    "option" : "Add the TLS option when mounting the file system with the EFS mount helper.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect: Because the EFS file system needs to enforce a default IAM policy.\nThe EFS resource policy should be used instead of the IAM identity policy.\nOption B is CORRECT: Because the file system policy is suitable to control permissions for various EFS actions.\nOption C is incorrect: Because the EFS service-linked role is managed by AWS and is used to call other AWS services on your behalf.\nIt cannot be used to enforce default security rules.\nOption D is incorrect: Because the resource policy should be attached in the EFS file system.\nIt is not enough to configure the EFS mount helper.\nReference:\nhttps://docs.aws.amazon.com/efs/latest/ug/iam-access-control-nfs-efs.html\n\nOption B is the most suitable method to configure a policy to enforce security rules on an Amazon Elastic File System (EFS).\nAmazon EFS is a fully managed service that provides shared file storage for use with Amazon EC2 instances. EFS supports the Network File System version 4 (NFSv4) protocol, which allows EFS to be mounted as a file system by EC2 instances. When mounting the EFS file system, you can use the EFS mount helper, which simplifies the process of mounting the file system by handling authentication and authorization on behalf of the client.\nTo configure a policy to enforce security rules on the EFS file system, you can use the EFS file system policy. This policy controls NFS client access to the EFS resource and allows you to specify which clients can access the file system and what actions they can perform. You can use the EFS file system policy to enforce default security rules, such as disabling root access and requiring connections from EFS clients to use TLS.\nOption A is incorrect because attaching an IAM identity policy to each IAM entity is used to grant permissions to access resources, but it does not enforce security rules on the EFS file system.\nOption C is incorrect because creating an EFS service-linked role is used to delegate permissions to AWS services that need to access EFS resources. It is not used to enforce security rules on the EFS file system.\nOption D is incorrect because adding the TLS option when mounting the file system with the EFS mount helper is used to enable encrypted connections between the client and the file system. However, it does not enforce security rules on the EFS file system, such as disabling root access.\n\n"
}, {
  "id" : 443,
  "question" : "Several teams share one AWS account, and there is a large number of IAM roles.\nYou need a method to periodically check each IAM role and identify unused roles based on a time window that you define.\nThen you can determine if some roles are obsolete and can be safely removed.\nWhich option is the most appropriate?\n",
  "answers" : [ {
    "id" : "2761f99d9b11424ab4859c52ea623214",
    "option" : "Periodically trigger a CloudWatch Event rule. Register a Lambda function to evaluate all the IAM roles.",
    "isCorrect" : "false"
  }, {
    "id" : "4adf547e7bab4e25aa014c15ddae2405",
    "option" : "Enable AWS Config and check the IAM usage of each IAM role in AWS Config resources.",
    "isCorrect" : "false"
  }, {
    "id" : "008c69e26bb347b39ea0c073dd40c653",
    "option" : "Periodically trigger an AWS Config rule that invokes a Lambda function. The Lambda function checks the status of each IAM role.",
    "isCorrect" : "true"
  }, {
    "id" : "2aa200fa20a84a948393c3df98c37acf",
    "option" : "Setup Amazon Athena to evaluate the usage of IAM roles from the AWS CloudTrail logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect: This option cannot provide you a view of which IAM roles are compliant or not.\nOption C is better.\nOption B is incorrect: AWS Config resources do not provide the usage status of IAM roles.\nInstead, you should set up an AWS Config rule.\nOption C is CORRECT: Because the Lambda function can check each IAM role, and you can view the compliance status in AWS Config.\nOption D is incorrect: Because this is not a straightforward solution if compared with option.\nC.\nIt also cannot provide you a clear view of the compliance status.\nReference:\nhttps://aws.amazon.com/blogs/security/continuously-monitor-unused-iam-roles-aws-config/\n\nThe question describes a scenario where multiple teams are sharing a single AWS account, and there are a large number of IAM roles in use. The task is to identify any unused IAM roles within a defined time window so that they can be safely removed. The objective is to optimize the IAM roles' usage and to avoid unnecessary costs.\nOption A suggests periodically triggering a CloudWatch Event rule and registering a Lambda function to evaluate all the IAM roles. CloudWatch Events is a service that enables you to respond to state changes in your AWS resources. You can create a rule that matches events and route them to a specific target. In this case, you would create a CloudWatch Event rule that triggers a Lambda function periodically. The Lambda function would then evaluate all IAM roles in the account to identify any unused ones.\nOption B suggests enabling AWS Config and checking the IAM usage of each IAM role in AWS Config resources. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. You can use AWS Config to monitor changes to your resources and to determine compliance against best practices. In this case, you would enable AWS Config and configure it to evaluate IAM roles. AWS Config would then periodically check the IAM usage of each IAM role to identify any unused ones.\nOption C suggests periodically triggering an AWS Config rule that invokes a Lambda function. The Lambda function checks the status of each IAM role. This option is similar to option B but with the addition of a custom AWS Config rule. AWS Config enables you to create custom rules that define the desired configuration of your resources. In this case, you would create a custom AWS Config rule that triggers a Lambda function. The Lambda function would then evaluate the status of each IAM role to identify any unused ones.\nOption D suggests setting up Amazon Athena to evaluate the usage of IAM roles from the AWS CloudTrail logs. Amazon Athena is a service that enables you to query data stored in Amazon S3 using SQL. AWS CloudTrail is a service that enables you to log, continuously monitor, and retain events related to API calls across your AWS account. In this case, you would configure AWS CloudTrail to log events related to IAM role usage. Then you would set up Amazon Athena to query the CloudTrail logs to identify any unused IAM roles.\nOf these four options, option B or C is the most appropriate for this scenario. AWS Config is designed specifically for resource configuration monitoring, and creating a custom rule can be very useful in identifying the usage of IAM roles. However, all the options have their own benefits and drawbacks, and the best option depends on the specific use case and preferences.\n\n"
}, {
  "id" : 444,
  "question" : "You are building a Lambda function that connects to an AWS Aurora cluster and performs SQL queries.\nYou want to use IAM database authentication to connect to the DB cluster so that the Lambda function does not need to use a username and password.\nIAM should manage the authentication process.\nHow would you implement this?\n",
  "answers" : [ {
    "id" : "908395b9eba84a9e9ca3aa6498df1b09",
    "option" : "Enable IAM database authentication in the DB cluster and assign the IAM permission to the Lambda execution role.",
    "isCorrect" : "true"
  }, {
    "id" : "611c75427df94fdd9215f8258408de92",
    "option" : "Generate a credential in Secrets Manager and configure the Lambda function to perform database authentication using it.",
    "isCorrect" : "false"
  }, {
    "id" : "5cb6084566ed4f0daebdd64cbfbadd38",
    "option" : "Create an IAM user for the database and enable the IAM database authentication with the user. Assign the user to the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "7045db4897ea48c6b5eec343931db401",
    "option" : "Enable IAM database authentication in the Aurora cluster. In the Lambda execution role, add the database as the trusted entity.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT: Because with IAM database authentication, if the Lambda function has the IAM permission, it can receive the authentication token from IAM and connect to the DB.Option B is incorrect: Because Secrets Manager is not suitable as the question mentioned that IAM database authentication should be used.\nOption C is incorrect: Because you should attach the IAM policy to the Lambda execution role.\nYou cannot assign an IAM user to the Lambda function.\nOption D is incorrect: Because the description is incorrect.\nAfter IAM database authentication is enabled, you should add the IAM permission to the role, allowing the Lambda to request an authentication token.\nReference:\nhttps://aws.amazon.com/blogs/database/iam-role-based-authentication-to-amazon-aurora-from-serverless-applications/\n\nThe correct answer is A. Enable IAM database authentication in the DB cluster and assign the IAM permission to the Lambda execution role.\nExplanation: AWS Aurora is a relational database engine that is fully managed and highly scalable. It is compatible with MySQL and PostgreSQL and provides several features such as automated backups, read replicas, and automated scaling.\nWhen a Lambda function needs to access an Aurora DB cluster, it can use IAM database authentication instead of providing a username and password. IAM database authentication is a more secure method of authentication that uses AWS Identity and Access Management (IAM) to manage database access.\nTo implement IAM database authentication, you should follow these steps:\nEnable IAM database authentication in the Aurora cluster: You can enable IAM database authentication using the AWS Management Console, AWS CLI, or AWS SDKs. When you enable IAM database authentication, you can also specify the AWS Identity and Access Management (IAM) role that Aurora will assume to authenticate users. Assign IAM permission to the Lambda execution role: You need to create an IAM role for your Lambda function that has permission to access the Aurora cluster. The IAM role should have permission to assume the IAM role that Aurora uses to authenticate users. You can assign the IAM permission to the Lambda execution role using the AWS Management Console, AWS CLI, or AWS SDKs. Modify the Lambda function to use IAM database authentication: In the Lambda function code, you need to specify the connection details for the Aurora cluster, including the cluster endpoint, database name, and IAM authentication option. You can use the AWS SDKs to connect to the Aurora cluster using IAM authentication.\nIn summary, to implement IAM database authentication for a Lambda function that connects to an Aurora cluster, you need to enable IAM database authentication in the Aurora cluster, assign IAM permission to the Lambda execution role, and modify the Lambda function to use IAM database authentication.\n\n"
}, {
  "id" : 445,
  "question" : "The AWS accounts in your company are managed in AWS Organizations and Organizational Units (OUs)\nIn one AWS account, there is an S3 bucket that stores sensitive documents.\nThe S3 bucket should only allow the AWS accounts from one OU of your Organization to get its objects.\nWhich of the following methods is the easiest one to achieve the requirement?\n",
  "answers" : [ {
    "id" : "02ab96a7de9c4510a7d11db20f11aefb",
    "option" : "In the S3 bucket policy, add all the IAM entities from the OU to the Principal field and allow read access.",
    "isCorrect" : "false"
  }, {
    "id" : "7acb346abdf14336b72abe34ae624abf",
    "option" : "Add the condition key “aws:PrincipalOrgPaths” in the S3 bucket policy to allow access based on the principal’s organization path.",
    "isCorrect" : "true"
  }, {
    "id" : "4841c5a321474e1cb2c38d2519922b6a",
    "option" : "Add a Service Control Policy (SCP) to the OU that allows the get object action.",
    "isCorrect" : "false"
  }, {
    "id" : "f03b867d65b549959c1d250167e009ba",
    "option" : "In the S3 Access Control List, allow access for all the AWS accounts in the OU and its child OUs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect: Because you have to fill in all the IAM entities in the OU.\nThis option is not the easiest solution.\nOption B is CORRECT: Because the condition key “aws:PrincipalOrgPaths” can check if the principal comes from the OU.\nOption C is incorrect: Because SCP adds a permission boundary and you still need to use the S3 bucket policy to assign permissions.\nOption D is incorrect: Because you have to set up all the required AWS accounts, and it is hard to maintain when the AWS accounts are changed in the OU.\nOption B is easier than this one.\nReference:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html\nhttps://aws.amazon.com/blogs/security/iam-share-aws-resources-groups-aws-accounts-aws-organizations/\n\nThe easiest method to restrict access to an S3 bucket to only specific AWS accounts within an OU in AWS Organizations is to use an S3 bucket policy with the appropriate IAM principals.\nOption A: In the S3 bucket policy, add all the IAM entities from the OU to the Principal field and allow read access. This option is correct as it is the easiest way to restrict access to an S3 bucket for a specific set of IAM entities, in this case, those in an OU. You can use a bucket policy to specify which AWS accounts or IAM users or roles are allowed to access the objects in the bucket. You can also use a policy to specify what actions are allowed (e.g., GET, PUT, DELETE). In this case, the policy should be set to allow read access for the IAM entities within the OU, and deny access to any other AWS accounts.\nOption B: Add the condition key “aws:PrincipalOrgPaths” in the S3 bucket policy to allow access based on the principal's organization path. This option is incorrect, as it is not the easiest method to restrict access to an S3 bucket to specific AWS accounts within an OU. Using \"aws:PrincipalOrgPaths\" can allow or deny access to the bucket based on the organization path of the IAM principal. However, it requires specifying the organization path of each IAM entity that should have access, which can be cumbersome and time-consuming.\nOption C: Add a Service Control Policy (SCP) to the OU that allows the get object action. This option is incorrect because SCPs are used to set permissions that apply to all AWS accounts within an OU or its child OUs. SCPs are not specific to S3 buckets, and they are not the easiest method to restrict access to a specific S3 bucket.\nOption D: In the S3 Access Control List, allow access for all the AWS accounts in the OU and its child OUs. This option is incorrect because using an ACL is not the easiest method to restrict access to an S3 bucket to specific AWS accounts within an OU. An ACL applies to a specific object within a bucket, whereas a bucket policy applies to all objects within the bucket. Also, using an ACL can be complicated, as you need to specify the AWS account IDs or IAM users or roles that should have access to each object in the bucket.\nTherefore, option A is the easiest and most straightforward method to restrict access to an S3 bucket to specific AWS accounts within an OU in AWS Organizations.\n\n"
}, {
  "id" : 446,
  "question" : "Your company owns an AWS Private Marketplace where third-party software is managed.\nThe AWS Organization includes multiple Organizational Units (OUs)\nYou need to ensure that only an IAM role from the procurement OU can access or configure the Private Marketplace, such as adding or removing products.\nWhich option is the most suitable?\n",
  "answers" : [ {
    "id" : "41b9f67915a14df98e2b268093464a9b",
    "option" : "Add the AWS Private MarketplaceAdminFullAccess permission to the specific IAM role.",
    "isCorrect" : "false"
  }, {
    "id" : "1cbf4b753fe945c4834eef63985bb77c",
    "option" : "Apply an SCP that allows Private Marketplace full access to the procurement OU.",
    "isCorrect" : "false"
  }, {
    "id" : "fbc3870f75094022ac8c48c3a5011d24",
    "option" : "Configure the AWS Private Marketplace to deny incoming requests unless the user is the IAM role.",
    "isCorrect" : "false"
  }, {
    "id" : "94e7c58336c5460483127b9e6957cf12",
    "option" : "Apply an SCP to restrict access to the Private Marketplace for all the organizational units. Ensure that only the IAM role from the Procurement OU has access to the Private Marketplace.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAn SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user.\nAny account has only those permissions permitted by every parent above it.\nIf permission is blocked at any level above the account, either implicitly (by not being included in an\nAllow.\npolicy statement) or explicitly (by being included in a\nDeny.\npolicy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the\nAdministratorAccess.\nIAM policy with */* permissions to the user.\nIf a user or role has an IAM permission policy that grants access to an action that is also allowed by the applicable SCPs, the user or role can perform that action.\nOption A is incorrect: Because other IAM roles can also have this permission.\nThere are no restrictions for other IAM entities.\nOption B is incorrect: Because the question states that only the \"IAM role from the procurement OU can access or configure the Private Marketplace\" and therefore applying the SCP that allows Private Marketplace full access to the Procurement OU is incorrect here.\nOption C is incorrect: Because AWS Private Marketplace does not have this configuration.\nOption D is CORRECT: With the SCP, you can deny the Private Marketplace access unless the AWS principal is the IAM role.\nReference:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_type-auth.html\n\nThe correct answer to this question is D: Apply an SCP to restrict access to the Private Marketplace for all the organizational units. Ensure that only the IAM role from the Procurement OU has access to the Private Marketplace.\nExplanation: AWS Private Marketplace is a custom-branded marketplace that enables AWS customers to easily discover and purchase third-party software from a catalog of products that have been pre-approved by their organization. To restrict access to the AWS Private Marketplace, an AWS administrator needs to apply an SCP (Service Control Policy) that restricts access to the Private Marketplace for all the organizational units. Then, the administrator can ensure that only the IAM role from the Procurement OU has access to the Private Marketplace.\nOption A: Add the AWS Private MarketplaceAdminFullAccess permission to the specific IAM role. This option is not the most suitable because it grants full administrative access to the Private Marketplace for the specified IAM role, which is not required in this case. The question only requires that the IAM role from the Procurement OU has access to the Private Marketplace, not full administrative access.\nOption B: Apply an SCP that allows Private Marketplace full access to the procurement OU. This option is not the most suitable because it allows the Procurement OU to have full access to the Private Marketplace, not just the specified IAM role from the Procurement OU. This could lead to security vulnerabilities if other users within the Procurement OU can also access the Private Marketplace.\nOption C: Configure the AWS Private Marketplace to deny incoming requests unless the user is the IAM role. This option is not the most suitable because it does not provide a way to restrict access to the Private Marketplace for all the organizational units. This would require individual configurations for each IAM user, which could be time-consuming and difficult to manage.\nOption D: Apply an SCP to restrict access to the Private Marketplace for all the organizational units. Ensure that only the IAM role from the Procurement OU has access to the Private Marketplace. This option is the most suitable because it applies an SCP to restrict access to the Private Marketplace for all the organizational units, ensuring that only the specified IAM role from the Procurement OU has access to the Private Marketplace. This is a secure and scalable solution that can be easily managed by the AWS administrator.\n\n"
}, {
  "id" : 447,
  "question" : "Certain users from your team need to assume an IAM role called AppAdminAccess for some configuration tasks.\nWhen they use the role, they know the specific actions that are required.\nYou want to restrict the permissions when the role is assumed so that only specific actions can be performed in the role session.\nHow would you achieve this requirement?\n",
  "answers" : [ {
    "id" : "46e90cb335d84c20add49469609f9f89",
    "option" : "Pass a session policy with AssumeRole API to limit the permissions.",
    "isCorrect" : "true"
  }, {
    "id" : "758e7ee881c94067b93c77157e3269a4",
    "option" : "Attach an IAM policy to the IAM users to limit their permissions.",
    "isCorrect" : "false"
  }, {
    "id" : "8dc7ac8e68114d4686c62ccd149c185d",
    "option" : "Configure a permission boundary to the IAM users to control their maximum permissions.",
    "isCorrect" : "false"
  }, {
    "id" : "a04d5a49800141109b5ff4c1401133dc",
    "option" : "This cannot be done using AssumeRole API. Create a new IAM role with the required permissions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - A.\nOption A is CORRECT: Because the AssumeRole API can add a session policy parameter to limit the total permissions granted by the assumed IAM role.\nOption B is incorrect: Because the IAM policy of the IAM users cannot limit the permissions when an IAM role is assumed.\nOption C is incorrect: Because the permission boundary does not influence the permissions when IAM users assume an IAM role.\nOption D is incorrect: Because there is no need to create a new IAM role since users can pass the session policy in the AssumeRole API.\nReference:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html#policies_session\n\nThe correct answer is A - Pass a session policy with AssumeRole API to limit the permissions.\nExplanation:\nWhen an IAM user or an AWS service assumes an IAM role, the permissions of the role are temporarily granted to that user or service. To restrict the permissions when the role is assumed so that only specific actions can be performed in the role session, we can use a session policy.\nA session policy is an optional IAM policy that can be passed when using the AssumeRole API to limit the permissions that are granted in the role session. This policy is evaluated in addition to the permissions granted by the IAM role itself.\nTo implement this solution, we need to follow these steps:\nCreate an IAM role called AppAdminAccess that grants the permissions required for the configuration tasks. Create a session policy that limits the permissions to only the specific actions required by the users. When the users need to perform the configuration tasks, they can use the AssumeRole API to assume the AppAdminAccess role and pass the session policy.\nHere's an example of how the session policy could look like in JSON format:\njson Copy code {     \"Version\": \"2012-10-17\",     \"Statement\": [         {             \"Effect\": \"Allow\",             \"Action\": [                 \"s3:GetObject\",                 \"s3:PutObject\",                 \"s3:ListBucket\"             ],             \"Resource\": [                 \"arn:aws:s3:::example-bucket/*\",                 \"arn:aws:s3:::example-bucket\"             ]         }     ] }\nThis policy allows the user to perform only the specified actions (GetObject, PutObject, and ListBucket) on the example-bucket S3 bucket.\nOption B (Attach an IAM policy to the IAM users to limit their permissions) and option C (Configure a permission boundary to the IAM users to control their maximum permissions) are not suitable solutions for this scenario because they do not restrict the permissions granted by the AppAdminAccess IAM role itself.\nOption D (Create a new IAM role with the required permissions) is not the best solution because it involves creating a new role every time there is a need to limit the permissions for a specific set of actions. It is better to use the session policy with the AssumeRole API to achieve the same result without creating unnecessary roles.\n\n"
}, {
  "id" : 448,
  "question" : "You write a Lambda function that needs to connect to an RDS MySQL database.\nYou use a CD pipeline to deploy the Lambda function in the same VPC subnet as the RDS database so that they can communicate.\nHowever, you find that the deployed Lambda function is not able to connect to the internet.\nHow would you resolve this problem?\n",
  "answers" : [ {
    "id" : "da5be59061a74b96aaaca64810590ba9",
    "option" : "Modify the pipeline to deploy the Lambda function in the default VPC rather than the same VPC as RDS.",
    "isCorrect" : "false"
  }, {
    "id" : "a0667676ad4b4802ab839ac931b7d17c",
    "option" : "Configure a NAT gateway in the VPC and route the outbound traffic to the NAT gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "93b0b0857fef4b36a894777da1f2dccc",
    "option" : "Deploy the Lambda function outside of the custom VP.",
    "isCorrect" : "false"
  }, {
    "id" : "76c1141244af472296b8f8bb033c09b6",
    "option" : "Open the outgoing port in the Lambda function access control list.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect: Because the Lambda function cannot communicate with the RDS database if it is in a different VPC.Option B is CORRECT: When the Lambda function is in a VPC, the VPC needs to access the internet.\nThe NAT gateway has a public IP address and connects to the internet.\nOption C is incorrect: Because the Lambda function will lose the connection with the RDS database if it is outside of the VPC.Option D is incorrect: Because the Lambda function does not have an access control list.\nIn this scenario, VPC should have internet access.\nReference:\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html\n\nThe issue is that the Lambda function, which needs to connect to an RDS MySQL database, is deployed in the same VPC subnet as the database, but it is unable to connect to the internet. This is because by default, Lambda functions deployed in a VPC do not have internet connectivity unless they are configured to do so explicitly.\nTo resolve this issue, there are a few possible options:\nOption A: Modify the pipeline to deploy the Lambda function in the default VPC rather than the same VPC as RDS.\nThis option would allow the Lambda function to connect to the internet by default, as the default VPC has an internet gateway attached to it. However, it would mean that the Lambda function and the RDS database would no longer be in the same VPC subnet, which could affect their ability to communicate efficiently.\nOption B: Configure a NAT gateway in the VPC and route the outbound traffic to the NAT gateway.\nThis option involves setting up a Network Address Translation (NAT) gateway in the VPC, which allows the Lambda function to access the internet through the NAT gateway. The NAT gateway acts as a proxy between the Lambda function and the internet, allowing the Lambda function to communicate with the RDS database in the same VPC subnet while also providing internet connectivity.\nOption C: Deploy the Lambda function outside of the custom VPC.\nThis option involves deploying the Lambda function outside of the VPC altogether, which would allow it to access the internet by default. However, this would also mean that the Lambda function would not be able to communicate directly with the RDS database in the same VPC subnet.\nOption D: Open the outgoing port in the Lambda function access control list.\nThis option involves modifying the security group associated with the Lambda function to allow outbound traffic on the required port(s) for connecting to the RDS database. However, this option alone may not be sufficient to allow the Lambda function to connect to the internet, as it would still be deployed in a VPC without internet connectivity by default.\nIn conclusion, the most appropriate solution to this problem would be option B: Configure a NAT gateway in the VPC and route the outbound traffic to the NAT gateway. This would allow the Lambda function to connect to the internet through the NAT gateway while still being able to communicate with the RDS database in the same VPC subnet.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 450,
  "question" : "You are a software developer and working on a Node.js web application.\nThe app is ready to be deployed for further testing.\nYou want to deploy it with AWS Elastic Beanstalk, and the application should be put in a custom VPC.\nYou plan to specify the VPC settings in a .ebextensions configuration file.\nWhich description is correct?\n",
  "answers" : [ {
    "id" : "b57be54b43474396ae0f856525cc22b8",
    "option" : "Define the VPC settings in the Resources section of the .ebextensions configuration files.",
    "isCorrect" : "false"
  }, {
    "id" : "ec067200b9314ddd8bb39fcfb9a6e3d6",
    "option" : "Define the VPC configurations in the aws:ec2:vpc namespace such as: - namespace: aws:ec2:vpc option_name: VPCId value: vpc-170647c",
    "isCorrect" : "true"
  }, {
    "id" : "2de0d30defaf4474a0ee8739c9851e38",
    "option" : "Create a saved configuration that contains the VPC settings and apply it using EB CLI.",
    "isCorrect" : "false"
  }, {
    "id" : "3aebb74fba8e427ca0fce9c4ca6cfede",
    "option" : "Configure the VPC in the option_settings namespace such as: - namespace: option_settings option_name: VPCId value: vpc-170647c.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - B.\nOption A is incorrect: Because the VPC settings should be put in the option_settings section instead of the Resources section.\nOption B is CORRECT: Because the aws:ec2:vpc namespace is the correct place to customize the VPC settings such as VPCId and subnets.\nOption C is incorrect: Because the question requires that the configurations should be put in the .ebextensions files.\nOption D is incorrect: Because for VPC settings, the namespace should be aws:ec2:vpc.\nFor details, please refer to the below reference.\nReference:\nhttps://docs.amazonaws.cn/en_us/elasticbeanstalk/latest/dg/vpc-rds.html\n\nSure, I'd be happy to provide a detailed explanation!\nThe correct answer for this question is option B: Define the VPC configurations in the aws:ec2:vpc namespace.\nHere's why:\nWhen you deploy a Node.js web application using AWS Elastic Beanstalk, you can use a .ebextensions configuration file to specify various settings and configurations for your application. One of the settings you can specify is the VPC (Virtual Private Cloud) where your application should run.\nTo specify VPC settings in your .ebextensions configuration file, you need to use the aws:ec2:vpc namespace. This namespace allows you to specify settings related to the VPC, such as the VPC ID, the security group ID, and the subnet ID.\nHere's an example of how to specify the VPC ID in the aws:ec2:vpc namespace:\nyaml Copy code option_settings:   aws:ec2:vpc:     VPCId: vpc-1234567890\nIn this example, vpc-1234567890 is the ID of the VPC where you want to run your application.\nOption A, defining the VPC settings in the Resources section of the .ebextensions configuration file, is incorrect. The Resources section is used to create AWS resources, such as an EC2 instance or an S3 bucket. It is not used to specify VPC settings.\nOption C, creating a saved configuration that contains the VPC settings and applying it using EB CLI, is also incorrect. While you can create a saved configuration with VPC settings, it's not necessary to do so in this case. You can specify the VPC settings directly in the .ebextensions configuration file.\nOption D, configuring the VPC in the option_settings namespace, is also incorrect. While you do need to use the option_settings namespace to specify VPC settings, you need to use the aws:ec2:vpc namespace specifically, not the generic option_settings namespace.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 452,
  "question" : "You are writing a CloudFormation template to configure multiple AWS resources including an application load balancer.\nYou need to launch the load balancer in two availability zones that have more free IP resources.\nHow would you configure the load balancer in the CloudFormation template?\n",
  "answers" : [ {
    "id" : "f38b94e24f5a4364a8ac4dd86b700e21",
    "option" : "Specify two subnets IDs that belong to two availability zones in the template.",
    "isCorrect" : "true"
  }, {
    "id" : "1fe377960c22411089e5a95c83789fe0",
    "option" : "Specify the availability zone names in the template such as ap-southeast-2a and ap-southeast-2b.",
    "isCorrect" : "false"
  }, {
    "id" : "805647c0033b4fb6a5deb13b22b6cbe6",
    "option" : "Use the availability zone IDs in the template such as apse2-az1 and apse2-az2.",
    "isCorrect" : "false"
  }, {
    "id" : "738dcdeeb31f4dfab5e9f11768dc0f77",
    "option" : "No actions required as CloudFormation automatically selects two availability zones that have more resources for the load balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT: Because you need to use Subnets or SubnetMappings for the application load balancer resources in CloudFormation.\nSubnets from at least two availability zones must be specified for application load balancers.\nOption B is incorrect: Availability zone names should not be used in this scenario.\nOption C is incorrect: Because you should use Subnets or SubnetMappings rather than the availability zone IDs.\nOption D is incorrect: Because you must specify either Subnets or SubnetMappings for application load balancer resources.\nReference:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-elasticloadbalancingv2-loadbalancer.html\n\nTo configure an application load balancer to launch in two availability zones with free IP resources using a CloudFormation template, you should:\nA. Specify two subnet IDs that belong to two availability zones in the template.\nWhen creating a CloudFormation template, you can specify two subnets for the application load balancer to be launched in two availability zones that have more free IP resources. A subnet is a subdivision of an IP network and a VPC (Virtual Private Cloud) is divided into several subnets that are deployed across multiple availability zones. By specifying two subnets that belong to different availability zones in the CloudFormation template, you ensure that the application load balancer is launched in two availability zones.\nB. Specify the availability zone names in the template such as ap-southeast-2a and ap-southeast-2b.\nWhile it is possible to specify the availability zone names in the CloudFormation template, this is not recommended as the availability zone names may change over time. Instead, it is recommended to use the availability zone IDs as they are more stable.\nC. Use the availability zone IDs in the template such as apse2-az1 and apse2-az2.\nUsing availability zone IDs in the CloudFormation template is a recommended approach. An availability zone ID is a unique identifier for an availability zone that remains the same over time. This ensures that the application load balancer is always launched in the same availability zones.\nD. No actions required as CloudFormation automatically selects two availability zones that have more resources for the load balancer.\nWhile CloudFormation does select two availability zones when creating resources, it cannot ensure that these availability zones have more free IP resources. Therefore, it is recommended to specify the subnets or availability zone IDs to ensure the application load balancer is launched in the desired availability zones.\n\n"
}, {
  "id" : 453,
  "question" : "You have a launch template where a subnet is specified in its network interface.\nNow you need to use AWS CLI (aws autoscaling create-auto-scaling-group) to create an Auto Scaling group with the launch template.\nHowever, the ASG should be launched in another subnet that is different from the one specified in the launch template.\nHow would you create the ASG with the AWS CLI command?\n",
  "answers" : [ {
    "id" : "f84cc6983bc7462abd5256a33271214a",
    "option" : "Modify the subnet in the launch template and create the ASG using the latest version of the launch template.",
    "isCorrect" : "false"
  }, {
    "id" : "8945060f03d8497b8271f6b3d3ccf540",
    "option" : "Change the subnet in the launch template. In AWS CLI (aws autoscaling create-auto-scaling-group), specify the subnet through the --launch-template option.",
    "isCorrect" : "false"
  }, {
    "id" : "1ce6eb53ce714647b573d1c27cbdeb5e",
    "option" : "Specify the subnet through the --vpc-zone-identifier option in AWS CLI (aws autoscaling create-auto-scaling-group). The subnet in the launch template is ignored.",
    "isCorrect" : "true"
  }, {
    "id" : "58b3575c4103453297d995c3621a762c",
    "option" : "Create another launch template with the new subnet in its network interface. In AWS CLI (aws autoscaling create-auto-scaling-group), specify the same subnet through the --vpc-zone-identifier option.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - C.\nOption A is incorrect: Because the subnet in the launch template is ignored when creating an Auto Scaling group.\nThe subnet needs to be specified in AWS CLI (AWS autoscaling create-auto-scaling-group).\nOption B is incorrect: Because the subnet should be specified in the --vpc-zone-identifier option of AWS CLI.\nOption C is CORRECT: When creating the ASG using AWS CLI command, the subnets should be specified in the --vpc-zone-identifier option.\nOption D is incorrect: Because there is no need to create a new launch template.\nYou just need to specify the subnet in the --vpc-zone-identifier option.\nReference:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/create-launch-template.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-launch-template.html\nhttps://docs.aws.amazon.com/cli/latest/reference/autoscaling/create-auto-scaling-group.html\n\nThe correct answer is B. Change the subnet in the launch template. In AWS CLI (aws autoscaling create-auto-scaling-group), specify the subnet through the --launch-template option.\nExplanation:\nWhen you create an Auto Scaling group using a launch template, the settings in the launch template are used as a blueprint for the instances launched by the Auto Scaling group. The launch template contains the configuration details, such as the AMI, instance type, and network settings, that the Auto Scaling group uses to launch EC2 instances.\nWhen launching an instance using a launch template, you can specify the subnet that the instance should be launched in. If you want to launch an Auto Scaling group in a different subnet than the one specified in the launch template, you can change the subnet in the launch template and create a new version of the launch template. However, this is not the recommended approach, as it changes the launch template, which could affect other Auto Scaling groups that use the same launch template.\nInstead, you should use the --launch-template option in the AWS CLI (aws autoscaling create-auto-scaling-group) command to specify the launch template to use and override the subnet specified in the launch template. You can pass the ID of the launch template and the subnet ID to launch the instances in the desired subnet. For example, the command would look like:\nsql Copy code aws autoscaling create-auto-scaling-group \\     --auto-scaling-group-name MyASG \\     --launch-template LaunchTemplateId=lt-0123456789abcdef,Version=1 \\     --vpc-zone-identifier subnet-0123456789abcdef\nIn this example, the launch template with the ID lt-0123456789abcdef and version 1 is used to launch instances, but the instances are launched in the subnet with the ID subnet-0123456789abcdef. The subnet specified in the launch template is ignored.\nOption C, specifying the subnet through the --vpc-zone-identifier option, is also correct, but it is less flexible than option B, as it does not allow you to specify other launch template settings, such as the instance type or security group, in the CLI command. Option A, modifying the launch template, and option D, creating another launch template, are both incorrect, as they modify the launch template, which could affect other Auto Scaling groups that use the same launch template.\n\n"
}, {
  "id" : 454,
  "question" : "You need to deploy an application in AWS EC2\nA CloudFormation template is required to prepare for the infrastructure including VPC, public subnet, and private subnet.\nThe EC2 instances should be placed into private subnets.\nThe application needs to connect to the internet to download software packages.\nWhich resources should be included in the CloudFormation template? (Select TWO).\n",
  "answers" : [ {
    "id" : "b4a25a37cc6c44e3824c6ccb6303460d",
    "option" : "VPC endpoint",
    "isCorrect" : "false"
  }, {
    "id" : "16526873ae37409b99ba71b8988884c2",
    "option" : "Route table for NAT Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "b9e405b4052a494bb356cb6b98214d05",
    "option" : "DHCP option set",
    "isCorrect" : "false"
  }, {
    "id" : "1da1f14e4c854bac949d1792f66e91dc",
    "option" : "Internet Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "48857050e22e4e4e9601e2d63eea9dd9",
    "option" : "Transit gateway route table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B, D.\nOption A is incorrect: Because the VPC endpoint is not required as EC2 in the private subnet needs to connect with the internet.\nOption B is CORRECT: Because a route table of the private subnet is required that routes the traffic to the NAT Gateway.\nOption C is incorrect: Because the DHCP option set is used to configure DHCP, which is unnecessary in this scenario.\nOption D is CORRECT: Because Internet Gateway is necessary for the public subnet.\nOption E is incorrect: Because a Transit Gateway is a network transit hub that interconnects attachments (VPCs and VPNs), which is not mentioned in the question.\nReference:\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/cloudformation-vpc-template.html\n\nTo deploy an application in AWS EC2, a CloudFormation template is required to prepare the infrastructure, which includes VPC, public subnet, and private subnet. The EC2 instances should be placed in private subnets.\nThe application needs to connect to the internet to download software packages, so two resources should be included in the CloudFormation template:\nNAT Gateway: Since the EC2 instances are in a private subnet, they can't access the internet directly. NAT Gateway acts as a proxy server to allow instances in the private subnet to connect to the internet. A route table for NAT Gateway should also be included, which routes traffic from the private subnet to the NAT Gateway. Internet Gateway: An Internet Gateway is required to allow traffic to and from the internet. It should be attached to the VPC and have a route table that directs traffic to it.\nHence, options B and D are the correct answers: B. Route table for NAT Gateway. D. Internet Gateway.\nOption A - VPC endpoint is used to connect to services such as S3 and DynamoDB without requiring an internet gateway, but it is not required for this scenario.\nOption C - DHCP option set is used to configure DHCP options for instances in the VPC, but it is not required for this scenario.\nOption E - Transit Gateway route table is used to route traffic between VPCs and on-premises networks via a Transit Gateway, but it is not required for this scenario.\n\n"
}, {
  "id" : 455,
  "question" : "An IT firm is storing critical data on multiple provisioned IOPS &amp; throughput optimized EBS volumes based upon various categories of data.\nFor quick restoration of this data in other regions, Operation Team is looking for data backup for all these EBS volumes.\nWhich of the following is the best option in performing backup for multiple EBS volumes?\n",
  "answers" : [ {
    "id" : "7c1a7600c79546eb98ded7d623203bdf",
    "option" : "Create a Multi-volume Snapshot for all EBS Volumes attached to EC2 instance by selecting Resource Type as Instance in Snapshot navigation pane.",
    "isCorrect" : "true"
  }, {
    "id" : "79c43962b77d4747aa42b90b0bfca40a",
    "option" : "Create a Multi-volume Snapshot for all EBS Volumes attached to EC2 instance by selecting Resource Type as Volumes in Snapshot navigation pane.",
    "isCorrect" : "false"
  }, {
    "id" : "1f62b8568156490e817cf5229db80bac",
    "option" : "Create a Snapshot for all EBS Volumes attached to EC2 instance individually by selecting Resource Type as Volumes in Snapshot navigation pane.",
    "isCorrect" : "false"
  }, {
    "id" : "c100c2f4e35c45429c33b91b70f5b08f",
    "option" : "Create a Snapshot for all EBS Volumes attached to EC2 instance individually by selecting Resource Type as Instance in Snapshot navigation pane.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nUsing Multi-Volume Snapshot, a point-in-time snapshot of all EBS volumes connected to an EC2 instance can be performed.\nFor this from the Snapshot navigation pane, resource type should be selected as Instance which will provide the option of performing snapshot for all EBS volumes connected to that EC2 instance.\nOption B is incorrect as Resource Type should be selected as Instance &amp; Not Volumes.\nOptions C &amp; D are incorrect.\nInstead of creating a snapshot for each EBS volume, a Multi-volume Snapshot can create multiple EBS volumes in a single operation.\nFor more information on Amazon EBS Multi-volume Snapshots, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html#ebs-create-snapshot-multi-volume\n\nTo perform backups for multiple EBS volumes, the best option would be to create a Multi-volume Snapshot for all the EBS volumes attached to the EC2 instance.\nOption A and Option D both suggest creating a Snapshot by selecting the Resource Type as Instance. However, this will only create a snapshot of the root EBS volume attached to the instance and not the additional EBS volumes attached to it. Therefore, both options A and D can be eliminated.\nOption C suggests creating a snapshot of each EBS volume individually by selecting the Resource Type as Volumes. While this option will provide backups for each EBS volume, it can be a time-consuming process if there are many EBS volumes to be backed up. Moreover, this option also increases the risk of inconsistencies if the EBS volumes are not synchronized at the time of snapshot creation.\nTherefore, the best option would be Option B, which suggests creating a Multi-volume Snapshot for all EBS Volumes attached to the EC2 instance by selecting the Resource Type as Volumes in the Snapshot navigation pane. This will create a consistent backup of all the EBS volumes attached to the EC2 instance in a single step, making it easier and quicker to restore the data in other regions.\nIt's worth noting that multi-volume snapshots are only available for EBS volumes that are in an attached state and not in a detached state. Also, multi-volume snapshots can be created using the AWS Management Console, AWS CLI, or AWS SDKs.\n\n"
}, {
  "id" : 456,
  "question" : "A start-up firm is using a Dedicated Host for the deployment of Oracle-based applications.\nSome of the instances on this Dedicated Host are using instance stored volume, while some are using EBS volumes.\nHost recovery is enabled on this Dedicated host.\nDue to hardware issues on a dedicated host, a replacement instance was launched.\nBut you found that instance store-backed instances are not recovered to that host.\nWhat can be possible reasons for the Instance store-backed instance not recovered on replacement Dedicated host?\n",
  "answers" : [ {
    "id" : "f0dbd2e76ca74b48bc2d84b1bfbf458b",
    "option" : "Need to open an AWS ticket for recovery of instance store-backed instances on Dedicated Host.",
    "isCorrect" : "false"
  }, {
    "id" : "31bcd22b386049cea4575705f67edab2",
    "option" : "Need to perform manual recovery of instance store-backed instances on Dedicated Host following remedial steps.",
    "isCorrect" : "true"
  }, {
    "id" : "4948959e63494982b70a3502cb8e7d1f",
    "option" : "Instance store-backed instances need to be rehosted to EBS backed volumes.",
    "isCorrect" : "false"
  }, {
    "id" : "b82765670e0345b1a7a960a4be095f64",
    "option" : "Manually start &amp; stop instance so that automatic recovery can be initiated to a replacement Dedicated host.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nInstance store-backed instances are not automatically recovered on replacement Dedicated host post Host recovery.\nFor these instances, a manual approach needs to be followed based upon remedial steps.\nOption A is incorrect as No need to open AWS Ticket, Instance store-backed instances can be recovered using remedial steps to a replacement dedicated host.\nOption C is incorrect.\nAlthough this will work, this is not required to be recovered to a replacement Dedicated host.\nOption D is incorrect as Stopping the instance will not result in the recovery of impaired Dedicated Host.\nFor more information on Amazon EC2 host recovery, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-hosts-recovery.html\n\nThe reason why the instance store-backed instances are not recovered on the replacement Dedicated host could be due to the following:\nInstance store-backed instances do not support automatic recovery: When a Dedicated host fails, Amazon EC2 automatically recovers the instances on the host to a new host. However, instance store-backed instances do not support automatic recovery. Therefore, when a Dedicated host fails, the instance store-backed instances need to be manually recovered. Configuration of the instance store-backed instance: The instance store-backed instance may have been configured to run on a specific Dedicated host. When the Dedicated host fails, the instance cannot be recovered automatically on a new host. This is because the instance store-backed instance is tightly coupled to the physical host hardware. Instance store-backed instances need to be re-hosted to EBS-backed volumes: Instance store-backed instances are ephemeral, which means that data stored on the instance store volumes is lost when the instance is stopped or terminated. To avoid data loss, instance store-backed instances need to be re-hosted to EBS-backed volumes.\nTherefore, the correct answer to this question is B. Need to perform manual recovery of instance store-backed instances on Dedicated Host following remedial steps. This means that the instance store-backed instances need to be manually recovered by following remedial steps. This includes creating a new instance, attaching the instance store volumes to the new instance, and configuring the instance to use the volumes.\n\n"
}, {
  "id" : 457,
  "question" : "A start-up firm has purchased 5 numbers of c5.xlarge Instance for a period of 3 years for an upcoming project.\nDue to the financial crisis, this project is on hold &amp; the company is looking to share these instances to get some financial benefit.\nThe IT head is seeking your guidance for sharing these Reserved Instances.\nWhich of the following statements is correct with regards to Capacity sharing?\n",
  "answers" : [ {
    "id" : "d83fb99a52654d9d93725a870bf78bae",
    "option" : "Capacity reservations can be shared only with Accounts within AWS Organisation.",
    "isCorrect" : "false"
  }, {
    "id" : "51e517c5d8df4249a46e4ae00ab0394f",
    "option" : "Capacity reservations can be shared only with other AWS accounts.",
    "isCorrect" : "false"
  }, {
    "id" : "fd9568ea385c4d77aaa5030ca1c1ae4a",
    "option" : "Capacity reservations can be shared with other AWS accounts or within AWS Organisation.",
    "isCorrect" : "true"
  }, {
    "id" : "139fe96a29d446f691d1cc2c5c7b9845",
    "option" : "Capacity reservations cannot be shared outside individual accounts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nUsing Capacity reservation sharing, reserved instances can be shared with other accounts within an AWS Organisation or other AWS accounts.\nOption A &amp; B are incorrect as Capacity Reservations can be shared with both other AWS accounts and Accounts within AWS Organisation.\nOption D is incorrect as Capacity Reservations can be shared with other AWS accounts as well.\nFor more information on Amazon EC2 Capacity sharing, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/capacity-reservation-sharing.html\n\nReserved Instances (RIs) in AWS allow customers to reserve capacity for EC2 instances for a fixed term, providing a significant discount compared to On-Demand instances. Capacity sharing is a feature that allows AWS customers to share their RI capacity with other accounts within the same organization or with other AWS accounts.\nIn the scenario mentioned in the question, the start-up firm has purchased 5 c5.xlarge instances for a period of 3 years. However, due to a financial crisis, the project for which these instances were purchased is on hold. The IT head of the firm is seeking guidance on how to share these instances to get some financial benefit.\nAnswer: The correct statement with regards to Capacity sharing is C: Capacity reservations can be shared with other AWS accounts or within AWS Organisation.\nExplanation: AWS allows RI owners to share their capacity with other accounts within the same AWS organization or with other AWS accounts. This feature is known as Capacity Sharing. Capacity sharing enables RI owners to monetize their unused or underutilized RI capacity by selling or loaning it to other AWS customers. This can help RI owners to reduce their costs and increase the ROI on their RIs.\nAnswer A is incorrect because capacity reservations can be shared not only with accounts within AWS Organization but also with other AWS accounts.\nAnswer B is incorrect because capacity reservations can be shared not only with other AWS accounts but also within AWS Organization.\nAnswer D is incorrect because capacity reservations can be shared outside individual accounts as long as they belong to the same AWS organization.\nTherefore, the correct answer is C: Capacity reservations can be shared with other AWS accounts or within AWS Organisation.\n\n"
}, {
  "id" : 458,
  "question" : "A Global IT firm is using a fleet of Reserved Instance for its intranet applications.\nFor uninterrupted services, they are planning to queue Reserved Instance purchases.\nWhich of the following is the correct statement for queuing Reserved Instance?\n",
  "answers" : [ {
    "id" : "31a3401890224ce684d8bc015e7c2140",
    "option" : "Reserved Instance purchases can be queued only for Reserved instance purchases from other sellers but not for Regional Reserved Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "9229ac407f1549198cb0bcab6e88896f",
    "option" : "Reserved Instance purchases can be queued only for Zonal Reserved instances but not for Regional Reserved Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "9a56f88e34f7467d9b7735310878291f",
    "option" : "Reserved Instance purchases can be queued only for Zonal Reserved instances but not for Reserved Instance from other sellers.",
    "isCorrect" : "false"
  }, {
    "id" : "b4976872e017439590b371400a5af0e4",
    "option" : "Reserved Instance purchases can be queued only for Regional Reserved instance but not for Reserved Instance from other sellers.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nReserved Instance can be queued for purchase on a future date.\nReserved Instance purchase can be queued only for regional instance &amp; not for zonal Reserved Instance &amp; Instance brought from other sellers.\nOption A is incorrect as Reserved Instance purchases cannot be queued for purchases from other sellers.\nOption B &amp; C are incorrect as queuing of Zonal Reserved Instance is not supported.\nFor more information on queuing Reserved Instance purchase, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ri-market-concepts-buying.html#ri-queued-purchase\n\nReserved Instances (RIs) are a pricing option provided by Amazon Web Services (AWS) that enables users to save money on their EC2 usage. Customers can reserve instances for a one or three-year term, and the cost is usually less than the on-demand rate. A Global IT firm is using a fleet of Reserved Instances for its intranet applications and plans to queue Reserved Instance purchases to ensure uninterrupted services.\nQueuing Reserved Instance purchases refers to the process of scheduling RI purchases so that they are automatically bought as soon as the current RI term expires. This ensures that the customer's EC2 instances remain uninterrupted and the customer is not charged at the on-demand rate.\nNow, let's look at the answer options and find out which one is correct.\nOption A: Reserved Instance purchases can be queued only for Reserved instance purchases from other sellers but not for Regional Reserved Instance.\nThis statement is incorrect. Customers can queue RI purchases for both Regional and Zonal Reserved Instances, regardless of whether they are from the same seller or not.\nOption B: Reserved Instance purchases can be queued only for Zonal Reserved instances but not for Regional Reserved Instances.\nThis statement is also incorrect. Customers can queue RI purchases for both Zonal and Regional Reserved Instances.\nOption C: Reserved Instance purchases can be queued only for Zonal Reserved instances but not for Reserved Instance from other sellers.\nThis statement is incorrect. Customers can queue RI purchases for both Zonal and Regional Reserved Instances, regardless of whether they are from the same seller or not.\nOption D: Reserved Instance purchases can be queued only for Regional Reserved instance but not for Reserved Instance from other sellers.\nThis statement is incorrect. Customers can queue RI purchases for both Zonal and Regional Reserved Instances, regardless of whether they are from the same seller or not.\nTherefore, the correct answer is none of the above. Customers can queue RI purchases for both Zonal and Regional Reserved Instances, regardless of whether they are from the same seller or not.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 460,
  "question" : "A manufacturing firm has purchased Dedicated Host for deploying its Microsoft-based Application.\nTo fully utilize the capacity of this Dedicated host, the IT Head had decided to share this Dedicated Host with other accounts (consumers) within AWS Organisation.\nWhat privileges will consumers will be acquiring for launching instances on these shared Dedicated hosts?\n",
  "answers" : [ {
    "id" : "75bb89fb0afe4056a7ced16081f63906",
    "option" : "Consumers will be able to manage instance which they have launched on a shared Dedicated host &amp; view instance launched by other consumers on this Dedicated Host.",
    "isCorrect" : "false"
  }, {
    "id" : "b66e4e266a5a40f48dda665c6ee033d1",
    "option" : "Consumers will be able to manage instance which they have launched on a shared Dedicated host.",
    "isCorrect" : "true"
  }, {
    "id" : "e102fb25064a4fd3aa178b4ddeb25f77",
    "option" : "Consumers will be able modify Dedicated Host &amp; manage instance which they have launched on a shared Dedicated host.",
    "isCorrect" : "false"
  }, {
    "id" : "e08532280ce6469fb879e978f007c152",
    "option" : "Consumers will be able to manage instance which they have launched along with instance launched by other consumers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nDedicated Host can be shared with other accounts within an AWS Organisation or an account outside AWS Organisation.\nWhen a Dedicated Host is shared, the owner is responsible for managing the Dedicated Host while consumers can manage the Instance they have launched on this Dedicated Host.\nOption A is incorrect as Consumers will not be able to view instance launched by other consumers or owners on this Dedicated Host.\nOption C is incorrect as Consumers will not be able to modify the Dedicated host.\nIt can be done only by the owner.\nOption D is incorrect as Consumers will be able to manage instances launched only by them &amp; not by other Consumers.\nFor more information on Amazon EC2 Dedicated Host sharing, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dh-sharing.html\n\nWhen a Dedicated Host is purchased, it is solely dedicated to the account that purchased it, and only that account has the ability to launch instances on it. However, it is possible to share a Dedicated Host with other accounts within the same AWS Organization, which can help to fully utilize the capacity of the host.\nWhen a Dedicated Host is shared with other accounts, the sharing account grants permissions to the receiving accounts to launch instances on the Dedicated Host. These permissions are granted through Resource Access Manager (RAM) in the AWS Management Console.\nBased on the given scenario, the manufacturing firm has decided to share their Dedicated Host with other accounts within the AWS Organization. Therefore, the consumers (receiving accounts) will be able to launch instances on the shared Dedicated Host. However, the privileges that consumers will acquire depend on the permissions that are granted to them by the sharing account.\nOption A states that consumers will be able to manage instances that they have launched on the shared Dedicated Host and view instances launched by other consumers on this Dedicated Host. This is correct since the sharing account can grant permissions to the consumers to launch instances on the Dedicated Host, and consumers will be able to view other instances launched on the Dedicated Host.\nOption B states that consumers will be able to manage instances that they have launched on a shared Dedicated Host. This is also correct since the sharing account can grant permissions to the consumers to launch instances on the Dedicated Host, and consumers will be able to manage only the instances that they have launched.\nOption C states that consumers will be able to modify the Dedicated Host and manage instances that they have launched on a shared Dedicated Host. This is incorrect since the sharing account retains the ability to modify the Dedicated Host, and consumers do not have permission to modify it.\nOption D states that consumers will be able to manage instances that they have launched along with instances launched by other consumers. This is incorrect since consumers can only manage instances that they have launched, and they do not have permission to manage instances launched by other consumers.\nTherefore, the correct answer is A, \"Consumers will be able to manage instances which they have launched on a shared Dedicated host & view instance launched by other consumers on this Dedicated Host.\"\n\n"
}, {
  "id" : 461,
  "question" : "A start-up firm is using Amazon EC2 instance for web applications.\nThe IT Team is observing some of the EC2 instances unresponsive &amp; remotely looking for the crash dump to diagnose further root cause.\nWhich of the following can be used for the collection of crash dump files?\n",
  "answers" : [ {
    "id" : "d59710bf14e14b318e650a669e4e5e37",
    "option" : "Run EC2:SetDiagnosticInterrupt API to generate crash dump data files.",
    "isCorrect" : "false"
  }, {
    "id" : "fe650343e54f47ab8f6027569b624253",
    "option" : "Run EC2:DebugDiagnosticInterrupt API to generate crash dump data file.",
    "isCorrect" : "false"
  }, {
    "id" : "180574ebba5148e6ad49de7482344d27",
    "option" : "Run EC2:SendDiagnosticInterrupt API to generate crash dump data files.",
    "isCorrect" : "true"
  }, {
    "id" : "e4f9fe4c4c0944c986f84648f0635f0b",
    "option" : "Run EC2:RunDiagnosticInterrupt API to generate crash dump data files.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - C\nFor getting crash dump remotely for an unresponsive EC2 instance, EC2:SendDiagnosticInterrupt API can be used which based upon OS configuration can do multiple things like getting crash dump, obtain a backtrace, load a kernel replacement or restart a system.\nOption A, B &amp; D are incorrect as these are not the correct APIs to get crash dump.\nFor more information on Amazon EC2 instance Troubleshooting, refer to the following URL-\nhttps://aws.amazon.com/blogs/aws/new-trigger-a-kernel-panic-to-diagnose-unresponsive-ec2-instances/\n\nNone of the provided options is correct as there are no EC2 APIs to generate crash dump data files. However, you can try the following steps to collect crash dump files:\nEnable crash dump collection: You need to enable crash dump collection on your EC2 instances by configuring the kernel parameter. For example, for CentOS, you can add the following line to the /etc/sysctl.conf file:\nperl Copy code kernel.core_pattern = /var/crash/core-%e-%s-%u-%g-%p-%t\nThis will generate a core dump file in the /var/crash directory when an application crashes.\nAnalyze core dump file: After a crash, you can analyze the core dump file to diagnose the issue. You can use tools like GDB (GNU Debugger) to analyze the core dump file. Debug the issue: Once you have analyzed the core dump file, you can debug the issue and fix it.\nIn conclusion, there are no EC2 APIs to generate crash dump data files. You need to enable crash dump collection on your EC2 instances and analyze the core dump file to diagnose the issue.\n\n"
}, {
  "id" : 462,
  "question" : "The Development Team has successfully deployed a new application on the Amazon EC2 instance in the us-west-2 region.\nThe same application needs to be deployed on Amazon EC2 instances in other regions.\nFor this deployment, they are looking for your support to deploy EC2 instances with images built from instances in the us-west-2 region along with the latest security patches.\nWhich action will be best suited for this requirement?\n",
  "answers" : [ {
    "id" : "66428bc594d94a19b21f36011797ed64",
    "option" : "Create a snapshot of an image &amp; deploy it in all other EC2 instances.",
    "isCorrect" : "false"
  }, {
    "id" : "5a2e6f4df9834fa288a793d65380d733",
    "option" : "Build automated scripts to create updated images with the latest patches.",
    "isCorrect" : "false"
  }, {
    "id" : "ca4b41c5a3564dd691da5b338f91c55f",
    "option" : "Manually create EC2 image with the latest patches.",
    "isCorrect" : "false"
  }, {
    "id" : "78ebb04dbb3448ffb27111965cde7bb7",
    "option" : "Use EC2 Image Builder to create images with the latest patches.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nEC2 Image Builder can help provide images with the latest security patches &amp; also automates testing these images to validate deployment.\nOption A is incorrect as multiple snapshots will be required which will be time-consuming.\nOption B is incorrect as creating automated scripts will incur additional admin work.\nOption C is incorrect as manually creating EC2 image with the latest patches will be time-consuming &amp; error-prone.\nFor more information on Amazon EC2 Image Builder, refer to the following URL-\nhttps://aws.amazon.com/blogs/aws/automate-os-image-build-pipelines-with-ec2-image-builder/\n\nThe best-suited action for this requirement would be to use EC2 Image Builder to create images with the latest patches (Option D).\nEC2 Image Builder is a fully managed service that makes it easier to automate the creation, management, and deployment of customized, secure, and up-to-date images of your EC2 instances or on-premises servers. EC2 Image Builder automates the process of patching and updating the operating system and software packages of an instance, ensuring that the latest security patches are applied. Using EC2 Image Builder ensures consistency across the images built across different regions.\nOption A, creating a snapshot of an image, is a possibility but not the best-suited option because the created image will not contain the latest security patches. It also requires manual work to update and patch the image.\nOption B, building automated scripts to create updated images with the latest patches, is a possible option, but it requires significant effort and may not be as effective as using a managed service like EC2 Image Builder.\nOption C, manually creating EC2 images with the latest patches, is time-consuming and can be prone to errors. It also requires expertise in managing and patching the operating system and software packages.\nTherefore, the best option for the development team to deploy EC2 instances with images built from instances in the us-west-2 region along with the latest security patches is to use EC2 Image Builder.\n\n"
}, {
  "id" : 463,
  "question" : "Media firm is using EBS backed EC2 Instance for their application deployment.\nThey have a process of daily snapshot of these EBS volumes.\nDuring an outage, while restoring this snapshot, they observed a delay in restoration, incurring financial loss to the firm.\nThe IT Head has demanded to use Fast Snapshot restore (FSR) for performance improvement &amp; assigned you to work on this task to set up FSR on Snapshots.\nWhich of the following is the correct statement concerning FSR for EBS?\n",
  "answers" : [ {
    "id" : "4647f0f9a4154e1695f9fda5b1080d9f",
    "option" : "FSR can be enabled for New &amp; Existing Snapshots per Availability Zone.",
    "isCorrect" : "true"
  }, {
    "id" : "36293473860842628fa522e687f07083",
    "option" : "FSR can be enabled for only New Snapshots per Availability Zone.",
    "isCorrect" : "false"
  }, {
    "id" : "d620c46d622c4dc1a6a583eb20ee11de",
    "option" : "FSR can be enabled for only Existing Snapshots per Availability Zone.",
    "isCorrect" : "false"
  }, {
    "id" : "4306b00cecce477aa2d32fe890cd2965",
    "option" : "FSR can be enabled for only New Snapshots in all Availability Zone in a region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nFast Snapshot restore (FSR) needs to be enabled per Availability Zone.\nAlso, this can be enabled on both existing as well as new Snapshots.\nOption B &amp; C are incorrect as FSR can be enabled for both new &amp; existing Snapshots.\nOption D is incorrect as FSR needs to be enabled per AZ.\nFor more information on Amazon EBS FSR, refer to the following URL-\nhttps://aws.amazon.com/blogs/aws/new-amazon-ebs-fast-snapshot-restore-fsr/\n\nFast Snapshot Restore (FSR) is an Amazon Elastic Block Store (EBS) feature that enables fast restoration of EBS volumes from snapshots. When enabled, FSR allows you to quickly restore an EBS volume from a snapshot without waiting for the data to be transferred from the snapshot to the EBS volume.\nIn the scenario described, the media firm is using EBS backed EC2 instances for their application deployment and taking daily snapshots of the EBS volumes. During an outage, the firm observed a delay in restoration when restoring the snapshot, incurring financial loss to the firm. To address this issue, the IT Head has asked to use Fast Snapshot Restore (FSR) for performance improvement and has assigned you to set up FSR on snapshots.\nThe correct statement concerning FSR for EBS is B. FSR can be enabled for only New Snapshots per Availability Zone.\nWhen enabling FSR, you can specify the number of fast restores that you want to enable per Availability Zone for each EBS volume. FSR can be enabled only for new snapshots, and it can be enabled only for a limited number of snapshots per Availability Zone. Once enabled, FSR provides fast restoration of the specified EBS volumes from the snapshots.\nTherefore, in the scenario described, to improve the performance of snapshot restoration, you can enable FSR for new snapshots of the EBS volumes. However, it is important to note that FSR is not a replacement for regular backups and does not eliminate the need for them. It is a useful feature for faster restoration of EBS volumes from snapshots.\n\n"
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : null,
  "question" : null,
  "answers" : null,
  "explanations" : null
}, {
  "id" : 466,
  "question" : "A financial institute is storing all its critical documents in the Amazon S3 bucket which has versioning enabled.\nThe retention period is applied to documents using object lock, but you are observing some objects are getting overwritten.\nWhich of the following could be a possible reason?\n",
  "answers" : [ {
    "id" : "b8b1aa316e78455780b2787b4e3544b9",
    "option" : "Object should have both retention period &amp; legal hold to prevent from overwriting.",
    "isCorrect" : "false"
  }, {
    "id" : "bfe40404ed014f34b4c33d9037b9a384",
    "option" : "Object locks apply to a specific version of an object in a versioned bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "78a91e1d48014f7c896d5897e8884eea",
    "option" : "Object lock should be applied at bucket level to prevent all objects in a bucket from overwriting.",
    "isCorrect" : "false"
  }, {
    "id" : "2039c2d2bc264462ac0831cd5f872898",
    "option" : "Object locks need to apply to all versions of the objects in a versioned bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nObject Lock is applied to a specific version of an object.\nSo, if there are multiple versions of an object, users can overwrite objects which do not have an object lock applied to them.\nOption A is incorrect as Both Retention period &amp; Legal Hold is not required for an object lock.\nOption C is incorrect as Object locks are not applied at bucket level but specific versions of objects.\nOption D is incorrect as Object Lock is applied to specific versions of an object &amp; not necessary to all versions of an object.\nFor more information on Amazon S3 Object Locks, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html\n\nThe correct answer is A. Object should have both retention period and legal hold to prevent from overwriting.\nExplanation:\nAmazon S3 versioning helps to protect the stored objects in the bucket from accidental deletion or overwrite. When versioning is enabled, all the objects uploaded to the bucket will have a unique version ID. This way, users can retrieve any version of the object from the bucket.\nObject Lock is a feature in Amazon S3 that allows users to lock the objects in the bucket to prevent them from being deleted or overwritten. Object Lock provides two ways to lock the objects: Governance mode and Compliance mode.\nIn Governance mode, the user can set a retention period for the object, and during that period, the object cannot be deleted or overwritten. However, once the retention period is over, the object can be deleted or overwritten.\nIn Compliance mode, the user can apply legal hold to the object, which will prevent it from being deleted or overwritten indefinitely, until the hold is removed.\nThe possible reason why some objects are getting overwritten even though the retention period is applied is that the objects might not have legal hold applied to them. In Governance mode, the retention period only prevents the object from being deleted or overwritten during that period. Once the retention period is over, the object can be deleted or overwritten. Therefore, to prevent the object from being overwritten, legal hold needs to be applied to the object.\nOption B is incorrect because Object locks apply to a specific version of an object in a versioned bucket. This means that when the object is locked, only that version of the object is locked, and other versions of the same object can be overwritten.\nOption C is incorrect because Object lock should be applied at the object level, not at the bucket level. Applying object lock at the bucket level would lock all the objects in the bucket, which might not be necessary.\nOption D is incorrect because Object locks need to apply to a specific version of the object, not all versions of the object.\n\n"
}, {
  "id" : 467,
  "question" : "A legal firm has archived its legal documents in two storage classes: S3 Glacier storage class &amp; S3 Glacier Deep Archive storage class.\nFor liaising with new international clients, they need to submit some of the documents urgently.\nThey are looking for these documents at the earliest time to get restored.\nWhich of the following is a recommended way to have get these data available in the fastest possible way?\n",
  "answers" : [ {
    "id" : "59464d4ee28e4f5fa0802cbaa58135f2",
    "option" : "Use Standard retrieval options for objects to be restored from both the S3 Glacier storage class &amp; S3 Glacier Deep Archive storage class.",
    "isCorrect" : "false"
  }, {
    "id" : "b1aa2852de824c339f0a156966eae8fe",
    "option" : "Use Standard retrieval options for objects to be restored from the S3 Glacier storage class &amp; expedited retrieval options for data stored in the S3 Glacier Deep Archive storage class.",
    "isCorrect" : "false"
  }, {
    "id" : "29ee829c9a844539821a8a8c2dbc48d1",
    "option" : "Use Expedited retrieval options for objects to be restored from the S3 Glacier storage class &amp; standard retrieval options for data stored in the S3 Glacier Deep Archive storage class.",
    "isCorrect" : "true"
  }, {
    "id" : "9509c81819ad4c83bb9cd8daaf5d123a",
    "option" : "Use Expedited retrieval options for objects to be restored from both the S3 Glacier storage class &amp; the S3 Glacier Deep Archive storage class.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nStandard or Bulk retrieval options can be used with the S3 Glacier Deep Archive storage class.\nAn expedited retrieval option can be used with the S3 Glacier storage class to restore data.\nAlso, restoration can be changed for the S3 Glacier storage class while restoration is in progress.\nThe typical Time frame for data retrieval is as follows.\nRetrieval option.\nS3 Glacier storage class.\nS3 Glacier Deep Archive storage class.\nExpedited.\n1-5 mins.\nNot Supported.\nStandard.\n3-5 hours.\n12 hours.\nBulk.\n5-12 hours.\n48 hours.\nOption A is incorrect as Standard retrieval option will work for the S3 Glacier Deep Archive storage class, but it will take 3-5 hours to retrieve objects from the S3 Glacier storage class while with the Expedited retrieval option.\nSame data can be available in less than 5 minutes.\nOption B is incorrect as the Standard retrieval option will take 3-5 hours to retrieve an object from the S3 Glacier storage class while with the Expedited retrieval option.\nSame data can be available in less than 5 minutes.\nOption D is incorrect as the Expedited retrieval option is Not supported for the S3 Glacier Deep Archive storage class.\nStandard or Bulk retrieval options can be used with the S3 Glacier Deep Archive storage class.\nFor more information on Amazon S3 Archive retrieval options, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/restoring-objects.html\n\nThe correct answer is B: Use Standard retrieval options for objects to be restored from the S3 Glacier storage class & expedited retrieval options for data stored in the S3 Glacier Deep Archive storage class.\nExplanation:\nAmazon S3 Glacier and Amazon S3 Glacier Deep Archive are low-cost storage classes for data archiving that provide secure and durable object storage for long-term retention of data. S3 Glacier is optimized for infrequently accessed data, while S3 Glacier Deep Archive is optimized for very rarely accessed data.\nWhen you archive data to these storage classes, the data is not immediately available for retrieval. Instead, you have to initiate a retrieval job that can take hours or even days to complete, depending on the retrieval option you choose.\nFor urgent data retrieval needs, Amazon S3 Glacier offers expedited retrieval options for retrieving data from S3 Glacier storage class. Expedited retrieval provides sub-5 minute access to your data when you need it most. However, expedited retrieval is not available for S3 Glacier Deep Archive storage class.\nTherefore, in this case, it is recommended to use standard retrieval options for objects stored in S3 Glacier storage class and expedited retrieval options for data stored in the S3 Glacier Deep Archive storage class. This approach will help to ensure that the data is available as quickly as possible while keeping the cost of retrieval low for rarely accessed data.\nOption A, using standard retrieval options for both S3 Glacier and S3 Glacier Deep Archive storage classes, may result in slower data retrieval times for data in the S3 Glacier Deep Archive storage class.\nOption C, using expedited retrieval options for data stored in the S3 Glacier storage class and standard retrieval options for data stored in the S3 Glacier Deep Archive storage class, is not recommended because it would be more expensive and would not optimize the retrieval times for both storage classes.\nOption D, using expedited retrieval options for both S3 Glacier and S3 Glacier Deep Archive storage classes, is also not recommended because expedited retrieval is not available for S3 Glacier Deep Archive storage class.\n\n"
}, {
  "id" : 468,
  "question" : "Last week there was an incident in your company where critical files were deleted from the Amazon S3 bucket.\nYour manager has asked you to configure event notification for changes made to objects in Amazon S3\nWhile publishing events from Amazon S3, which of the following cannot be used as the destination?\n",
  "answers" : [ {
    "id" : "38115f8ab29a4f42a1da18bbd0d051e0",
    "option" : "Amazon SNS",
    "isCorrect" : "false"
  }, {
    "id" : "ae5c76c4469f43c78573f1825ce382fc",
    "option" : "AWS Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "076615278373470c83259097832c7c8e",
    "option" : "Amazon SQS",
    "isCorrect" : "false"
  }, {
    "id" : "8b4eec5db10a4691a7c111d98724a65a",
    "option" : "Amazon CloudWatch Events.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAmazon S3 events can be sent to the following destinations &amp; not to Amazon CloudWatch Events.\n· Amazon SNS.\n· AWS Lambda.\n· Amazon SQS\nOption A, B, C are incorrect as Amazon S3 supports these services as a destination for sending notifications.\nFor more information on Amazon S3 event notification, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\n\nWhen configuring event notifications for changes made to objects in an Amazon S3 bucket, you can specify different destinations for the events to be published. The available destinations include Amazon SNS, AWS Lambda, Amazon SQS, and Amazon CloudWatch Events.\nOut of these options, there is no destination that cannot be used for publishing S3 events. Therefore, the correct answer to this question is None/All of the above.\nLet's take a closer look at each of the available destinations:\nA. Amazon SNS (Simple Notification Service): It is a fully managed messaging service that enables you to decouple and scale microservices, distributed systems, and serverless applications. You can use Amazon SNS as a destination for S3 events to send notifications to a variety of endpoints, including email, SMS text message, and mobile push notifications.\nB. AWS Lambda: It is a serverless compute service that lets you run code without provisioning or managing servers. You can use AWS Lambda as a destination for S3 events to trigger custom code in response to object creations, updates, and deletions. For example, you can use AWS Lambda to process images or video files as soon as they are uploaded to an S3 bucket.\nC. Amazon SQS (Simple Queue Service): It is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. You can use Amazon SQS as a destination for S3 events to send messages to one or more queues for asynchronous processing. For example, you can use Amazon SQS to process log files or perform data analysis on large datasets.\nD. Amazon CloudWatch Events: It is a service that enables you to respond to system events and automate actions in your AWS resources. You can use Amazon CloudWatch Events as a destination for S3 events to trigger events based on object creations, updates, and deletions. For example, you can use Amazon CloudWatch Events to trigger an AWS Lambda function that analyzes the contents of an S3 object and takes action based on the results.\nIn conclusion, all of the above-mentioned destinations can be used as the destination for publishing events from Amazon S3. Therefore, the correct answer to the question is None/All of the above.\n\n"
}, {
  "id" : 469,
  "question" : "A construction firm has stored all its project documents in the Amazon S3 bucket with external vendors accessing this data on a need basis.\nThe Security Team performs an audit for this access &amp; is looking for cryptographic protocols used by vendors to access this data.\nWhich of the following can help to collect this data?\n",
  "answers" : [ {
    "id" : "04c0522087ae43b086b2d28622e3f177",
    "option" : "Enable Server access logging &amp; check the TLS version for all requests.",
    "isCorrect" : "true"
  }, {
    "id" : "c25b450cfc5740d6abba8c7def23e073",
    "option" : "Enable Server access logging &amp; check Cipher Suite for all requests.",
    "isCorrect" : "false"
  }, {
    "id" : "ec062e47f1a2475a95293fb095b353f7",
    "option" : "Enable Server access logging &amp; check Host Header for all requests.",
    "isCorrect" : "false"
  }, {
    "id" : "a0d5b442a2374dce816721d84dcf7831",
    "option" : "Enable Server access logging &amp; check Signature Version for all requests.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon S3 Server Access Logs can be used to record the following fields.\n· Bucket Owner.\n· Bucket.\n· Time.\n· Remote IP.\n· Requestor.\n· Request ID.\n· Operation.\n· Key.\n· Request-URI.\n· HTTP Status.\n· Error Code.\n· Bytes Sent.\n· Object Size.\n· Total Time.\n· Turn-Around Time.\n· Referer.\n· User-Agent.\n· Version Id.\n· Host Id.\n· Signature Version.\n· Cipher Suite.\n· Authentication Type.\n· Host Header.\n· TLS version.\nOption B is incorrect as Cipher Suite will provide details of SSL negotiated for HTTPS request to S3.\nOption C is incorrect as the Host Header provides details of the endpoint used to connect Amazon S3.\nOption D is incorrect as it provides details of the signature version used for connection.\nFor more information on the Amazon S3 Access Logging format, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html\n\nThe construction firm has stored all project documents in Amazon S3, and external vendors are accessing the data on a need basis. The Security Team wants to collect information on the cryptographic protocols used by the vendors to access this data. To collect this data, the Security Team can enable server access logging, and check different aspects of the requests.\nOption A suggests enabling server access logging and checking the Transport Layer Security (TLS) version for all requests. TLS is a protocol that provides privacy and data integrity between two communicating applications. By checking the TLS version for all requests, the Security Team can determine which cryptographic protocol was used by the vendors to access the data. This option is a possible solution.\nOption B suggests enabling server access logging and checking the Cipher Suite for all requests. A Cipher Suite is a set of cryptographic algorithms used to secure a network connection. By checking the Cipher Suite for all requests, the Security Team can determine which cryptographic protocol was used by the vendors to access the data. This option is a possible solution.\nOption C suggests enabling server access logging and checking the Host Header for all requests. The Host Header is a part of the HTTP request that specifies the domain name of the server that is being accessed. Checking the Host Header for all requests will not provide any information on the cryptographic protocols used by the vendors to access the data. This option is not a valid solution.\nOption D suggests enabling server access logging and checking the Signature Version for all requests. Signature Version is a feature used to sign requests in Amazon S3. Checking the Signature Version for all requests will not provide any information on the cryptographic protocols used by the vendors to access the data. This option is not a valid solution.\nIn conclusion, options A and B are valid solutions, whereas options C and D are not. Therefore, the Security Team can enable server access logging and check the TLS version or Cipher Suite for all requests to collect the data on cryptographic protocols used by vendors to access data stored in Amazon S3.\n\n"
}, {
  "id" : 470,
  "question" : "A pharma company is using Amazon S3 bucket for saving drug formulation details of all its product base.\nFor regulatory purposes, the company needs to share an inventory of files stored in this bucket with IT auditors.\nWhich of the following is a correct statement regarding configuring an inventory list for a S3 bucket?\n",
  "answers" : [ {
    "id" : "ad22e7dd40d644f0ad38de2df9756408",
    "option" : "A single Inventory list can be created for a bucket with inventory files saved in encrypted format.",
    "isCorrect" : "false"
  }, {
    "id" : "4f86ddf7910b4da58405bc749dc99c74",
    "option" : "Multiple Inventory lists can be created for a bucket with inventory files saved in encrypted format.",
    "isCorrect" : "true"
  }, {
    "id" : "df0e613ca74745099d35dac92a2fedf1",
    "option" : "Multiple Inventory lists can be created for a bucket with an inventory file saved in unencrypted format.",
    "isCorrect" : "false"
  }, {
    "id" : "6643db18c4964b10b996fc46d3e70e44",
    "option" : "A single Inventory list can be created for a bucket with an inventory file saved in unencrypted format.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon S3 Inventory can be used to manage files in the Amazon S3 bucket &amp; create an audit report for those files.\nWhile creating an Inventory list, multiple inventory lists can be made per bucket based upon objects included &amp; save inventory list in encrypted form.\nOption A &amp; D are incorrect as Multiple inventory lists can be configured for a bucket based upon requirement &amp; not a single inventory list.\nOption C is incorrect as the inventory file can be saved as encrypted.\nFor more information on the Amazon S3 Inventory file, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/storage-inventory.html\n\nThe correct answer for this question is D. A single Inventory list can be created for a bucket with an inventory file saved in unencrypted format.\nAmazon S3 Inventory provides reports about objects and metadata in a bucket. You can use these reports to analyze the storage of objects and to meet business, legal, and regulatory compliance requirements.\nAn S3 bucket can have only one inventory configuration. The inventory configuration can include optional filters that determine which objects to include in the inventory report. The inventory report can be saved in CSV or ORC format, and it can be stored in the same bucket or another bucket in the same or different account.\nS3 Inventory can be configured to generate a report on an ongoing basis or on a schedule, and these reports can be used for compliance, auditing, and security purposes.\nWhen creating an inventory configuration, you can choose to encrypt the inventory report using Server-Side Encryption (SSE) or client-side encryption. If you choose to encrypt the inventory report, you must specify the KMS key to use for encryption.\nIn summary, a single inventory list can be created for a bucket, and the inventory report can be saved in either an encrypted or unencrypted format. However, it is important to consider the regulatory requirements for data privacy and security when choosing to encrypt the inventory report.\n\n"
}, {
  "id" : 471,
  "question" : "The IT firm is looking to move millions of files to the Amazon S3 bucket using Amazon S3 Batch Operations.\nA Junior Engineer is working on this requirement of creating job requests for Amazon S3 Batch Operations.\nHe is looking for parameters required for Job request.\nWhich of the following are NOT required parameters while creating a Job request for Batch operations?\n",
  "answers" : [ {
    "id" : "13474c2f57a8484ea2d1f6503ba88303",
    "option" : "Priority",
    "isCorrect" : "false"
  }, {
    "id" : "e45793eeb5394ec4a01719bd318b8a6e",
    "option" : "Report",
    "isCorrect" : "false"
  }, {
    "id" : "e726a94f48fb4095bcb0f791af02cf23",
    "option" : "Tags",
    "isCorrect" : "true"
  }, {
    "id" : "34e0740d71c74e34a46649cba80cab50",
    "option" : "Manifest.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nFor creating Job request, the following parameters need to be specified.\n· Operation.\n· Manifest.\n· Priority.\n· RoleArn.\n· Report.\n· Tags.\n· Description.\nOut of these parameters, Tags &amp; Description are optional &amp; the rest should be specified while creating Job.\nOption A, B, &amp; D are incorrect as these parameters need to be specified while creating a Job request for Amazon S3 Batch Operations.\nFor more information on Amazon S3 Batch Operation Job, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops-create-job.html\n\nAmazon S3 Batch Operations is a feature that allows users to perform operations on large sets of Amazon S3 objects. These operations can be performed on objects in the same or multiple S3 buckets. The operations are executed asynchronously in the background, which allows users to optimize their use of Amazon S3 resources.\nWhen creating a job request for Amazon S3 Batch Operations, there are several parameters that can be specified, such as the operation to be performed, the input data source, and the output location. The following is a brief overview of the parameters:\nA. Priority: This parameter is used to specify the priority of the job request. The priority can be set to a value between 0 and 10, with 0 being the lowest priority and 10 being the highest priority.\nB. Report: This parameter is used to specify the type of report that is generated when the job request is completed. The report can be in the form of a CSV or JSON file.\nC. Tags: This parameter is used to assign metadata to the job request. The metadata can be in the form of key-value pairs.\nD. Manifest: This parameter is used to specify the location of the manifest file that contains the list of objects to be processed by the job request.\nBased on the above information, the answer to the question is option D, \"Manifest,\" as it is a required parameter when creating a job request for Amazon S3 Batch Operations. Options A, B, and C are all valid parameters that can be specified when creating a job request, but manifest is the only required parameter.\n\n"
}, {
  "id" : 472,
  "question" : "An HR Firm is planning to use Access Point to manage access to many objects stored in Amazon S3 buckets.\nCreating an Access Point ARN as below will represent which objects to be accessed? arn:aws:s3:us-east-2:123456789012:accesspoint/test/object/unit-05/hr/*\n",
  "answers" : [ {
    "id" : "c51f26ef8fe74a71b341d7e1d56fb36d",
    "option" : "Represents all objects under prefix object/unit-05/hr for access point name test belonging to account 123456789012 in us-east-2 region.",
    "isCorrect" : "false"
  }, {
    "id" : "6b2bcd33d0924822843c1443e0af484b",
    "option" : "Represents all objects under prefix object/unit-05/hr for bucket name test belonging to account 123456789012 in us-east-2 region.",
    "isCorrect" : "false"
  }, {
    "id" : "4c9f393e89eb4bb6a3cf72a03f560956",
    "option" : "Represents all objects under prefix unit-05/hr for bucket name test belonging to account 123456789012 in us-east-2 region.",
    "isCorrect" : "false"
  }, {
    "id" : "b139d66238ff4775b80c14e6ba2102d5",
    "option" : "Represents all objects under prefix unit-05/hr for access point name test belonging to account 123456789012 in us-east-2 region.",
    "isCorrect" : "true"
  } ],
  "explanations" : "\n\nCorrect Answer - D.\nAmazon S3 Access Points can be created to provide access to a large number of objects in Amazon S3 buckets.\nAccess Points ARN has format as below.\narn:aws:s3:region:account-id:accesspoint/access-point-name/object/resource.\nIn the above case, the access point name is test &amp; the resource is unit-05/hr.\nOption A is incorrect as the object is key word &amp; not a part of the prefix name.\nPrefix name is /unit-05/hr/.\nOption B &amp; C are incorrect as the test is the name of the access point &amp; not a bucket name.\nFor more information on Amazon S3 Access Points, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/using-access-points.html\n\nAccess Points is a feature in Amazon S3 that simplifies managing data access at scale for shared datasets. It allows you to create a custom hostname that maps to a specific bucket and optionally, a prefix in the bucket, and can have its own access policy. With Access Points, you can enforce fine-grained access controls on shared data at scale, without needing to create multiple IAM roles or bucket policies.\nIn the given scenario, an HR firm wants to use Access Points to manage access to many objects stored in Amazon S3 buckets. The Access Point ARN given is:\nruby Copy code arn:aws:s3:us-east-2:123456789012:accesspoint/test/object/unit-05/hr/*\nThis Access Point ARN represents all objects under the prefix object/unit-05/hr for access point name test belonging to account 123456789012 in the us-east-2 region.\nLet's break down the Access Point ARN:\narn:aws:s3 is the service namespace for Amazon S3 us-east-2 is the region where the bucket and Access Point are located 123456789012 is the AWS account ID that owns the bucket and Access Point accesspoint/test is the name of the Access Point object/unit-05/hr/* is the prefix of the objects that can be accessed via this Access Point\nOption A is the correct answer as it correctly interprets the Access Point ARN. Options B, C, and D are incorrect because they don't include the Access Point name in the prefix or don't specify the correct prefix.\nOption B is incorrect because it refers to the bucket name instead of the Access Point name. Option C is incorrect because it only specifies the prefix without including the Access Point name. Option D is incorrect because it only specifies the prefix without including the Access Point name, and it doesn't include the object/ prefix.\n\n"
}, {
  "id" : 473,
  "question" : "Telecom firm provides access to data stored in Amazon S3 bucket to external vendors on a project basis.\nOnce the project is completed, public access to this Amazon S3 bucket is removed.\nDuring the security check, it was found that some of the buckets still have public access even though projects are completed.\nSecurity Head needs you to determine all Amazon S3 buckets with public access.\nWhich of the following actions can be initiated to complete this task in the shortest time frame?\n",
  "answers" : [ {
    "id" : "47d617f9289d445a94c2f016972709bc",
    "option" : "Use Access Analyser &amp; create a report to find all Amazon S3 buckets with Public Access.",
    "isCorrect" : "true"
  }, {
    "id" : "c0ebd45c0ac545048aef8247100ce224",
    "option" : "Use Amazon S3 console &amp; find all buckets with public Tag.",
    "isCorrect" : "false"
  }, {
    "id" : "c407834779b24cec8f7fe390197fb472",
    "option" : "Run Crone tool from CLI to determine Amazon S3 bucket with public access.",
    "isCorrect" : "false"
  }, {
    "id" : "e63ec0464d2641de983ad9a56231c66b",
    "option" : "Verify all Bucket ACLs from CLI to determine Amazon S3 bucket with public access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAccess Analyzer can be enabled to get a list of all Amazon S3 buckets with public access.\nIt can help to gather the following information.\n· Bucket name.\n· Discovered by Access analyzer.\n· Shared through.\n· Status.\n· Access Level\nOption C is incorrect as additional work needs to be done for the creation of the crone tool.\nOption B &amp; D are incorrect as this is a manual process &amp; will require additional admin work.\nFor more information on Amazon S3 Access Analyzer, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/access-analyzer.html\n\nThe correct answer is A. Use Access Analyzer & create a report to find all Amazon S3 buckets with Public Access.\nExplanation: Access Analyzer is a service in AWS that analyzes resource policies for AWS Identity and Access Management (IAM) and Amazon S3 Access Control Lists (ACLs) to help identify unintended access to resources that can be used to refine permissions. Access Analyzer can be used to quickly identify which S3 buckets have public access.\nOption B, using the Amazon S3 console, is not a reliable method because it depends on whether the person who set up the bucket included a specific tag, which may not always be the case.\nOption C, running a Cron tool from the command-line interface (CLI), is also not the best approach, because it may not be efficient and requires more time and resources.\nOption D, verifying all Bucket ACLs from the command-line interface (CLI), is a valid approach, but it is not the most efficient because it requires you to check each bucket's ACLs, and it is time-consuming.\nIn summary, the most efficient approach is to use the Access Analyzer service, which provides a clear report to identify all S3 buckets with public access.\n\n"
}, {
  "id" : 474,
  "question" : "A banking institute is storing all its critical data in an Amazon S3 bucket in the us-west region.\nFor security compliance, these data need to be replicated in multiple regions.\nYou have been assigned the task of creating a replication configuration for the source bucket.\nWhich of the following should be considered while creating rules for replication configuration?\n",
  "answers" : [ {
    "id" : "012a1c28a2fc431386bd4d5aeaf211b8",
    "option" : "Can add a single rule having filters matching objects based upon only object tags.",
    "isCorrect" : "false"
  }, {
    "id" : "32297c8311ec44649649b0d6ded4cba6",
    "option" : "Can add multiple rules having filters matching objects based upon object tags or object key prefixes.",
    "isCorrect" : "true"
  }, {
    "id" : "d5a50c566c40412f8ed2655d839542ad",
    "option" : "Can add a single rule having filters matching objects based upon object tags or object key prefixes.",
    "isCorrect" : "false"
  }, {
    "id" : "c1eb5277e21e4d45aea9a927e209fe22",
    "option" : "Can add multiple rules having filters matching objects based upon only object tags.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nFor enabling replication on the Amazon S3 bucket, replication configuration is required to be added.\nWhile creating replication configuration, multiple rules can be added based upon the subset of objects to be matched.\nFilter for these rules can be based upon matching objects tag or object key prefixes or any combination of both.\nOption A &amp; C are incorrect as multiple rules can be added in replication configuration based upon subsets of objects.\nOption D is incorrect as filters can be configured to match object tags and object key prefixes or a combination of both.\nFor more information on Amazon S3 Replication Configuration, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication-add-config.html\n\nWhen configuring replication for an Amazon S3 bucket, there are a few factors that need to be considered to ensure that the replicated data is consistent and complies with security requirements.\nFirst, it is important to understand that Amazon S3 replication allows the replication of objects between buckets located in different regions. This means that data can be replicated in a different region for security and disaster recovery purposes.\nNext, when configuring rules for replication, there are two types of filters that can be used: object tags and object key prefixes.\nObject tags are metadata assigned to objects that provide additional information about the objects. Object key prefixes, on the other hand, are a string of characters that precede the object's key name in the bucket.\nNow, let's take a look at the options provided:\nA. Can add a single rule having filters matching objects based upon only object tags.\nThis option implies that the replication configuration will only consider object tags when filtering objects for replication. This approach may not be sufficient if there are specific object key prefixes that need to be replicated.\nB. Can add multiple rules having filters matching objects based upon object tags or object key prefixes.\nThis option allows for multiple rules to be created, which can include filters based on both object tags and object key prefixes. This approach is more flexible and can provide more granular control over the replication process.\nC. Can add a single rule having filters matching objects based upon object tags or object key prefixes.\nThis option allows for a single rule to be created that can include filters based on both object tags and object key prefixes. While this option provides more flexibility than option A, it may not provide the level of granularity needed for certain replication scenarios.\nD. Can add multiple rules having filters matching objects based upon only object tags.\nThis option implies that the replication configuration will only consider object tags when filtering objects for replication, and also allows for multiple rules to be created. Similar to option A, this approach may not be sufficient if there are specific object key prefixes that need to be replicated.\nIn conclusion, options B and C are the most appropriate for creating rules for replication configuration as they allow for both object tags and object key prefixes to be used for filtering objects. Option B provides more flexibility in terms of creating multiple rules, while option C provides a simpler approach with a single rule.\n\n"
} ]