[ {
  "id" : 1,
  "question" : "Your company is planning on the following architecture for their application. A set of EC2 Instances hosting the web part of the application. A relational database for the backend. A Load balancer for distribution of traffic. A NAT gateway for routing traffic from the database server to the Internet. Which of the following architecture ensures high availability across all components?\n",
  "answers" : [ {
    "id" : "653980ca89604abf86c67a41fda6d2eb",
    "option" : "A Load balancer with one public subnet, one private subnet. The EC2 Instances placed in one Availability Zone. RDS with Multi-AZ Enabled. NAT Gateway in one availability zone.",
    "isCorrect" : "false"
  }, {
    "id" : "1e85564f13f7443c8f192bcc1edd7cb6",
    "option" : "A Load balancer with 2 public subnets, 2 private subnets. The EC2 Instances placed across 2 Availability Zones. RDS with Multi-AZ Enabled. NAT Gateways in each availability zone.",
    "isCorrect" : "true"
  }, {
    "id" : "bdd23a02f47c4789bbc6841d59b655e8",
    "option" : "A Load balancer with 2 public subnets, 2 private subnets. The EC2 Instances placed in 2 Availability Zones. RDS with Multi-AZ Enabled. NAT Gateway in one availability zone.",
    "isCorrect" : "false"
  }, {
    "id" : "266ffc46a6564dcc84881b74398238a7",
    "option" : "A Load balancer with 2 public subnets, 2 private subnets. The EC2 Instances placed in one Availability Zone. RDS with Multi-AZ Enabled. NAT Gateway in one availability zone.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nIn a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone.\nRunning a DB instance with high availability can enhance availability during planned system maintenance and help protect your databases against DB instance failure and Availability Zone disruption.\nOption A is incorrect because High Availability is not ensured.\nWe recommend that you enable multiple Availability Zones.\nWith this configuration, if one Availability Zone becomes unavailable or has no healthy targets, the load balancer can continue to route traffic to the healthy targets in another Availability Zone.\nOption B is Correct because High Availability is ensured.\nIf one of the AZs should fail, then the EC2 instances in the remaining private subnet will still be able to communicate with the Internet because they have their own NAT Gateway in the same AZ.\nOptions C and D are incorrect because High Availability is not ensured as either if we have EC2 in a single AZ or multiple AZ.\nWe have NAT Gateway in single AZ is a cause for not ensuring High Availability.\nHowever, if there is a failure with Availability Zone A (rare, but can happen), then the NAT Gateway is not reachable from Private-Subnet-B.\nThus, the system may be impacted even though it runs across two AZs or a single AZ.\nFor more information on Elastic Load Balancing, Multi-AZ, and NAT gateway, please refer to the below URLs:-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nOption B is the correct answer as it ensures high availability across all components. Let's go through each component and see how it contributes to high availability:\nLoad Balancer: The load balancer distributes incoming traffic across multiple EC2 instances. This provides fault tolerance and helps prevent any single point of failure. By having the load balancer in multiple availability zones, it ensures that the application remains available even if one availability zone goes down.\nEC2 Instances: The EC2 instances host the web part of the application. By placing them across multiple availability zones, it ensures that the application remains available even if one availability zone goes down.\nRDS: The relational database is an important component of the application. Enabling Multi-AZ ensures that a standby instance is available in a different availability zone, which is automatically promoted in case the primary database instance goes down. This ensures that the application remains available even in the event of a database failure.\nNAT Gateway: The NAT gateway allows the database server to access the internet. By having the NAT gateway in each availability zone, it ensures that the database server remains reachable even if one availability zone goes down.\nOption A is incorrect because the EC2 instances are placed in one availability zone, which can cause a single point of failure. Also, the NAT gateway is placed in one availability zone, which can cause issues if that availability zone goes down.\nOption C is incorrect because the NAT gateway is placed in one availability zone, which can cause issues if that availability zone goes down.\nOption D is incorrect because the EC2 instances are placed in one availability zone, which can cause a single point of failure.\n\n"
}, {
  "id" : 2,
  "question" : "An AWS Solutions Architect designing a solution to store and archive corporate documents has determined Amazon Glacier as the right choice.\nAn important requirement is that the data must be delivered within 5 minutes of a retrieval request. Which feature in Amazon Glacier could help to meet this requirement?\n",
  "answers" : [ {
    "id" : "3410f1a6089a4298b9d1577f53b76420",
    "option" : "Vault Lock",
    "isCorrect" : "false"
  }, {
    "id" : "fe58f53e874c40a593a286ea57da3b53",
    "option" : "Expedited retrieval",
    "isCorrect" : "true"
  }, {
    "id" : "ac2d49cae4ae44d7b0ac476e9bafcd31",
    "option" : "Bulk retrieval",
    "isCorrect" : "false"
  }, {
    "id" : "6bd0c97cb0ff47918445f33bde718dae",
    "option" : "Standard retrieval.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following:\nExpedited retrievals allow you to access data in 1-5 minutes for a flat rate of $0.03 per GB retrieved.\nExpedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required.\nThe Vault Lock and Standard Retrieval are standard with 3-5 hours retrieval time while Bulk retrievals which can be considered the cheapest option have 5-12 hours retrieval time.\nFor more information on AWS Glacier Retrieval, please visit the following URL:\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html\n\nThe correct answer is B. Expedited retrieval.\nAmazon Glacier is a low-cost, secure, and durable storage service for data archiving and long-term backup. It is optimized for data that is infrequently accessed and for which retrieval times of several hours are acceptable. However, the scenario in the question requires the data to be delivered within 5 minutes of a retrieval request.\nAmazon Glacier provides three retrieval options for accessing archived data: Standard, Expedited, and Bulk.\nStandard retrieval is the default option and provides access to the lowest-cost data retrieval. It typically takes 3-5 hours to retrieve data.\nBulk retrieval is optimized for large data sets and can take up to 12 hours to retrieve data.\nExpedited retrieval is designed for situations where fast access to data is needed, and it typically delivers data within minutes. It incurs a higher cost than Standard or Bulk retrieval, but it guarantees data retrieval within 1-5 minutes.\nTherefore, the feature in Amazon Glacier that could help to meet the requirement of delivering data within 5 minutes of a retrieval request is Expedited retrieval (Option B).\n\n"
}, {
  "id" : 3,
  "question" : "A Solutions Architect is designing a highly scalable system to track records.\nThese records must remain available for immediate download for up to three months and then must be deleted.\nWhat is the most appropriate decision for this use case?\n",
  "answers" : [ {
    "id" : "baf74e74f8ab413a8616740c6f2736ee",
    "option" : "Store the files in Amazon EBS and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "false"
  }, {
    "id" : "cdc6495c815d48f6a336432d37e81fed",
    "option" : "Store the files in Amazon S3 and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "true"
  }, {
    "id" : "53095ce97f034a9193bf4fbe40014c94",
    "option" : "Store the files in Amazon Glacier and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "false"
  }, {
    "id" : "ea1a0da591704c6cb074cdf5b7360ffb",
    "option" : "Store the files in Amazon EFS and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect since the records need to be stored in a highly scalable system.\nOption C is incorrect since the records must be available for immediate download.\nOption D is incorrect since EFS lifecycle management is used to migrate files that have not been accessed for a certain period of time to the Infrequent Access storage class.\nFiles moved to this storage remain indefinitely and not get deleted.\nAnd due to this reason, this option is not correct.\nAWS Documentation mentions the following about Lifecycle Policies:\nLifecycle configuration enables you to specify the Lifecycle Management of objects in a bucket.\nThe configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects.\nThese actions can be classified as follows:\nTransition actions - In which you define when the transition of the object occurs to another storage class.\nFor example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation or archive objects to the GLACIER storage class one year after creation.\nExpiration actions - In which you specify when the objects will expire.\nThen Amazon S3 deletes the expired objects on your behalf.\nFor more information on AWS S3 Lifecycle Policies, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\nTo know more about EFS Lifecycle Management, please check the following URL:\nhttps://docs.aws.amazon.com/efs/latest/ug//lifecycle-management-efs.html.\n\nFor the given use case of tracking records that must remain available for immediate download for up to three months and then must be deleted, the most appropriate decision is to store the files in Amazon S3 and create a Lifecycle Policy to remove files after 3 months (Option B).\nAmazon S3 (Simple Storage Service) is an object storage service that provides industry-leading scalability, data availability, security, and performance. It is designed to store and retrieve any amount of data from anywhere on the web, and is highly durable and resilient, with data stored across multiple devices and facilities within a region to provide high availability and fault tolerance.\nOption A, storing the files in Amazon EBS (Elastic Block Store) and creating a Lifecycle Policy to remove files after 3 months, is not the best option because EBS volumes are designed to be used as a block device for EC2 instances and are not intended for long-term data storage. Additionally, EBS volumes are not highly scalable or fault-tolerant by themselves, and require additional configuration and management to achieve such capabilities.\nOption C, storing the files in Amazon Glacier and creating a Lifecycle Policy to remove files after 3 months, is also not the best option because Amazon Glacier is designed for long-term data archival and retrieval, and is intended for data that is infrequently accessed and has a lower priority for availability. Retrieving data from Glacier can also take several hours, which is not ideal for a use case where immediate download is required.\nOption D, storing the files in Amazon EFS (Elastic File System) and creating a Lifecycle Policy to remove files after 3 months, is also not the best option because EFS is a scalable and highly available file system that is designed for shared access across multiple instances, but is not intended for long-term data storage or archival. Additionally, EFS can be more expensive than S3 for this use case.\nTherefore, the most appropriate decision for this use case is to store the files in Amazon S3 and create a Lifecycle Policy to remove files after 3 months. The Lifecycle Policy can be configured to automatically transition the objects to Amazon Glacier after a certain period of time, if required, to reduce storage costs while still maintaining data availability.\n\n"
}, {
  "id" : 4,
  "question" : "You have developed a new web application on AWS for a real estate firm.\nIt has a web interface where real estate employees upload photos of newly constructed houses in S3 buckets.\nProspective buyers log in to the website and access photos.\nThe marketing team has initiated an intensive marketing event to promote new housing schemes which will lead to customers who frequently access these images.\nAs this is a new application, you have no projection of traffic on the S3 bucket.\nYou need an S3 storage class that can automatically optimize the storage costs with changing access patterns.\nWhich of the following is a recommended storage solution to meet this requirement?\n",
  "answers" : [ {
    "id" : "024d266bcacb46b0bba4bba15fd40626",
    "option" : "Use One Zone-IA storage class to store all images.",
    "isCorrect" : "false"
  }, {
    "id" : "d7e193d955dc4bd59db20b887ebf46d4",
    "option" : "Use Standard-IA to store all images.",
    "isCorrect" : "false"
  }, {
    "id" : "ef664e3d32ba4d378a73509ac3c89dbe",
    "option" : "Use S3 Intelligent-Tiering storage class.",
    "isCorrect" : "true"
  }, {
    "id" : "ebe8ff3727f74e088b21e1f19ecbd640",
    "option" : "Use Standard storage class and use Storage class analytics to identify &amp; move objects using lifecycle policies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nWhen the access pattern to web applications using S3 storage buckets is unpredictable, you can use S3 intelligent-Tiering storage class.\nS3 Intelligent-Tiering storage class includes two access tiers: frequent access and infrequent access.\nBased upon access patterns, it moves data between these tiers, which helps in cost saving.\nS3 Intelligent-Tiering storage class has the same performance as that of Standard storage class.\nOption A is incorrect.\nAlthough it will save costs, it will not provide any protection in case of AZ failure.\nAlso, this class is suitable for infrequently accessed data &amp; not for frequently access data.\nOption B is incorrect as Standard-IA storage class is for infrequently accessed data &amp; there are retrieval charges associated.\nIn the above requirement, you do not know the data access pattern, which may result in a higher cost.\nOption D is incorrect.\nIt has operational overhead to set up Storage class analytics &amp; moves objects between various classes.\nAlso, since the access pattern is undetermined, this will run into a costlier option.\nFor more information on S3 Intelligent-Tiering, refer to the following URLs:\nhttps://aws.amazon.com/blogs/aws/new-automatic-cost-optimization-for-amazon-s3-via-intelligent-tiering/\n\nThe recommended storage solution to meet the requirement of optimizing storage costs with changing access patterns in the given scenario is C. Use S3 Intelligent-Tiering storage class.\nS3 Intelligent-Tiering is a storage class in Amazon S3 that is designed to optimize costs for data with unknown or changing access patterns. This storage class automatically moves objects between two access tiers - frequent access and infrequent access - based on changing access patterns, without any performance impact or operational overhead. S3 Intelligent-Tiering is suitable for workloads with unknown or changing access patterns, where data access frequency can vary widely and unpredictably.\nIn the given scenario, the web application is new, and there is no projection of traffic on the S3 bucket. Additionally, the marketing team has initiated an intensive marketing event to promote new housing schemes, which will lead to customers who frequently access these images. As a result, the access patterns to the S3 bucket are unknown or changing, and it is not clear how frequently the images will be accessed in the future.\nUsing One Zone-IA storage class (Option A) or Standard-IA storage class (Option B) may not be optimal in this scenario, as these storage classes are designed for data that is infrequently accessed but must be available immediately when needed. In other words, these storage classes are suitable for data that is accessed less frequently and can tolerate a slightly longer retrieval time when accessed. In contrast, S3 Intelligent-Tiering is designed for workloads with unknown or changing access patterns, where data access frequency can vary widely and unpredictably.\nUsing Standard storage class and using Storage class analytics to identify & move objects using lifecycle policies (Option D) is a viable option, but it requires more manual intervention and is less automated than using S3 Intelligent-Tiering. With S3 Intelligent-Tiering, the movement of objects between access tiers is automated, and there is no need for manual intervention or lifecycle policies.\nTherefore, the recommended storage solution to meet the requirement of optimizing storage costs with changing access patterns in the given scenario is C. Use S3 Intelligent-Tiering storage class.\n\n"
}, {
  "id" : 5,
  "question" : "You host a static website in an S3 bucket, and there are global clients from multiple regions.\nYou want to use an AWS service to store cache for frequently accessed content so that the latency is reduced and the data transfer rate increases.\nWhich of the following options would you choose?\n",
  "answers" : [ {
    "id" : "7a408e9f09ad4a698e0423e458de7272",
    "option" : "Use AWS SDKs to horizontally scale parallel requests to the Amazon S3 service endpoints.",
    "isCorrect" : "false"
  }, {
    "id" : "c4fda38a150e4bd09e0a65681c279684",
    "option" : "Create multiple Amazon S3 buckets and put Amazon EC2 and S3 in the same AWS Region.",
    "isCorrect" : "false"
  }, {
    "id" : "8a4d0607d08842ba9dd19c142b40bf87",
    "option" : "Enable Cross-Region Replication to several AWS Regions to serve customers from different locations.",
    "isCorrect" : "false"
  }, {
    "id" : "20266668933c449eadf8a5f882e8e87d",
    "option" : "Configure CloudFront to deliver the content in the S3 bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nCloudFront can store the frequently accessed content as a cache, and the performance is optimized.\nOther options may help with the performance.\nHowever, they do not store cache for the S3 objects.\nOption A is incorrect: This option may increase the throughput.\nHowever, it does not store cache.\nOption B is incorrect: Because this option does not use cache.\nOption C is incorrect: This option creates multiple S3 buckets in different regions.\nIt does not improve the performance using cache.\nOption D is CORRECT: Because CloudFront caches copies the S3 files in its edge locations.\nUsers are routed to the edge location that has the lowest latency.\n\nThe best option for this scenario would be to configure Amazon CloudFront to deliver the content in the S3 bucket.\nCloudFront is a Content Delivery Network (CDN) that can cache content at edge locations around the world to reduce latency and improve data transfer rates. When a user requests content from your website, CloudFront will automatically route the request to the nearest edge location, which will serve the cached content if available. If the content is not cached, CloudFront will retrieve it from the origin (in this case, the S3 bucket) and cache it for future requests.\nUsing AWS SDKs to horizontally scale parallel requests to the Amazon S3 service endpoints, as suggested in option A, may increase the number of requests that S3 can handle simultaneously, but it will not reduce the latency or improve data transfer rates for global clients.\nCreating multiple Amazon S3 buckets and putting Amazon EC2 and S3 in the same AWS Region, as suggested in option B, may improve performance for clients within the same region as the EC2 instances and S3 buckets, but it will not improve performance for clients in other regions.\nEnabling Cross-Region Replication to several AWS Regions to serve customers from different locations, as suggested in option C, may improve availability and durability by replicating data to multiple regions, but it will not reduce the latency or improve data transfer rates for global clients.\nTherefore, the best option to reduce latency and improve data transfer rates for global clients accessing a static website hosted in an S3 bucket is to configure CloudFront to deliver the content in the S3 bucket.\n\n"
}, {
  "id" : 6,
  "question" : "You have planned to host a web application on AWS.\nYou create an EC2 Instance in a public subnet that needs to connect to an EC2 Instance that will host an Oracle database.\nWhich steps would ensure a secure setup? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "806ac97062a54013a1be3fd92da36804",
    "option" : "Place the EC2 Instance with the Oracle database in the same public subnet as the Webserver for faster communication.",
    "isCorrect" : "false"
  }, {
    "id" : "987a59dbd884424f8e8f4e8cfec57d91",
    "option" : "Place the ec2 instance that will host the Oracle database in a private subnet.",
    "isCorrect" : "true"
  }, {
    "id" : "38503bb33ce549ddb7b3cfb2e44dac8e",
    "option" : "Create a database Security group which allows incoming traffic only from the Web server`s security group.",
    "isCorrect" : "true"
  }, {
    "id" : "c7a6ee4fb9344c438677cc688c40c208",
    "option" : "Ensure that the database security group allows incoming traffic from 0.0.0.0/0.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nThe best and most secure option is to place the database in a private subnet.\nThe below diagram from AWS Documentation shows this setup.\nAlso, you ensure that access is not allowed from all sources but only from the web servers.\nOption A is incorrect because DB instances are placed in Private subnets and allowed to communicate with web servers in the public subnet as per the best practice guidelines.\nOption D is incorrect because allowing all incoming traffic from the Internet to the DB instance is a security risk.\nFor more information on this type of setup, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\n\n\nWhen hosting a web application on AWS, it's essential to ensure that the setup is secure. One of the critical aspects of security is securing the connection between the web server and the database server. The following steps would ensure a secure setup:\nPlace the EC2 instance that will host the Oracle database in a private subnet:\nA private subnet is a subnet that doesn't have a route to the Internet Gateway. This means that instances in a private subnet can't be accessed directly from the internet, which provides an additional layer of security.\nCreate a database security group that allows incoming traffic only from the Webserver's security group:\nA security group acts as a virtual firewall for your instances, controlling inbound and outbound traffic. By creating a database security group that only allows incoming traffic from the web server's security group, you ensure that only the web server can communicate with the database server.\nTherefore, options B and C are the correct answers.\nOption A is incorrect because it's not recommended to place the database server and web server in the same public subnet. By doing so, you increase the attack surface because both servers are accessible from the internet.\nOption D is incorrect because allowing incoming traffic from 0.0.0.0/0 means that anyone can access the database server, which is not secure. You should restrict access to the database server to only the necessary entities.\n\n"
}, {
  "id" : 7,
  "question" : "Your company needs to develop an application that needs to have a login module in place.\nTheir key requirement is to ensure that users can also use their current identities with various providers such as Facebook to log into the application.\nWhich of the following can help you accomplish this?\n",
  "answers" : [ {
    "id" : "fcba8d8108c04401b872d9d0fec6b251",
    "option" : "Using the AWS Cognito service",
    "isCorrect" : "true"
  }, {
    "id" : "bc824ca5dcdc485b9faac7e343a30d6b",
    "option" : "Using the AWS Config service",
    "isCorrect" : "false"
  }, {
    "id" : "75ae5a318b434215a0c809fffeafec70",
    "option" : "Using the AWS SQS service",
    "isCorrect" : "false"
  }, {
    "id" : "6dde8aff0c7d47c7837df8317af02a3a",
    "option" : "Using the AWS WAF service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon Cognito provides authentication, authorization, and user management for your web and mobile apps.\nYour users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, or Google.\nThe two main components of Amazon Cognito are user pools and identity pools.\nUser pools are user directories that provide sign-up and sign-in options for your app users.\nIdentity pools enable you to grant your users access to other AWS services.\nYou can use identity pools and user pools separately or together.\nOption B is incorrect since this is a configuration service.\nOption C is incorrect since this is a messaging service.\nOption D is incorrect since this is a web application firewall service.\nFor more information on AWS Cognito, please refer to the below URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\n\nThe correct answer is A. Using the AWS Cognito service.\nAWS Cognito is a fully managed service that provides user authentication, registration, and user management for your web and mobile apps. It allows users to sign up and sign in using different identity providers such as Facebook, Google, and Amazon.\nCognito is designed to work with any application or service that supports OAuth 2.0 or OpenID Connect, which are industry-standard protocols for user authentication and authorization. With Cognito, you can add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily.\nAdditionally, Cognito provides other features such as user profile management, multi-factor authentication, and fine-grained access control, making it a comprehensive solution for user authentication and management.\nAWS Config is a service that enables you to assess, audit, and evaluate the configuration of your AWS resources. It does not provide any functionality related to user authentication and identity management.\nAWS SQS is a message queue service that enables decoupling and scaling of microservices and distributed systems. It does not provide any functionality related to user authentication and identity management.\nAWS WAF is a web application firewall that helps protect web applications from common web exploits. It does not provide any functionality related to user authentication and identity management.\nTherefore, the best option to accomplish the requirement of using various providers for user authentication is to use AWS Cognito.\n\n"
}, {
  "id" : 8,
  "question" : "A company currently hosts a Redshift cluster in AWS.\nIt should ensure that all traffic from and to the Redshift cluster does not go through the Internet for security reasons.\nWhich features can be used to fulfill this requirement in an efficient manner?\n",
  "answers" : [ {
    "id" : "80f735cbc9ce49ab853c529a13fd1239",
    "option" : "Enable Amazon Redshift Enhanced VPC Routing.",
    "isCorrect" : "true"
  }, {
    "id" : "278a292063ec4b32b2e746f8b068be11",
    "option" : "Create a NAT Gateway to route the traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "7a900b4f29da4c2eb6a48044efa45425",
    "option" : "Create a NAT Instance to route the traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "e8776cc1a40649088bacf18ed8ef401e",
    "option" : "Create a VPN Connection to ensure traffic does not flow through the Internet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following:\nWhen you use Amazon Redshift Enhanced VPC Routing, Amazon Redshift forces all COPY and UNLOAD traffic between your cluster and your data repositories through your Amazon VPC.If Enhanced VPC Routing is not enabled, Amazon Redshift routes traffic through the Internet, including traffic to other services within the AWS network.\nFor more information on Redshift Enhanced Routing, please visit the following URL:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/enhanced-vpc-routing.html\n\nThe correct answer is A. Enable Amazon Redshift Enhanced VPC Routing.\nExplanation:\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud. It is designed for data warehousing and analytics, and it can be used to store and analyze large amounts of data. When a Redshift cluster is created, it is deployed in an Amazon Virtual Private Cloud (VPC).\nIn this scenario, the requirement is to ensure that all traffic from and to the Redshift cluster does not go through the Internet for security reasons. There are a few options to achieve this:\nA. Enable Amazon Redshift Enhanced VPC Routing: Enhanced VPC Routing allows Redshift clusters to route traffic over private IP addresses within a VPC, instead of using public IP addresses over the Internet. This ensures that all traffic to and from the Redshift cluster stays within the VPC, providing an additional layer of security.\nB. Create a NAT Gateway to route the traffic: A NAT Gateway allows traffic from a private subnet to the Internet, and vice versa. However, this option requires traffic to pass through the Internet, which is not ideal for this scenario.\nC. Create a NAT Instance to route the traffic: Similar to a NAT Gateway, a NAT Instance allows traffic from a private subnet to the Internet, and vice versa. However, this option requires more management and maintenance overhead compared to a NAT Gateway.\nD. Create a VPN Connection to ensure traffic does not flow through the Internet: A VPN Connection allows traffic to travel securely over the Internet between a VPC and an on-premises location. However, this option still requires traffic to pass through the Internet, which is not ideal for this scenario.\nTherefore, the correct and most efficient option is to enable Amazon Redshift Enhanced VPC Routing to ensure that all traffic from and to the Redshift cluster does not go through the Internet for security reasons.\n\n"
}, {
  "id" : 9,
  "question" : "You have implemented AWS Cognito services to require users to sign in and sign up to your app through social identity providers like Facebook, Google, etc.\nYour marketing department wants users to anonymously try out the app because the current log-in requirement is excessive, which may reduce the demand for products and services offered through the app.\nWhat would you suggest to the marketing department in this regard?\n",
  "answers" : [ {
    "id" : "a68116f976334a92b42ae3eed8a58364",
    "option" : "Itâ€™s too much of a security risk to allow unauthenticated users access to the app.",
    "isCorrect" : "false"
  }, {
    "id" : "1e8db64cceb745cba0e1d6ad520b3b65",
    "option" : "Cognito Identity supports guest users for the ability to enter the app and have limited access.",
    "isCorrect" : "true"
  }, {
    "id" : "f3d47557c7e34b04a2e8caa4c6ca4564",
    "option" : "A second version of the app will need to be offered for unauthenticated users.",
    "isCorrect" : "false"
  }, {
    "id" : "bc256cb2f0914bff977b219f9f08441b",
    "option" : "This is possible only if we remove the authentication from everywhere.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption B is correct.\nAmazon Cognito Identity Pools can support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an identity provider.\nUnauthenticated users can be associated with a role with limited access to resources compared to a role for authenticated users.\nOption A is incorrect.\nCognito will allow unauthenticated users without being a security risk.\nOption C is incorrect.\nCognito supports both authenticated and unauthenticated users.\nReferences:\nhttps://aws.amazon.com/cognito/\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\nhttps://aws.amazon.com/cognito/getting-started/\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/concepts.html\n\nThe marketing department wants users to try out the app without requiring them to sign in or sign up, but you have implemented AWS Cognito services to require users to authenticate through social identity providers like Facebook, Google, etc.\nOption A states that it's too much of a security risk to allow unauthenticated users access to the app. This is generally true since allowing unauthenticated users to access your app may lead to data breaches or unauthorized access to your app's resources. Therefore, this option is not a viable solution for the marketing department's request.\nOption B states that Cognito Identity supports guest users for the ability to enter the app and have limited access. This option is correct because Cognito Identity provides the ability to create unauthenticated guest users who can access the app with limited permissions. You can set up specific policies to restrict access to certain resources or features of the app. This way, the marketing department can allow users to try out the app anonymously, but with limited access.\nOption C suggests offering a second version of the app for unauthenticated users. This option is not practical because it would require creating and maintaining two separate versions of the app, which would be costly and time-consuming.\nOption D suggests removing authentication from everywhere, which is not recommended as it would put the app at a significant security risk.\nIn summary, option B is the most suitable solution to the marketing department's request. It allows the anonymous use of the app while limiting access to certain features, maintaining the app's security, and avoiding the creation of a separate app version.\n\n"
}, {
  "id" : 10,
  "question" : "You have created an AWS Lambda function that will write data to a DynamoDB table.\nWhich of the following must be in place to ensure that the Lambda function can interact with the DynamoDB table?\n",
  "answers" : [ {
    "id" : "bbb3c6d8fb6444edbefbfedda87afd40",
    "option" : "Ensure an IAM Role is attached to the Lambda function which has the required DynamoDB privileges.",
    "isCorrect" : "true"
  }, {
    "id" : "f87e5b473e4241ab8c5d0aa0a34dad61",
    "option" : "Ensure an IAM User is attached to the Lambda function which has the required DynamoDB privileges.",
    "isCorrect" : "false"
  }, {
    "id" : "aef0abfd8d614f4b8d4d889744d3c87b",
    "option" : "Ensure the Access keys are embedded in the AWS Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "f18186e6d8b546cb89084d26f1802bda",
    "option" : "Ensure the IAM user password is embedded in the AWS Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following to support this requirement:\nEach Lambda function has an IAM role (execution role) associated with it.\nYou specify the IAM role when you create your Lambda function.\nPermissions you grant to this role determine what AWS Lambda can do when it assumes the role.\nThere are two types of permissions that you grant to the IAM role:\nIf your Lambda function code accesses other AWS resources, such as reading an object from an S3 bucket or writing logs to CloudWatch Logs, you need to grant permissions for relevant Amazon S3 and CloudWatch actions to the role.\nIf the event source is stream-based (Amazon Kinesis Data Streams and DynamoDB streams), AWS Lambda polls these streams on your behalf.\nAWS Lambda needs permissions to poll the stream and read new records on the stream.\nSo you need to grant the relevant permissions to this role.\nFor more information on the Permission Role model for AWS Lambda, please refer to the URL below:\nhttps://docs.aws.amazon.com/lambda/latest/dg/intro-permission-model.html\n\nThe correct answer is A. Ensure an IAM Role is attached to the Lambda function which has the required DynamoDB privileges.\nExplanation:\nWhen you create an AWS Lambda function that needs to interact with a DynamoDB table, you need to ensure that the Lambda function has the necessary permissions to access the DynamoDB table. The best practice is to use an IAM role instead of an IAM user or access keys.\nAn IAM role is an AWS Identity and Access Management (IAM) entity that defines a set of permissions for making AWS service requests. IAM roles do not have permanent credentials associated with them, and they are not associated with a specific user or group. Instead, permissions are granted to the role, and then the role is assumed by trusted entities, such as AWS services or IAM users. By using IAM roles, you can securely delegate access to AWS services and resources without sharing long-term access keys.\nTo give a Lambda function permissions to access a DynamoDB table, you need to create an IAM role with the appropriate permissions and attach the role to the Lambda function. The IAM role must have permissions to access the DynamoDB table, and the Lambda function must be granted permission to assume the IAM role.\nOption B, \"Ensure an IAM User is attached to the Lambda function which has the required DynamoDB privileges,\" is incorrect because IAM users should not be used for Lambda functions. IAM users are for humans and applications that require long-term access to AWS services.\nOption C, \"Ensure the Access keys are embedded in the AWS Lambda function,\" is incorrect because access keys are a security risk and should not be embedded in code. Access keys should be managed securely using AWS Identity and Access Management (IAM).\nOption D, \"Ensure the IAM user password is embedded in the AWS Lambda function,\" is incorrect because passwords should never be embedded in code. Instead, IAM users should be managed securely using AWS Identity and Access Management (IAM), and IAM roles should be used for granting permissions to AWS services and resources.\n\n"
}, {
  "id" : 11,
  "question" : "Your company is planning to use Route 53 as the DNS provider.\nThere is a need to ensure that the company's domain name points to an existing CloudFront distribution.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "b69088bd34924c53a84d8707b71aadde",
    "option" : "Create an Alias record which points to the CloudFront distribution.",
    "isCorrect" : "true"
  }, {
    "id" : "19d1f6b6e03349428cf70fb62c761b23",
    "option" : "Create a host record which points to the CloudFront distribution.",
    "isCorrect" : "false"
  }, {
    "id" : "fc3b8116d2454d65970e2214d2e0f71e",
    "option" : "Create a CNAME record which points to the CloudFront distribution.",
    "isCorrect" : "false"
  }, {
    "id" : "16dd645591c74466a23be15236090a65",
    "option" : "Create a Non-Alias Record which points to the CloudFront distribution.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following.\nWhile ordinary Amazon Route 53 records are standard DNS records, alias records provide a Route 53-specific extension to the DNS functionality.\nInstead of an IP address or a domain name, an alias record contains a pointer to a CloudFront distribution, an Elastic Beanstalk environment, an ELB Classic, Application, or Network Load Balancer, an Amazon S3 bucket that is configured as a static website, or another Route 53 record in the same hosted zone.\nWhen Route 53 receives a DNS query that matches the name and type in an alias record, Route 53 follows the pointer and responds with the applicable value.\nFor more information on Route 53 Alias records, please visit the following URL-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\nNote:\nRoute 53 uses \"Alias Name\" to connect to the CloudFront as Alias Record is a Route 53 extension to DNS.Also, an Alias record is similar to a CNAME record, but the main difference is - you can create an Alias record for both root domain &amp; subdomain.\nIn contrast, a CNAME record can be created only to subdomain.\nCheck the below link to get more information:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html\n\nThe correct answer is A. Create an Alias record which points to the CloudFront distribution.\nExplanation:\nAmazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. DNS is used to convert human-friendly domain names into IP addresses that computers use to identify each other on the internet.\nCloudFront is a content delivery network (CDN) that distributes data to multiple edge locations worldwide, providing low-latency performance and high data transfer speeds.\nTo ensure that a company's domain name points to an existing CloudFront distribution using Route 53, an Alias record should be created. Alias records are used to map a domain name to an Amazon resource, such as an Amazon S3 bucket, an Amazon EC2 instance, an Elastic Load Balancer, or a CloudFront distribution.\nAn Alias record provides the following benefits:\nIt is an Amazon Route 53-specific feature that allows you to map a DNS record to an AWS resource using its DNS name. It automatically routes traffic to the nearest CloudFront distribution edge location, based on the geographic location of the user requesting the content. It provides seamless failover to a backup CloudFront distribution in the event of an outage.\nTherefore, to ensure that a company's domain name points to an existing CloudFront distribution using Route 53, an Alias record should be created, which points to the CloudFront distribution. This ensures that the domain name resolves to the correct IP address and improves the performance and reliability of the website or application.\nOption B, creating a host record, is incorrect as it does not exist as a type of Route 53 record.\nOption C, creating a CNAME record, is also incorrect because CNAME records are used to create an alias of one domain name to another, but they cannot be used to point to a CloudFront distribution directly.\nOption D, creating a Non-Alias record, is also incorrect because Non-Alias records are used to point to a fixed IP address and are not recommended for use with AWS resources.\n\n"
}, {
  "id" : 12,
  "question" : "You are building an automated transcription service where Amazon EC2 worker instances process an uploaded audio file and generate a text file.\nYou must store both of these files in the same durable storage until the text file is retrieved.\nCustomers fetch the text files frequently.\nYou do not know about the storage capacity requirements.\nWhich storage option would be both cost-efficient and highly available in this situation?\n",
  "answers" : [ {
    "id" : "3e104c12137847338d2d02d50761626b",
    "option" : "Multiple Amazon EBS Volume with snapshots",
    "isCorrect" : "false"
  }, {
    "id" : "3d617eac98dd49afa7bc4d54288162c3",
    "option" : "A single Amazon Glacier Vault",
    "isCorrect" : "false"
  }, {
    "id" : "2515c0a47ba4467db296947e5f17086c",
    "option" : "A single Amazon S3 bucket",
    "isCorrect" : "true"
  }, {
    "id" : "2742f90c163c44f89fc8b08aa0af534e",
    "option" : "Multiple instance stores.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon S3 is the perfect storage solution for audio and text files.\nIt is a highly available and durable storage device.\nOption A is incorrect because storing files in EBS is not cost-efficient.\nOption B is incorrect because files need to be retrieved frequently so Glacier is not suitable.\nOption D is incorrect because instance store is not highly available compared with S3.\nFor more information on Amazon S3, please visit the following URL:\nhttps://aws.amazon.com/s3/\n\nThe most suitable storage option for this use case would be a single Amazon S3 bucket (option C).\nExplanation:\nAmazon S3 (Simple Storage Service) is a highly scalable, durable, and secure object storage service that can store and retrieve any amount of data from anywhere on the web. It provides a simple web services interface that can be used to store and retrieve any amount of data, at any time, from anywhere on the web.\nIn this use case, the audio and text files need to be stored in the same durable storage until the text file is retrieved. Amazon S3 provides highly durable and highly available storage for objects, with automatic and unlimited scalability. It is also designed to be accessed frequently and provides low latency retrieval, making it ideal for this use case where customers frequently fetch the text files.\nOption A, Multiple Amazon EBS Volume with snapshots, would not be cost-efficient as it requires multiple EBS volumes, each with snapshots. Additionally, EBS volumes are not highly available by default, so additional configurations would need to be made to ensure high availability.\nOption B, A single Amazon Glacier Vault, would also not be suitable for this use case because Glacier is designed for long-term storage of infrequently accessed data. Retrieving data from Glacier can take several hours, which is not suitable for this use case where customers frequently access the text files.\nOption D, Multiple instance stores, is not a durable storage option as instance stores are ephemeral, meaning that the data stored on them is lost when the instance is stopped or terminated. This would not be suitable for a use case where data needs to be stored until the text file is retrieved.\nTherefore, option C, a single Amazon S3 bucket, is the most suitable and cost-efficient option for this use case.\n\n"
}, {
  "id" : 13,
  "question" : "You are deploying an application to track the GPS coordinates of delivery trucks in the United States.\nCoordinates are transmitted from each delivery truck once every three seconds.\nYou need to design an architecture that will enable real-time processing of these coordinates from multiple consumers.\nWhich service should you use to implement data ingestion?\n",
  "answers" : [ {
    "id" : "0752cb091eba4d43b527afc9c5600bfc",
    "option" : "Amazon Kinesis",
    "isCorrect" : "true"
  }, {
    "id" : "4a1c4a2af04f43619a5fcc0d78c68dc2",
    "option" : "AWS Data Pipeline",
    "isCorrect" : "false"
  }, {
    "id" : "eb28f3e03ac947b5ac06bd2da466d6da",
    "option" : "Amazon AppStream",
    "isCorrect" : "false"
  }, {
    "id" : "3f3645fe0c79461ab10096fce204ec99",
    "option" : "Amazon Simple Queue Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS documentation mentions the following:\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to process streaming data cost-effectively at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications.\nAmazon Kinesis enables you to process and analyze data as it arrives and responds instantly instead of waiting until all your data is collected before the processing can begin.\nFor more information on Amazon Kinesis, please visit the following URL:\nhttps://aws.amazon.com/kinesis/\n\nFor the given scenario, the most suitable service to implement data ingestion for real-time processing of GPS coordinates is Amazon Kinesis (Option A).\nAmazon Kinesis is a fully managed service by AWS that allows you to collect, process, and analyze real-time, streaming data such as video, audio, application logs, website clickstreams, and IoT telemetry data. It is highly scalable, reliable, and can handle high-volume, high-velocity, and high-variety data streams.\nIn this scenario, the GPS coordinates are transmitted from each delivery truck once every three seconds, which indicates a high-velocity data stream. Amazon Kinesis can handle this data ingestion and process it in real-time, allowing multiple consumers to consume the data simultaneously.\nAdditionally, Amazon Kinesis provides various ingestion methods such as Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics, which can be used to capture, process, and analyze the data.\nAWS Data Pipeline (Option B) is a web service that allows you to automate the movement and transformation of data. It is used to move data between different AWS services, on-premises data sources, and third-party data sources. However, it is not a suitable service for real-time data ingestion and processing.\nAmazon AppStream (Option C) is a fully managed application streaming service that allows you to securely stream desktop applications to any device. It is not suitable for data ingestion and processing.\nAmazon Simple Queue Service (Option D) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. However, it is not suitable for real-time data ingestion and processing as it is designed for asynchronous message passing between distributed systems.\nTherefore, the correct answer is Option A: Amazon Kinesis.\n\n"
}, {
  "id" : 14,
  "question" : "Your current architecture consists of a set of web servers spun up as part of an Autoscaling group.\nThese web servers then communicate with a set of database servers.\nYou need to ensure that the database servers' security groups are set properly to accept traffic from the web servers.\nWhich of the following is the best way to accomplish this?\n",
  "answers" : [ {
    "id" : "629d3ec9a31b4ea3a6671d088f4ef4fb",
    "option" : "Ensure that the Private IP addresses of the web servers are put as sources for the incoming rules in the database server security group.",
    "isCorrect" : "false"
  }, {
    "id" : "0c4bb6ed01ac4beaac6bad2d9e665e73",
    "option" : "Ensure that the Public IP addresses of the web servers are put as sources for the incoming rules in the database server security group.",
    "isCorrect" : "false"
  }, {
    "id" : "140e594b231049c3ab853d4a1969ede0",
    "option" : "Ensure that the web server security group is placed as the source for the incoming rules in the database server security group.",
    "isCorrect" : "true"
  }, {
    "id" : "6d227e239a804939982563594432a541",
    "option" : "Ensure that the Instance ID of the web servers are put as sources for the incoming rules in the database server security group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe below example from the AWS Documentation also shows the Source of the database security group involving the web server security groups' ID.Options A and B are invalid or not the best practice.\nSince they are part of the Autoscaling Group, the IP addresses of the instances can change.\nOption D is incorrect since normally you don't specify the Instance ID in Security Groups.\nFor more information on the Security Groups for the VPC, please refer to the below URL-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\n\n\nIn this scenario, you have a set of web servers that are part of an Auto Scaling group and communicate with a set of database servers. The task is to ensure that the database servers' security groups are set properly to accept traffic from the web servers.\nTo achieve this, we need to understand the concept of security groups in AWS. A security group acts as a virtual firewall that controls inbound and outbound traffic for one or more instances. In AWS, security groups are stateful, which means that any traffic that is allowed to flow in is automatically allowed to flow out.\nNow let's analyze the given options:\nA. Ensure that the Private IP addresses of the web servers are put as sources for the incoming rules in the database server security group.\nThis option is not a good approach because the IP addresses of the instances within an Auto Scaling group may change dynamically, making it difficult to maintain the security group rules.\nB. Ensure that the Public IP addresses of the web servers are put as sources for the incoming rules in the database server security group.\nThis option is also not a good approach because the public IP addresses of the web servers can change each time the instances are launched, and using public IP addresses is less secure.\nC. Ensure that the web server security group is placed as the source for the incoming rules in the database server security group.\nThis option is the correct approach. By placing the web server security group as the source for the incoming rules in the database server security group, you allow any instance that is part of the web server security group to communicate with the database server. This is a better approach because it is based on the group membership and is independent of the IP addresses of the instances.\nD. Ensure that the Instance ID of the web servers are put as sources for the incoming rules in the database server security group.\nThis option is not a good approach because the instance IDs can also change when the Auto Scaling group scales up or down.\nIn conclusion, the best way to accomplish this task is to ensure that the web server security group is placed as the source for the incoming rules in the database server security group.\n\n"
}, {
  "id" : 15,
  "question" : "While managing permissions for the API Gateway, what could be used to ensure that the right level of permissions is given to Developers, IT Admins, and users? Also, the permissions should be easily managed.\n",
  "answers" : [ {
    "id" : "6798380c2ad44b248d5f674aa74277d2",
    "option" : "Use the secure token service to manage the permissions for different users.",
    "isCorrect" : "false"
  }, {
    "id" : "ea08292608a2448d990b2e8d9edd981b",
    "option" : "Use IAM Policies to create different policies for different types of users.",
    "isCorrect" : "true"
  }, {
    "id" : "bf42ab0bd9434259a65dfe1772054790",
    "option" : "Use the AWS Config tool to manage the permissions for different users.",
    "isCorrect" : "false"
  }, {
    "id" : "6c09f1280c93479799130e6559407f83",
    "option" : "Use IAM Access Keys to create sets of keys for different types of users.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following:\nYou control access to Amazon API Gateway with IAM permissions by controlling access to the following two API Gateway component processes.\nTo create, deploy, and manage an API in API Gateway, you must grant the API developer permissions to perform the required actions supported by the API management component of API Gateway.\nTo call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway.\nFor more information on permissions with the API Gateway, please visit the following URL:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html\n\nWhen managing permissions for API Gateway, the appropriate level of access should be granted to different users such as Developers, IT Admins, and users. The permissions should also be easy to manage.\nAmong the given options, the best approach for managing permissions for API Gateway is to use IAM policies (Option B). IAM policies are a powerful and flexible way to grant permissions to different users or groups. IAM policies define the actions that can be taken on a particular resource, such as an API Gateway, and the conditions under which those actions can be taken.\nIAM policies can be assigned to individual IAM users or groups, and they can be easily updated or revoked as needed. For example, you can create an IAM policy that allows a developer to create, update, and delete API Gateway resources but restricts access to only a certain API Gateway stage. Another policy could be created for IT admins to allow them to modify or delete any API Gateway resource. IAM policies can be customized to fit the needs of different types of users and can be easily managed through the AWS Management Console or the AWS CLI.\nUsing the Secure Token Service (Option A) is not appropriate for managing permissions for API Gateway as it is primarily used to manage temporary credentials for accessing AWS services. It can be used to authenticate and authorize API Gateway clients, but it does not provide fine-grained access control over API Gateway resources.\nAWS Config (Option C) is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It is not a tool for managing permissions for different users.\nIAM Access Keys (Option D) are used to provide programmatic access to AWS resources. They are used to authenticate requests to AWS services made using the AWS CLI or SDKs. While access keys can be used to restrict access to particular API Gateway resources, they do not provide the same fine-grained control over access as IAM policies. Furthermore, access keys are not designed to manage permissions for different types of users, making them an unsuitable choice for this scenario.\n\n"
}, {
  "id" : 16,
  "question" : "A global sports news company has hosted its website on Amazon EC2 instance using a single Public IP address &amp; is front-ended by TLS-enabled Application Load Balancer.\nFor an upcoming mega sports event, they plan to launch a new website on the existing Amazon EC2 instance.\nThe company has registered a different domain name &amp; possesses a separate TLS certificate for this new website. As an AWS consultant to this company, which of the following recommendations will you provide to support multiple certificates with existing Public IP addresses in the most cost-effective way?\n",
  "answers" : [ {
    "id" : "235d5f2d3ff34cfba123b453e4974f67",
    "option" : "Launch an additional TLS-enabled ALB front ending Amazon EC2 instance with different certificates for each domain.",
    "isCorrect" : "false"
  }, {
    "id" : "7588bf207dec42c9a47c02db97a9b486",
    "option" : "Use Wildcard certificates on ALB matching old &amp; new domain name.",
    "isCorrect" : "false"
  }, {
    "id" : "4664e0c0b11343bdbf2da2af6c711c6b",
    "option" : "Use a single certificate on ALB &amp; add Subject Alternative Name (SAN) for additional domain name.",
    "isCorrect" : "false"
  }, {
    "id" : "7a606362cd3b43d4b88653151b2f1174",
    "option" : "Use multiple TLS certificates on ALB using Server Name Indication (SNI).",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nALB supports Server Name Indication (SNI), enabling hosting multiple domain names with different TLS certificates behind a single ALB.\nWith SNI, multiple certificates can be associated with listeners in ALB, enabling each web application to use separate certificates.\nThe below diagrams shows the process which takes place when a client tries to access a website.\nClient Browser starts a TLS handshake by sending a ClientHello message which consists of protocol version, extensions, cipher suites, and compression techniques.\nBased on browser capabilities, ALB responds with a valid certificate for a domain name of the requested web application.\nOption A is incorrect as launching additional ALB will work.\nBut it will incur additional cost &amp; admin work for setup.\nOption B is incorrect as using wildcard certificates can be used for related sub-domains &amp; not different domains.\nOption C is incorrect as using SAN certificates, for any new addition of domain, all certificates need to revalidate with certificate authority.\nFor more information on Amazon ALB SNI support, refer to the following URL-\nhttps://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/\n\n\nAs the sports news company plans to launch a new website with a different domain name and a separate TLS certificate on an existing Amazon EC2 instance, the company needs to support multiple certificates with existing public IP addresses in a cost-effective way. The options available to achieve this are:\nA. Launch an additional TLS-enabled ALB front-ending Amazon EC2 instance with different certificates for each domain. This option requires launching a new Amazon EC2 instance and an Application Load Balancer (ALB) with a separate TLS certificate for the new website. However, this option will increase the cost of hosting as a new EC2 instance and ALB will be needed, leading to higher operational costs.\nB. Use Wildcard certificates on ALB matching old and new domain names. This option involves using a single Wildcard certificate that covers both the existing and new domain names. This option requires only one certificate to be managed, and thus it reduces the operational costs. However, there are potential security risks associated with the use of wildcard certificates, as it may expose all subdomains to the same level of risk.\nC. Use a single certificate on ALB and add Subject Alternative Name (SAN) for additional domain name. This option involves using a single TLS certificate with a Subject Alternative Name (SAN) extension that includes both the existing and new domain names. This option requires only one certificate to be managed, and it also reduces operational costs. However, not all TLS clients support SAN extensions, and therefore this option may not work with older browsers and clients.\nD. Use multiple TLS certificates on ALB using Server Name Indication (SNI). This option involves using multiple TLS certificates on a single ALB using Server Name Indication (SNI). SNI allows the ALB to serve multiple SSL certificates for different domain names using the same IP address. This option is cost-effective as it requires only one EC2 instance and one ALB, and it provides strong security as it does not use wildcard certificates. However, not all clients support SNI, and this option may not work with older browsers and clients.\nIn conclusion, the most cost-effective option that also provides strong security is to use multiple TLS certificates on ALB using Server Name Indication (SNI).\n\n"
}, {
  "id" : 17,
  "question" : "You work in a large organization.\nYour team creates AWS resources such as Amazon EC2 dedicated hosts and reserved capacities that need to be shared by other AWS accounts.\nYou need an AWS service to centrally manage these resources so that you can easily specify which accounts or Organizations can access the resources.\nWhich AWS service would you choose to meet this requirement?\n",
  "answers" : [ {
    "id" : "996fa31c2ed64e6eb7ec3a5fe4e762f7",
    "option" : "IAM",
    "isCorrect" : "false"
  }, {
    "id" : "7da8359664084bfb88e43a7a8a68e4f4",
    "option" : "Resource Access Manager",
    "isCorrect" : "true"
  }, {
    "id" : "bf556b480aff4369a73ca67f5e1fbd96",
    "option" : "Service Catalog",
    "isCorrect" : "false"
  }, {
    "id" : "7d424b2378ce4bda83a1e04afa348c77",
    "option" : "AWS Single Sign-On.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Resource Access Manager (AWS RAM) helps users to share resources with other AWS accounts or Organizations.\nRefer to the reference in https://docs.aws.amazon.com/ram/latest/userguide/what-is.html.\nOption A is incorrect: Because IAM cannot be used to manage and share these resources.\nOption B is CORRECT: EC2 dedicated hosts and reserved capacities are shareable resources that are supported by Resource Access Manager.\nCheck the reference in https://docs.aws.amazon.com/ram/latest/userguide/shareable.html.\nOption C is incorrect: Because Service Catalog is used to manage catalogs and cannot share resources with others.\nOption D is incorrect: Because AWS Single Sign-On is used for SSO access and does not share the mentioned resources.\n\nThe AWS service that allows you to centrally manage shared resources and specify access to them is Resource Access Manager (RAM). Therefore, the correct answer is B.\nResource Access Manager (RAM) is an AWS service that allows you to share your resources such as Amazon EC2 dedicated hosts, Amazon RDS databases, and Amazon Redshift clusters with other AWS accounts or AWS Organizations. RAM enables you to create resource shares that define the resources you want to share and the AWS accounts or AWS Organizations with which you want to share them.\nIAM (A) is an AWS service that enables you to manage access to your AWS resources. However, IAM is not designed to manage shared resources across multiple AWS accounts or AWS Organizations.\nService Catalog (C) is an AWS service that enables you to create and manage catalogs of IT services that are approved for use on AWS. Service Catalog is not designed to manage shared resources across multiple AWS accounts or AWS Organizations.\nAWS Single Sign-On (D) is an AWS service that provides a central location to manage single sign-on access to multiple AWS accounts and business applications. While it enables centralized access management, AWS SSO is not designed to manage shared resources across multiple AWS accounts or AWS Organizations.\nIn summary, Resource Access Manager (RAM) is the correct choice to centrally manage shared resources across multiple AWS accounts or AWS Organizations.\n\n"
}, {
  "id" : 18,
  "question" : "A company is planning to host an active-active site.\nOne site will be deployed in AWS, and the other one on their On-premise data center.\nThey need to ensure that the traffic is distributed to multiple resources, proportionately between both sites.\nWhich of the following routing policy would you use for this purpose?\n",
  "answers" : [ {
    "id" : "802467d5ae4645f39674d140fd70214f",
    "option" : "Simple Routing",
    "isCorrect" : "false"
  }, {
    "id" : "a8a723069a844fc89a7e032e0018de63",
    "option" : "Failover Routing",
    "isCorrect" : "false"
  }, {
    "id" : "ae844a02115b46478ea013327aef4456",
    "option" : "Latency Routing",
    "isCorrect" : "false"
  }, {
    "id" : "1fa42df0f9874adc863bb0f8c81772e5",
    "option" : "Weighted Routing.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nWeighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource.\nThis can be useful for various purposes, including load balancing and testing new versions of software.\nTo configure weighted routing, you create records with the same name and type for each of your resources.\nYou assign each record a relative weight that corresponds with how much traffic you want to send to each resource.\nAmazon Route 53 sends traffic to a resource based on the weight you assign to the record as a proportion of the total weight for all the group records.\nOption A is incorrect since this should be used when you want to configure standard DNS records.\nOption B is incorrect since this should be used when you want to route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy.\nOption C is incorrect since this should be used when you want to improve your users' performance by serving their requests from the AWS Region that provides the lowest latency.\nFor more information on a Routing policy, please refer to the below URL-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nFor this scenario, we need a routing policy that can distribute the traffic to both sites proportionately. A routing policy is a configuration that determines how Amazon Route 53 responds to DNS queries.\nThe routing policy that best suits this scenario is Weighted Routing, which allows distributing traffic to multiple resources in a weighted manner based on the specified value.\nWith Weighted Routing, we can create multiple records with the same name and associate them with different IP addresses of the resources in different locations. Then we can assign a weight to each record. The weight determines the proportion of traffic that goes to each resource.\nFor instance, if we have two resources, one in AWS and one in the On-premise data center, we can assign each resource a weight of 50%. Then, when a user requests access to the site, Amazon Route 53 will return both IP addresses, and 50% of the traffic will go to each resource.\nTherefore, the correct answer is D. Weighted Routing.\n\n"
}, {
  "id" : 19,
  "question" : "You are planning to use Auto Scaling groups to maintain the performance of your web application.\nHow would you ensure that the scaling activity has sufficient time to stabilize without executing another scaling action?\n",
  "answers" : [ {
    "id" : "1ba7dc5c56294b56958c23fe12838052",
    "option" : "Modify the Instance User Data property with a timeout interval.",
    "isCorrect" : "false"
  }, {
    "id" : "a1eeabdbbc76431891a51e93cde63cf2",
    "option" : "Increase the Auto Scaling Cooldown timer value.",
    "isCorrect" : "true"
  }, {
    "id" : "3a3e051fc3de4f71b5b193351ba86c8d",
    "option" : "Enable the Auto Scaling cross zone balancing feature.",
    "isCorrect" : "false"
  }, {
    "id" : "3f0192136c9147659211eee31a644bb3",
    "option" : "Disable CloudWatch alarms till the application stabilizes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nThe Cooldown period is a configurable setting for your Auto Scaling group, ensuring that it doesn't launch or terminate additional instances before the previous scaling activity takes effect.\nAfter the Auto Scaling group dynamically scales using a simple Scaling Policy, it waits for the Cooldown period to complete before resuming scaling activities.\nFor more information on Auto Scaling Cooldown, please visit the following URL-\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/Cooldown.html\n\nWhen using Auto Scaling groups to maintain the performance of a web application, it is important to ensure that the scaling activity has sufficient time to stabilize before executing another scaling action. This is necessary to avoid a cycle of rapid scaling actions that can lead to instability in the application.\nThe following are the options provided for ensuring that scaling activity has sufficient time to stabilize without executing another scaling action:\nA. Modify the Instance User Data property with a timeout interval: This option is incorrect as the Instance User Data property is used to pass data to instances when they are launched. It does not have any impact on the Auto Scaling cooldown timer or the scaling activity.\nB. Increase the Auto Scaling Cooldown timer value: This is the correct option as the Auto Scaling cooldown timer determines the amount of time that Auto Scaling waits before executing another scaling action. By increasing the cooldown timer value, we can ensure that sufficient time is given for the scaling activity to stabilize before executing another scaling action.\nC. Enable the Auto Scaling cross-zone balancing feature: This option is incorrect as the Auto Scaling cross-zone balancing feature is used to distribute instances evenly across multiple Availability Zones. It does not have any impact on the Auto Scaling cooldown timer or the scaling activity.\nD. Disable CloudWatch alarms till the application stabilizes: This option is incorrect as disabling CloudWatch alarms will not provide sufficient time for the scaling activity to stabilize. CloudWatch alarms are used to trigger scaling actions based on predefined thresholds. Disabling the alarms will prevent scaling actions from being triggered, but it will not provide sufficient time for the scaling activity to stabilize.\nIn conclusion, the correct option for ensuring that scaling activity has sufficient time to stabilize without executing another scaling action is to increase the Auto Scaling cooldown timer value.\n\n"
}, {
  "id" : 20,
  "question" : "You have 2 development environments hosted in 2 different VPCs in an AWS account in the same region.\nThere is now a requirement to access the resources of one VPC from another.\nHow could this be accomplished?\n",
  "answers" : [ {
    "id" : "694876976fee4547b43e54deacb4c5e9",
    "option" : "Establish a Direct Connect connection.",
    "isCorrect" : "false"
  }, {
    "id" : "984bdd39f2f8438c8611b1298d89988e",
    "option" : "Establish a VPN connection.",
    "isCorrect" : "false"
  }, {
    "id" : "9e5490ebc10442d1942b0f073bcb7084",
    "option" : "Establish VPC Peering.",
    "isCorrect" : "true"
  }, {
    "id" : "880ce4d770904cd290bf2e4687b843ca",
    "option" : "Establish Subnet Peering.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the following.\nA VPC peering connection is a networking connection between two VPCs that enable you to route traffic between them privately.\nInstances in either VPC can communicate with each other as if they are within the same network.\nYou can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.\nFor more information on VPC peering, please visit the URL below.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html\n\nTo enable access between two VPCs, there are several methods available in AWS. Here are the details on each of the proposed options:\nA. Establish a Direct Connect connection: AWS Direct Connect is a dedicated network connection service that provides a private, high-speed, and secure connection between an on-premises data center and AWS cloud services. This method can be used to connect two VPCs in different accounts or regions as well, but it can be an expensive solution and requires dedicated network equipment to be installed on-premises.\nB. Establish a VPN connection: AWS Virtual Private Network (VPN) allows you to connect your VPCs to your on-premises network securely over the internet. VPNs can be established between two VPCs in different accounts, regions, or even between a VPC and an on-premises network. This method is less expensive than Direct Connect but might have some limitations in terms of bandwidth and network latency.\nC. Establish VPC Peering: VPC peering allows you to connect two VPCs within the same AWS account or different AWS accounts within the same region, using private IP addresses. VPC peering is a cost-effective and easy-to-set-up solution for connecting VPCs within the same region. However, it cannot be used to connect VPCs in different regions, and there are some limitations in terms of routing, security, and IP address ranges.\nD. Establish Subnet Peering: Subnet Peering allows you to connect two subnets in different VPCs, which can be within the same AWS account or different AWS accounts within the same region, using private IP addresses. Subnet peering is a cost-effective solution and can be used to connect specific resources in different VPCs, instead of the entire VPCs. However, it cannot be used to connect subnets in different regions, and there are some limitations in terms of routing and security.\nTherefore, in the given scenario, the best solution for connecting two VPCs hosted in the same AWS account and region would be to establish VPC peering (option C). It is cost-effective, easy to set up, and provides a private, secure, and direct connection between VPCs, allowing resources in both VPCs to communicate with each other using private IP addresses.\n\n"
}, {
  "id" : 21,
  "question" : "Your company is planning to store sensitive documents in an S3 bucket.\nThey want to keep the documents private but serve content only to selected users based on a particular time frame.\nWhich of the following can help you accomplish this?\n",
  "answers" : [ {
    "id" : "e7cd3082aa954073aeac424a5d5f3598",
    "option" : "Enable CORS for the S3 bucket",
    "isCorrect" : "false"
  }, {
    "id" : "c9614cb160d44d5099d42e4bab3eb4fc",
    "option" : "Use KMS and enable encryption for the files",
    "isCorrect" : "false"
  }, {
    "id" : "70e8a9cfcfbc4c9dbaefe4e55acf3f5f",
    "option" : "Create pre-signed URLâ€™s",
    "isCorrect" : "true"
  }, {
    "id" : "87b362986a924459b99eb5eb0793fd2e",
    "option" : "Enable versioning for the S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nA pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permissions to access that object.\nThat is, if you receive a pre-signed URL to upload an object, you can upload the object only if the creator of the pre-signed URL has the necessary permissions to upload that object.\nAll objects and buckets by default are private.\nThe pre-signed URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don't require them to have AWS security credentials or permissions.\nWhen you create a pre-signed URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time.\nThe pre-signed URLs are valid only for the specified duration.\nOption A is incorrect since this is used for Cross-origin access.\nOption B is incorrect since this is used for encryption purposes.\nOption D is incorrect since this is used for versioning.\nFor more information on pre-signed URL's, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\n\nThe correct answer to this question is C. Create pre-signed URLs.\nPre-signed URLs provide a secure way to share private content without exposing them to the public. With pre-signed URLs, you can generate a URL that provides time-limited access to a private S3 object. You can also specify the user or group of users who can access the object by providing AWS IAM user credentials or by creating a special URL for a specific AWS IAM role.\nTo generate pre-signed URLs, you can use the AWS SDKs, AWS CLI, or the AWS Management Console. When generating pre-signed URLs, you can specify an expiration time, after which the URL will no longer be valid. You can also specify other parameters, such as the HTTP method (GET, PUT, etc.) and the IP address range of the client who will access the object.\nCORS (Cross-Origin Resource Sharing) is used to allow web applications to access resources from a different domain. Enabling CORS for an S3 bucket does not provide any additional security or control over who can access the objects in the bucket.\nKMS (Key Management Service) is a service that enables you to create and manage cryptographic keys that can be used to encrypt and decrypt data. While encrypting sensitive documents using KMS is a good security practice, it does not provide a way to control access to the objects based on a particular time frame.\nEnabling versioning for an S3 bucket allows you to store multiple versions of an object in the same bucket. It does not provide a way to control access to the objects based on a particular time frame.\nIn summary, pre-signed URLs are the best option for controlling access to private S3 objects based on a particular time frame.\n\n"
}, {
  "id" : 22,
  "question" : "You have a requirement to get a snapshot of the current configuration of resources in your AWS Account.\nWhich service can be used for this purpose?\n",
  "answers" : [ {
    "id" : "7c54ea534f984469a92eb6d6a382cda4",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "false"
  }, {
    "id" : "f7647ce2caa746f1acd389d342675d8a",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "345e121a4feb4605bb770d3b3a455ba0",
    "option" : "AWS Config",
    "isCorrect" : "true"
  }, {
    "id" : "f9ee184e09d24cb996f5a7cdbf9a6cbe",
    "option" : "AWS IAM.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the following.\nWith AWS Config, you can do the following.\nEvaluate your AWS resource configurations for desired settings.\nGet a snapshot of the current configurations of the supported resources that are associated with your AWS account.\nRetrieve configurations of one or more resources that exist in your account.\nRetrieve historical configurations of one or more resources.\nReceive a notification whenever a resource is created, modified or deleted.\nView relationships between resources.\nFor example, you might want to find all resources that use a particular security group.\nFor more information on AWS Config, please visit the below URL-\nhttp://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\n\nThe correct answer is C. AWS Config.\nAWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. It continuously tracks changes to resources and allows you to assess the impact of those changes on your resources and compliance with your organization's policies.\nAWS Config can be used to automate the evaluation of recorded configurations against desired configurations and to monitor resource changes over time. It also allows you to take remedial actions if needed.\nAWS CodeDeploy is a service that automates code deployments to any instance, including EC2 instances and on-premises servers. It is not used for configuration management or for providing a snapshot of the current configuration of resources in your AWS account.\nAWS Trusted Advisor is a service that inspects your AWS environment and provides recommendations on cost optimization, security, fault tolerance, and performance improvement. It is not designed to provide a snapshot of the current configuration of resources in your AWS account.\nAWS IAM is a service that enables you to manage access to AWS services and resources securely. It is not used for configuration management or for providing a snapshot of the current configuration of resources in your AWS account.\n\n"
}, {
  "id" : 23,
  "question" : "You work for a company that has a set of EC2 Instances.\nThere is an internal requirement to create another instance in another availability zone.\nOne of the EBS volumes from the current instance needs to be moved from one of the older instances to the new instance.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "d066497ef83344d896653a1e74cd63a0",
    "option" : "Detach the volume and attach to an EC2 instance in another AZ.",
    "isCorrect" : "false"
  }, {
    "id" : "cc50535856f94c90929ed2b12fb3ecab",
    "option" : "Create a new volume in the other AZ and specify the current volume as the source.",
    "isCorrect" : "false"
  }, {
    "id" : "033cb49d7ac74f3eaa202c85f9263723",
    "option" : "Create a snapshot of the volume and then create a volume from the snapshot in the other AZ",
    "isCorrect" : "true"
  }, {
    "id" : "2a878f5b43c246aba32fef196de4f959",
    "option" : "Create a new volume in the AZ and do a disk copy of contents from one volume to another.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nFor a volume to be available in another availability zone, you need to create a snapshot from the volume.\nThen in the snapshot from creating a volume, you can then specify the new availability zone accordingly.\nThe EBS Volumes attached to the EC2 Instance will always have to remain in the same availability zone as the EC2 Instance.\nA possible reason for this could be because EBS Volumes are present outside of the host machine, and instances have to be connected over the network.\nIf the EBS Volumes are present outside the Availability Zone, there can be potential latency issues and subsequent performance degradation.\nWhat one can do in such a scenario is to get the Snapshot of the EBS Volume.\nSnapshot sequentially captures the state of your EBS Volume.\nYou can create an EBS Volume from this snapshot in your desired Availability Zone and attach it to your new Instance.\nLater you can detach the volume from the older instance and delete then.\nOption A is invalid because the Instance and Volume have to be in the same AZ to be attached to the instance.\nAfter all, we have to specify AZ while creating Volume.\nOption B is invalid because there is no way to specify a volume as a source.\nOption D is invalid because the Diskcopy would be a tedious process.\nFor more information on snapshots, please visit the below URLs-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ebs-detaching-volume.html\n\n\nSure, I can provide you with a detailed explanation of how to achieve the given task.\nTo move an EBS volume from one EC2 instance in one availability zone to another EC2 instance in a different availability zone, you can follow any of the below options:\nOption A: Detach the volume and attach to an EC2 instance in another AZ.\nStop the current EC2 instance to which the EBS volume is attached. Go to the Amazon EC2 console, select the EBS volume you want to move, and choose \"Detach Volume\" from the \"Actions\" dropdown menu. After detaching the volume, go to the EC2 console in the other availability zone where you want to move the volume and choose \"Attach Volume\" from the \"Actions\" dropdown menu. Specify the instance ID of the EC2 instance in the other availability zone to which you want to attach the volume. Mount the EBS volume on the new EC2 instance as required.\nOption B: Create a new volume in the other AZ and specify the current volume as the source.\nGo to the Amazon EC2 console, select the EBS volume you want to move, and choose \"Create Snapshot\" from the \"Actions\" dropdown menu. Once the snapshot is completed, go to the EC2 console in the other availability zone where you want to move the volume and choose \"Create Volume\" from the \"Actions\" dropdown menu. Specify the snapshot you created in the previous step as the source for the new volume. After creating the new volume, go to the EC2 console in the availability zone where you want to move the volume, and attach the new volume to the EC2 instance. Mount the EBS volume on the new EC2 instance as required.\nOption C: Create a snapshot of the volume and then create a volume from the snapshot in the other AZ.\nGo to the Amazon EC2 console, select the EBS volume you want to move, and choose \"Create Snapshot\" from the \"Actions\" dropdown menu. Once the snapshot is completed, go to the EC2 console in the other availability zone where you want to move the volume and choose \"Create Volume\" from the \"Actions\" dropdown menu. Specify the snapshot you created in the previous step as the source for the new volume. After creating the new volume, go to the EC2 console in the availability zone where you want to move the volume, and attach the new volume to the EC2 instance. Mount the EBS volume on the new EC2 instance as required.\nOption D: Create a new volume in the AZ and do a disk copy of contents from one volume to another.\nGo to the Amazon EC2 console, select the EBS volume you want to move, and choose \"Create Snapshot\" from the \"Actions\" dropdown menu. Once the snapshot is completed, go to the EC2 console in the same availability zone where you want to move the volume and choose \"Create Volume\" from the \"Actions\" dropdown menu. Attach the new volume to the EC2 instance from which you want to copy the data. Copy the contents of the original EBS volume to the new EBS volume using a disk copy tool. After copying the contents, detach the original EBS volume and attach the new EBS volume to the new EC2 instance in the other availability zone. Mount the EBS volume on the new EC2 instance as required.\nIn summary, you can move an EBS volume from one EC2 instance in one availability zone to another EC2 instance in a different availability zone using any of the above-mentioned options based on your preference and\n\n"
}, {
  "id" : 24,
  "question" : "You work as an architect for a company.\nAn application is going to be deployed on a set of EC2 instances in a VPC.\nThe Instances will be hosting a web application.\nYou need to design the security group to ensure that users have the ability to connect from the Internet via HTTPS.\nWhich of the following needs to be configured for the security group?\n",
  "answers" : [ {
    "id" : "061c46510b764c4d98aa3f162d75e36b",
    "option" : "Allow Inbound access on port 443 for 0.0.0.0/0",
    "isCorrect" : "true"
  }, {
    "id" : "cdb0b71d0a5347879fdd3ca093d89e1a",
    "option" : "Allow Outbound access on port 443 for 0.0.0.0/0",
    "isCorrect" : "false"
  }, {
    "id" : "4923faf587a942129f3381df71b0957b",
    "option" : "Allow Inbound access on port 80 for 0.0.0.0/0",
    "isCorrect" : "false"
  }, {
    "id" : "b8fd725668b649ac8f6530e5f4a6f41d",
    "option" : "Allow Outbound access on port 80 for 0.0.0.0/0",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nA security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nWhen you launch an instance in a VPC, you can assign five security groups to the instance.\nSecurity groups act at the instance level, not the subnet level.\nTherefore, each instance in a subnet in your VPC could be assigned to a different set of security groups.\nIf you don't specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.AWS Security groups are stateful.\nIt means that you do not need to open the outbound for responses - open only inbound for requests.\nIf you think your instances will be sending requests to certain IPs (for example: to upgrade/install a package), then you need to open the IP/port for that request.\nBy default, it is open for all traffic.\nOption B is incorrect since security groups are stateful.\nYou don't need to define the rule for outbound traffic.\nOptions C and D are incorrect since you need to ensure access for HTTPS.\nHence you should not configure rules for port 80.\nFor more information on security groups, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html\n\nTo allow users to connect to the web application hosted on EC2 instances via HTTPS from the internet, you need to allow inbound traffic on port 443 (which is the default port for HTTPS) in the security group that is associated with the EC2 instances. Therefore, the correct answer is A: Allow Inbound access on port 443 for 0.0.0.0/0.\nExplanation of each option: A. Allow Inbound access on port 443 for 0.0.0.0/0 - This option is the correct answer. It allows inbound traffic on port 443 (HTTPS) from any source IP address (0.0.0.0/0). B. Allow Outbound access on port 443 for 0.0.0.0/0 - This option allows outbound traffic on port 443 (HTTPS) to any destination IP address (0.0.0.0/0), but it does not allow inbound traffic. C. Allow Inbound access on port 80 for 0.0.0.0/0 - This option allows inbound traffic on port 80 (HTTP) from any source IP address (0.0.0.0/0), which is not what the question is asking for. The question specifically asks for HTTPS, which uses port 443. D. Allow Outbound access on port 80 for 0.0.0.0/0 - This option allows outbound traffic on port 80 (HTTP) to any destination IP address (0.0.0.0/0), but it does not allow inbound traffic.\nIt's important to note that while allowing inbound traffic on port 443 (HTTPS) is necessary to allow users to connect to the web application via HTTPS, it is not sufficient to fully secure the application. You should also consider implementing SSL/TLS certificates, using secure coding practices, and implementing other security measures to fully protect the web application.\n\n"
}, {
  "id" : 25,
  "question" : "You have been designing a CloudFormation template that creates one elastic load balancer fronting two EC2 instances.\nWhich section of the template should you edit so that the load balancer's DNS is returned upon creating the stack?\n",
  "answers" : [ {
    "id" : "ebbc32db16ec4dcaa94bf5cbeb1e2fc0",
    "option" : "Resources",
    "isCorrect" : "false"
  }, {
    "id" : "0af1ea2264f04311a5916f3434572a10",
    "option" : "Parameters",
    "isCorrect" : "false"
  }, {
    "id" : "18c9081b5b094f779fefd7a49da0e946",
    "option" : "Outputs",
    "isCorrect" : "true"
  }, {
    "id" : "380a51658a67438a822a77b937419036",
    "option" : "Mappings.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - C.\nThe below example shows a simple CloudFormation template.\nIt creates an EC2 instance based on the AMI - ami-d6f32ab5\nWhen the instance is created, it will output the AZ in which it is created.\n{\n\"Resources\": {\n\"MyEC2Instance\": {\n\"Type\": \"AWS::EC2::Instance\",\n\"Properties\": {\n\"ImageId\": \"ami-d6f32ab5\"\n}\n}\n},\n\"Outputs\": {\n\"Availability\": {\n\"Description\": \"The Instance ID\",\n\"Value\":\n{ \"Fn::GetAtt\" : [ \"MyEC2Instance\", \"AvailabilityZone\" ]}\n}\n}\n}\nOption A is incorrect because this is used to define the main resources in the template.\nOption B is incorrect because this is used to define parameters that can be taken during template deployment.\nOption D is incorrect because this is used to map key-value pairs in a template.\nTo understand more on CloudFormation, please visit the URL-\nhttps://aws.amazon.com/cloudformation/\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\n\nThe section of the CloudFormation template that should be edited to return the load balancer's DNS upon creating the stack is the \"Outputs\" section.\nThe \"Outputs\" section is used to specify the values that should be exported from the stack after it has been created. These outputs can be used by other stacks, AWS services, or even external systems to consume the resources created by the CloudFormation stack.\nTo specify the load balancer's DNS as an output, you would need to add a new section to the CloudFormation template similar to the following:\nyaml Copy code Outputs:   LoadBalancerDNSName:     Description: The DNS name of the load balancer     Value: !GetAtt [MyLoadBalancer, DNSName]\nIn this example, the output is named \"LoadBalancerDNSName\" and its value is obtained using the !GetAtt function, which retrieves the DNS name of the load balancer resource named \"MyLoadBalancer\".\nOnce this section is added to the CloudFormation template, the load balancer's DNS name will be returned as an output when the stack is created. This output can be viewed in the AWS Management Console, retrieved using the AWS CLI, or accessed programmatically using the AWS SDKs.\n\n"
}, {
  "id" : 26,
  "question" : "You work as an architect for a company.\nThere is a requirement for an application to be deployed on a set of EC2 Instances.\nThese would be part of a compute cluster that requires low inter-node latency.\nWhich of the following would you use for this requirement?\n",
  "answers" : [ {
    "id" : "fd0df5d3c16c45c1b67495df10d22f09",
    "option" : "Multiple Availability Zones",
    "isCorrect" : "false"
  }, {
    "id" : "0a0c8d03877642adbe9ef5e3e9205894",
    "option" : "AWS Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "44e4cef0fe0a488cbdf1adc0b668dca4",
    "option" : "EC2 Dedicated Instances",
    "isCorrect" : "false"
  }, {
    "id" : "73b7dda9d8d44f55a5f2f3a9b8c7dc53",
    "option" : "Cluster placement Groups",
    "isCorrect" : "true"
  }, {
    "id" : "9622cb7ad651431c9bb5a59b832e141e",
    "option" : "VPC private subnets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAmazon Web Services' solution to reduce latency between instances involves the use of placement groups.\nAs the name implies, a placement group is just that -- a group.\nAWS instances that exist within a common availability zone can be grouped into a placement group.\nGroup members can communicate with one another in a way that provides low latency and high throughput.\nCluster Placement groups are recommended for applications that benefit from low network latency, high network throughput, or both.\nThe majority of the network traffic is between the instances in the group.\nTo provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.\nBecause of what is mentioned in the documentation, all other options are incorrect.\nFor more information on AWS placement groups, please visit the URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nTo deploy an application on a set of EC2 instances that require low inter-node latency, the most appropriate solution would be to use Cluster Placement Groups (CPGs).\nA Cluster Placement Group is a logical grouping of instances within a single Availability Zone (AZ), and provides low-latency network connectivity between instances in the group. CPGs are recommended for applications that require high network throughput and low latency, as they are optimized for low-latency inter-instance communication.\nOption A (Multiple Availability Zones) would not be suitable for this requirement as it is not designed to provide low-latency inter-instance communication. Instead, Multiple Availability Zones are used for disaster recovery and high availability, allowing you to deploy resources in multiple geographically separate data centers.\nOption B (AWS Direct Connect) is a network service that provides dedicated network connections between your on-premises data center and AWS, bypassing the internet. While this can improve network performance, it is not designed to specifically address low inter-node latency.\nOption C (EC2 Dedicated Instances) provides instances that run on hardware dedicated to a single customer. While this can improve security and compliance requirements, it does not provide any specific networking optimizations.\nOption E (VPC private subnets) are used to isolate resources within a VPC and provide additional security. They do not provide any specific networking optimizations for inter-instance communication.\nTherefore, the correct answer is D, Cluster Placement Groups.\n\n"
}, {
  "id" : 27,
  "question" : "You are working as an AWS Architect for a software company.\nYou are working on a new project which involves an application deployed on twenty C5 EC2 On-demand Instances with Elastic IP attached to each instance.\nDuring peak hours, when you are initiating new instances, a considerable delay is observed.\nYou perform a pilot test for the option of initiating these Instances and hibernating so that during peak hours, these instances could be quickly launched. It works fine during the pilot phase.\nYou are recommending this option to be implemented in production.\nThe management team is concerned about the pricing of many EC2 instances in the Hibernate state.\nWhat is considered to calculate the pricing for an EC2 instance in the Hibernate state?\n",
  "answers" : [ {
    "id" : "381cd646ceda4bde8db8bc38a5f9e197",
    "option" : "Elastic IP address and EBS volumes attached to EC2 Instance",
    "isCorrect" : "true"
  }, {
    "id" : "515c7f0552c7435a9c98d0b0d05e4aca",
    "option" : "Total Compute capacity per hour, Elastic IP address and EBS volumes attached to EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "43ebc968ecc14f1ba18161a9ba9f04b7",
    "option" : "Total Compute capacity per hour and EBS volumes attached to EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "13f50211823f430894d7ea8b0f62ca0d",
    "option" : "Total Compute capacity per hour &amp; Elastic IP address attached to EC2 Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer -A.\nWhen an EC2 instance is in the Hibernate state, you pay only for the EBS volumes and Elastic IP Addresses attached to it.\nOptions B, C, and D are incorrect because when an EC2 instance is in a hibernate state, compute capacity charges are not applicable.\nThe charges are only applicable for the EBS volumes and Elastic IP Addresses attached to it.\nFor more information on pricing for an EC2 instance in Hibernate state, refer to the following URL-\nhttps://aws.amazon.com/blogs/aws/new-hibernate-your-ec2-instances/\n\nWhen an EC2 instance is hibernated, it is considered to be in the \"Stopped\" state. In this state, Amazon EC2 does not charge for the instance usage, but there are still some charges that apply.\nIn this scenario, the EC2 instances have Elastic IP addresses and EBS volumes attached to them. Therefore, the charges that apply for a hibernated EC2 instance are related to these resources.\nThe correct answer is A. Elastic IP address and EBS volumes attached to EC2 Instance.\nThe charges for a hibernated EC2 instance include:\nElastic IP address charges: When an Elastic IP address is allocated to an EC2 instance, Amazon EC2 charges for the Elastic IP address when it is not associated with a running instance. However, if an Elastic IP address is associated with a running instance, there are no charges. In this scenario, since the Elastic IP addresses are attached to the EC2 instances, charges for the Elastic IP addresses will apply when the instances are hibernated. EBS volume charges: When an EC2 instance is hibernated, the data in the instance memory is saved to the instance's EBS root volume. The charges for the EBS volume apply even when the instance is hibernated. No Compute capacity charges: When an EC2 instance is hibernated, the compute capacity is not being used, and there are no charges for the compute capacity.\nTherefore, option A, Elastic IP address and EBS volumes attached to EC2 Instance, is the correct answer as it includes the resources that incur charges for a hibernated EC2 instance. Option B, Total Compute capacity per hour, Elastic IP address, and EBS volumes attached to EC2 Instance, includes the Compute capacity per hour which is not charged during hibernation, making it an incorrect option. Option C, Total Compute capacity per hour and EBS volumes attached to EC2 Instance, misses the Elastic IP address charges. Option D, Total Compute capacity per hour and Elastic IP address attached to EC2 Instance, misses the EBS volume charges.\n\n"
}, {
  "id" : 28,
  "question" : "You are launching the AWS ECS instance.\nYou would like to set the ECS container agent configuration during the ECS instance launch.\nWhat should you do?\n",
  "answers" : [ {
    "id" : "7db60837f3e442719ec50f4d8ddcd0d1",
    "option" : "Set configuration in the ECS metadata parameter during cluster creation.",
    "isCorrect" : "false"
  }, {
    "id" : "efed109c07e942118b19496d14f37816",
    "option" : "Set configuration in the user data parameter of EC2 instance.",
    "isCorrect" : "true"
  }, {
    "id" : "ab56590599f4402cbb2700a8e75d799a",
    "option" : "Define configuration in the task definition.",
    "isCorrect" : "false"
  }, {
    "id" : "d206e12d79344f3eb50912ff615cbd16",
    "option" : "Define configuration in the service definition.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWhen you launch an Amazon ECS container instance, you have the option of passing user data to the instance.\nThe data can be used to perform common automated configuration tasks and even run scripts when the instance boots.\nFor Amazon ECS, the most common use cases for user data are to pass configuration information to the Docker daemon and the Amazon ECS container agent.\nReference:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n\n\nWhen launching an Amazon Elastic Container Service (ECS) instance, you may want to set specific configurations for the ECS container agent during launch time. These configurations may include parameters such as log level, ECS cluster name, or Docker registry credentials.\nTo set the ECS container agent configuration during instance launch, you have a few options:\nA. Set configuration in the ECS metadata parameter during cluster creation: This option is not applicable for setting the ECS container agent configuration during instance launch. ECS metadata parameters are used to provide container instances with metadata that describes the task or service that they are running.\nB. Set configuration in the user data parameter of EC2 instance: This is the recommended option for setting the ECS container agent configuration during instance launch. You can use the user data parameter to pass scripts or configuration data to an EC2 instance when it launches. You can include the ECS container agent configuration in the user data script, and the agent will pick up the configuration when it starts up.\nC. Define configuration in the task definition: This option is not applicable for setting the ECS container agent configuration during instance launch. Task definitions are used to define the containers that run as part of a task, including their Docker images, CPU and memory requirements, and network settings.\nD. Define configuration in the service definition: This option is not applicable for setting the ECS container agent configuration during instance launch. Service definitions are used to define how tasks are run and scaled, including the number of tasks to run and the load balancer to use.\nIn summary, the correct option for setting the ECS container agent configuration during instance launch is B. Set configuration in the user data parameter of EC2 instance.\n\n"
}, {
  "id" : 29,
  "question" : "Which of the following actions is required by Lambda execution role to write the logs into AWS CloudWatch? (choose 3 options)\n",
  "answers" : [ {
    "id" : "bf08c00c5f5e4b398dee6f1689d907ab",
    "option" : "logs:CreateLogGroup",
    "isCorrect" : "true"
  }, {
    "id" : "9cd57778739741608c02f2def09fc3d0",
    "option" : "logs:GetLogEvents",
    "isCorrect" : "false"
  }, {
    "id" : "4c3690ef75ba4130be8933c03f299c5b",
    "option" : "logs:CreateLogStream",
    "isCorrect" : "true"
  }, {
    "id" : "87fcf6b3404b4acd8f782ee3ae05f96d",
    "option" : "logs:DescribeLogStreams",
    "isCorrect" : "false"
  }, {
    "id" : "7dbd8815320a4fa2b3cf7ee02167a6bd",
    "option" : "logs:PutLogEvents.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A, C, E.\nhttps://docs.aws.amazon.com/lambda/latest/dg/intro-permission-model.html?shortFooter=true#lambda-intro-execution-role.\n\n\nTo write the logs into AWS CloudWatch, a Lambda function requires certain permissions to be set in its execution role. These permissions determine what the function can do when interacting with CloudWatch Logs.\nOut of the given options, the three required actions that must be added to the Lambda execution role are:\nlogs:CreateLogGroup: This permission allows the Lambda function to create a new log group in CloudWatch Logs. A log group is a container for log streams that share the same retention, monitoring, and access control settings. logs:CreateLogStream: This permission allows the Lambda function to create a new log stream within a log group. A log stream is a sequence of log events that share the same source. logs:PutLogEvents: This permission allows the Lambda function to write log events to a log stream. A log event represents a single occurrence of an event in a log stream.\nThe remaining options are not required for writing logs into AWS CloudWatch:\nlogs:GetLogEvents: This permission is used to retrieve log events from a log stream, but it is not required for writing logs. logs:DescribeLogStreams: This permission allows the Lambda function to list the available log streams in a log group, but it is not required for writing logs.\nTherefore, options A, C, and E are the correct answers as they contain the required actions that are needed by the Lambda execution role to write the logs into AWS CloudWatch.\n\n"
}, {
  "id" : 30,
  "question" : "Your company has setup EC2 Instances in a VPC for their application.\nThe IT Security department has advised that all traffic be monitored to the EC2 Instances.\nWhich of the following features can be used to capture information for outgoing and incoming IP traffic from network interfaces in a VPC.\n",
  "answers" : [ {
    "id" : "0ed1fb93df3e4529875d7e02c3417642",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "679769dd123f4b909412719ca92df0be",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "f3a8fe74e86944b3b1334887904e75dd",
    "option" : "AWS SQS",
    "isCorrect" : "false"
  }, {
    "id" : "0ea1044ebf4c49f69beeac581a3f3c60",
    "option" : "AWS VPC Flow Logs.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.\nFlow log data can be published to Amazon CloudWatch Logs and Amazon S3\nAfter you've created a flow log, you can retrieve and view its data in the chosen destination.\nOption A is incorrect since this is a monitoring service.\nOption B is incorrect since this is a compute service.\nOption C is incorrect since this is a messaging service.\nFor more information on VPC flow logs, please visit the below URL.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html\n\nThe feature that can be used to capture information for incoming and outgoing IP traffic from network interfaces in a VPC is AWS VPC Flow Logs.\nVPC Flow Logs is a feature that captures information about the IP traffic going to and from network interfaces in a VPC. This feature allows you to collect and store network flow data for analysis and troubleshooting purposes.\nWhen VPC Flow Logs is enabled, it will capture the following information about the network traffic:\nThe source and destination IP addresses The source and destination ports The protocol used The number of bytes and packets transferred The start and end time of the flow The action taken (accepted or rejected)\nVPC Flow Logs can be configured at the VPC level, subnet level, or network interface level. Once enabled, the captured flow logs can be stored in Amazon S3 or Amazon CloudWatch Logs for further analysis and monitoring.\nAWS CloudWatch is a monitoring and logging service that can be used to monitor EC2 instances and other AWS resources, but it does not provide the same level of detail as VPC Flow Logs.\nAWS EC2 is a service that provides virtual servers in the cloud, but it is not used for capturing network flow data.\nAWS SQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It is not related to capturing network flow data.\nTherefore, the correct answer is D. AWS VPC Flow Logs.\n\n"
}, {
  "id" : 31,
  "question" : "A media firm uses the Amazon S3 bucket to save all videos shared by reporters across the globe.\nOperation Team has instructed all reporters to use only Multipart Uploads while uploading these large-sized videos to Amazon S3 bucket in each region.\nMost of the reporters are working from remote areas &amp; face challenges in uploading videos.\nThe Finance Team is concerned about high costs incurred by saving data in the Amazon S3 bucket &amp; seeking your guidance.\nPost verification, you observe a large number of incomplete uploads in Amazon S3 buckets in each region.\nThe uncompleted uploads can be deleted after a certain period of time. Which of the following actions can minimize charges for saving video files in the Amazon S3 bucket?\n",
  "answers" : [ {
    "id" : "53152a3c67a941a1bd9dde7869f7d1bd",
    "option" : "Reporter`s need to compress video files locally before uploading to Amazon S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "7e89c699a3e249618645eecfc27f4b78",
    "option" : "Reporterâ€™s need to upload Videos to Amazon S3 Glacier to save additional charges.",
    "isCorrect" : "false"
  }, {
    "id" : "c5d5380401594c0eaeca4167ed0058c4",
    "option" : "Create a Lifecycle Policy to move all incomplete Multipart uploads to Amazon S3 Glacier after weeksâ€™ time from initiation.",
    "isCorrect" : "false"
  }, {
    "id" : "58eb6b530c0b4769a3db625f835369cd",
    "option" : "Create a Lifecycle Policy to delete all incomplete Multipart uploads after weeksâ€™ time from initiation.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nIncomplete Multipart Uploads incur storage charges on the Amazon S3 bucket.\nLifecycle rules can be used to abort the uploading of multipart uploads that are incomplete since a specific time frame &amp; also deletes these parts to free up storage, reducing costs for this storage.\nOption A &amp; B are incorrect as Incomplete Multipart Uploads incur charges.\nThese charges can be stopped by stopping multipart uploads.\nOption C is incorrect as Moving all incomplete Multipart uploads to Amazon S3 Glacier would not completely reduce the cost for storing data.\nAs data in Amazon S3 Glacier would incur cost, it would be less than data storing the Amazon S3 bucket.\nAlso, incomplete Multipart uploads would not be used until fully uploaded.\nFor more information on Amazon S3 Multipart Upload, refer to the following URLs-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html\nhttps://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/\n\nThe correct answer to this question is D. Create a Lifecycle Policy to delete all incomplete Multipart uploads after a certain period of time from initiation.\nExplanation: The media firm is using Amazon S3 bucket to store videos shared by reporters across the globe. Operation Team has instructed reporters to use Multipart Uploads, which is a recommended approach for uploading large-sized files to Amazon S3. However, due to remote working conditions, reporters are facing challenges in uploading videos, resulting in a large number of incomplete uploads.\nThe Finance Team is concerned about high costs incurred by storing data in the Amazon S3 bucket, which is a valid concern as Amazon S3 pricing is based on storage, data transfer, and number of requests. Incomplete uploads consume storage space in the S3 bucket and contribute to the storage costs.\nTo minimize charges for saving video files in the Amazon S3 bucket, a Lifecycle Policy can be created. Amazon S3 Lifecycle policies automate the movement of objects between different storage tiers (e.g., S3 Standard, S3 Glacier) based on specific criteria. In this case, the Lifecycle Policy can be used to delete all incomplete Multipart uploads after a certain period of time from initiation, reducing the amount of storage used by incomplete uploads and therefore minimizing storage costs.\nOption A, i.e., compressing video files locally before uploading to Amazon S3 bucket, may reduce the size of video files and reduce data transfer costs but will not reduce storage costs.\nOption B, i.e., uploading Videos to Amazon S3 Glacier, is not recommended for active data that is frequently accessed as Glacier has a slower retrieval time and additional retrieval fees. It is a suitable option for data that is accessed infrequently and is not expected to be retrieved often.\nOption C, i.e., moving all incomplete Multipart uploads to Amazon S3 Glacier after a certain period of time from initiation, will reduce storage costs but will not address the issue of incomplete uploads consuming storage space. Therefore, it is not the most appropriate solution for this scenario.\n\n"
}, {
  "id" : 32,
  "question" : "Your team has deployed an application that consists of a web and database tier hosted on separate EC2 Instances.\nBoth EC2 Instances are using General Purpose SSD for their underlying volume type.\nOf late, there are performance issues related to the read and writes of the database EC2 Instance.\nWhich of the following could be used to alleviate the issue?\n",
  "answers" : [ {
    "id" : "ff7fa1ff86e34805b0e63b8fd57b5f68",
    "option" : "Change the Instance type to a higher Instance Type.",
    "isCorrect" : "false"
  }, {
    "id" : "7d870f1142604ee19e48d7e29f1a558f",
    "option" : "Change the EBS volume to Provisioned IOPS SS.",
    "isCorrect" : "true"
  }, {
    "id" : "fcd58be72ba545a1b91b2a688aa78609",
    "option" : "Enable Enhanced Networking on the Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "a70021baa5a246b8a9218c13a4deb862",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe Provisioned IOPS SSD EBS volume type is perfect for these types of workloads.\nThe below excerpt from the documentation shows the key differences between the different volume types.\nOption A is incorrect since the primary issue is that the volume type is not correct.\nOption C is incorrect since networking is not an issue here.\nOption D is incorrect since this option is applicable for the AWS RDS service.\nFor more information on EBS volume types, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\n\n\nSure, I'll be happy to explain each of the options and their potential impact on the performance of the database EC2 instance.\nA. Change the Instance type to a higher Instance Type: Upgrading the instance type to a higher performance instance may provide more CPU and memory resources to the database EC2 instance. This can improve the application's performance, including the read and write operations. However, upgrading the instance type alone may not address the underlying issue of storage performance. It may be worth considering this option only after ruling out other potential causes for the performance issues.\nB. Change the EBS volume to Provisioned IOPS SSD: Provisioned IOPS SSD volumes are designed for applications with high I/O requirements, such as databases. Unlike General Purpose SSD volumes, Provisioned IOPS volumes allow you to specify a consistent level of IOPS (input/output operations per second) based on your application's needs. This can help improve the storage performance of the database EC2 instance and reduce the latency of read and write operations. However, Provisioned IOPS SSD volumes can be more expensive than General Purpose SSD volumes, so it is important to consider your application's requirements and budget before making the switch.\nC. Enable Enhanced Networking on the Instance: Enhanced Networking uses a network interface that provides higher throughput, lower latency, and lower jitter. This can improve the network performance of the EC2 instance and reduce the latency of read and write operations. However, enabling Enhanced Networking alone may not address the underlying issue of storage performance.\nD. Enable Multi-AZ for the database: Multi-AZ (Availability Zone) deployment provides high availability and data durability for database workloads. When you enable Multi-AZ deployment, AWS automatically provisions and maintains a synchronous standby replica in a different Availability Zone. If the primary database instance fails, AWS automatically fails over to the standby replica. Multi-AZ deployment can improve the availability and durability of the database, but may not have a significant impact on storage performance.\nIn summary, option B (Change the EBS volume to Provisioned IOPS SSD) is likely to have the most significant impact on the performance of the database EC2 instance. However, it is important to consider the application's requirements and budget before making any changes.\n\n"
}, {
  "id" : 33,
  "question" : "Your company is planning to make use of the Elastic Container service for managing their container-based applications.\nThey are going to process both critical and non-critical workloads with these applications.\nWhich of the following COST effective setup would they consider?\n",
  "answers" : [ {
    "id" : "37ba0df41f034d46b181953feec81c70",
    "option" : "Use ECS orchestration and Spot Instances for processing critical data and On-Demand for the non-critical data.",
    "isCorrect" : "false"
  }, {
    "id" : "236de1d2100b437d874ae7be87a1523c",
    "option" : "Use ECS orchestration and On-Demand Instances for processing critical data and Spot Instances for the non-critical data.",
    "isCorrect" : "true"
  }, {
    "id" : "08c5bb8992634fffb5eb691e7bb9883d",
    "option" : "Use ECS orchestration and Spot Instances for both the processing of critical data and non-critical data.",
    "isCorrect" : "false"
  }, {
    "id" : "ca2f00320e7145ba831918237b889f6e",
    "option" : "Use ECS orchestration and On-Demand Instances for both the processing of critical data and non-critical data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nSpot Instance and On-demand Instance are very similar in nature.\nThe main difference between these is a commitment.\nIn Spot Instance, there is no commitment.\nAs soon as the Bid price exceeds the Spot price, a user gets the Instance.\nIn an On-demand Instance, a user has to pay the On-demand rate specified by Amazon.\nOnce they have bought the Instance, they have to use it by paying that rate.\nIn Spot Instance, once the Spot price exceeds the Bid price, Amazon will shut the instance.\nThe benefit to the user is that they will not be charged for the partial hour in which the Instance was taken back from them.\nSpot instances are not always cheaper than on-demand, they can and do sometimes fluctuate wildly, even to very high per hour amounts, higher than the on-demand price at times.\nA Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.\nBecause Spot Instances enable you to request unused EC2 instances at steep discounts, you can significantly lower your Amazon EC2 costs.\nThe hourly price for a Spot Instance is called a Spot price.\nThe Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances.\nYour Spot Instance runs whenever capacity is available, and the maximum price per hour for your request exceeds the Spot price.\nOptions A and C are incorrect since Spot Instances can be taken back or interrupted and should not be used for critical workloads.\nOption D is not a cost-effective solution.\nYou can use Spot Instances for non-critical workloads.\nFor more information on Spot Instances, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nElastic Container Service (ECS) is a fully managed container orchestration service provided by AWS. It allows running Docker containers on a cluster of EC2 instances. ECS can be used to deploy both critical and non-critical workloads, but the cost-effective setup will depend on the workload characteristics and business needs.\nOption A: Use ECS orchestration and Spot Instances for processing critical data and On-Demand for non-critical data. This option uses Spot Instances for processing critical data and On-Demand Instances for non-critical data. Spot Instances are available at a lower price compared to On-Demand Instances but are subject to availability, and their prices may vary based on demand. This makes them a good fit for non-critical workloads, but not ideal for critical workloads. Critical workloads require more reliability and availability, which On-Demand Instances provide. Thus, this option is not a cost-effective setup for both critical and non-critical workloads.\nOption B: Use ECS orchestration and On-Demand Instances for processing critical data and Spot Instances for non-critical data. This option uses On-Demand Instances for processing critical data and Spot Instances for non-critical data. On-Demand Instances provide higher reliability and availability, which are important for critical workloads. Spot Instances can be used for non-critical workloads since they are cheaper and can be interrupted without impacting the application. This option is a cost-effective setup for both critical and non-critical workloads.\nOption C: Use ECS orchestration and Spot Instances for both the processing of critical data and non-critical data. This option uses Spot Instances for both critical and non-critical workloads. As mentioned earlier, Spot Instances are cheaper but can be interrupted, which is not ideal for critical workloads. This option is not a cost-effective setup for critical workloads since the risk of interruptions may lead to increased downtime and costs associated with restarting the application.\nOption D: Use ECS orchestration and On-Demand Instances for both the processing of critical data and non-critical data. This option uses On-Demand Instances for both critical and non-critical workloads. On-Demand Instances provide higher reliability and availability, making them a better fit for critical workloads. However, On-Demand Instances are more expensive than Spot Instances. This option is a cost-effective setup for critical workloads, but not for non-critical workloads.\nIn conclusion, option B is the most cost-effective setup for managing both critical and non-critical workloads in ECS. It uses On-Demand Instances for critical workloads and Spot Instances for non-critical workloads, providing a balance between cost and reliability.\n\n"
}, {
  "id" : 34,
  "question" : "A company has its major applications deployed in AWS.\nThe company is building a new office and requires a high-performance network connection between the local office network and the AWS network.\nThe connection needs to have high bandwidth throughput and allow users in the office to connect with multiple AWS VPCs of multiple AWS Regions.\nHow would you establish the connection in the most appropriate way?\n",
  "answers" : [ {
    "id" : "2a8f39cea6b741fbadee6fbb6c2bcdad",
    "option" : "For each AWS Region, create an AWS Direct Connect by configuring a public VIF between the VPC Virtual Private Gateway and the Customer Router.",
    "isCorrect" : "false"
  }, {
    "id" : "948970c47c714f53b6ec393be30634c5",
    "option" : "Create a Direct Connect Gateway to connect the local network with multiple Amazon VPCs across different regions.",
    "isCorrect" : "true"
  }, {
    "id" : "8dc4e11bc3e94f82ad8e4fe64d47b80b",
    "option" : "Configure two Direct Connects with two private VIFs to provide highly-available and dedicated private connections.",
    "isCorrect" : "false"
  }, {
    "id" : "ddfb5a38ab5f405cb9aae7bc046a2f15",
    "option" : "Create an AWS Direct Connect dedicated network connection on top of Amazon VPN to establish an end-to-end secure IPSec connection.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect because in this option users need to configure an AWS Direct Connect for each AWS Region, which is not the most appropriate method.\nBesides, private VIF should be set up in Direct Connection instead of public VIF.\nPrivate virtual interface is used to access Amazon VPC using private IP addresses and public virtual interface is used to access AWS public services.\nIn this scenario, the connections need to be secured, so private VIF should be used.\nOption B is CORRECT because Direct Connect Gateway, as a globally available resource, can be used to establish high-performance network connections to different AWS Regions and reduce management loads.\nPlease check the following figure:\nOption C is incorrect because this option is suitable for scenarios in which highly available and redundant connections are needed.\nThere is no such requirement in the question.\nOption D is incorrect because the question does not require a secure IPSec connection therefore VPN is not needed.\nThis option also does not address the requirements to connect with different AWS Regions.\nReferences:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\n\n\nThe most appropriate way to establish a high-performance network connection between a local office network and multiple AWS VPCs of different AWS Regions would be to create a Direct Connect Gateway. Option B is the correct answer.\nHere's why:\nAWS Direct Connect is a dedicated network connection service that provides a high-bandwidth, low-latency connection between on-premises data centers and AWS VPCs. Direct Connect Gateway is a service that allows you to connect multiple VPCs in different AWS Regions or different AWS accounts to a single Direct Connect connection. With Direct Connect Gateway, you can centralize and simplify your network architecture and reduce operational costs.\nOption A suggests creating a separate Direct Connect for each AWS Region. While this option can provide high performance and dedicated connectivity, it would result in a complex and expensive network architecture, as you would need to manage multiple Direct Connect connections.\nOption C suggests configuring two Direct Connects with two private VIFs for highly available and dedicated private connections. While this option provides high availability and dedicated connectivity, it is more complex and expensive than the Direct Connect Gateway option. You would need to manage multiple Direct Connect connections and redundant infrastructure.\nOption D suggests creating an AWS Direct Connect dedicated network connection on top of Amazon VPN to establish an end-to-end secure IPSec connection. While this option provides a secure connection, it does not address the requirement of high bandwidth throughput and connection with multiple VPCs of multiple AWS Regions.\nIn summary, the most appropriate way to establish the connection in the given scenario would be to create a Direct Connect Gateway to connect the local network with multiple Amazon VPCs across different regions. This approach provides a centralized and simplified network architecture, reduces operational costs, and addresses the requirement of high bandwidth throughput and connection with multiple VPCs of different AWS Regions.\n\n"
}, {
  "id" : 35,
  "question" : "Your company owns several EC2 Windows servers in production.\nIn order to be compliant with recent company security policies, you need to create an EC2 Windows bastion host for users to connect to the instances via the Remote Desktop Protocol (RDP)\nHow would you ensure that users can perform remote administration for the Windows servers ONLY through the new bastion host?\n",
  "answers" : [ {
    "id" : "8b0634d4eeed48f5809d15c3f3e415b8",
    "option" : "Configure the security groups of the Windows server instances to only accept TCP/3389 connections from the security group of the Windows bastion host.",
    "isCorrect" : "true"
  }, {
    "id" : "30f30a6841654327a0864409f2b2c530",
    "option" : "Configure the security group of the Windows bastion host to only allow RDP from the companyâ€™s IP addresses.",
    "isCorrect" : "false"
  }, {
    "id" : "07d7f25ec3064b7f92bdce4d91d8b592",
    "option" : "Add a NACL rule in the subnets of the Windows server instances to deny TCP/443 and TCP/22.",
    "isCorrect" : "false"
  }, {
    "id" : "7c3aa6a446d84c4c84ba261b9a0c9b2f",
    "option" : "In the NACL of the bastion host server, allow the inbound and outbound traffic for TCP/3389.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT because with this option, Windows server instances only allow the RDP traffic from the bastion host instance.\nUsers need to login to the bastion host to connect to the Windows servers.\nOption B is incorrect because this option only allows the connections to the bastion host.\nIt does not provide the RDP connections to the Windows servers through the bastion host.\nOption C is incorrect because this option only denies ports 443 and 22\nIt does not allow any rules for the inbound RDP connections.\nOption D is incorrect because similar to option B, it only controls the RDP traffic of the bastion host and there are no controls or limitations on the Windows instances.\nUsers can bypass the bastion host to connect to the Windows servers via RDP and there is no NACL rule to deny it.\nThe NACL of the bastion host server cannot block the unexpected connections.\nInstead, the control should be applied in the Windows instances to only allow the connections from the Windows bastion host.\nReference:\nhttps://aws.amazon.com/blogs/security/controlling-network-access-to-ec2-instances-using-a-bastion-server/\n\nSure! Let me provide you with a detailed explanation of the answer options and the correct approach to ensure that users can perform remote administration for the Windows servers ONLY through the new bastion host.\nA. Configure the security groups of the Windows server instances to only accept TCP/3389 connections from the security group of the Windows bastion host. This option suggests that you can configure the security groups of the Windows server instances to only accept incoming TCP/3389 connections from the security group of the Windows bastion host. By doing this, you are restricting the RDP traffic to only come from the specific security group of the bastion host. This option is partially correct as it helps to ensure that RDP traffic only comes from a specific source, but it does not guarantee that users can perform remote administration for the Windows servers ONLY through the new bastion host. It is possible that a user could still directly connect to the Windows server instances if they have access to the instance's security group.\nB. Configure the security group of the Windows bastion host to only allow RDP from the company's IP addresses. This option suggests that you can configure the security group of the Windows bastion host to only allow incoming RDP traffic from the company's IP addresses. By doing this, you are limiting RDP traffic to only come from a specific set of IP addresses, which would prevent unauthorized access. However, this option does not ensure that users can perform remote administration for the Windows servers ONLY through the new bastion host. Users who are authorized to access the company's IP addresses could still directly connect to the Windows server instances if they have access to the instance's security group.\nC. Add a NACL rule in the subnets of the Windows server instances to deny TCP/443 and TCP/22. This option suggests that you can add a Network Access Control List (NACL) rule to the subnets of the Windows server instances to deny incoming TCP/443 (HTTPS) and TCP/22 (SSH) traffic. This would help to prevent unauthorized access to the Windows server instances from the internet. However, this option does not ensure that users can perform remote administration for the Windows servers ONLY through the new bastion host.\nD. In the NACL of the bastion host server, allow the inbound and outbound traffic for TCP/3389. This option suggests that you can configure the NACL of the bastion host server to allow inbound and outbound traffic for TCP/3389. By doing this, you are ensuring that only the bastion host can initiate RDP connections to the Windows server instances. Any other RDP connections would be denied. This option is the correct approach as it ensures that users can perform remote administration for the Windows servers ONLY through the new bastion host.\nTherefore, the correct answer is D. In the NACL of the bastion host server, allow the inbound and outbound traffic for TCP/3389.\n\n"
}, {
  "id" : 36,
  "question" : "Your team uses Amazon ECS to manage containers for several micro-services.\nTo save cost, multiple ECS tasks should run at a single container instance.\nWhen a task is launched, the host port should be dynamically chosen from the container instance's ephemeral port range.\nThe ECS service should select a load balancer that supports dynamic mapping.\nWhich types of load balancers are appropriate?\n",
  "answers" : [ {
    "id" : "51a56113e63b4f44988f8d44d5de7bfb",
    "option" : "Application Load Balancer or Network Load Balancer.",
    "isCorrect" : "true"
  }, {
    "id" : "ef7bbd62e0cc4019ab1bf806b05c3908",
    "option" : "Application Load Balancer only.",
    "isCorrect" : "false"
  }, {
    "id" : "9326ad71a144418cbac17e21a3e5b281",
    "option" : "Network Load Balancer only.",
    "isCorrect" : "false"
  }, {
    "id" : "8bd944a4b35d4bc7b3793a648984f8a3",
    "option" : "Application Load Balancer or Classic Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT because both Application Load Balancer and Network Load Balancer support dynamic mapping.\nYou can configure the ECS service to use the load balancer, and a dynamic port will be selected for each ECS task automatically.\nWith Dynamic mapping, multiple copies of a task can run on the same instance.\nOption B and C are incorrect: Please check the below references.\nOption D is incorrect because Classic Load Balancer does not support dynamic mapping.\nWith Classic Load Balancer, you have to define the port mappings on a container instance statically.\nReference:\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/load-balancer-types.html.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/.\n\nThe appropriate load balancers for this scenario are the Application Load Balancer (ALB) or the Network Load Balancer (NLB), so the correct answer is A.\nWhen running multiple ECS tasks on a single container instance, it's important to ensure that they don't interfere with each other. To achieve this, each task should use a different ephemeral port to communicate with the outside world. An ephemeral port is a temporary port number that's dynamically assigned by the operating system to a network communication session, and it's typically in the range of 49152 to 65535.\nIn this scenario, the host port that's used by each ECS task should be dynamically chosen from the container instance's ephemeral port range. This means that the host port will be different for each task and will change each time the task is launched.\nTo enable this dynamic port mapping, the ECS service needs to select a load balancer that supports it. Both the Application Load Balancer (ALB) and the Network Load Balancer (NLB) support dynamic port mapping.\nThe ALB operates at the application layer (layer 7) of the OSI model and supports HTTP, HTTPS, and WebSocket protocols. It can route requests based on content-based routing, path-based routing, host-based routing, and query string parameters. The ALB also supports containerized applications and integrates with Amazon ECS and EKS natively.\nThe NLB operates at the transport layer (layer 4) of the OSI model and supports TCP, UDP, and TLS protocols. It's optimized for high throughput, low latency, and handling millions of requests per second. The NLB supports static IP addresses, which can be useful for applications that require fixed IP addresses.\nTherefore, both ALB and NLB support dynamic port mapping and are appropriate for this scenario. Classic Load Balancer (CLB) does not support dynamic port mapping, and hence it's not appropriate for this scenario.\n\n"
}, {
  "id" : 37,
  "question" : "A large amount of structured data is stored in Amazon S3 using the JSON format.\nYou need to use a service to analyze the S3 data directly with standard SQL.\nIn the meantime, the data should be easily visualized through data dashboards.\nWhich of the following services is the most appropriate?\n",
  "answers" : [ {
    "id" : "1620a5a52de145c38ce20e01abfaf53c",
    "option" : "Amazon Athena and Amazon QuickSight.",
    "isCorrect" : "true"
  }, {
    "id" : "f7aa10933e8546d7880c43f879c42c47",
    "option" : "AWS Glue and Amazon Athena.",
    "isCorrect" : "false"
  }, {
    "id" : "e44df0447fa34d98acb88ef676606e3d",
    "option" : "AWS Glue and Amazon QuickSight.",
    "isCorrect" : "false"
  }, {
    "id" : "81f5782c98dd4f26aca55e8abc1ac848",
    "option" : "Amazon Kinesis Data Stream and Amazon QuickSight.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT because Amazon Athena is the most suitable to run ad-hoc queries to analyze data in S3\nAmazon Athena is serverless, and you are charged for the amount of scanned data.\nBesides, Athena can integrate with Amazon QuickSight that visualizes the data via dashboards.\nOption B is incorrect because AWS Glue is an ETL (extract, transform, and load) service that organizes, cleanses, validates, and formats data in a data warehouse.\nThis service is not required in this scenario.\nOption C is incorrect because it is the same as Option.\nB.\nAWS Glue is not required.\nOption D is incorrect because, with Amazon Kinesis Data Stream, users cannot perform queries for the S3 data through standard SQL.\nReference:\nhttps://aws.amazon.com/athena/pricing/.\nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html.\n\nThe most appropriate services to use in this scenario would be Amazon Athena and Amazon QuickSight (option A).\nAmazon Athena is a serverless interactive query service that allows users to easily analyze data stored in S3 using standard SQL. Since the data is already stored in S3, no additional data loading is required. Athena supports various data formats including JSON, CSV, ORC, and Parquet, making it ideal for analyzing structured data stored in S3 in the JSON format.\nAmazon QuickSight is a cloud-based business intelligence service that allows users to easily create and publish interactive dashboards, reports, and charts. QuickSight integrates seamlessly with Athena, allowing users to visualize data directly from the Athena query results. QuickSight also supports various data sources including S3, Athena, RDS, Redshift, and more, making it easy to combine data from different sources in a single dashboard.\nOption B (AWS Glue and Amazon Athena) is not the most appropriate choice since AWS Glue is an ETL (extract, transform, load) service that is used to prepare and transform data for analysis, not to analyze data directly. While AWS Glue can also integrate with Athena to create and maintain data catalogs, it is not necessary for this scenario.\nOption C (AWS Glue and Amazon QuickSight) is similar to option B but with the addition of QuickSight. While QuickSight is a good choice for data visualization, AWS Glue is not necessary for this scenario since the data is already structured and stored in S3.\nOption D (Amazon Kinesis Data Stream and Amazon QuickSight) is not the most appropriate choice since Kinesis Data Stream is a real-time data streaming service, which is not necessary for this scenario. Additionally, Kinesis Data Stream does not support SQL queries, which are required to analyze the structured data stored in S3.\n\n"
}, {
  "id" : 38,
  "question" : "You have an Amazon Route 53 alias record that routes the traffic to an Application Load Balancer.\nLater on, the availability zones enabled for the load balancer are changed by a team member.\nWhen you check the load balancer using the dig command, you find that the IPs of the ELB have changed.\nWhat kind of change do you need to do for the alias record in Route 53?\n",
  "answers" : [ {
    "id" : "0551047ddbf54affb7de56b344250fd2",
    "option" : "Change the record type from A to CNAM.",
    "isCorrect" : "false"
  }, {
    "id" : "6d8bf55f277b4fcabcf881cdb01efea5",
    "option" : "Modify the destination to the DNS name of the Application Load Balancer.",
    "isCorrect" : "false"
  }, {
    "id" : "a5cff1780ac04b55b70775f099d76480",
    "option" : "Add the new IP addresses in the destination of the alias record.",
    "isCorrect" : "false"
  }, {
    "id" : "0c2dcba3d8db40b89a885d9f85b7cecb",
    "option" : "Nothing as Route 53 automatically recognizes changes in the resource for the alias record.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect because there is no need to change the type to CNAME as the alias record continues to work, although the IP addresses are changed for the ELB.Option B is incorrect because Route 53 can find out the new IPs of the ELB.\nThis change is not required.\nOption C is incorrect because you cannot add any extra IPs to this record after creating the alias record.\nRoute 53 is responsible for routing the traffic to the correct IP addresses.\nOption D is CORRECT because Route 53 automatically routes the traffic to the new ELB IP addresses.\nWith alias records, users do not need to change the record sets even if they have some configuration changes.\nReference:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html.\n\nWhen you configure an Amazon Route 53 alias record to route traffic to an Application Load Balancer, the alias record is associated with the DNS name of the load balancer. The DNS name for the load balancer resolves to one or more IP addresses. The IP addresses for the load balancer are typically associated with Elastic Network Interfaces (ENIs) in the subnets of the Availability Zones that the load balancer is configured to use.\nIn this scenario, if the availability zones enabled for the load balancer are changed by a team member, the IP addresses associated with the load balancer might change. As a result, the DNS name associated with the load balancer will also change. This means that the alias record in Route 53 needs to be updated to reflect the new DNS name of the load balancer.\nThe correct answer to the question is B. Modify the destination to the DNS name of the Application Load Balancer. This means that you need to update the alias record in Route 53 to use the new DNS name associated with the load balancer. To do this, you can edit the alias record in the Route 53 console or by using the AWS Command Line Interface (CLI) or SDKs. You should replace the old DNS name associated with the load balancer with the new DNS name.\nOption A, Change the record type from A to CNAME is not correct because the alias record type needs to remain as \"A\". This is because the alias record is pointing to an AWS resource that has one or more IP addresses associated with it.\nOption C, Add the new IP addresses in the destination of the alias record is not correct because the alias record is associated with the DNS name of the load balancer, not with the IP addresses. The IP addresses can change frequently as the load balancer scales up or down, so it's better to use the DNS name associated with the load balancer.\nOption D, Nothing as Route 53 automatically recognizes changes in the resource for the alias record is not correct because Route 53 does not automatically update the DNS name associated with the load balancer. It is your responsibility as an administrator to update the alias record to use the new DNS name associated with the load balancer.\n\n"
}, {
  "id" : 39,
  "question" : "You have an S3 bucket that is used to store important data for a web application.\nYou want to receive an email notification whenever an object removal event happens in the S3 bucket.\nHow would you configure the S3 bucket to achieve this requirement?\n",
  "answers" : [ {
    "id" : "24e26881dee84024b9f8498418ab1ea6",
    "option" : "Configure the object-level logging for the S3 bucket and register an SNS topic to provide notifications.",
    "isCorrect" : "false"
  }, {
    "id" : "ccd5fd5c30a6458b8440d147d23f9e9f",
    "option" : "Configure the server access logging for the object removal events. Add an SNS topic to notify the team via emails.",
    "isCorrect" : "false"
  }, {
    "id" : "d8269ba7292d410f9a17d20db061cc4f",
    "option" : "Set up an AWS Config rule to check the object deletion events. Register a Lambda function to send notifications.",
    "isCorrect" : "false"
  }, {
    "id" : "8f62d481fe33457ba414737f1441d788",
    "option" : "Configure an S3 event notification for the object removal events. Send the events to an SNS topic.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect because object-level logging is used to record object-level API activities in CloudTrail.\nUsers cannot register an SNS topic for object-level logging.\nOption B is incorrect because server access logging does not trigger an SNS topic for the object removal events.\nOption C is incorrect because you would need to write a custom Lambda function in the AWS Config rule to check the object removal events.\nThis method is more complicated than option.\nD.Option D is CORRECT because with an S3 event notification, you can select which events are enabled for the notification.\nThe events can be sent to an SNS topic, an SQS queue, or a Lambda function.\nReference:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html.\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-event-notifications.html.\n\n\nThe correct answer is D: Configure an S3 event notification for the object removal events. Send the events to an SNS topic.\nExplanation:\nTo receive an email notification whenever an object removal event happens in an S3 bucket, you can use Amazon S3 event notifications. These notifications can be configured to send an email to an Amazon SNS topic when an object is deleted from the S3 bucket.\nTo configure S3 event notifications for the S3 bucket, follow these steps:\nOpen the Amazon S3 console. Choose the S3 bucket that you want to configure for event notifications. Click on the \"Properties\" tab. Click on the \"Events\" section. Click on the \"Add notification\" button. Choose the event type \"Object removal\" and provide the prefix or suffix of the object key for which you want to receive notifications. Select the Amazon SNS topic to which you want to send the notification emails. Click on the \"Save\" button.\nOnce you have configured the S3 event notification, you will receive an email notification to the SNS topic whenever an object is deleted from the S3 bucket. You can then configure the SNS topic to send an email notification to the appropriate team members.\nOption A is incorrect because object-level logging does not provide the ability to send notifications. It only logs the object-level activity in the S3 bucket.\nOption B is incorrect because server access logging only logs the requests made to the S3 bucket. It does not provide the ability to send notifications for object removal events.\nOption C is incorrect because AWS Config is used to track resource configurations and changes over time. It is not designed to send notifications for object removal events in an S3 bucket.\n\n"
}, {
  "id" : 40,
  "question" : "Your organization had asked to be cost-efficient in designing AWS solutions.\nYou have created three VPCs(VPC A, VPC B, VPC C), peered VPC A to VPC B and VPC B to VPC C.\nYou have created a NAT gateway in VPC B and would like to use the same NAT Gateway for resources within VPC A and VPC C.\nHowever, the resources within VPC A and VPC C cannot communicate to the internet through NAT Gateway, but resources in VPC B can communicate.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "395e4d3fb68249beb6d5eaef6bbeca22",
    "option" : "Route tables in VPC A and VPC C are not configured to have VPC Bâ€™s NAT gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "8b8d566935114908955bacaa1ad9225a",
    "option" : "Using another VPC`s NAT Gateway is not supported in AWS.",
    "isCorrect" : "true"
  }, {
    "id" : "b442a6b762614109930e12596b0ffbab",
    "option" : "VPC Bâ€™s subnet which contains NAT gateway is not configured in VPC A and VPC C route tables.",
    "isCorrect" : "false"
  }, {
    "id" : "8b290ab61cfd4e40bf8a1441d38e8d47",
    "option" : "NAT Gateway is not created inside VPC Bâ€™s public subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nIn a VPC peering connection, using the NAT Gateway of another VPC becomes transitive routing and is not supported in AWS.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html#nat-\nFor.\nOption A, in VPC's route table, only NAT Gateway of the belonging VPC can be configured.\nVPC A and VPC C cannot configure VPC B's NAT Gateway in their respective route tables.\nThis option is incorrect.\nFor Option B, as explained above, transitive routing is not supported.\nThis option is correct.\nFor Option C, even if two VPCs have peered and configured route tables with their entire IP range, as explained above, transitive routing is not supported.\nThis option is incorrect.\nFor Option D, the question says VPC B resources can communicate with the internet, for which NAT gateway should be on a public subnet.\nSo this option is not valid.\n\n\nThe correct answer is A. Route tables in VPC A and VPC C are not configured to have VPC B's NAT gateway.\nExplanation:\nWhen VPCs are peered, they create a private connection between them, and instances within each VPC can communicate with each other as if they were on the same network. However, by default, instances in a VPC cannot communicate with the internet directly. To enable internet connectivity, you need to create a NAT Gateway or a NAT Instance in a public subnet within the VPC.\nIn this scenario, you have created a NAT Gateway in VPC B, which means instances in VPC B can communicate with the internet. However, instances in VPC A and VPC C cannot communicate with the internet through the NAT Gateway, even though VPCs are peered and the NAT Gateway is located in VPC B.\nThis issue could be due to the fact that the route tables in VPC A and VPC C are not configured to have VPC B's NAT Gateway. When you create a NAT Gateway, it is associated with a specific subnet, and the route tables of the subnets within the VPC should be updated to route traffic to the NAT Gateway. Therefore, in this scenario, the route tables in VPC A and VPC C need to be updated to route traffic to the NAT Gateway in VPC B.\nOption B is incorrect because using another VPC's NAT Gateway is supported in AWS.\nOption C is incorrect because VPC B's subnet containing the NAT Gateway needs to be added to the route tables of VPC A and VPC C, not the other way around.\nOption D is incorrect because if the NAT Gateway is not created in a public subnet in VPC B, then instances in VPC B would not be able to communicate with the internet either.\n\n"
}, {
  "id" : 41,
  "question" : "As a Cloud Architect, you have deployed an existing application from the local server to an On-demand EC2 instance.\nYou found out that there is an issue while connecting the application using the HTTPS Protocol.\nAfter troubleshooting the issue, you added port 443 to the security group of the instance.\nHow much time will it take to update changes to all of the resources related to Security groups?\n",
  "answers" : [ {
    "id" : "544fa54c0e434d83ad622fc13b30da0c",
    "option" : "It can take up to 10 minutes depending on the number of resources.",
    "isCorrect" : "false"
  }, {
    "id" : "61dddf45a8d94a4292f54dc0c34a2fe0",
    "option" : "You just need to restart the EC2 Server.",
    "isCorrect" : "false"
  }, {
    "id" : "d57b938ff6ac4126a706dfa3df786d2c",
    "option" : "You cannot make any change to existing security group, you have to create new Security group.",
    "isCorrect" : "false"
  }, {
    "id" : "aa7e87d54ff344618096534f54426e26",
    "option" : "Immediately without restart.",
    "isCorrect" : "true"
  }, {
    "id" : "0fbe2d164ba6439f861ab555e1492637",
    "option" : "You have to deploy your application again.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - D.\nYou can assign a security group to an instance when you launch the instance.\nWhen you add or remove rules, those changes are automatically applied to all instances to which you've assigned the security group.\nOption A is incorrect.\nAny changes made to the Security Group are immediately effected.\nOption B is incorrect because you don't need to restart the server to check any Security Group updates.\nOption C is incorrect because you can modify rules in the security group.\nOption D is CORRECT because any changes made to the security group are taken into effect immediately.\nOption E is incorrect because this security group works at the instance level, not at the application level.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#adding-security-group-rule\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/working-with-security-groups.html\n\nWhen you add a new rule to a security group, it is applied immediately to all the resources associated with that security group. Therefore, the correct answer is D - \"Immediately without restart.\"\nSecurity groups are virtual firewalls that control inbound and outbound traffic to your AWS resources such as EC2 instances. When you add a new rule to a security group, the change takes effect immediately, and it applies to all instances associated with that security group.\nHowever, keep in mind that changes to security group rules may take some time to propagate to all instances in some rare cases. In such cases, the delay may depend on the size of the security group, network latency, and other factors, but it is usually no more than a few minutes.\nIn summary, you do not need to restart the EC2 server, create a new security group, or redeploy your application. Adding a rule to a security group takes effect immediately and applies to all resources associated with that security group.\n\n"
}, {
  "id" : 42,
  "question" : "One of your colleagues, who is new to the company where you work as a cloud Architect, has some issues with IP Addresses.\nHe has created an Amazon VPC with an IPV4 CIDR block 10.0.0.0/24, but now there is a requirement of hosting a few more resources to that VPC.\nAs per his knowledge, he is thinking of creating a new VPC with a greater range.\nCould you suggest to him a better way that should be reliable?\n",
  "answers" : [ {
    "id" : "b313c18eef464f409e824c10d67fede3",
    "option" : "Delete the existing subnets in the VPC and create new Subnets in VP.",
    "isCorrect" : "false"
  }, {
    "id" : "408909d974f54cda9a19a9dbb9a7c705",
    "option" : "He is thinking of the right approach.",
    "isCorrect" : "false"
  }, {
    "id" : "c9342f7811ca4873868b141f2517928b",
    "option" : "You can create new VPC and connect old VPC with a new one.",
    "isCorrect" : "false"
  }, {
    "id" : "9d9966f1b35e49f09d04ffb16e267940",
    "option" : "You can expand existing VPC by adding Secondary CIDR to your current VP.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOptions A, B, C are incorrect because it is not reliable to go for this type of approach as VPC to VPC connection will take new resources like VPC Peering.\nCreating a new VPC or Subnet is also not suggested.\nOption D is correct because you can associate Secondary CIDR to your current VPC to accommodate more hosts.\nRefer: https://docs.aws.amazon.com/vpc/latest/userguide/working-with-vpcs.html#add-ipv4-cidr.\n\nThe best option for hosting additional resources in an Amazon VPC with an IPV4 CIDR block 10.0.0.0/24 is to expand the existing VPC by adding Secondary CIDR to the current VPC.\nExpanding the VPC by adding Secondary CIDR to the current VPC is a reliable and straightforward solution for this scenario. With Secondary CIDR, you can extend the CIDR block range of your existing VPC without having to create a new VPC. This will enable you to add more resources to the existing VPC without the need to reconfigure the network settings.\nBy using Secondary CIDR blocks, you can have multiple IP address ranges in your VPC. Each of these CIDR blocks is associated with a subnet, allowing you to create multiple subnets with different IP address ranges within the same VPC.\nTo add a Secondary CIDR block to your VPC, follow these steps:\nOpen the Amazon VPC console. In the navigation pane, choose \"Your VPCs.\" Select the VPC for which you want to add a Secondary CIDR block. Choose the \"CIDR blocks\" tab, and then choose \"Edit CIDRs.\" In the \"Add CIDR\" dialog box, enter the new CIDR block range that you want to add. Choose \"Save.\"\nOnce you add a Secondary CIDR block, you can create new subnets within the range of that block. You can also modify existing subnets to use the new CIDR block range.\nIn conclusion, adding a Secondary CIDR block to your current VPC is the recommended solution for expanding your VPC's IP address range and hosting additional resources in your VPC without having to create a new VPC.\n\n"
}, {
  "id" : 43,
  "question" : "You are a solutions architect working for a financial services firm.\nYour firm requires a very low latency response time for requests via API Gateway and Lambda integration to your securities master database.\nThe securities master database, housed in Aurora, contains data about all of the securities your firm trades.\nThe data consists of the security ticker, the trading exchange, trading partner firm for the security, etc.\nAs this securities data is relatively static, you can improve the performance of your API Gateway REST endpoint by using API Gateway caching.\nYour REST API calls for equity security request types and fixed income security request types to be cached separately. Which of the following options is the most efficient way to separate your cache responses via request type using API Gateway caching?\n",
  "answers" : [ {
    "id" : "daa362e370424b20b8e38790a7cd0866",
    "option" : "Payload compression",
    "isCorrect" : "false"
  }, {
    "id" : "c8eb962e04be4953806f3d6a3b3017a8",
    "option" : "Custom domain name",
    "isCorrect" : "false"
  }, {
    "id" : "be208264360447ad90b45e8c9a88b868",
    "option" : "API Stage",
    "isCorrect" : "false"
  }, {
    "id" : "a9291f8ec21c4f40a4261f05b33b7fd6",
    "option" : "Query string.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption A is incorrect.\nPayload compression is used to compress and decompress the payload to and from your API Gateway.\nIt is not used to separate cache responses.\nOption B is incorrect.\nCustom domain names are used to provide more readable URLs for the users of your AIPs.\nThey are not used to separate cache responses.\nOption C is incorrect.\nAn API stage is used to create a name for your API deployments.\nThey are used to deploy your API in an optimal way.\nOption D is correct.\nYou can use your query string parameters as part of your cache key.\nThis allows you to separate cache responses for equity requests from fixed income request responses.\nReferences:\nPlease see the Amazon API Gateway developer guide titled Enabling API caching to enhance responsiveness (https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html),\nthe Amazon API Gateway REST API Reference page titled Making HTTP Requests to Amazon API Gateway (https://docs.aws.amazon.com/apigateway/api-reference/making-http-requests/),\nthe Amazon API Gateway developer guide titled Enabling payload compression for an API (https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-gzip-compression-decompression.html),\nthe Amazon API Gateway developer guide titled Setting up custom domain names for REST APIs (https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nand the Amazon API Gateway developer guide titled Setting up a stage for a REST API (https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html)\n\nThe most efficient way to separate cache responses via request type using API Gateway caching is by using the Query string.\nQuery strings are a part of the URL and are used to pass additional parameters to the backend of the API Gateway. By using query strings, we can pass a unique identifier to each request type that we want to cache separately. This unique identifier can then be used by API Gateway to cache the responses separately for each request type.\nFor example, let's assume that we have two request types, \"equity\" and \"fixed income\", and we want to cache their responses separately. We can modify the URLs for these request types to include a query string parameter such as \"?request_type=equity\" or \"?request_type=fixed_income\". This query string parameter will act as a unique identifier for each request type.\nWhen API Gateway receives a request with a query string parameter, it will check if the response is already cached for that particular request type. If the response is cached, API Gateway will return the cached response, which improves the response time. If the response is not cached, API Gateway will execute the backend Lambda function and cache the response for future requests.\nUsing API Gateway caching with query string parameters can significantly improve the performance of REST endpoints that have static data, such as the securities master database in this case.\nTo summarize, the most efficient way to separate cache responses via request type using API Gateway caching is by using query strings. This allows us to pass a unique identifier for each request type, which API Gateway can use to cache responses separately.\n\n"
}, {
  "id" : 44,
  "question" : "You are a solutions architect working for a data analytics company that delivers analytics data to politicians that need the data to manage their campaigns.\nPolitical campaigns use your company's analytics data to decide on where to spend their campaign money to get the best results for the efforts.\nYour political campaign users access your analytics data through an Angular SPA via API Gateway REST endpoints.\nYou need to manage the access and use of your analytics platform to ensure that the individual campaign data is separate.\nSpecifically, you need to produce logs of all user requests and responses to those requests, including request payloads, response payloads, and error traces.\nWhich type of AWS logging service should you use to achieve your goals?\n",
  "answers" : [ {
    "id" : "3a76c7d75d1348f5a568285f34fe95b4",
    "option" : "Use CloudWatch access logging",
    "isCorrect" : "false"
  }, {
    "id" : "4e079d7373af4930b101bfecc0a7b982",
    "option" : "Use CloudWatch execution logging",
    "isCorrect" : "true"
  }, {
    "id" : "bee5a95fa86c4105b8678f0b3e1834c4",
    "option" : "Use CloudTrail logging",
    "isCorrect" : "false"
  }, {
    "id" : "fc261844e6864aaeb5e0afa69669d4c5",
    "option" : "Use CloudTrail execution logging.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect.\nCloudWatch access logging captures which resource accessed an API and the method used to access the API.\nIt is not used for execution traces, such as capturing request and response payloads.\nOption B is correct.\nCloudWatch execution logging allows you to capture user request and response payloads as well as error traces.\nOption C is incorrect.\nCloudTrail captures actions by users, roles, and AWS services.\nCloudTrail records all AWS account activity.\nCloudTrail does not capture error traces.\nOption D is incorrect.\nCloudTrail does not have a feature called execution logging.\nReferences:\nPlease see the Amazon API Gateway developer guide titled Setting up CloudWatch logging for a REST API in API Gateway (https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html), and the AWS CloudTrail user guide titled How CloudTrail works (https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html)\n\nFor the given scenario, the recommended AWS logging service to use is CloudTrail logging (option C).\nCloudTrail is an AWS service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It records and logs all API calls made to AWS resources in your account, including those made via the AWS Management Console, AWS Command Line Interface (CLI), and AWS SDKs.\nBy using CloudTrail logging, you can ensure that individual campaign data is separate by tracking and auditing all API calls made to your analytics platform, and you can also produce logs of all user requests and responses, including request and response payloads, and error traces. This makes it easier to identify and troubleshoot issues with your API endpoints.\nOn the other hand, CloudWatch access logging (option A) is used to capture information about who has accessed your Amazon API Gateway APIs. It provides you with detailed access logs that can help you monitor and troubleshoot your APIs. However, it doesn't capture the request and response payloads, which are required in this scenario.\nCloudWatch execution logging (option B) captures logs for Lambda function executions. It records information such as the execution time, duration, memory used, and other details about the execution of your Lambda functions. This logging service is not applicable in the given scenario.\nCloudTrail execution logging (option D) is a type of logging that is used to track changes made to AWS resources, including Lambda functions, AWS Identity and Access Management (IAM) roles, and Amazon EC2 instances. This logging service is not suitable for the given scenario, as it does not capture API call logs.\nIn summary, CloudTrail logging (option C) is the best choice for achieving the goals of the given scenario, as it captures all API calls made to AWS resources, including the analytics data platform, and records the request and response payloads, which are needed to produce logs of user requests and responses.\n\n"
}, {
  "id" : 45,
  "question" : "You are a solutions architect working for a social media company that provides a place for civil discussion of political and news-related events.\nDue to the ever-changing regulatory requirements and restrictions placed on social media apps that provide these services, you need to build your app in a very highly flexible environment that you can change instantly without updating code.\nYou have chosen to build the REST API endpoints used by your social media app user interface code using Lambda.\nHow can you securely configure your Lambda functions so that you can achieve the flexibility required by your app? (Select TWO)\n",
  "answers" : [ {
    "id" : "54982bdc0965423d9a343c001cf28d09",
    "option" : "Pass environment variables to your Lambda function via the request header sent to your API Gateway methods.",
    "isCorrect" : "false"
  }, {
    "id" : "00551225683d4d4d9ca1981e03694900",
    "option" : "Configure your Lambda functions to use key configuration.",
    "isCorrect" : "true"
  }, {
    "id" : "a32b0de9d66e40249df5814934f76e78",
    "option" : "Use encryption helpers",
    "isCorrect" : "true"
  }, {
    "id" : "c10436bde18b400288450380f1db4238",
    "option" : "Use Lambda layers",
    "isCorrect" : "false"
  }, {
    "id" : "c59cd0fdd4c54b9983a50d973817a94c",
    "option" : "Use Lambda aliases.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: B and C.\nOption A is incorrect.\nSending environment variables to your Lambda function as request parameters would expose the environment variables as plain text.\nThis is not a secure approach.\nOption B is correct.\nLambda key configuration allows you to have your Lambda functions use an encryption key.\nYou create the key in AWS KMS.\nThe key is used to encrypt the environment variables that you can use to change your function without deploying any code.\nOption C is correct.\nEncryption helpers make your lambda function more secure by allowing you to encrypt your environment variables before they are sent to Lambda.\nOption D is incorrect.\nLambda layers are used to package common code such as libraries, configuration files, or custom runtime images.\nLayers will not give you the same flexibility as environment variables for use in managing change without deploying any code.\nOption E is incorrect.\nLambda aliases are used to refer to a specific version of your Lambda function.\nYou could switch between many versions of your Lambda function, but you would have to deploy new code to create a different version of your Lambda function.\nReferences:\nPlease see the AWS Lambda developer guide titled Data protection in AWS Lambda (https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html), the AWS Lambda developer guide titled Lambda concepts (https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-concepts.html#gettingstarted-concepts-layer), the AWS Lambda developer guide titled Lambda function aliases (https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html), and the AWS Lambda developer guide titled Using AWS Lambda environment variables (https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html)\n\nSure, I'd be happy to provide a detailed explanation of how you can securely configure your Lambda functions for a flexible environment in a social media app.\nFirstly, it's important to note that Lambda provides a serverless compute service that allows you to run code without provisioning or managing servers. You only pay for the compute time that you consume, which makes it a cost-effective and flexible solution for a social media app that may experience varying traffic levels.\nTo securely configure your Lambda functions for flexibility, you can use the following two approaches:\nUse Lambda Layers: Lambda Layers allow you to separate out common code from your Lambda functions and share it across multiple functions. This can help you achieve flexibility by enabling you to update your common code separately from your function code, without having to update the function code every time there is a change. You can use Layers to manage your dependencies, such as libraries and frameworks, and ensure that they are up-to-date and secure. By keeping your dependencies in Layers, you can quickly update them without changing your function code, which helps you achieve the flexibility you need. You can also use Layers to share security configurations and logging logic across your functions, which can help you maintain a consistent level of security and visibility across your application. Use Lambda Aliases: Lambda Aliases allow you to create multiple versions of your Lambda function, each with a unique ARN (Amazon Resource Name), and map them to friendly names, such as \"prod\" or \"test\". You can use Aliases to achieve flexibility by enabling you to roll out new versions of your function without changing the function code or the ARN used by your clients. You can assign different weights to each version, which determines the percentage of traffic that each version receives, and gradually shift traffic from one version to another, which helps you achieve seamless updates and minimize the impact on your users. You can also use Aliases to manage your environment variables, such as database credentials and API keys, and ensure that they are securely stored and encrypted. By keeping your environment variables in Aliases, you can quickly update them without changing your function code, which helps you achieve the flexibility you need.\nIn summary, using Lambda Layers and Lambda Aliases can help you achieve a highly flexible environment for your social media app, while ensuring that your Lambda functions are secure and up-to-date. By separating out common code and managing your dependencies, and by creating multiple versions of your function and managing your environment variables, you can achieve flexibility without compromising on security.\n\n"
}, {
  "id" : 46,
  "question" : "You are working as an AWS Architect for a start-up company.\nThey have a two-tier production website.\nDatabase servers are spread across multiple Availability Zones and are stateful. You have configured Auto Scaling Group for these database servers with a minimum of 2 instances &amp; a maximum of 6 instances.\nDuring post-peak hours, you observe some data loss.\nWhich feature needs to be configured additionally to avoid future data loss (and copy data before instance termination)?\n",
  "answers" : [ {
    "id" : "e77218b1373241c0a0f081b4feb94b10",
    "option" : "Modify the cooldown period to complete custom actions before the Instance terminates.",
    "isCorrect" : "false"
  }, {
    "id" : "428571d5865e48d8aaf57492eef37e14",
    "option" : "Add lifecycle hooks to Auto Scaling group.",
    "isCorrect" : "true"
  }, {
    "id" : "bc7229505fe04f68b26548543b91847e",
    "option" : "Customize Termination policy to complete data copy before termination.",
    "isCorrect" : "false"
  }, {
    "id" : "155d210bc5b447cd888ff2c318fe129f",
    "option" : "Suspend Terminate process that will avoid data loss.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAdding Lifecycle Hooks to the Auto Scaling group puts the instance into a wait state before termination.\nDuring this wait state, you can perform custom activities to retrieve critical operational data from a stateful instance.\nThe Default Wait period is 1 hour.\nOption A is incorrect as the cooldown period will not help copy data from the instance before termination.\nOption C is incorrect as the Termination policy is used to specify which instances to terminate first during scale in.\nConfiguring termination policy for the Auto Scaling group will not copy data before instance termination.\nOption D is incorrect as the Suspending Terminate policy will not prevent data loss but disrupt other processes &amp; prevent scale-in.\nFor more information on lifecycle-hooks, refer to the following URLs:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/lifecycle-hooks.html\nhttps://aws.amazon.com/ec2/autoscaling/faqs/\n\nThe correct answer is C. Customize Termination policy to complete data copy before termination.\nExplanation: Auto Scaling Group (ASG) allows you to automatically adjust the number of instances in a group according to the demand of your application. When an instance in an ASG is terminated, it can cause data loss if the data is not saved or backed up.\nIn this scenario, the database servers are stateful, which means they store data that needs to be saved before the instances are terminated. Additionally, the ASG has a minimum of 2 instances and a maximum of 6 instances, which indicates that instances are regularly terminated and replaced.\nTo avoid data loss, a customized Termination policy needs to be configured that performs data backup before terminating an instance. The Termination policy can be customized using a combination of the following methods:\nLifecycle hooks: Lifecycle hooks can be added to the ASG to pause the termination process and perform a data backup before terminating the instance. CloudWatch events: CloudWatch events can be configured to trigger a Lambda function that performs a data backup before the instance is terminated. Custom scripts: Custom scripts can be written to perform data backup before the instance is terminated.\nOption A (Modify the cooldown period to complete custom actions before the Instance terminates) and Option B (Add lifecycle hooks to Auto Scaling group) are partially correct, but they do not specifically address the issue of data loss. Modifying the cooldown period can provide additional time to complete custom actions before the instance terminates, but it does not guarantee data backup. Adding lifecycle hooks to the ASG can provide additional control over the termination process, but it requires additional configuration to ensure data backup.\nOption D (Suspend Terminate process that will avoid data loss) is incorrect because suspending the Terminate process can cause a delay in replacing terminated instances, which can impact the availability of the application.\nTherefore, the correct answer is C (Customize Termination policy to complete data copy before termination).\n\n"
}, {
  "id" : 47,
  "question" : "You have an application running in us-west-2 that requires 6 EC2 Instances running at all times.\nWith 3 Availability Zones in the region viz.\nus-west-2a, us-west-2b, and us-west-2c, which of the following deployments provides fault tolerance if ONE Availability Zone in us-west-2 becomes unavailable? (SELECT TWO.)\n",
  "answers" : [ {
    "id" : "495c23dee34e43d59772a38e129091de",
    "option" : "2 EC2 Instances in us-west-2a, 2 EC2 Instances in us-west-2b, and 2 EC2 Instances in us-west-2c",
    "isCorrect" : "false"
  }, {
    "id" : "33491cc46dba49589c31b80fd4ce9170",
    "option" : "3 EC2 Instances in us-west-2a, 3 EC2 Instances in us-west-2b, and no EC2 Instances in us-west-2c",
    "isCorrect" : "false"
  }, {
    "id" : "476aba93e1d54927b8b2a7306cb83d23",
    "option" : "4 EC2 Instances in us-west-2a, 2 EC2 Instances in us-west-2b, and 2 EC2 Instances in us-west-2c",
    "isCorrect" : "false"
  }, {
    "id" : "4ed0efc70e6f4d4a84b34e25f299b633",
    "option" : "6 EC2 Instances in us-west-2a, 6 EC2 Instances in us-west-2b, and no EC2 Instances in us-west-2c",
    "isCorrect" : "true"
  }, {
    "id" : "b7716f6cf45042e3b40c331143fdb85f",
    "option" : "3 EC2 Instances in us-west-2a, 3 EC2 Instances in us-west-2b, and 3 EC2 Instances in us-west-2c.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D and E.\nOption D- US West 2a-6 , US West 2b - 6, US West 2c-0\nIf US West 2a goes down, we will still have 6 instances running in US West 2b.\nIf US West 2b goes down, we will still have 6 instances running in US West 2a.\nIf US West 2c goes down, we will still have 6 instances running in US West 2a, 6 instances running in US West 2b.\nOption E- US West 2a-3 , US West 2b - 3, US West 2c-3\nIf US West 2a goes down, we will still have 3 instances running in US West 2b and 3 instances running in US West 2c.\nIf US West 2b goes down, we will still have 3 instances running in US West 2a and 3 instances running in US West 2c.\nIf US West 2c goes down, we will still have 3 instances running in US West 2a and 3 instances running in US West 2b.\nOption A is incorrect because, even if one AZ becomes unavailable, we will only have 4 instances available.\nThis does not meet the specified requirements.\nOption B is incorrect because, when either us-west-2a or us-west-2b is unavailable, you would only have 3 instances available.\nThis does not meet the specified requirements.\nOption C is incorrect because, if us-west-2a becomes unavailable, you would only have 4 instances available.\nThis also does not meet the specified requirements.\nFor more information on AWS Regions and Availability Zones, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html\nNote:\nIn this scenario, we need to have 6 instances running all the time, even when 1 AZ is down.\nHence options D &amp; E are correct.\n\nSure, I'd be happy to help you understand the answer to this question.\nThe question is asking which of the given deployments provides fault tolerance if one availability zone in us-west-2 becomes unavailable, while also ensuring that 6 EC2 instances are running at all times.\nTo understand the answer, it's important to first understand what an availability zone is in AWS. An availability zone (AZ) is a data center in a specific geographic region that is isolated from other AZs in terms of power, cooling, and network connectivity. This isolation helps ensure that if there is an issue in one AZ, it does not affect the other AZs in the region.\nNow, let's look at each of the answer options:\nA. 2 EC2 Instances in us-west-2a, 2 EC2 Instances in us-west-2b, and 2 EC2 Instances in us-west-2c This deployment spreads the EC2 instances evenly across all three availability zones. However, if one AZ becomes unavailable, there will only be 4 EC2 instances remaining, which is not enough to meet the requirement of 6 EC2 instances running at all times. Therefore, this option is not a correct answer.\nB. 3 EC2 Instances in us-west-2a, 3 EC2 Instances in us-west-2b, and no EC2 Instances in us-west-2c This deployment puts all 6 EC2 instances in two availability zones, us-west-2a and us-west-2b, and does not use the third availability zone, us-west-2c. If one of these two AZs becomes unavailable, there will still be 3 EC2 instances running in the other AZ, which meets the requirement of having at least 6 EC2 instances running at all times. This option provides fault tolerance and is therefore a correct answer.\nC. 4 EC2 Instances in us-west-2a, 2 EC2 Instances in us-west-2b, and 2 EC2 Instances in us-west-2c This deployment puts the majority of the EC2 instances in us-west-2a, with 4 instances, and splits the remaining 2 instances across the other two AZs. If us-west-2a becomes unavailable, there will only be 4 EC2 instances remaining across the other two AZs, which is not enough to meet the requirement of 6 EC2 instances running at all times. Therefore, this option is not a correct answer.\nD. 6 EC2 Instances in us-west-2a, 6 EC2 Instances in us-west-2b, and no EC2 Instances in us-west-2c This deployment puts all 12 EC2 instances in two AZs, us-west-2a and us-west-2b, and does not use the third AZ, us-west-2c. Similar to option B, if one of these two AZs becomes unavailable, there will still be 6 EC2 instances running in the other AZ, which meets the requirement of having at least 6 EC2 instances running at all times. However, this option uses more resources than necessary, with 12 EC2 instances running instead of the required 6, and is therefore not a practical solution.\nE. 3 EC2 Instances in us-west-2a, 3 EC2 Instances in us-west-2b, and 3 EC2 Instances in us-west-2c. This deployment evenly distributes the EC2 instances across all three availability zones. However, if one AZ becomes unavailable, there will only be 2 EC2 instances running in each of the remaining AZs, which is not enough to meet the requirement of 6 EC2 instances running at all times.\n\n"
}, {
  "id" : 48,
  "question" : "An application allows a manufacturing site to upload files.\nEach uploaded 2500 MB file is processed to extract metadata, and this process takes a few seconds per file.\nThe frequency at which the uploading happens is unpredictable.\nFor instance, there may be no upload for hours, followed by several files being uploaded concurrently. Which architecture will address this workload in the most cost-efficient manner?\n",
  "answers" : [ {
    "id" : "d451fec862fa4ec58c5ea01e7dab31af",
    "option" : "Use a Kinesis Data Delivery Stream to store the file. Use Lambda for processing.",
    "isCorrect" : "false"
  }, {
    "id" : "80aafc438b8b426a912dd676d3034d1e",
    "option" : "Use an SQS queue to store the file to be accessed by a fleet of EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "43c76cf259724c09836ec907c1d8980b",
    "option" : "Store the file in an EBS volume, which can then be accessed by another EC2 Instance for processing.",
    "isCorrect" : "false"
  }, {
    "id" : "448a3dfa9ee64ddead2a8e37cfc3e9dd",
    "option" : "Store the file in an S3 bucket. Use Amazon S3 event notification to invoke a Lambda function for file processing.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nYou can first create a Lambda function with the code to process the file.\nThen, you can use an Event Notification from the S3 bucket to invoke the Lambda function whenever a file is uploaded.\nOption A is incorrect as Kinesis is used to collect, process, and analyze real-time data.\nOption B is incorrect as SQS cannot store a message that is 3GB.\nThe maximum payload supported by SQS is 2GB.To manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the Amazon SQS Extended Client Library for Java.\nThis is especially useful for storing and consuming messages up to 2 GB.Option C is incorrect as EBS is a service to provide block-level storage.\nS3 is more suitable in this scenario.\nNote: The total volume of data and the number of objects you can store are unlimited.\nIndividual Amazon S3 objects can range in size from a minimum of 0 bytes to a maximum of 5 terabytes.\nThe largest object that can be uploaded in a single PUT is 5 gigabytes.\nFor more information on Amazon S3 event notification, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\nhttps://aws.amazon.com/s3/faqs/\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html\n\n\nThis question is asking for an architecture that can handle the unpredictable upload of large files, process the files by extracting metadata, and is cost-efficient. Let's look at each answer option in detail:\nA. Use a Kinesis Data Delivery Stream to store the file. Use Lambda for processing. Kinesis is a real-time data streaming service that is primarily used for ingesting and processing large amounts of data. Using Kinesis for this use case might not be the most cost-efficient option because it is designed for high-volume streaming data. Additionally, Kinesis is not well-suited for processing large files like the 2500 MB files in this use case. Lambda can process the files, but it may not be the most cost-effective option since it charges per execution.\nB. Use an SQS queue to store the file to be accessed by a fleet of EC2 Instances. SQS is a message queue service that enables decoupling of components in a distributed system. Using an SQS queue to store the files can help with the unpredictable workload since messages will be stored until they are processed. Using a fleet of EC2 instances to process the files can help scale the workload as needed. However, this approach may not be the most cost-effective option since it requires maintaining and managing a fleet of EC2 instances.\nC. Store the file in an EBS volume, which can then be accessed by another EC2 Instance for processing. EBS is a block storage service that provides persistent storage volumes for use with EC2 instances. Storing the files in an EBS volume can provide persistent storage for the files until they are processed. However, this approach may not be the most cost-effective option since it requires maintaining and managing an EC2 instance to process the files.\nD. Store the file in an S3 bucket. Use Amazon S3 event notification to invoke a Lambda function for file processing. S3 is an object storage service that can store and retrieve large amounts of data at a low cost. Storing the files in an S3 bucket can provide durable and cost-effective storage for the files. Using S3 event notifications to trigger a Lambda function can provide a scalable and cost-effective way to process the files. Lambda charges per execution, but the cost can be minimized by using the correct memory allocation and optimizing the code.\nOverall, option D, storing the files in an S3 bucket and using S3 event notifications to trigger a Lambda function, is the best option for this use case. It provides durable and cost-effective storage, scalable and efficient processing, and is easy to manage.\n\n"
}, {
  "id" : 49,
  "question" : "You are part of the IT team of a streaming application.\nYour application is hosted in two separate regions, us-east-1(N Virginia) and ap-south-1 (Mumbai)\nYour application recently became very popular, and now you have users from all around the world.\nHowever, these new users have been experiencing high latency in the application.\nHow can you solve this problem, keeping in mind that possible failovers in the app need to be solved very quickly?\n",
  "answers" : [ {
    "id" : "6ae785a70be74f0b86c6359c65af2a4e",
    "option" : "Enable a DNS-based traffic management solution with Geolocation route policies in Route53.",
    "isCorrect" : "false"
  }, {
    "id" : "43bab7cb76c9498da1f7ec11970692ea",
    "option" : "Enable AWS WAF to securely serve your application content to the nearest Edge Locations to the users.",
    "isCorrect" : "false"
  }, {
    "id" : "e4d6c953f7294437b9f301a67df398d9",
    "option" : "Enable Global Accelerator endpoint for your two regions.",
    "isCorrect" : "true"
  }, {
    "id" : "831f1281b1cd408a8d1bbc1496fbf5bc",
    "option" : "Enable Direct Connect.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect because if there is a failover, you will need to modify the source application's IP address or configure Route53 records.\nThat will take time to solve the failover.\nMore details please check https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo.\nOption B is incorrect because AWS WAF is a service to protect applications from attacks.\nIt does not help to improve the performance or reduce latency.\nOption C is CORRECT because AWS Global Accelerator is a service that redirects users requests to the nearest edge location and then routes the data to the Amazon global network, increasing the speed and security of data transfer, therefore, increasing the performance of our applications.\nIt also reroutes requests to healthy IPs if it fails and changes propagations.\nIt is automatic and lasts some seconds.\nMore details please check https://aws.amazon.com/global-accelerator/faqs/.\nOption D is incorrect because Direct Connect is a service used to increase data transfer between On-Premise data centers and AWS services.\nMore details: https://aws.amazon.com/directconnect/.\nHere is the additional explanation on AWS website on Global accelerator FAQ.\nWhy DNS/Route 53 is an inferior option as compared to Global accelerator.\nQ: How is AWS Global Accelerator different from a DNS-based traffic management solution?\nA: First, some client devices and internet resolvers cache DNS answers for long periods of time.\nSo when you make a configuration update, or there's an application failure or change in your routing preference, you don't know how long it will take before all of your users receive updated IP addresses.\nWith AWS Global Accelerator, you don't have to rely on the IP address caching settings of client devices.\nChange propagation takes a matter of seconds, which reduces your application downtime.\nSecond, with Global Accelerator, you get static IP addresses that provide a fixed entry point to your applications.\nThis lets you easily move your endpoints between Availability Zones or between AWS Regions, without having to update the DNS configuration or client-facing applications.\n\nThe issue faced by the streaming application is high latency, which means the delay between the user's request and the response from the application is significant. This delay could be due to various factors, including the distance between the user and the application servers, network congestion, and other factors.\nTo solve this issue, we need to consider the global nature of the application's users and the need for fast failover in case of any disruptions. Let's explore the answer choices and see how they can address the issue:\nA. Enable a DNS-based traffic management solution with Geolocation route policies in Route53. Route53 is a DNS-based service provided by AWS that allows users to route traffic to different resources based on various policies, including Geolocation. Geolocation routing policies allow us to route traffic based on the user's geographic location, which can help reduce latency. By using Route53, we can create multiple DNS records, each pointing to a different endpoint of the application deployed in the two regions, and then configure geolocation routing policies based on the user's location. However, this solution does not provide fast failover in case of disruptions.\nB. Enable AWS WAF to securely serve your application content to the nearest Edge Locations to the users. AWS WAF is a web application firewall service provided by AWS that can help protect web applications from common web exploits. It can also help route traffic to the nearest Edge location using AWS Global Accelerator. By using AWS WAF, we can direct traffic to the nearest Edge location, which can help reduce latency. However, like the previous solution, it does not provide fast failover in case of disruptions.\nC. Enable Global Accelerator endpoint for your two regions. AWS Global Accelerator is a service provided by AWS that can help improve the availability and performance of applications. It can help route traffic to the optimal AWS endpoint based on the user's location, health, and other factors. By using Global Accelerator, we can direct traffic to the nearest available endpoint, which can help reduce latency. Additionally, Global Accelerator provides fast failover by automatically redirecting traffic to healthy endpoints in case of disruptions in one or more regions. Therefore, this solution provides both latency reduction and fast failover.\nD. Enable Direct Connect. AWS Direct Connect is a service provided by AWS that allows users to establish a dedicated network connection between their data center and AWS. Direct Connect can help improve network performance, reduce latency, and provide a more consistent network experience. However, this solution may not be suitable for this particular scenario as the application is already hosted on AWS, and the issue is not related to the network connection between the data center and AWS.\nIn summary, the most suitable solution for this scenario is to enable Global Accelerator endpoint for the two regions. This solution can help reduce latency and provide fast failover in case of disruptions, ensuring a seamless user experience.\n\n"
}, {
  "id" : 50,
  "question" : "For which of the following scenarios should a Solutions Architect consider using ElasticBeanStalk? (Choose TWO.)\n",
  "answers" : [ {
    "id" : "97a7435d5cb047c1964ffe516d9c7c4a",
    "option" : "A Java web application using Amazon Linux EC2 instances",
    "isCorrect" : "true"
  }, {
    "id" : "5e54b4e6e9e64f7883f739ff186d2b8d",
    "option" : "An Enterprise Data Warehouse",
    "isCorrect" : "false"
  }, {
    "id" : "7bb4c7363b3542d98af1346bc91150cf",
    "option" : "Configuring AWS resources using Chef",
    "isCorrect" : "false"
  }, {
    "id" : "9dc32350fecd491690bfcaee132eb0e7",
    "option" : "A worker environment with an SQS queue and an Auto Scaling group",
    "isCorrect" : "true"
  }, {
    "id" : "5a592b08aabe449ca5def84415116f6a",
    "option" : "A management task run once on nightly basis.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and D.\nAWS Documentation clearly mentions that the Elastic Beanstalk component can create Web Server environments and Worker environments.\nThe following diagram illustrates an example of Elastic Beanstalk architecture for a web server environment tier and shows how the components in that type of environment tier work together.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features-managing-env-tiers.html\nFor more information on AWS Elastic beanstalk Web server environments, please visit the URL https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-webserver.html.\nOption B is incorrect.\nElastcibeanstalk is used to deploy and manage the applications on AWS.\nIt's not used to store the data.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html.\nFor more information on AWS Elastic beanstalk Worker environments, please visit the following URL: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts-worker.html.\nOption C is incorrect.\nOpsWorks is more suitable when Chef is used to configure AWS resources.\nOption D is correct.\nThe worker environments in Elastic Beanstalk include an Auto Scaling group and an SQS queue.\nThis option is suitable.\nOption E is incorrect.\nWhen you launch an Elastic Beanstalk environment, you first choose an environment tier.\nThe environment tier that you choose determines whether Elastic Beanstalk provisions resources to support an application that handles HTTP requests or an application that pulls tasks from a queue.\nAn application that serves HTTP requests runs in a web server environment.\nAn environment that pulls tasks from an Amazon Simple Queue Service queue runs in a worker environment.\nFurther, when you create an environment, Elastic Beanstalk provisions the resources required to run your application.\nAWS resources created for an environment include one elastic load balancer (ELB in the diagram), an Auto Scaling group, and one or more Amazon EC2 instances.\nSo, these resources must run the application 24/7, not for only at night or day.\n\n\nElastic Beanstalk is a fully-managed service provided by AWS that makes it easy to deploy and run applications in multiple programming languages, including Java, .NET, Node.js, PHP, Python, Ruby, and Go. It handles deployment, capacity provisioning, load balancing, and automatic scaling, allowing developers to focus on writing code without worrying about infrastructure.\nA Solutions Architect should consider using Elastic Beanstalk for the following scenarios:\nA. A Java web application using Amazon Linux EC2 instances Elastic Beanstalk is an ideal choice for deploying and managing web applications built using Java programming language. It simplifies the process of deploying and scaling web applications by automatically provisioning and configuring the underlying Amazon EC2 instances, load balancers, and other AWS services.\nD. A worker environment with an SQS queue and an Auto Scaling group Elastic Beanstalk can also be used to manage worker environments that process tasks in the background. In this scenario, Elastic Beanstalk can be used to deploy worker instances that listen to a SQS queue and automatically scale the number of instances based on the number of messages in the queue. This ensures that the workload is evenly distributed and the system can handle large volumes of work efficiently.\nThe other options are not suitable for Elastic Beanstalk:\nB. An Enterprise Data Warehouse Elastic Beanstalk is not designed for deploying and managing data warehousing solutions. Data warehousing requires specialized infrastructure and configuration that is better handled by AWS services such as Amazon Redshift or Amazon EMR.\nC. Configuring AWS resources using Chef Chef is a popular configuration management tool that can be used to automate the deployment and management of AWS resources. However, Elastic Beanstalk is a higher-level service that abstracts away the details of managing infrastructure, and therefore is not suitable for use with Chef.\nE. A management task run once on nightly basis. Elastic Beanstalk is designed for running web applications and background workers that need to be available all the time. Running a management task once a night is better suited to AWS Lambda or AWS Batch, which are serverless computing services that allow you to run code without having to provision or manage servers.\n\n"
}, {
  "id" : 51,
  "question" : "You need to install a 150 GB volume on an EC2 Instance for a new application.\nWhile the data in the volume is used less frequently with small peaks in the morning and evening, Which storage type would be the most cost-effective option for the given requirement?\n",
  "answers" : [ {
    "id" : "b45879869a3846e5997a64d852e477aa",
    "option" : "Amazon EBS provisioned IOPS SS.",
    "isCorrect" : "false"
  }, {
    "id" : "b64f3163f54f485f9f13753c7b9e873b",
    "option" : "Amazon EBS Cold HD.",
    "isCorrect" : "true"
  }, {
    "id" : "99016cd3ad0d4f1b9b55e917acfd059b",
    "option" : "Amazon EBS General Purpose SS.",
    "isCorrect" : "false"
  }, {
    "id" : "f681df1ef61b4f1ab10922472bec813e",
    "option" : "Amazon EFS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe volume data is used infrequently, not throughout the day, and the question requires the MOST cost-effective storage type.\nSo the preferred choice would be Amazon Cold HDD.\nCold HDD is suitable for the following scenarios:\nThroughput-oriented storage for data that is infrequently accessed.\nScenarios where the lowest storage cost is important.\nThe volume size of EBS Cold HDD is 125 GiB - 16 TiB.\nThe database size is 150G and is suitable in this scenario.\nOption D is incorrect because EFS is file storage, not block or volume storage.\nNote: IOPS measures the number of reads and writes operations per second, while throughput measures the number of bits read or written per second.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html\nhttps://aws.amazon.com/ebs/features/\n\nBased on the given requirements of needing a 150 GB volume on an EC2 instance for a new application, with data that is used less frequently with small peaks in the morning and evening, the most cost-effective storage option would be Amazon EBS Cold HDD.\nAmazon EBS Cold HDD is a cost-effective storage option designed for workloads that need infrequent access to data. It is optimized for large, sequential workloads and is designed for infrequent access, such as backups and archives. The data retrieval times may be longer compared to other EBS storage options, but it provides cost savings.\nThe other options listed in the question are as follows:\nA. Amazon EBS Provisioned IOPS SSD: This option is designed for high-performance workloads that require low-latency and consistent I/O performance. It is a good option for applications that need high IOPS and throughput.\nB. Amazon EBS Cold HD: This option is designed for infrequent access workloads with large sequential workloads.\nC. Amazon EBS General Purpose SSD: This option is a good fit for small and medium-sized workloads with moderate I/O requirements.\nD. Amazon EFS: This option is a fully-managed file system for use with AWS Cloud services and on-premises resources. It is suitable for shared access to files across multiple instances.\nTherefore, based on the given requirements, Amazon EBS Cold HDD would be the most cost-effective option for the EC2 instance with a 150 GB volume.\n\n"
}, {
  "id" : 52,
  "question" : "You are working as an AWS consultant for a start-up company.\nThey have developed a web application, that requires a lot of memory, for their employees to share files with external vendors securely.\nThey created an AutoScaling group for the web servers that require two m4.large EC2 instances running at all times, scaling up to a maximum of twelve instances.\nPost-deployment of the application, a huge rise in cost was observed.\nDue to a limited budget, the CTO has requested your advice to optimize the usage of instances in the Auto Scaling groups.\nWhat do you suggest for reducing the costs and minimizing the risk of adverse impact on the performance?\n",
  "answers" : [ {
    "id" : "d410933b49f34e799faf1db96e9cc96c",
    "option" : "Create an Auto Scaling group with t2. micro On-Demand instances.",
    "isCorrect" : "false"
  }, {
    "id" : "04e19fe4f0f14b43a13d3194eff85503",
    "option" : "Create an Auto Scaling group with a mix of On-Demand &amp; Spot Instance. Select the On-Demand base as zero. Above On-Demand base, select 100% of On-Demand instance &amp; 0% of Spot Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "65487a1a7d624c519a770624e3c0e7df",
    "option" : "Create an Auto Scaling group with all Spot Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "0240c38073b9455a8f1309541376dc7d",
    "option" : "Create an Auto Scaling group with a mix of On-Demand &amp; Spot Instance. Select the On-Demand base as 2. Above On-Demand base, select 20% of On-Demand instance &amp; 80% of Spot Instance.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAuto Scaling group supports a mix of On-Demand &amp; Spot instances, which helps design a cost-optimized solution without impacting the performance.\nYou can choose the percentage of On-Demand &amp; Spot instances based on the application requirements.\nWith Option D, the Auto Scaling group will have 2 instances initially as the On-Demand instances.\nIn contrast, the remaining instances will be launched in a 20 % On-Demand instance &amp; 80% Spot Instance ratio.\nNo matter, if 80% of spot instances get terminated.\nThe required 20% on-demand will be available with 2 on-demand instances always running.\nOption A is incorrect.\nWith t2\nMicro, there would be a cost reduction, but it will impact the performance of the application.\nThe question requires that the performance is not impacted, so changing the instance type is not suitable.\nOption B is incorrect as there would not be any cost reduction with all On-Demand instances.\nOption C is incorrect.\nAlthough this will reduce cost, all spot instances in an auto-scaling group may cause inconsistencies in the application &amp; lead to poor performance.\nFor more information on Auto Scaling with multiple Instance types &amp; purchase options, refer to the following URLs:\nhttps://aws.amazon.com/blogs/aws/new-ec2-auto-scaling-groups-with-multiple-instance-types-purchase-options/\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html#asg-purchase-options\n\nWhen optimizing the usage of instances in the Auto Scaling group, there are several factors to consider, such as cost, performance, and availability. In this scenario, the goal is to reduce costs while minimizing the risk of adverse impact on performance.\nOption A: Create an Auto Scaling group with t2.micro On-Demand instances.\nThis option suggests using On-Demand instances, which are instances that are purchased at a fixed rate and can be used for as long as needed. The t2.micro instance type is a cost-effective option with a low hourly rate, but it only has 1GB of memory, which may not be sufficient for the web application's memory requirements. Additionally, since the application requires a lot of memory, running multiple t2.micro instances may not be optimal for performance.\nOption B: Create an Auto Scaling group with a mix of On-Demand and Spot Instances. Select the On-Demand base as zero. Above On-Demand base, select 100% of On-Demand instance & 0% of Spot Instance.\nThis option suggests using a mix of On-Demand and Spot Instances. Spot Instances are instances that are available for purchase at a lower hourly rate but can be interrupted by AWS if the spot price goes above the user's bid. This option selects a zero On-Demand base, which means all instances will be Spot Instances unless there is insufficient capacity, in which case On-Demand instances will be used. This option is not recommended since it has a high risk of interruptions, which may negatively impact performance.\nOption C: Create an Auto Scaling group with all Spot Instances.\nThis option suggests using only Spot Instances, which have a lower hourly rate than On-Demand instances. However, as mentioned earlier, Spot Instances can be interrupted by AWS if the spot price goes above the user's bid, which may negatively impact performance. Using only Spot Instances may also increase the risk of capacity constraints, which could result in reduced availability.\nOption D: Create an Auto Scaling group with a mix of On-Demand and Spot Instances. Select the On-Demand base as 2. Above On-Demand base, select 20% of On-Demand instances & 80% of Spot Instances.\nThis option suggests using a mix of On-Demand and Spot Instances, with a base of two On-Demand instances and 20% of On-Demand instances above the base. This option provides a balance between cost, performance, and availability. The two On-Demand instances ensure that there is always a minimum capacity available, while the use of Spot Instances helps reduce costs. The 20% On-Demand instances above the base provide additional capacity to handle spikes in demand. This option is recommended as it minimizes the risk of interruptions and capacity constraints while also reducing costs.\nIn summary, option D is the recommended solution for reducing costs and minimizing the risk of adverse impact on performance.\n\n"
}, {
  "id" : 53,
  "question" : "You are working as an AWS Architect for a start-up company.\nThe company has a two-tier production website on AWS with web servers in the front end &amp; database servers in the back end.\nThe third-party firm has been looking after the operations of these database servers.\nThey need to access these database servers in private subnets on the SSH port.\nAs per standard operating procedure provided by the Security team, all access to these servers should be over a jumpbox accessible from internet.\nWhat will be the best solution to meet this requirement?\n",
  "answers" : [ {
    "id" : "e587d966dcdc43dfb8157cde86517b4e",
    "option" : "Deploy Bastion hosts in Private Subnet",
    "isCorrect" : "false"
  }, {
    "id" : "6c6bcbf2bcf94588bc3bf4b52c93dfd6",
    "option" : "Deploy NAT Instance in Private Subnet",
    "isCorrect" : "false"
  }, {
    "id" : "06984d50a894462dabbdaa3f20b42e07",
    "option" : "Deploy NAT Instance in Public Subnet",
    "isCorrect" : "false"
  }, {
    "id" : "d388d5356dbd47a08d96ade5660ba5df",
    "option" : "Deploy Bastion hosts in Public Subnet.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nExternal users will be unable to access the instance in private subnets directly.\nTo provide such access, we need to deploy Bastion hosts in public subnets.\nIn case of the above requirement, third-party users will initiate a connection to Bastion hosts in public subnets &amp; from there, they will access SSH connection to database servers in private subnets.\nOption A is incorrect as Bastion hosts need to be in Public subnets &amp; not in Private subnets, as third-party users will be accessing these servers from the internet.\nOption B is incorrect as NAT instance is used to provide internet traffic to hosts in private subnets.\nUsers from the internet will not be able to do SSH connections to hosts in private subnets using NAT instance.\nNAT instance is always present in Public subnets.\nOption C is incorrect as NAT instance is used to provide internet traffic to hosts in private subnets.\nUsers from the internet will not be able to do SSH connections to hosts in private subnets using NAT instance.\nFor more information on bastion instance, refer to the following URL:\nhttps://docs.aws.amazon.com/quickstart/latest/linux-bastion/architecture.html\n\nThe best solution to meet the requirement of providing third-party access to the database servers in private subnets over SSH port via a jumpbox accessible from the internet is to deploy a Bastion host in the private subnet.\nExplanation: A Bastion host, also known as a Jump host or Jump box, is a secure gateway server that allows users to connect to a private network securely. The Bastion host acts as a single point of entry to access the resources in the private subnet, in this case, the database servers.\nOption A: Deploying a Bastion host in the private subnet is the best solution as it provides secure access to the database servers without exposing them to the internet. The Bastion host acts as a gateway for SSH traffic to the database servers, and all traffic is routed through it. This approach ensures that only authorized users can access the database servers via the Bastion host.\nOption B: Deploying a NAT instance in the private subnet is not an appropriate solution for this scenario, as NAT instances are used to provide internet access to resources in private subnets. They are not used for accessing resources from the internet.\nOption C: Deploying a NAT instance in the public subnet is not an appropriate solution for this scenario, as NAT instances are used to provide internet access to resources in private subnets. They are not used for accessing resources from the internet.\nOption D: Deploying a Bastion host in the public subnet is not an appropriate solution for this scenario, as it exposes the Bastion host and the database servers to the internet. This approach is not secure, and it violates the security standards provided by the security team.\nTherefore, Option A is the best solution to meet the requirement of providing third-party access to the database servers in private subnets over SSH port via a jumpbox accessible from the internet.\n\n"
}, {
  "id" : 54,
  "question" : "An AWS Solutions Architect designing a solution to store and archive corporate documents has determined Amazon Glacier as the right choice.\nAn important requirement is that the data must be delivered within 5 minutes of a retrieval request. Which feature in Amazon Glacier could help to meet this requirement?\n",
  "answers" : [ {
    "id" : "7e97b0f90c464dc3bfbdbffa676b3fb7",
    "option" : "Vault Lock",
    "isCorrect" : "false"
  }, {
    "id" : "af9cd1531d10468ab428ebf97ab79563",
    "option" : "Expedited retrieval",
    "isCorrect" : "true"
  }, {
    "id" : "e50902eef53a4d55a385eae735ae8c5c",
    "option" : "Bulk retrieval",
    "isCorrect" : "false"
  }, {
    "id" : "d2de8f2ffc094c3993c4de3dfbb46a4c",
    "option" : "Standard retrieval.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following:\nExpedited retrievals allow you to access data in 1-5 minutes for a flat rate of $0.03 per GB retrieved.\nExpedited retrievals allow you to quickly access your data when occasional urgent requests for a subset of archives are required.\nThe Vault Lock and Standard Retrieval are standard with 3-5 hours retrieval time while Bulk retrievals which can be considered the cheapest option have 5-12 hours retrieval time.\nFor more information on AWS Glacier Retrieval, please visit the following URL:\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html\n\nThe correct answer is B. Expedited retrieval.\nAmazon Glacier is a cloud-based storage service that is optimized for data archiving and backup. It is designed for storing data that is infrequently accessed and has a long retention period. When data is stored in Amazon Glacier, it is stored in archives. An archive is a collection of one or more files that are grouped together into a single unit.\nAmazon Glacier provides three retrieval options for archives:\nStandard retrieval: This is the default option for retrieving archives. It typically takes 3-5 hours to retrieve an archive using this option. Bulk retrieval: This option is designed for retrieving large amounts of data. It can take several hours to retrieve an archive using this option. Expedited retrieval: This option is designed for retrieving archives quickly. Archives can be retrieved in as little as 1-5 minutes using this option.\nGiven the requirement that the data must be delivered within 5 minutes of a retrieval request, the best retrieval option to choose is Expedited retrieval (Option B).\nVault Lock (Option A) is a feature that allows you to lock down your Amazon Glacier vault with a policy that cannot be modified or deleted. This can help to prevent data loss or tampering.\nBulk retrieval (Option C) is a retrieval option that is designed for retrieving large amounts of data. It can take several hours to retrieve an archive using this option.\nStandard retrieval (Option D) is the default option for retrieving archives. It typically takes 3-5 hours to retrieve an archive using this option.\n\n"
}, {
  "id" : 55,
  "question" : "You are working for a start-up company that develops mobile gaming applications using AWS resources.\nFor creating AWS resources, the project team is using CloudFormation Templates.\nThe Project Team is concerned about the changes made in EC2 instance properties by the Operations Team, apart from parameters specified in CloudFormation Templates.\nTo observe changes in AWS EC2 instance, you advise using CloudFormation Drift Detection.\nAfter Drift detection, when you check drift status for all AWS EC2 instances, drift for certain property values with default values for resource properties is not displayed.\nWhat would you do to include these resource properties to be captured in CloudFormation Drift Detection?\n",
  "answers" : [ {
    "id" : "58d0b271e3674a539e8825719999c7f8",
    "option" : "Run CloudFormation Drift Detection on individual stack resources instead of entire CloudFormation stack.",
    "isCorrect" : "false"
  }, {
    "id" : "8972ac1837de43838f0958ef7c7e47e0",
    "option" : "Explicitly set the property value, which can be the same as the default value.",
    "isCorrect" : "true"
  }, {
    "id" : "d4c7fb2ef2ec41d4b9e5da48dfed40f0",
    "option" : "Manually check these resources as this is not supported in CloudFormation Drift Detection.",
    "isCorrect" : "false"
  }, {
    "id" : "a573d34e8eb74820ac10ccfe697d136a",
    "option" : "Assign Read permission to CloudFormation Drift Detection to determine drift.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS CloudFormation Drift Detection can be used to detect changes made to AWS resources outside the CloudFormation Templates.\nAWS CloudFormation Drift Detection only checks property values explicitly set by stack templates or by specifying template parameters.\nIt does not determine drift for property values that are set by default.\nTo determine drift for these resources, you can explicitly set property values that can be the same as that of the default value.\nOption A is incorrect.\nIf property values are assigned explicitly to these properties, running AWS CloudFormation Drift Detection would be detected in both individuals and the entire CloudFormation Stack.\nOption C is incorrect as CloudFormation Drift Detection supports the AWS EC2 instance.\nOption D is incorrect.\nSince for all other resources, CloudFormation Drift Detection has already determined drift, there is no other read permission to be granted further.\nFor more information on CloudFormation Drift Detection, refer to the following URL:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-drift.html\n\nCloudFormation is an AWS service that helps you model and set up your AWS resources so that you can spend less time managing those resources and more time focusing on your applications. When you use CloudFormation to create and manage AWS resources, you define a template that describes the resources and their configurations. CloudFormation uses this template to provision and manage the resources.\nDrift detection is a CloudFormation feature that enables you to detect whether the actual resources in your stack differ from the expected resources defined in your template. When you detect drift, you can compare the actual stack resources to the expected stack resources and take corrective action, such as updating or deleting resources.\nIn this scenario, the project team is concerned about changes made in EC2 instance properties by the operations team that are not specified in the CloudFormation templates. The project team wants to observe these changes and take corrective action if necessary. To do this, you suggest using CloudFormation drift detection.\nHowever, when you check drift status for all AWS EC2 instances, drift for certain property values with default values for resource properties is not displayed. This is because CloudFormation only detects drift for properties that are explicitly set in the CloudFormation template. If a property has a default value and is not set in the template, CloudFormation assumes that the property has not changed and does not detect drift for that property.\nTo include these resource properties to be captured in CloudFormation drift detection, you have a few options:\nA. Run CloudFormation Drift Detection on individual stack resources instead of the entire CloudFormation stack. This allows you to focus on specific resources and detect drift for properties that are not explicitly set in the CloudFormation template.\nB. Explicitly set the property value, which can be the same as the default value. This ensures that CloudFormation detects drift for that property even if it has a default value.\nC. Manually check these resources as this is not supported in CloudFormation Drift Detection. This option requires manual effort, but it ensures that you are aware of any changes made to the resources, even if they are not captured by CloudFormation drift detection.\nD. Assign Read permission to CloudFormation Drift Detection to determine drift. This option does not address the issue of properties with default values, but it ensures that CloudFormation has the necessary permissions to detect drift for the resources.\nIn summary, to include resource properties with default values in CloudFormation drift detection, you can run drift detection on individual stack resources, explicitly set the property value, manually check the resources, or assign read permission to CloudFormation drift detection.\n\n"
}, {
  "id" : 56,
  "question" : "You are responsible for performing a migration from your company's on-premise data to the AWS cloud.\nYou have about 400 GB of data stored in an NFS.\nOne requirement of this migration is to transfer some of this data to AWS EFS and the other part to S3\nWhich is the easiest to use and with the most cost-effective solution?\n",
  "answers" : [ {
    "id" : "ce81cecc0d674488b2ee58bc6ce753e5",
    "option" : "Use AWS Storage gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "423d6ac208d347c19ea90699065db5d8",
    "option" : "Use S3 Transfer Acceleration.",
    "isCorrect" : "false"
  }, {
    "id" : "c61ce487931647dfa8b209a1caad13b5",
    "option" : "Use AWS DataSync.",
    "isCorrect" : "true"
  }, {
    "id" : "69176bd350fd43eea4ec222ec4edf4b9",
    "option" : "Use AWS Database Migration Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect.\nStorage Gateway is a hybrid cloud storage service to share and access data between your on-premise resources and AWS resources.\nIt is not mainly designed to migrate data from On-Premise to AWS.\nAdditionally, all three storage gateway patterns are backed by S3, not EFS.\nMore details: https://aws.amazon.com/storagegateway/faqs/.\nOption B is incorrect.\nS3 Transfer Acceleration is used to transfer data to S3 using Amazon CloudFront's globally distributed edge locations, increasing data transfer speed.\nHowever, this option doesn't work to transfer data to AWS EFS.\nMore details: https://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html.\nOption C is CORRECT.\nDataSync is a service used to transfer data between on-premise storage to AWS S3, EFS and FSx.\nIt is cost-effective, easy to use, and can handle the transfer to EFS and S3\nMore details: https://aws.amazon.com/datasync/faqs/.\nOption D is incorrect because AWS Database Migration Service is used to migrate databases to AWS databases like Aurora, DynamoDB, etc.\nIn this scenario, we don't mention a database migration, and with this service, you can not transfer data to EFS nor S3\nMore details: https://aws.amazon.com/dms/.\n\nThe best option for transferring data from on-premise to AWS depends on several factors, such as the amount of data, the transfer speed, the cost, and the requirements for data storage in AWS.\nIn this case, the question mentions that you have about 400 GB of data stored in an NFS, and you need to transfer some of it to EFS and the rest to S3. Therefore, you need a solution that can handle this transfer and provide cost-effective storage in AWS.\nOption A, AWS Storage Gateway, is a hybrid storage service that enables you to securely store data in AWS cloud for on-premise access. However, it is not the best option for this specific scenario, as it doesn't support direct transfer to EFS and S3. Instead, it requires additional configuration and management overhead to set up an EBS volume or a file gateway to access EFS and S3.\nOption B, S3 Transfer Acceleration, is a service that accelerates transfers to and from Amazon S3 by using Amazon CloudFront's globally distributed edge locations. While it could be used to transfer data to S3, it doesn't provide support for EFS, which is also a requirement for this migration. Additionally, S3 Transfer Acceleration can incur additional costs, especially if the data is not transferred frequently.\nOption C, AWS DataSync, is a fully-managed service that makes it easy to move data between on-premises storage and AWS services. AWS DataSync can be used to transfer data directly to EFS and S3, which makes it a good fit for this scenario. AWS DataSync supports NFS as a source file system and can transfer data over the internet or AWS Direct Connect. However, AWS DataSync may not be the most cost-effective solution for small data migrations due to the minimum duration of the service being 10 hours.\nOption D, AWS Database Migration Service, is a service that can be used to migrate databases to AWS. However, it is not suitable for transferring NFS data to EFS and S3, which are file-based storage services.\nBased on the requirements, the best option for this scenario is Option C, AWS DataSync. It supports direct transfer to EFS and S3, can handle NFS data, and provides a cost-effective solution for data migration.\n\n"
}, {
  "id" : 57,
  "question" : "A company hosts a popular web application that connects to an Amazon RDS MySQL DB instance running in a default VPC private subnet with NACL settings that was created by AWS as default.\nThe web servers must be accessible only to customers on HTTPS connections, and the database must only be accessible to web servers in a public subnet.\nWhich solution would meet these requirements without impacting other applications? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "b8cd31569f1e4fbea6d25160b2511dca",
    "option" : "Create a network ACL on the Web Server`s subnets, allow HTTPS port 443 inbound and specify the source as 0.0.0.0/0.",
    "isCorrect" : "false"
  }, {
    "id" : "2904556ca2704bd58dd7371cd7469d7e",
    "option" : "Create a Web Server security group that allows HTTPS port 443 inbound traffic from anywhere (0.0.0.0/0) and apply it to the Web Servers.",
    "isCorrect" : "true"
  }, {
    "id" : "22160dac678a4f6b8964b2600f213286",
    "option" : "Create a DB Server security group that allows MySQL port 3306 inbound and specify the source as the Web Server security group.",
    "isCorrect" : "true"
  }, {
    "id" : "c646c3e174e94eaeb89be7e43310ea38",
    "option" : "Create a network ACL on the DB subnet, allow MySQL port 3306 inbound for Web Servers and deny all outbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "7707de8b237345e6bd737b40b71fe44b",
    "option" : "Create a DB Server security group that allows HTTPS port 443 inbound and specify the source as a Web Server security group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nThis sort of setup is explained in the AWS documentation.\n1) To ensure that traffic can flow into your webserver from anywhere on secure traffic, you need to allow inbound security at 443.\n2) And then, you need to ensure that traffic can flow from the webserver to the database server via the database security group.\nThe below snapshots from the AWS Documentation show rule tables for security groups related to the same requirements as in the question.\nFor more information on this use case scenario, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\nOptions A and D are invalid answers.\nNetwork ACLs are stateless.\nSo we need to set rules for both inbound and outbound traffic for Network ACLs.\nOption E is also invalid because, in order to communicate with the MySQL servers, we need to allow traffic to flow through port 3306.\nNote: The above correct options are the combination of steps required to secure your web and database servers.\nBesides, the company may implement additional security measures from their end.\n\n\nTo meet the given requirements, we need to allow HTTPS traffic to the web server and restrict access to the database server to only the web servers in a public subnet. We can achieve this by implementing the following solutions:\nA. Create a network ACL on the Web Server's subnets, allow HTTPS port 443 inbound and specify the source as 0.0.0.0/0. This option allows incoming HTTPS traffic from anywhere, which is not recommended as it could open up the web server to attacks from unauthorized sources. Therefore, this option is not recommended.\nB. Create a Web Server security group that allows HTTPS port 443 inbound traffic from anywhere (0.0.0.0/0) and apply it to the Web Servers. This option allows incoming HTTPS traffic only to the web server and restricts access from unauthorized sources. It is a recommended solution.\nC. Create a DB Server security group that allows MySQL port 3306 inbound and specify the source as the Web Server security group. This option allows incoming traffic to the database server only from the web server security group, which restricts access from unauthorized sources. It is a recommended solution.\nD. Create a network ACL on the DB subnet, allow MySQL port 3306 inbound for Web Servers and deny all outbound traffic. This option allows incoming traffic to the database server only from the web server and restricts outbound traffic from the database server. However, it does not restrict access from other sources, which makes it an incomplete solution. Therefore, it is not a recommended solution.\nE. Create a DB Server security group that allows HTTPS port 443 inbound and specify the source as a Web Server security group. This option allows incoming HTTPS traffic only to the database server and restricts access from unauthorized sources. However, it does not restrict access to the MySQL port, which is required to access the database server. Therefore, it is not a recommended solution.\nIn conclusion, options B and C are the recommended solutions to meet the given requirements. Option B restricts incoming traffic to HTTPS traffic only, and option C restricts access to the database server only from the web server security group.\n\n"
}, {
  "id" : 58,
  "question" : "You lead a team to develop a new web application in AWS EC2\nThe application will have a large number of users globally.\nFor a great user experience, this application requires very low network latency and jitter.\nIf the network speed is not fast enough, you will lose customers.\nWhich tool would you choose to improve the application performance? (Select TWO.)\n",
  "answers" : [ {
    "id" : "8dcf8fcc9d0b4d6084351c81bf198ff3",
    "option" : "AWS VPN",
    "isCorrect" : "false"
  }, {
    "id" : "ed3c7715b42b4aa9a54f5f49245616b6",
    "option" : "AWS Global Accelerator",
    "isCorrect" : "true"
  }, {
    "id" : "5ae2cf1a58a1424fbeb3a9fa2d4ef0ac",
    "option" : "Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "28fcd6b753a546848f9e0d0ef4e1ce94",
    "option" : "API Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "a2c33d0c9e084c4283e3ad2b0c7ccc2f",
    "option" : "CloudFront.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - B, E.\nThis web application has global users and needs low latency.\nBoth CloudFront and Global Accelerator can speed up the distribution of contents over the AWS global network.\nOption A is incorrect: AWS VPN links on-premise network to AWS network.\nHowever, no on-premise services are mentioned in this question.\nOption B is CORRECT: AWS Global Accelerator works at the network layer and can direct traffic to optimal endpoints.\nFor reference, check the link https://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html.\nOption C is incorrect: Direct Connect links on-premise network to AWS network.\nHowever, no on-premise services are mentioned in this question.\nOption D is incorrect: API Gateway is a regional service and cannot improve the application performance.\nAPI Gateway is suitable for serverless applications such as Lambda.\nOption E is CORRECT: Because CloudFront delivers content through edge locations, and users are routed to the edge location with the lowest time delay.\n\nFor a web application that requires very low network latency and jitter, there are a few tools available on AWS that can be used to improve performance.\nThe two tools that would be suitable for this scenario are:\nAWS Global Accelerator: AWS Global Accelerator is a networking service that provides static IP addresses that can be used to route traffic to the optimal AWS endpoint based on geographic location, health checks, and routing policies. It improves the availability and performance of your applications by routing traffic over the AWS global network instead of the public internet. It can also automatically reroute traffic to healthy endpoints in case of an outage or degradation in performance. By using AWS Global Accelerator, you can reduce network latency and improve the user experience for customers globally. Amazon CloudFront: Amazon CloudFront is a content delivery network (CDN) that caches and delivers static and dynamic web content, including images, videos, and APIs, to end-users from the edge locations closest to them. This reduces the distance that requests must travel and improves the speed at which content is delivered to users. CloudFront also provides features like SSL/TLS encryption, custom domain names, and geo-restriction to help improve the security, reliability, and performance of your web application.\nTherefore, in order to improve the performance of the web application with low latency and jitter, it is recommended to use AWS Global Accelerator and Amazon CloudFront in combination.\n\n"
}, {
  "id" : 59,
  "question" : "You are deploying an application on Amazon EC2 that must call AWS APIs.\nWhich method would you use to allow the application access to the APIs securely?\n",
  "answers" : [ {
    "id" : "38032dc9f60b400fbe5a4bc1e5419991",
    "option" : "Pass API credentials to the instance using Instance userdata.",
    "isCorrect" : "false"
  }, {
    "id" : "7c4d53601ee14127aa6d6f4fc0f03381",
    "option" : "Store API credentials as an object in Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "7678015647bf4bb6b5c1b67fdf3595b3",
    "option" : "Embed the API credentials into your application.",
    "isCorrect" : "false"
  }, {
    "id" : "2d9325086892494a9a9af708bd9b8da9",
    "option" : "Assign IAM roles to the EC2 Instances.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions the following:\nYou can use roles to delegate access to users, applications, or services that don't normally have access to your AWS resources.\nIt is not a good practice to use IAM credentials for a production-based application.\nHowever, it is a good practice to use IAM Roles.\nFor more information on IAM Roles, please visit the following URL:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\n\nThe most secure and recommended method for granting an application running on an Amazon EC2 instance access to AWS APIs is to assign IAM roles to the EC2 instances. Therefore, the correct answer is D.\nHere is a more detailed explanation of the four options:\nA. Passing API credentials to the instance using instance userdata: This is not a recommended method as userdata is not secure and can be intercepted. Also, passing credentials in clear text can be risky since anyone who can access the instance can potentially retrieve these credentials.\nB. Storing API credentials as an object in Amazon S3: This is also not a recommended method since S3 is not secure by default, and anyone with access to the S3 bucket can retrieve the credentials. Additionally, accessing the S3 bucket will require the credentials, so you end up in a catch-22 situation.\nC. Embedding the API credentials into your application: This method is also not secure as the credentials are embedded in clear text in the application code. Therefore, anyone who can access the application code can retrieve the credentials.\nD. Assigning IAM roles to the EC2 instances: This is the most secure and recommended method as it does not require the application to store credentials at all. Instead, the application can make API calls using temporary security credentials obtained from the EC2 instance metadata service. IAM roles allow you to define a set of permissions for the EC2 instances that the application can use. These permissions are based on policies attached to the role. The instance retrieves these temporary credentials from the metadata service and uses them to make API calls on behalf of the application.\nIn summary, IAM roles are the recommended and most secure method of granting applications running on EC2 instances access to AWS APIs.\n\n"
}, {
  "id" : 60,
  "question" : "A website runs on EC2 Instances behind an Application Load Balancer.\nThe instances run in an Auto Scaling Group across multiple Availability Zones and deliver several static files stored on a shared Amazon EFS file system.\nThe company needs to avoid serving the files from EC2 Instances every time a user requests these digital assets. What should the company do to improve the user experience of the website?\n",
  "answers" : [ {
    "id" : "fc4dcd04dd5f45a28ee4724c78c7c409",
    "option" : "Move the digital assets to Amazon Glacier.",
    "isCorrect" : "false"
  }, {
    "id" : "2285c09a8b5641f2bb4e602d20251245",
    "option" : "Cache static content using CloudFront.",
    "isCorrect" : "true"
  }, {
    "id" : "d937dbe623ea497c871baa06665be8b5",
    "option" : "Resize the images so that they are smaller.",
    "isCorrect" : "false"
  }, {
    "id" : "015a9044874f48ffa459ba76a0fb3760",
    "option" : "Use reserved EC2 Instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAWS Documentation mentions the following about the benefits of using CloudFront:\nAmazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files to your users.\nCloudFront delivers your content through a worldwide network of data centers called edge locations.\nWhen a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay), so that the content is delivered with the best possible performance.\nIf the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.\nFor more information on AWS CloudFront, please visit the following URL on page 3 under the section \"Accelerate Static Website Content Delivery\":\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\nThe glacier is not used for frequent retrievals.\nSo.\nOption A is not a good solution.\nOptions C &amp; D will also not help in this situation.\n\nOption A: Moving the digital assets to Amazon Glacier is not a suitable solution for improving the user experience of the website. Amazon Glacier is a low-cost storage service designed for long-term data archiving and backup, and it is not intended for frequently accessed files. Retrieving files from Amazon Glacier can take several hours, making it unsuitable for serving static files to users who expect a quick response time.\nOption B: Caching static content using CloudFront is the recommended solution for improving the user experience of the website. CloudFront is a content delivery network (CDN) that caches content in edge locations around the world, reducing the latency and improving the performance of the website. By configuring CloudFront to cache static files stored on the Amazon EFS file system, the company can serve the files directly from the edge locations, without relying on the EC2 instances every time a user requests these digital assets. This results in a faster response time for users, lower server load, and cost savings.\nOption C: Resizing the images so that they are smaller can improve the website's performance to some extent, but it may not be sufficient to meet the company's needs. While smaller images require less bandwidth and faster to load, they may still need to be served from the EC2 instances, causing high server load and slow response times during high traffic periods.\nOption D: Using reserved EC2 instances is not relevant to the question, as it does not address the issue of serving static files from EC2 instances every time a user requests these digital assets. Reserved instances are a billing option that allows companies to reserve EC2 capacity for one or three-year terms at a discounted price, which can be a cost-saving measure for companies with predictable workloads.\n\n"
}, {
  "id" : 61,
  "question" : "A Solutions Architect is designing a highly scalable system to track records.\nThese records must remain available for immediate download for up to three months and then must be deleted.\nWhat is the most appropriate decision for this use case?\n",
  "answers" : [ {
    "id" : "772e0ebb7b37490e965aebd3aa2b5761",
    "option" : "Store the files in Amazon EBS and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "false"
  }, {
    "id" : "771f479d1be44f65b7dba8e4797352ad",
    "option" : "Store the files in Amazon S3 and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "true"
  }, {
    "id" : "db0ddeddab80476aa16fa85f8ae7f81f",
    "option" : "Store the files in Amazon Glacier and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "false"
  }, {
    "id" : "e46aae23f6dd47e4806b3d3f918c1785",
    "option" : "Store the files in Amazon EFS and create a Lifecycle Policy to remove files after 3 months.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect since the records need to be stored in a highly scalable system.\nOption C is incorrect since the records must be available for immediate download.\nOption D is incorrect since EFS lifecycle management is used to migrate files that have not been accessed for a certain period of time to the Infrequent Access storage class.\nFiles moved to this storage remain indefinitely and not get deleted.\nAnd due to this reason, this option is not correct.\nAWS Documentation mentions the following about Lifecycle Policies:\nLifecycle configuration enables you to specify the Lifecycle Management of objects in a bucket.\nThe configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects.\nThese actions can be classified as follows:\nTransition actions - In which you define when the transition of the object occurs to another storage class.\nFor example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation or archive objects to the GLACIER storage class one year after creation.\nExpiration actions - In which you specify when the objects will expire.\nThen Amazon S3 deletes the expired objects on your behalf.\nFor more information on AWS S3 Lifecycle Policies, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\nTo know more about EFS Lifecycle Management, please check the following URL:\nhttps://docs.aws.amazon.com/efs/latest/ug//lifecycle-management-efs.html.\n\nThe most appropriate decision for this use case is to store the files in Amazon S3 and create a Lifecycle Policy to remove files after 3 months.\nAmazon S3 is a highly scalable object storage service that provides durability, availability, and security for storing and retrieving any amount of data from anywhere on the web. It is designed to scale up or down according to the needs of the application and provides high availability and durability of objects stored in it.\nIn this use case, the records need to be available for immediate download for up to three months and then must be deleted. Amazon S3 provides the necessary features to store these records for the required period and then delete them.\nTo achieve this, a Lifecycle Policy can be created in Amazon S3 that will automatically delete the files after three months. A Lifecycle Policy is a set of rules that can be defined to automatically move objects to different storage classes or delete them based on their age, size, or other criteria.\nAmazon EBS is a block storage service designed for use with Amazon EC2 instances, and Amazon EFS is a fully-managed network file system designed for use with AWS cloud services and on-premises resources. These services may not be appropriate for storing files that need to be available for immediate download and then deleted after a specific time.\nAmazon Glacier is a low-cost storage service designed for data archiving and long-term backup that is not immediately available for download. It is designed for use cases where data retrieval can take hours or days, which is not suitable for this use case. Therefore, Amazon S3 is the best option for storing the records in this use case.\n\n"
}, {
  "id" : 62,
  "question" : "A consulting firm repeatedly builds large architectures for their customers using AWS resources from several AWS services, including IAM, Amazon EC2, Amazon RDS, DynamoDB and Amazon VPC.\nThe consultants have architecture diagrams for each of their architectures and are frustrated that they cannot use them to create their resources automatically. Which service should provide immediate benefits to the organization?\n",
  "answers" : [ {
    "id" : "21a370aa13ba4cc68ce8a805c481e0d5",
    "option" : "AWS Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "e455863ed38f4377b02a46bec8416aee",
    "option" : "AWS CloudFormation",
    "isCorrect" : "true"
  }, {
    "id" : "933c040978e4497aa60ddf114e25c88b",
    "option" : "AWS CodeBuild",
    "isCorrect" : "false"
  }, {
    "id" : "17490b99cae7471d87fbbfbe03ae58cc",
    "option" : "AWS CodeDeploy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAWS CloudFormation: This supplements the requirement in the question and enables consultants to use their architecture diagrams to construct CloudFormation templates.\nAWS Documentation mentions the following on AWS CloudFormation:\nAWS CloudFormation is a service that helps you model and set up your Amazon Web Service resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS.\nYou create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances), and AWS CloudFormation takes care of provisioning and configuring those resources for you.\nFor more information on AWS Cloudformation, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js etc.\nYou can upload your code, and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring.\nIn question mentioned that \"A consulting firm repeatedly builds large architectures for their customers using AWS resources from several AWS services including IAM, Amazon EC2, Amazon RDS, DynamoDB and Amazon VPC.\"\nWhen you are building large architectures repeatedly, you can use the CloudFormation template to create or modify an existing AWS CloudFormation template.\nA template describes all of your resources and their properties.\nWhen you use that template to create an AWS CloudFormation stack, AWS CloudFormation provisions the Auto Scaling group, load balancer, and database for you.\nAfter the stack has been successfully created, your AWS resources are up and running.\nYou can delete the stack just as easily, which deletes all the resources in the stack.\nBy using AWS CloudFormation, you easily manage a collection of resources as a single unit.\nWhenever working with more number of AWS resources together, CloudFormation is the best option.\n\nThe service that can help the consulting firm create their AWS resources automatically from their architecture diagrams is AWS CloudFormation.\nAWS CloudFormation is a service that allows you to define and provision AWS infrastructure as code. It uses templates in either JSON or YAML format to create and manage a collection of related AWS resources, including IAM, EC2, RDS, DynamoDB, and VPC.\nWith CloudFormation, you can create a single template that describes all the resources needed for your application, and then use that template to provision and manage those resources. This helps to eliminate manual provisioning processes and reduces the potential for human error.\nIn this case, the consulting firm can use CloudFormation to create templates based on their existing architecture diagrams, and then use those templates to automatically provision the required resources for each new customer engagement. This would help to streamline their process and ensure that each new architecture is consistent with their standard practices.\nAWS Elastic Beanstalk is a service that helps deploy, manage, and scale web applications, while AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeDeploy automates code deployments to any instance, including EC2 instances and instances running on-premises. However, none of these services offer the same benefits as CloudFormation when it comes to automating the creation and management of AWS resources.\nTherefore, the best choice for this consulting firm looking to automate their resource provisioning is AWS CloudFormation.\n\n"
}, {
  "id" : 63,
  "question" : "The security policy of an organization requires an application to encrypt data before writing to the disk.\nWhich solution should the organization use to meet this requirement?\n",
  "answers" : [ {
    "id" : "f19c8f4562c4409e80b5a80acebb8635",
    "option" : "AWS KMS API",
    "isCorrect" : "true"
  }, {
    "id" : "854560450bb7419b9d8bf1219a782be4",
    "option" : "AWS Certificate Manager",
    "isCorrect" : "false"
  }, {
    "id" : "f19f2acc6ea0417ca3acd6a65dec8638",
    "option" : "API Gateway with STS",
    "isCorrect" : "false"
  }, {
    "id" : "a4b2f5cb8e3440e281ab77673accb253",
    "option" : "IAM Access Key.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption B is incorrect.\nThe AWS Certificate Manager can be used to generate SSL certificates to encrypt traffic in transit, but not at rest.\nOption C is incorrect.\nIt is used for issuing tokens while using the API gateway for traffic in transit.\nOption D is used for providing secure access to EC2 Instances.\nAWS Documentation mentions the following on AWS KMS:\nAWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data.\nAWS KMS is integrated with other AWS services including Amazon Elastic Block Store (Amazon EBS), Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elastic Transcoder, Amazon WorkMail, Amazon Relational Database Service (Amazon RDS), and others to make it simple to encrypt your data with encryption keys that you manage.\nFor more information on AWS KMS, please visit the following URL:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/overview.html\nThe AWS KMS API can be used to encrypt, decrypt and reencrypt content.\nPlease refer the following guide:\nhttps://docs.aws.amazon.com/kms/latest/APIReference/kms-api-reference.pdf#Welcome\n\nThe solution that an organization should use to meet the requirement of encrypting data before writing to the disk is AWS KMS API.\nAWS KMS (Key Management Service) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. With AWS KMS, you can create, rotate, and manage customer master keys (CMKs) and use them to protect your data. AWS KMS integrates with AWS services, including Amazon EBS, Amazon S3, and Amazon Redshift, to enable you to easily encrypt data at rest.\nTo encrypt data before writing to the disk, you can use AWS KMS API to generate a data encryption key (DEK) and use it to encrypt the data. You can then store the encrypted data on the disk. When you need to read the data, you can use the same DEK to decrypt the data.\nThe other options provided in the answer choices are not appropriate for meeting the requirement of encrypting data before writing to the disk:\nAWS Certificate Manager is used to manage SSL/TLS certificates for use with AWS services. It does not provide encryption for data at rest. API Gateway with STS (Security Token Service) provides temporary security credentials to access AWS services, but it does not provide encryption for data at rest. IAM (Identity and Access Management) Access Key is used to authenticate and authorize access to AWS services, but it does not provide encryption for data at rest.\n\n"
}, {
  "id" : 64,
  "question" : "You are an AWS Solutions Architect.\nYour company has a successful web application deployed across multiple AWS Regions.\nThe application attracts more and more global customers.\nHowever, the application's performance is impacted.\nYour manager asks you how to improve the performance and availability of the application.\nWhich of the following AWS services would you recommend?\n",
  "answers" : [ {
    "id" : "58666ff945e44e508c19b74903cb37b4",
    "option" : "AWS DataSync",
    "isCorrect" : "false"
  }, {
    "id" : "b8e912b842934252ae0a667b58829f6f",
    "option" : "Amazon DynamoDB Accelerator",
    "isCorrect" : "false"
  }, {
    "id" : "a9fdd18cae354500b2e246e234d3f5ed",
    "option" : "AWS Lake Formation",
    "isCorrect" : "false"
  }, {
    "id" : "652192e075374ee8a9f933f5a797cfa9",
    "option" : "AWS Global Accelerator.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Global accelerator provides static IP addresses that are anycast in the AWS edge network.\nIncoming traffic is distributed across endpoints in AWS regions.\nThe performance and availability of the application are improved.\nOption A is incorrect: Because DataSync is a tool to automate the data transfer and does not improve the performance.\nOption B is incorrect: DynamoDB is not mentioned in this question.\nOption C is incorrect: Because AWS Lake Formation is used to manage a large amount of data in AWS, which would not help in this situation.\nOption D is CORRECT: Check the AWS Global Accelerator use cases in https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-benefits-of-migrating.html.\nThe Global Accelerator service can improve both the application performance and availability.\n\nIn this scenario, the company has a web application deployed across multiple AWS Regions and is experiencing performance issues due to the growing number of global customers. To improve the application's performance and availability, we need to recommend an appropriate AWS service.\nOut of the given options, the recommended AWS service is AWS Global Accelerator (option D).\nAWS Global Accelerator is a networking service that routes traffic over the AWS global network to the optimal AWS endpoint based on the geographic location of the user, health of the endpoint, and routing policies configured by the user. It uses anycast IP addresses to provide a static IP that clients can use to access the application, which allows traffic to be distributed to the optimal endpoint based on proximity, improving performance and availability.\nAWS Global Accelerator also provides health checks and automatic failover, ensuring that client requests are always routed to healthy endpoints, improving the application's availability.\nThe other options provided, AWS DataSync (option A), Amazon DynamoDB Accelerator (option B), and AWS Lake Formation (option C), are not relevant to the performance and availability issues of a web application deployed across multiple AWS Regions.\nAWS DataSync is a data transfer service that moves large amounts of data between on-premises storage and AWS services. Amazon DynamoDB Accelerator is an in-memory cache that improves the performance of DynamoDB queries. AWS Lake Formation is a service that helps you set up a secure data lake in minutes.\nTherefore, the recommended AWS service for improving the performance and availability of a web application deployed across multiple AWS Regions is AWS Global Accelerator.\n\n"
}, {
  "id" : 65,
  "question" : "A retailer exports data daily from its transactional databases into an S3 bucket in the Sydney region.\nThe retailer's Data Warehousing team wants to import this data into an existing Amazon Redshift cluster in their VPC in Sydney.\nCorporate security policy mandates that data can only be transported within the AWS's private network. Which steps would satisfy the security policy? (SELECT TWO.)\n",
  "answers" : [ {
    "id" : "0c4d599c6ce74ead8bf9331ae6bfde50",
    "option" : "Enable Amazon Redshift Enhanced VPC Routing.",
    "isCorrect" : "true"
  }, {
    "id" : "6ae91279b00d45ff900fac23fdb88659",
    "option" : "Create a Cluster Security Group to allow the Amazon Redshift cluster to access Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "ee508645b8a746bd8fe442419aace069",
    "option" : "Create a NAT gateway in a public subnet to allow the Amazon Redshift cluster to access Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "6a6172e6f825409db891e7940418f776",
    "option" : "Create and configure an Amazon S3 VPC endpoint.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - A and D.\nAmazon Redshift Enhanced VPC Routing provides VPC resources access to Redshift.\nRedshift will not be able to access the S3 VPC endpoints without enabling Enhanced VPC routing.\nSo one option will not support the scenario if another is not selected.\nNAT instance (the proposed answer) cannot be reached by Redshift without enabling Enhanced VPC Routing.\nhttps://aws.amazon.com/about-aws/whats-new/2016/09/amazon-redshift-now-supports-enhanced-vpc-routing/\nOption D:\nVPC Endpoints - It enables you to privately connect your VPC to the supported AWS Services and VPC Endpoint services powered by PrivateLink without requiring an IGW, NAT Device, VPN Connection or AWS Direct Connect connections.\nInstances in VPC do not require Public IP addresses to communicate with resources in the service and traffic between your VPC and other service does not leave the Amazon network.\nS3 VPC Endpoint - it is a feature that will allow you to make even better use of VPC and S3.\nI recommend you to look into the following URLs to know the concept further.\nhttps://aws.amazon.com/blogs/aws/new-vpc-endpoint-for-amazon-s3/\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints-s3.html\n\nTo satisfy the corporate security policy that mandates that data can only be transported within AWS's private network, we need to use private networking options available within AWS. Two such options are:\nEnable Amazon Redshift Enhanced VPC Routing: This feature allows Amazon Redshift to access the S3 bucket over the VPC endpoint without using the public Internet. It creates a VPC endpoint for Amazon S3 in your VPC and routes all Amazon S3 traffic through that endpoint, which ensures that the traffic remains within your VPC and doesn't traverse the public internet. Create and configure an Amazon S3 VPC endpoint: An S3 VPC endpoint allows communication between Amazon Redshift in a VPC and S3 over an Amazon network instead of the public Internet. By creating a VPC endpoint, traffic between Amazon Redshift and S3 stays within the Amazon network, which is a private connection and therefore satisfies the security policy.\nHence, the correct answers are A and D. Option B is incorrect because it only creates a Cluster Security Group to allow the Amazon Redshift cluster to access Amazon S3, but it does not ensure private network transport. Option C is incorrect because it requires a NAT gateway in a public subnet, which defeats the purpose of having a private network transport.\n\n"
}, {
  "id" : 66,
  "question" : "A team is building an application that must persist and index JSON data in a highly available data store.\nThe latency of data access must remain consistent despite very high application traffic. Which service would help the team to meet the above requirement?\n",
  "answers" : [ {
    "id" : "048955a975264694a8800a7b358f7fa2",
    "option" : "Amazon EFS",
    "isCorrect" : "false"
  }, {
    "id" : "d817bcb5ce0845129c03398ee91baf97",
    "option" : "Amazon Redshift",
    "isCorrect" : "false"
  }, {
    "id" : "7f234fb7ad4e470c9252dd4033925277",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "dc3e0a6727cf4688aab01f0912bf0fbb",
    "option" : "AWS CloudFormation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the following about DynamoDB:\nAmazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.\nThe data in DynamoDB is stored in JSON format.\nHence it is the perfect data storage to meet the requirement mentioned in the question.\nFor more information on AWS DynamoDB, please visit the following URL:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\n\nThe best service to meet the requirements of persisting and indexing JSON data in a highly available data store, while maintaining consistent latency during high application traffic, is Amazon DynamoDB (Option C).\nAmazon DynamoDB is a fully-managed NoSQL database service that offers high performance, scalability, and availability. It supports document data models, and can store JSON documents natively. DynamoDB is designed to provide consistent single-digit millisecond latency at any scale, which makes it an ideal choice for high-traffic applications.\nHere are some reasons why DynamoDB is a good fit for the requirements listed in the question:\nHighly Available: DynamoDB is designed to be highly available by replicating data across multiple availability zones. This ensures that if one availability zone goes down, the database can still be accessed from another availability zone. Scalable: DynamoDB can handle a virtually unlimited amount of traffic and data, making it ideal for high-traffic applications. It can also automatically scale up or down based on the application's needs. Low Latency: DynamoDB provides single-digit millisecond latency at any scale, which ensures consistent performance even during high traffic periods. Document Data Model: DynamoDB supports storing JSON documents natively, which means that the application can persist and index JSON data without the need for complex transformations. Fully Managed: DynamoDB is a fully-managed service, which means that AWS handles the management and scaling of the database, allowing the team to focus on building the application.\nAmazon EFS (Option A) is a managed file storage service, but it is not optimized for highly available, low-latency data access. Amazon Redshift (Option B) is a fully-managed data warehouse service, which is designed for running complex queries on large datasets. It is not optimized for highly available, low-latency data access. AWS CloudFormation (Option D) is a service that allows you to define and deploy infrastructure as code, but it is not a data store service.\n\n"
}, {
  "id" : 67,
  "question" : "An organization hosts a multi-language website on AWS, which is served using CloudFront.\nLanguage is specified in the HTTP request as shown below: http://d11111f8.cloudfront.net/main.html?language=de http://d11111f8.cloudfront.net/main.html?language=en http://d11111f8.cloudfront.net/main.html?language=es How should AWS CloudFront be configured to deliver cached data in the correct language?\n",
  "answers" : [ {
    "id" : "5584c15effb3440cb117e978e79dc476",
    "option" : "Forward cookies to the origin",
    "isCorrect" : "false"
  }, {
    "id" : "b5b452f4cdef41ab900f5c7e4356c838",
    "option" : "Based on query string parameters",
    "isCorrect" : "true"
  }, {
    "id" : "c0372d37412346faac494db8e47afaa4",
    "option" : "Cache objects at the origin",
    "isCorrect" : "false"
  }, {
    "id" : "5ff04549de3547bcb709968d2dad3bd5",
    "option" : "Serve dynamic content.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nSince language is specified in the query string parameters, CloudFront should be configured for the same.\nFor more information on configuring CloudFront via query string parameters, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html\n\nTo deliver cached data in the correct language, AWS CloudFront should be configured based on query string parameters. This can be done by creating separate cache behaviors for each language in the CloudFront distribution settings.\nHere's how this can be done:\nCreate a new cache behavior: In the CloudFront distribution settings, click on \"Behaviors\", then \"Create Behavior\". Specify the path pattern: Enter the path pattern for the language-specific pages, for example \"/main.html?language=en\" for English pages. Specify the query string parameters: In the \"Cache Based on Selected Request Headers or Query Strings\" section, select \"Whitelist\" and enter \"language\" as the query string parameter. Specify the TTL: Set the Time-to-Live (TTL) for the cache behavior to an appropriate value for the language-specific content. Repeat for other languages: Repeat steps 1-4 for each language-specific cache behavior that you want to create.\nBy creating separate cache behaviors for each language, CloudFront will cache and serve the correct version of each page based on the \"language\" query string parameter in the HTTP request.\nForwarding cookies to the origin (option A) may not be necessary or desirable in this case, as the language is already specified in the query string parameter. Caching objects at the origin (option C) would not be effective in delivering cached data in the correct language. Serving dynamic content (option D) would be a more resource-intensive option, and may not be necessary if the language-specific pages can be served from cache.\n\n"
}, {
  "id" : 68,
  "question" : "You have developed a new web application on AWS for a real estate firm.\nIt has a web interface where real estate employees upload photos of newly constructed houses in S3 buckets.\nProspective buyers log in to the website and access photos.\nThe marketing team has initiated an intensive marketing event to promote new housing schemes which will lead to customers who frequently access these images.\nAs this is a new application, you have no projection of traffic on the S3 bucket.\nYou need an S3 storage class that can automatically optimize the storage costs with changing access patterns.\nWhich of the following is a recommended storage solution to meet this requirement?\n",
  "answers" : [ {
    "id" : "2e4f9ac0b82c4033bf50377b7bf8e265",
    "option" : "Use One Zone-IA storage class to store all images.",
    "isCorrect" : "false"
  }, {
    "id" : "e13b100e2bca4112974443c0b19534ed",
    "option" : "Use Standard-IA to store all images.",
    "isCorrect" : "false"
  }, {
    "id" : "ddffa9168dc2474ca3ae154b8ec2490b",
    "option" : "Use S3 Intelligent-Tiering storage class.",
    "isCorrect" : "true"
  }, {
    "id" : "d2a174bd76ce407b9e4ad7dd1fab1f42",
    "option" : "Use Standard storage class and use Storage class analytics to identify &amp; move objects using lifecycle policies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nWhen the access pattern to web applications using S3 storage buckets is unpredictable, you can use S3 intelligent-Tiering storage class.\nS3 Intelligent-Tiering storage class includes two access tiers: frequent access and infrequent access.\nBased upon access patterns, it moves data between these tiers, which helps in cost saving.\nS3 Intelligent-Tiering storage class has the same performance as that of Standard storage class.\nOption A is incorrect.\nAlthough it will save costs, it will not provide any protection in case of AZ failure.\nAlso, this class is suitable for infrequently accessed data &amp; not for frequently access data.\nOption B is incorrect as Standard-IA storage class is for infrequently accessed data &amp; there are retrieval charges associated.\nIn the above requirement, you do not know the data access pattern, which may result in a higher cost.\nOption D is incorrect.\nIt has operational overhead to set up Storage class analytics &amp; moves objects between various classes.\nAlso, since the access pattern is undetermined, this will run into a costlier option.\nFor more information on S3 Intelligent-Tiering, refer to the following URLs:\nhttps://aws.amazon.com/blogs/aws/new-automatic-cost-optimization-for-amazon-s3-via-intelligent-tiering/\n\nThe recommended storage solution for this scenario is the S3 Intelligent-Tiering storage class (option C).\nS3 Intelligent-Tiering is a storage class that uses machine learning to automatically move objects between two access tiers: frequent and infrequent access. It is designed for data with unknown or changing access patterns, such as in this case where there is no projection of traffic on the S3 bucket.\nWith S3 Intelligent-Tiering, objects that are accessed frequently will be stored in the frequent access tier, which has a higher storage cost but a lower retrieval cost. Objects that are accessed infrequently will be moved to the infrequent access tier, which has a lower storage cost but a higher retrieval cost.\nThis means that as the marketing event leads to more frequent access to the images, they will be automatically moved to the frequent access tier, and as access patterns change, the storage class will adjust accordingly, optimizing storage costs.\nOption A, using the One Zone-IA storage class, is not recommended because it stores data in a single availability zone, making it less resilient to failures.\nOption B, using the Standard-IA storage class, is also not recommended because it is optimized for infrequent access, and there is no way to automatically move objects to a more cost-effective storage class if access patterns change.\nOption D, using the Standard storage class and using Storage class analytics to identify and move objects using lifecycle policies, is a possible solution, but it requires manual intervention to move objects between storage classes, which may not be optimal for a scenario with changing access patterns.\nTherefore, option C, using the S3 Intelligent-Tiering storage class, is the recommended storage solution for this scenario.\n\n"
}, {
  "id" : 69,
  "question" : "You are part of the IT team of an assurance company.\nYou have been having a consistent amount of usage of your EC2 instances and Fargate.\nHowever, there is also a consistent amount of usage increase.\nBecause of this, you can predict that you may need to increase the size of the instances in 2 or 3 years.\nThe finance team has asked you if there is a way to save costs in the EC2 instances and Fargate.\nWhat do you suggest?\n",
  "answers" : [ {
    "id" : "cafbaa10c4cb4928a7a4e22c3adc5738",
    "option" : "Purchase a Compute Saving plan.",
    "isCorrect" : "true"
  }, {
    "id" : "76549452f7b949789e9304f02f5dfcee",
    "option" : "Purchase an EC2 instance saving plan.",
    "isCorrect" : "false"
  }, {
    "id" : "bea002c19a604129a93b0d17f1853a9c",
    "option" : "Purchase a Convertible Reserved Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "5e9a3d6f38934b4aaa8dd562e3ccb4c0",
    "option" : "Purchase a Standard Reserved Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nWith a Compute Saving Plan, you can save up to 66% and it applies to Fargate and EC2 instances.\nIt also lets you change the instance's family and size.\nMore details-\nhttps://aws.amazon.com/savingsplans/faq/\nOption B is incorrect.\nThe EC2 instance saving plan does not apply automatically to Fargate instances.\nMore details-\nhttps://aws.amazon.com/savingsplans/faq/\nOptions C and D are incorrect.\nReserved instances do not save the cost of Fargate.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\n\nIn this scenario, the IT team is experiencing consistent usage of EC2 instances and Fargate, and can predict a potential increase in usage in 2-3 years. The finance team has requested suggestions to save costs on these resources.\nThe options provided in the answer choices are all related to AWS cost optimization mechanisms for EC2 instances:\nA. Compute Saving Plan: Compute Savings Plans provide flexibility and help reduce costs by offering a flexible discount model that applies to a specific instance family in a specific region. This option could be helpful if the IT team expects consistent usage for a particular instance family in a specific region for a long period of time. However, this might not be an ideal solution if the IT team expects usage to change over time or if they plan to switch to another instance family in the future.\nB. EC2 Instance Saving Plan: EC2 Instance Saving Plans provide a discounted rate for EC2 instances that require a one or three-year commitment. This option could be a viable solution if the IT team expects to use a particular instance type and size for a longer period of time. However, if the IT team expects usage to change over time or if they plan to switch to another instance family, this might not be the best solution.\nC. Convertible Reserved Instance: Convertible Reserved Instances offer flexibility and can be converted to other instance types or sizes within the same instance family, without the need to pay any upfront cost difference. This option could be an ideal solution if the IT team expects usage to change over time and may need to change instance types or sizes, providing more flexibility and cost savings in the long run.\nD. Standard Reserved Instance: Standard Reserved Instances provide a discounted rate for EC2 instances that require a one or three-year commitment. However, unlike Convertible Reserved Instances, they cannot be modified to another instance type or size within the same instance family. This option may not be the best solution if the IT team expects usage to change over time or if they plan to switch to another instance family.\nTherefore, considering the potential increase in usage in the next 2-3 years and the need for flexibility, the most suitable option in this scenario would be to Purchase a Convertible Reserved Instance (Option C).\n\n"
}, {
  "id" : 70,
  "question" : "A company is generating large datasets with millions of rows to be summarized column-wise.\nTo build daily reports from these data sets, Business Intelligence tools would be used. Which storage service would meet these requirements?\n",
  "answers" : [ {
    "id" : "f42cf66fb203456a8446dce5cd1b9b94",
    "option" : "Amazon Redshift",
    "isCorrect" : "true"
  }, {
    "id" : "f4339de35fc64227a161154ce184d86d",
    "option" : "Amazon RDS",
    "isCorrect" : "false"
  }, {
    "id" : "3f7064054f004d889a332113244b4ce0",
    "option" : "ElastiCache",
    "isCorrect" : "false"
  }, {
    "id" : "73c8b69d27674bfeb6c3eaa890e5d362",
    "option" : "DynamoDB.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following:\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.\nYou can start with just a few hundred gigabytes of data and scale to a petabyte or more.\nThis enables you to use your data to acquire new insights for your business and customers.\nFor more information on AWS Redshift, please visit the following URL:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html\nColumnar storage for database tables is an important factor in optimizing analytic query performance because it drastically reduces the overall disk I/O requirements and the amount of data you need to load from disk.\nAmazon Redshift uses a block size of 1 MB, which is more efficient and further reduces the number of I/O requests needed to perform any database loading or other operations that are part of query execution.\nFor more information on how redshift manages the columnar storage, please visit the following URL:\nhttps://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html\n\nBased on the requirement of storing large datasets with millions of rows and performing column-wise summarization, and using Business Intelligence (BI) tools for generating daily reports, the most suitable storage service would be Amazon Redshift.\nAmazon Redshift is a data warehousing service provided by AWS. It is designed to handle large datasets, perform complex queries, and deliver fast results. It is a highly scalable service that can handle petabyte-scale data warehouses, making it suitable for storing large datasets with millions of rows.\nAmazon Redshift allows for columnar storage, which is highly optimized for querying large datasets. It stores data in a columnar format, meaning that data is organized by column rather than by row. This makes it easier to perform column-wise summarization, which is a requirement for the given scenario.\nMoreover, Amazon Redshift is optimized to work with BI tools such as Tableau, MicroStrategy, and others, making it an excellent choice for generating daily reports using BI tools. It can also integrate with other AWS services such as AWS Glue and AWS Data Pipeline for data integration and ETL (extract, transform, load) tasks.\nOn the other hand, Amazon RDS and DynamoDB are primarily designed for transactional workloads, where data is frequently updated and accessed in real-time. They may not be suitable for storing large datasets that require column-wise summarization.\nElastiCache, on the other hand, is a caching service that can be used to improve the performance of applications that access data frequently. It is not designed for storing large datasets or performing complex queries.\nTherefore, the correct answer to the given scenario would be option A, Amazon Redshift.\n\n"
}, {
  "id" : 71,
  "question" : "A company is developing a web application to be hosted in AWS.\nThis application needs a data store for session data.\nAs an AWS Solution Architect, what would you recommend as an ideal option to store session data? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "f28950781d58431bb2eb3e8419e04286",
    "option" : "CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "617e5467c8f5472f9ab407ba260b2861",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "ace1749496ad4a4e97bf95a0f674ad76",
    "option" : "Elastic Load Balancing",
    "isCorrect" : "false"
  }, {
    "id" : "2065c61f0ed64b1199153b175ad6b575",
    "option" : "ElastiCache",
    "isCorrect" : "true"
  }, {
    "id" : "e3232c0f6f054e4dad11c3dcedea54b9",
    "option" : "Storage Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and D.\nDynamoDB and ElastiCache are perfect options for storing session data.\nAWS Documentation mentions the following on Amazon DynamoDB:\nAmazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale.\nIt is a fully managed cloud database and supports both document and key-value store models.\nIts flexible data model, reliable performance, and automatic scaling of throughput capacity make it a great fit for mobile, web, gaming, ad tech, IoT, and many other applications.\nFor more information on AWS DynamoDB, please visit the following URL:\nhttps://aws.amazon.com/dynamodb/\nAWS Documentation mentions the following on AWS ElastiCache:\nAWS ElastiCache is a web service that makes it easy to set up, manage, and scale a distributed in-memory data store or cache environment in the cloud.\nIt provides a high-performance, scalable, and cost-effective caching solution while removing the complexity associated with the deployment and management of a distributed cache environment.\nFor more information on AWS Elasticache, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/UserGuide/WhatIs.html\nOption A is incorrect.\nAWS CloudWatch offers cloud monitoring services for the customers of AWS resources.\nOption C is incorrect.\nAWS Elastic Load Balancing automatically distributes incoming application traffic across multiple targets.\nOption E is incorrect.\nAWS Storage Gateway is a hybrid storage service that enables your on-premises applications to use AWS cloud storage seamlessly.\n\nOut of the given options, the two ideal options to store session data in AWS are:\nB. DynamoDB: Amazon DynamoDB is a fully managed NoSQL database service that can provide low-latency, high-scalability, and high-availability data storage for web applications. DynamoDB can be used to store session data as key-value pairs and can be accessed from anywhere in the world. DynamoDB offers consistent, single-digit millisecond latency, and can automatically scale up or down as per the traffic demands. DynamoDB also provides various features like data encryption at rest, backup and restore, and global tables for disaster recovery.\nD. ElastiCache: Amazon ElastiCache is a fully managed, in-memory caching service that can provide high-performance data storage for web applications. ElastiCache can be used to store session data in a cache cluster, and the data can be accessed from anywhere in the world. ElastiCache supports two popular caching engines: Memcached and Redis. Memcached is a simple caching engine that is ideal for small data sets, whereas Redis is a more feature-rich caching engine that supports advanced data structures and features like replication and persistence.\nCloudWatch (A) is a monitoring and logging service, and it is not ideal for storing session data. Elastic Load Balancing (C) is a service that helps distribute incoming traffic across multiple targets, and it is not designed for storing session data. Storage Gateway (E) is a hybrid storage service that can be used to connect on-premises environments with AWS cloud storage, and it is not designed for storing session data.\nIn summary, DynamoDB and ElastiCache are the ideal options to store session data in AWS, depending on the specific requirements of the application.\n\n"
}, {
  "id" : 72,
  "question" : "A company needs to store images that are uploaded by users via a mobile application.\nThere is also a need to ensure that security measures are in place to avoid data loss. What step should be taken for protection against unintended user actions?\n",
  "answers" : [ {
    "id" : "310d567dce234ffc86ffc92bb94b1818",
    "option" : "Store data in an EBS volume and create snapshots once a week.",
    "isCorrect" : "false"
  }, {
    "id" : "bc2488c49f364df8b482f29be6084b37",
    "option" : "Store data in an S3 bucket and enable versioning.",
    "isCorrect" : "true"
  }, {
    "id" : "c06798f837d347eba029300661c6dee9",
    "option" : "Store data on Amazon EFS storage.",
    "isCorrect" : "false"
  }, {
    "id" : "a4e32f5865fc48f09708a018f139fcd8",
    "option" : "Store data on EC2 instance storage.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon S3 has an option for versioning, as shown below.\nVersioning is on the bucket level and can be used to recover prior versions of an object.\nFor more information on AWS S3 versioning, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Versioning.html\nOption A is incorrect as it does not offer protection against accidental deletion of files.\nOption C is incorrect.\nIt is not the ideal solution because multiple EC2 instances can access the file system.\nOption D is ephemeral.\n\n\nOption B. Store data in an S3 bucket and enable versioning is the best option for storing images that are uploaded by users via a mobile application, while ensuring security measures are in place to avoid data loss due to unintended user actions.\nAmazon S3 (Simple Storage Service) is a highly scalable and durable object storage service offered by AWS. It provides secure, reliable, and highly available storage for objects such as images, videos, and other files.\nEnabling versioning on an S3 bucket allows you to preserve, retrieve, and restore every version of an object in the bucket. This feature ensures that if a user accidentally overwrites or deletes an object, you can easily recover the previous version of the object. This can help avoid data loss due to unintended user actions.\nFurthermore, S3 provides a range of security features such as encryption of data at rest and in transit, access control policies, and integration with AWS Identity and Access Management (IAM) for authentication and authorization.\nOption A. Storing data in an EBS (Elastic Block Store) volume and creating snapshots once a week is not an appropriate option for storing images uploaded by users via a mobile application. EBS volumes are designed to provide block-level storage for use with Amazon EC2 instances. This option does not provide the scalability and flexibility required for storing large volumes of data such as images.\nOption C. Storing data on Amazon EFS (Elastic File System) storage is also not an appropriate option for storing images uploaded by users via a mobile application. EFS is a fully-managed file system service that provides scalable and highly available storage for use with EC2 instances. This option is not designed for object storage and is not an appropriate solution for storing images.\nOption D. Storing data on EC2 instance storage is not an appropriate option for storing images uploaded by users via a mobile application. EC2 instance storage provides temporary block-level storage that is attached to an EC2 instance. This storage is not persistent and is not suitable for storing large volumes of data such as images. Additionally, data stored on instance storage is lost when the instance is stopped or terminated.\n\n"
}, {
  "id" : 73,
  "question" : "An application needs to have a relational Datastore hosted in AWS.\nThe following requirements are in place for the Datastore: a) The initial storage capacity of 8 TB b) The ability to accommodate a database growth of 8GB per day c) The ability to have 4 Read Replicas Which of the following Datastore is the best for this requirement?\n",
  "answers" : [ {
    "id" : "2f26330c2f9145c7b117768e3936de5f",
    "option" : "DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "581f2fa9734a4badbd5febce94bec4c1",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "c6de468f21964bf2b1e59d65eb9d8628",
    "option" : "Amazon Aurora",
    "isCorrect" : "true"
  }, {
    "id" : "a6c3d3b6c6804fc7ab32f1e990cdfc2a",
    "option" : "ElastiCache.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAurora can have a storage limit of 64TB and can easily accommodate the initial 8TB plus a database growth of 8GB/day for nearly a period of 20+ years.\nIt can have up to 15 Aurora Replicas that can be distributed across the Availability Zones that a DB cluster spans within an AWS Region.\nAurora Replicas work well for read scaling because they are fully dedicated to read operations on the cluster volume.\nWrite operations are managed by the primary instance.\nBecause the cluster volume is shared among all DB instances in your DB cluster, no additional work is required to replicate a copy of each Aurora Replica data.\nFor more information on AWS Aurora, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Replication.html\nNote:\nOur DB choice needs to fulfill the 3 criteria.\nInitial Storage capacity 8 TB.\nDaily DB growth of 8GB/day.\nNeed 4 Read replicas.\nDynamoDB is incorrect because it is not a relational database.\nElastiCache is incorrect because it is a distributed in-memory cache service.\nIn this scenario, a datastore is required.\n\nThe best option for hosting a relational Datastore with the specified requirements would be Amazon Aurora (option C).\nExplanation:\na) The initial storage capacity of 8 TB: Amazon Aurora allows you to provision up to 64 TB of storage. Therefore, the specified initial storage capacity of 8 TB can easily be accommodated with Amazon Aurora.\nb) The ability to accommodate a database growth of 8GB per day: Amazon Aurora is designed to automatically grow its storage capacity as needed based on the database growth rate. It can also automatically replicate data across multiple Availability Zones for high availability and durability.\nc) The ability to have 4 Read Replicas: Amazon Aurora provides support for up to 15 read replicas per database instance. Therefore, the requirement of 4 read replicas can be easily met with Amazon Aurora.\nOption A (DynamoDB) is a NoSQL database and may not be suitable for a relational datastore.\nOption B (Amazon S3) is an object storage service and does not provide the capabilities required for a relational datastore.\nOption D (ElastiCache) is an in-memory data store and is not suitable for storing large amounts of data over a long period of time.\n\n"
}, {
  "id" : 74,
  "question" : "There is a requirement to host a database on an EC2 Instance.\nIt is also required that the EBS volume should support 32,000 IOPS. Which Amazon EBS volume type would meet the performance requirements of this database?\n",
  "answers" : [ {
    "id" : "fc6ba68366274c028d678bb1390620df",
    "option" : "EBS Provisioned IOPS SSD",
    "isCorrect" : "true"
  }, {
    "id" : "d42ca2b3080b4f2b8eefa601766c33e2",
    "option" : "EBS Throughput Optimized HDD",
    "isCorrect" : "false"
  }, {
    "id" : "adfaa9ba37b3449e88ab0ff0bdf18586",
    "option" : "EBS General Purpose SSD",
    "isCorrect" : "false"
  }, {
    "id" : "da509e41bf2946628dad43d27ae02bbe",
    "option" : "EBS Cold HDD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nFor high performance and high IOPS requirements, as in this case, the ideal choice would be to choose EBS Provisioned IOPS SSD.The below snapshot from the AWS Documentation shows the usage of Provisioned IOPS for better IOPS performance in database-based applications.\nFor more information on AWS EBS Volume types, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\n\n\nThe Amazon Elastic Block Store (EBS) is a block-level storage service designed for use with Amazon Elastic Compute Cloud (EC2) instances. EBS provides persistent block-level storage volumes for use with EC2 instances in the AWS Cloud. Amazon EBS volumes are highly available and reliable storage volumes that can be attached to any running EC2 instance in the same Availability Zone and are designed for use with EC2 instances that require persistent storage.\nBased on the requirement of hosting a database on an EC2 instance and supporting 32,000 IOPS, the best EBS volume type that meets the performance requirements of the database is the EBS Provisioned IOPS SSD.\nEBS Provisioned IOPS SSD is designed for I/O-intensive applications that require high IOPS and low latency. This EBS volume type delivers consistent and low-latency performance that is ideal for use with databases, NoSQL databases, and transactional workloads.\nProvisioned IOPS SSD provides predictable and consistent performance, where customers can specify the IOPS they need, and Amazon EBS delivers the requested performance, regardless of the workload.\nThe EBS Throughput Optimized HDD volume type is designed for big data workloads that require low-cost and high throughput performance. It is optimized for streaming workloads that require consistent, fast throughput at a low cost per GB. This volume type is not ideal for hosting a database that requires high IOPS and low latency.\nThe EBS General Purpose SSD volume type is a good fit for small to medium-sized databases and other workloads that require a balance of price and performance. It is designed for a broad range of transactional workloads, including development and testing environments, low-latency interactive applications, and boot volumes.\nThe EBS Cold HDD volume type is designed for infrequent access and low-latency throughput workloads. It is suitable for cold data, such as backups and archives, and not for hosting databases that require high IOPS and low latency.\nIn summary, the best Amazon EBS volume type that meets the requirement of hosting a database on an EC2 instance and supporting 32,000 IOPS is the EBS Provisioned IOPS SSD volume type.\n\n"
}, {
  "id" : 75,
  "question" : "In your organization, development teams use S3 buckets to store log files for various applications hosted in AWS development environments.\nThe developers intend to keep the logs for a month for troubleshooting purposes and subsequently purge the logs. Which feature should be used to enable this requirement?\n",
  "answers" : [ {
    "id" : "143f6315e8a54c7e98fbcf86aaf01470",
    "option" : "Adding a bucket policy on the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "6110eda7fa264534ad6328bce59d249a",
    "option" : "Configuring lifecycle configuration rules on the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "7b970951362546088153937fa57a3fb3",
    "option" : "Creating an IAM policy for the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "f57497cf36fe4ca7871557db40ba4afb",
    "option" : "Enabling CORS on the S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following on Lifecycle policies:\nLifecycle configuration enables you to specify the Lifecycle management of objects in a bucket.\nThe configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects.\nThese actions can be classified as follows:\nTransition actions - In which you define when objects transition to another storage class.\nFor example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation or archive objects to the GLACIER storage class one year after creation.\nExpiration actions - In which you specify when the objects expire.\nThen, Amazon S3 deletes the expired objects on your behalf.\nFor more information on AWS S3 Lifecycle policies, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\n\nThe correct answer is B. Configuring lifecycle configuration rules on the S3 bucket.\nExplanation: Lifecycle configuration rules are used to automate the management of objects in an S3 bucket. They allow you to define actions that should be taken on objects in the bucket based on their age, size, or other attributes.\nIn this scenario, the requirement is to keep the logs for a month for troubleshooting purposes and subsequently purge them. To accomplish this, a lifecycle configuration rule can be configured on the S3 bucket to automatically delete objects that are older than a specified time period.\nThe other options listed are not relevant to the given requirement:\nA bucket policy is used to control access to an S3 bucket. It does not provide a mechanism for automatically purging objects based on their age. An IAM policy is used to grant permissions to AWS resources. It does not provide a mechanism for automatically purging objects based on their age. Enabling CORS (Cross-Origin Resource Sharing) allows web pages to make cross-origin AJAX requests to AWS resources. It does not provide a mechanism for automatically purging objects based on their age.\nTherefore, the correct answer is B. Configuring lifecycle configuration rules on the S3 bucket.\n\n"
}, {
  "id" : 76,
  "question" : "You are creating a new architecture for a financial firm.\nThe architecture consists of some EC2 instances of different types and sizes.\nThe management team has asked you to create this architecture by ensuring the reduction of the risk of simultaneous failures.\nWhich placement group option could you suggest for the instances?\n",
  "answers" : [ {
    "id" : "b24d150a9f154c8abe82f0b268e0d669",
    "option" : "Clustered Placement Group",
    "isCorrect" : "false"
  }, {
    "id" : "680dc9508de94ed3988240fced3fbaa4",
    "option" : "Partition Placement Group",
    "isCorrect" : "false"
  }, {
    "id" : "b4c73ddfe2f44ca4a2df5da9eb31b127",
    "option" : "Multi-AZ Placement Group",
    "isCorrect" : "false"
  }, {
    "id" : "ee2325eb00ad49d09de758e556959eb0",
    "option" : "Spread Placement Group.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect.\nIn Clustered Placement Groups, all the instances are placed in the same rack in the same availability zone.\nTherefore, they are very susceptible to hardware failures and simultaneous failures.\nAlso, it is suggested to have the same size and types of instances in this Placement Group.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nOption B is incorrect.\nIn partition placement groups, the chances of hardware failures and simultaneous failure are reduced compared to the Clustered placement group.\nStill, the Spread Placement group has fewer chances of this kind of failure.\nIt is also suggested to have the same size and types of instances in this Placement Group.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nOption C is incorrect.\nThe Multi-AZ Placement Group does not exist.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nOption D is CORRECT.\nSpread Placement Groups place the instances in different racks.\nEvery rack has its own hardware and power source.\nSo, there is a minimum chance of simultaneous failure.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nThe correct answer is D. Spread Placement Group.\nExplanation: A placement group is a logical grouping of instances within a single Availability Zone. Amazon EC2 offers four types of placement groups: Clustered, Partition, Spread, and Multi-AZ.\nClustered Placement Group: This type of placement group is used for applications that need low network latency, high network throughput, or both. Clustered placement groups are best suited for applications that require a large number of instances to communicate with each other with high network performance.\nPartition Placement Group: This type of placement group is used for large distributed and replicated workloads. Partition placement groups allow you to spread your instances across logical partitions, which can span multiple AZs within a region.\nMulti-AZ Placement Group: This type of placement group is used for high availability applications that need automatic failover capability across multiple AZs within a region.\nSpread Placement Group: This type of placement group is used for applications that need to minimize the risk of simultaneous failure. Spread placement groups spread instances across underlying hardware and offer the best possible separation of instances.\nIn this scenario, since the management team has requested to reduce the risk of simultaneous failures, the best option would be to use a Spread Placement Group. This is because spread placement groups ensure that instances are placed on different underlying hardware to reduce the risk of simultaneous failure.\n\n"
}, {
  "id" : 77,
  "question" : "You are creating a new architecture for a financial firm.\nThe architecture consists of some EC2 instances with the same type and size (M5.large)\nIn this architecture, all the EC2 mostly communicate with each other.\nBusiness people have asked you to create this architecture keeping in mind low latency as a priority.\nWhich placement group option could you suggest for the instances?\n",
  "answers" : [ {
    "id" : "eaebcdf5cf7e41c688bd0a751d70d44b",
    "option" : "Partition Placement Group",
    "isCorrect" : "false"
  }, {
    "id" : "413e1e26b06e4624a3547d004a8045b1",
    "option" : "Clustered Placement Group",
    "isCorrect" : "true"
  }, {
    "id" : "140d3e22a2fa4ea7b8f493588ba7583b",
    "option" : "Spread Placement Group",
    "isCorrect" : "false"
  }, {
    "id" : "33943cd22194433c91e4fcc04dcd55df",
    "option" : "Enhanced Networking Placement Group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect.\nPartition Placement Groups distribute the instances in different partitions.\nThe partitions are placed in the same AZ, but do not share the same rack.\nThis type of placement group does not provide low latency throughput to the instances.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nOption B is CORRECT.\nClustered Placement Group places all the instances on the same rack.\nThis placement group option provides 10 Gbps connectivity between instances ( Internet connectivity in the instances has a maximum of 5 Gbps)\nThis option of placement group is perfect for the workload that needs low latency.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nOption C is incorrect.\nPlacement Groups place all the instances in different racks in the same AZ.\nThese types of placement groups do not provide low latency throughput to the instances.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\nOption D is incorrect.\nEnhanced Networking Placement Group does not exist.\nMore details-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nBased on the requirement of low latency as a priority, the best option for the placement group would be a \"Clustered Placement Group\" (option B).\nA placement group is a logical grouping of instances within a single Availability Zone. The choice of placement group type can have a significant impact on the performance, reliability, and availability of your applications.\nHere's a brief explanation of the different placement group options:\nA. Partition Placement Group: This type of placement group spreads instances across logical partitions, which helps to reduce the risk of simultaneous failures. It is ideal for applications that need to maximize their fault tolerance.\nB. Clustered Placement Group: This type of placement group places instances in a low-latency group in a single Availability Zone. The instances in a clustered placement group are tightly packed together and connected to a low-latency network. This option is ideal for applications that require high network throughput and low latency, such as HPC (High-Performance Computing) applications or tightly coupled workloads.\nC. Spread Placement Group: This type of placement group spreads instances across distinct underlying hardware to reduce the risk of simultaneous failures. It is ideal for applications that need to maximize their fault tolerance.\nD. Enhanced Networking Placement Group: This type of placement group leverages enhanced networking capabilities to provide higher packet per second (PPS) performance, lower network jitter, and lower network latency. It is ideal for applications that require high network performance.\nIn summary, since the EC2 instances in this financial firm mostly communicate with each other, and low latency is a priority, the \"Clustered Placement Group\" is the best option for this architecture.\n\n"
}, {
  "id" : 78,
  "question" : "A company has a media processing application deployed in a local data center.\nIts file storage is built on a Microsoft Windows file server.\nThe application and file server need to be migrated to AWS.\nYou want to set up the file server in AWS quickly.\nThe application code should continue working to access the file systems.\nWhich method should you choose to create the file server?\n",
  "answers" : [ {
    "id" : "d1ba58dc1eaa45d6ba78dd79553a64b8",
    "option" : "Create a Windows File Server from Amazon WorkSpaces.",
    "isCorrect" : "false"
  }, {
    "id" : "ef231a263c75459a82ee18e085f0606e",
    "option" : "Configure a high performance Windows File System in Amazon EFS.",
    "isCorrect" : "false"
  }, {
    "id" : "44f3eee7570940f39852c6a0611d78dd",
    "option" : "Create a Windows File Server in Amazon FSx.",
    "isCorrect" : "true"
  }, {
    "id" : "bba0fdbdf0d240b0ac1f3ad5faa5c1c1",
    "option" : "Configure a secure enterprise storage through Amazon WorkDocs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nIn this question, a Windows file server is required in AWS, and the application should continue to work unchanged.\nAmazon FSx for Windows File Server is the correct answer as it is backed by a fully native Windows file system.\nOption A is incorrect: Because Amazon WorkSpace configures a desktop server which is not required in this question.\nOnly a Windows file server is needed.\nOption B is incorrect: Because EFS cannot be used to configure a Windows file server.\nOption C is CORRECT: Because Amazon FSx provides fully managed Microsoft Windows file servers.\nCheck the reference in https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html.\nOption D is incorrect: Because Amazon WorkDocs is a file sharing service in AWS.\nIt cannot provide a native Windows file system.\n\nThe best option for creating a file server in AWS, while also ensuring that the application code continues to function properly, would be to use Amazon FSx for Windows File Server. Therefore, option C is the correct choice.\nAmazon FSx is a fully-managed native Microsoft Windows file system that can be used to migrate existing applications that rely on Windows file shares. It is built on Windows Server and provides compatibility with Windows applications and services, including Active Directory integration. This means that migrating the existing file server to Amazon FSx would not require any changes to the application code, making the migration process easier and faster.\nOption A, creating a Windows File Server from Amazon WorkSpaces, is not a suitable choice because Amazon WorkSpaces is a Desktop-as-a-Service (DaaS) solution designed to provide remote access to desktop environments, rather than providing a file server.\nOption B, configuring a high-performance Windows File System in Amazon EFS, is not the best choice because Amazon EFS is not natively a Windows file system. It is designed to be a scalable, fully managed, cloud-native file storage service that can be accessed from multiple instances across different Availability Zones. Although it can support Windows workloads, it may not be the best option for a migration scenario where compatibility is critical.\nOption D, configuring a secure enterprise storage through Amazon WorkDocs, is also not a suitable choice because Amazon WorkDocs is a fully managed, secure content creation, storage, and collaboration service, but it is not designed to provide file server functionality.\n\n"
}, {
  "id" : 79,
  "question" : "There is a requirement to get the source IP addresses that access resources in a private subnet.\nWhich of the following cost-optimized service could be used to fulfill this purpose?\n",
  "answers" : [ {
    "id" : "2f0f71e3d5c24efca83a2af23daaff88",
    "option" : "Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "8cea3c228a3743d2a3c6f4f2e53ac507",
    "option" : "VPC Flow Logs",
    "isCorrect" : "true"
  }, {
    "id" : "673662ffcc244fa9b455b83a6be0eac8",
    "option" : "Use CloudWatch metrics",
    "isCorrect" : "false"
  }, {
    "id" : "500c16e00f6e4610bcff4901cbc7ec93",
    "option" : "Use CloudTrail.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe AWS Documentation mentions the following:\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.\nFlow log data is stored using Amazon CloudWatch Logs.\nAfter you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs.\nFor more information on VPC Flow Logs, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html\nOption A is INCORRECT because AWS Trusted Advisor is your customized cloud expert! It helps you to observe the best practices for using AWS by inspecting your AWS environment to save money, improve system performance and reliability, and close security gaps.\nOption C is INCORRECT because CloudWatch Metric is mainly used for performance metrics and cannot provide the source IP addresses.\nOption D is INCORRECT because AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nHowever, it is costly to use a CloudTrail.\nhttps://aws.amazon.com/about-aws/whats-new/2018/08/aws-cloudtrail-adds-vpc-endpoint-support-to-aws-privatelink/#:~:text=This%20enables%20you%20to%20connect,VPC%20through%20the%20Amazon%20network.&amp;text=By%20using%20AWS%20CloudTrail%20with,your%20compliance%20and%20regulatory%20requirements\n\nThe correct answer to the given question is B. VPC Flow Logs.\nExplanation:\nWhen a resource is launched in a private subnet, it cannot be accessed from the internet directly. Instead, the resource can be accessed through a NAT Gateway, VPN Connection or Direct Connect connection. In such a scenario, it becomes important to monitor and track the source IP addresses that are accessing the resources in the private subnet. This can be achieved through VPC Flow Logs.\nVPC Flow Logs is a feature provided by AWS which captures information about the IP traffic going to and from network interfaces in a VPC. It captures the metadata such as the source and destination IP addresses, ports, protocol, and timestamps for each network packet. This information can then be used for security analysis, troubleshooting, and auditing purposes.\nUsing VPC Flow Logs, one can capture the IP addresses of the resources that are accessing the resources in a private subnet. It provides visibility into the network traffic, and enables one to monitor the traffic that is going to and from the resources in the private subnet.\nThe other options mentioned in the question are not relevant to the requirement mentioned in the question.\nTrusted Advisor is a service that provides recommendations to optimize the cost, security, performance, and fault tolerance of AWS resources. It does not provide information on the source IP addresses that are accessing the resources in a private subnet.\nCloudWatch metrics are used to monitor the performance of AWS resources such as EC2 instances, RDS databases, and ELB load balancers. It does not provide information on the source IP addresses that are accessing the resources in a private subnet.\nCloudTrail is used to log the API activity for an AWS account. It provides a record of the actions taken by a user, role, or AWS service in the account. It does not provide information on the source IP addresses that are accessing the resources in a private subnet.\nHence, the correct option is B. VPC Flow Logs.\n\n"
}, {
  "id" : 80,
  "question" : "You are part of the IT team of a small car manufacturer company.\nThe company is starting to move its On-Premise resources to the cloud.\nThe Marketing department was the first department to migrate its applications to the cloud.\nNow the finance team wants to do the same.\nEach department should have its own AWS account but you need one management account to pay for the bills of all the AWS accounts.\nWhat do you suggest to solve this?\n",
  "answers" : [ {
    "id" : "2eed30bc33b74865be836ed26d25f5be",
    "option" : "Create a different VPC for the Finance Department and limit their access to resources with IAM Roles and Policies.",
    "isCorrect" : "false"
  }, {
    "id" : "be9ec5d43d784568ace7dfada6508933",
    "option" : "Use AWS Control Tower.",
    "isCorrect" : "false"
  }, {
    "id" : "d4a57d4aa9d5467497aa04063f688f11",
    "option" : "Use AWS Organizations to manage both AWS accounts.",
    "isCorrect" : "true"
  }, {
    "id" : "b138ad0fe1344ad085f9f805f04e3c71",
    "option" : "Use AWS Cost Explorer to divide the bills and use IAM policies to limit the access to resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect.\nThis option does not cover the splitting bill requirements.\nAlso, a different VPC is not the best option to separate projects or environments.\nOption B is incorrect.\nControl Tower is more suitable to automate a deployment in multi-account environments.\nMore details: https://aws.amazon.com/controltower/faqs/\nOption C is CORRECT.\nWith AWS Organizations, you can have separate bills for every account and pay it with the same account using consolidating billing.\nMore details: https://aws.amazon.com/organizations/faqs/.\nOption D is incorrect.\nWith AWS Cost Explorer, you can analyze and explore your bills and service usage in the account.\nBut in the end, there will be one bill for the account.\nMore details- https://aws.amazon.com/aws-cost-management/faqs/\n\nWhen moving resources to the cloud, it is important to organize them in a way that is scalable, secure, and manageable. In this scenario, the company wants to migrate each department's resources to its own AWS account but have a single management account to pay the bills. To achieve this, the best solution is to use AWS Organizations.\nAWS Organizations is a service that allows you to centrally manage and govern multiple AWS accounts. With AWS Organizations, you can create a hierarchy of accounts, which can be organized by department, project, or any other criteria that make sense for your organization. Each account in the hierarchy can have its own billing, resource access policies, and IAM users and roles.\nUsing AWS Organizations, you can create a management account that is used to pay the bills for all the other accounts. This account is called the payer account, and it is used to consolidate billing information across all the linked accounts. The finance team can create its own AWS account within the organization, and it will be linked to the payer account for billing purposes.\nAWS Control Tower is another option for managing multiple AWS accounts, but it is more focused on setting up new accounts and enforcing compliance policies across them. While it could be used in this scenario, it might be more complex than necessary.\nUsing a separate VPC for the Finance Department and limiting their access to resources with IAM roles and policies (option A) is a good security practice, but it does not address the issue of managing multiple AWS accounts.\nAWS Cost Explorer (option D) is a tool that can help you visualize and analyze your AWS spending, but it is not a solution for managing multiple accounts. IAM policies can be used to limit access to resources, but they do not provide the centralized management capabilities that AWS Organizations does.\nIn summary, the best solution for this scenario is to use AWS Organizations to manage both AWS accounts and create a payer account to consolidate billing information.\n\n"
}, {
  "id" : 81,
  "question" : "Your team is developing a high-performance computing (HPC) application.\nThe application resolves complex, compute-intensive problems and needs a high-performance and low-latency Lustre file system.\nYou need to configure this file system in AWS at a low cost.\nWhich method is the most suitable?\n",
  "answers" : [ {
    "id" : "c9b042969e024ee496dc55615c5a541b",
    "option" : "Create a Lustre file system through Amazon FSx.",
    "isCorrect" : "true"
  }, {
    "id" : "4224653612da4786b7afa8944af090ee",
    "option" : "Launch a high performance Lustre file system in Amazon EBS.",
    "isCorrect" : "false"
  }, {
    "id" : "1c72dc5d93ca4f18b8fe64f870bc03f8",
    "option" : "Create a high-speed volume cluster in EC2 placement group.",
    "isCorrect" : "false"
  }, {
    "id" : "01eceea3f34543ab974ac33938ed6fe1",
    "option" : "Launch the Lustre file system from AWS Marketplace.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThe Lustre file system is an open-source, parallel file system that can be used for HPC applications.\nRefer to http://lustre.org/ for its introduction.\nIn Amazon FSx, users can quickly launch a Lustre file system at a low cost.\nOption A is CORRECT: Amazon FSx supports Lustre file systems, and users pay for only the resources they use.\nOption B is incorrect: Although users may be able to configure a Lustre file system through EBS, it needs lots of extra configurations.\nOption A is more straightforward.\nOption C is incorrect: Because the EC2 placement group does not support a Lustre file system.\nOption D is incorrect: Because products in AWS Marketplace are not cost-effective.\nFor Amazon FSx, there are no minimum fees or set-up charges.\nCheck its pricing in.\nhttps://aws.amazon.com/fsx/lustre/pricing/.\n\nOption A is the most suitable method to configure a high-performance Lustre file system in AWS at a low cost.\nAmazon FSx is a fully-managed service that makes it easy to create and run Lustre file systems that are optimized for HPC workloads. With Amazon FSx, users can launch and scale Lustre file systems quickly and easily, without having to manage the underlying infrastructure.\nOption B, launching a high-performance Lustre file system in Amazon EBS, is not an appropriate method because Amazon EBS is designed for block storage, not file storage, and it is not optimized for HPC workloads.\nOption C, creating a high-speed volume cluster in an EC2 placement group, is not an appropriate method for configuring a Lustre file system because it does not provide a file system solution that can be used by multiple EC2 instances simultaneously.\nOption D, launching the Lustre file system from AWS Marketplace, is not an appropriate method because AWS Marketplace is a platform for buying and selling software, not for configuring file systems.\nTherefore, option A is the most suitable method for configuring a high-performance Lustre file system in AWS at a low cost.\n\n"
}, {
  "id" : 82,
  "question" : "A Redshift cluster currently contains 60TB of data.\nThere is a requirement that a disaster recovery site is put in place in another region.\nWhich solution would help ensure that this requirement is fulfilled?\n",
  "answers" : [ {
    "id" : "fb5000d987ea4ca2ba5a3afe4cfdccb7",
    "option" : "Take a copy of the underlying EBS volumes to S3, and then do Cross-Region Replication.",
    "isCorrect" : "false"
  }, {
    "id" : "b421da527d6e44f190468be6498c38ba",
    "option" : "Enable Cross-Region snapshots for the Redshift Cluster.",
    "isCorrect" : "true"
  }, {
    "id" : "819a936a792744bb8a4eab796a562a76",
    "option" : "Create a CloudFormation template to restore the Cluster in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "63e48e8105564194bb43e41e11918a0e",
    "option" : "Enable Cross Availability Zone snapshots for the Redshift Cluster.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe below diagram shows that snapshots are available for Redshift clusters enabling them to be available in different regions.\nFor more information on managing Redshift snapshots, please visit the following URLs:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html\nhttps://aws.amazon.com/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/\n\n\nThe correct answer for this question is B. Enable Cross-Region snapshots for the Redshift Cluster.\nExplanation: The question requires a disaster recovery solution for a Redshift cluster that contains 60TB of data. The solution should also be located in another region to ensure data redundancy in the event of a disaster. In this scenario, cross-region snapshots are the most suitable solution because they offer several benefits, including:\nAutomatic replication: Cross-region snapshots automatically replicate data to a secondary region, providing a reliable disaster recovery solution. Easy restore: In the event of a disaster, restoring the cluster in the secondary region is as simple as restoring from a snapshot. Reduced downtime: Since the data is already replicated in another region, restoring the cluster is quicker, reducing downtime.\nOption A, taking a copy of the underlying EBS volumes to S3 and then doing Cross-Region Replication is not a recommended approach for Redshift disaster recovery because EBS volumes are not the primary storage mechanism for Redshift. Redshift uses a distributed storage system, where data is spread across multiple nodes, and copying the underlying EBS volumes to S3 may not capture all of the data.\nOption C, creating a CloudFormation template to restore the cluster in another region, is not a recommended approach for Redshift disaster recovery because it is time-consuming and does not offer automatic replication.\nOption D, enabling Cross Availability Zone snapshots for the Redshift Cluster, only offers protection against single AZ failures, and not across regions, which is the requirement in this scenario.\nTherefore, option B, enabling Cross-Region snapshots for the Redshift Cluster, is the most suitable solution for this scenario.\n\n"
}, {
  "id" : 83,
  "question" : "A company is using a Redshift cluster to store its data warehouse.\nThere is a requirement from the Internal IT Security team to encrypt data in the Redshift database.\nHow could this be achieved? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "4352beeb2f4943dcb80fb5a61a85af68",
    "option" : "Encrypt the EBS volumes of the underlying EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "cd9e10d54ccc452daba36fe3715ce79c",
    "option" : "Use AWS KMS Customer Default master key.",
    "isCorrect" : "true"
  }, {
    "id" : "b60b3ac9bae743e18b7491ed7dc946e9",
    "option" : "Use SSL/TLS for encrypting the data.",
    "isCorrect" : "false"
  }, {
    "id" : "fe338c05cbfc4655bf95af5b349fbe0d",
    "option" : "Use hardware security module (HSM) to manage the top-level encryption keys .",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - B and D.\nAWS documentation mentions the following:\nAmazon Redshift uses a hierarchy of encryption keys to encrypt the database.\nYou can use either AWS Key Management Service (AWS KMS) or a hardware security module (HSM) to manage the top-level encryption keys in this hierarchy.\nThe process that Amazon Redshift uses for encryption differs depending on how you manage keys.\nOption D is correct.\nWe can use the hardware security module (HSM) to manage the top-level encryption keys for key management with Amazon Redshift.\nReference:\nhttps://aws.amazon.com/blogs/big-data/encrypt-your-amazon-redshift-loads-with-amazon-s3-and-aws-kms/\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html\n\nTo encrypt data in a Redshift database, there are a few options available:\nEncrypt the EBS volumes of the underlying EC2 instances: This is a valid option, but it only encrypts the data at rest, meaning that the data is only encrypted when it is not being actively accessed. This does not provide end-to-end encryption. Use AWS KMS Customer Default master key: This option allows the user to create a master key using AWS KMS (Key Management Service), which is a fully managed service that makes it easy to create and control encryption keys. Using the Customer Default master key, the user can encrypt the data in the Redshift database, and this encryption can be managed by AWS KMS. Use SSL/TLS for encrypting the data: This option is used for encrypting data in transit, meaning the data is encrypted when it is being transferred from one place to another. SSL/TLS is used to establish a secure connection between the client and the Redshift cluster, and this encryption can be used to encrypt data in transit. Use hardware security module (HSM) to manage the top-level encryption keys: This option provides the highest level of security for encryption keys. HSM is a hardware device that is designed to securely store encryption keys. Using HSM, the user can manage the top-level encryption keys that are used to encrypt the data in the Redshift database.\nTherefore, the correct options to achieve the encryption of data in the Redshift database are:\nB. Use AWS KMS Customer Default master key, which encrypts the data at rest and allows for easy management of the encryption.\nC. Use SSL/TLS for encrypting the data, which encrypts the data in transit and adds an additional layer of security.\n\n"
}, {
  "id" : 84,
  "question" : "An EC2 instance in the private subnet needs access to the S3 bucket placed in the same region as that of the EC2 instance.\nThe EC2 instance needs to upload and download bigger files to the S3 bucket frequently. As an AWS Solutions Architect, what quick and cost-effective solution would you suggest to your customers? You need to consider that the EC2 instances are present in the private subnet, and the customers do not want their data to be exposed over the internet.\n",
  "answers" : [ {
    "id" : "720e818b83b845f6b9a61c890f2da4c0",
    "option" : "Place the S3 bucket in another public subnet of the same region and create a VPC peering connection to this private subnet where the EC2 instance is placed. The traffic to upload and download files will go through secure Amazon`s private network.",
    "isCorrect" : "false"
  }, {
    "id" : "36531623a9e24619b277f8734243ca8f",
    "option" : "Create an IAM role having access over the S3 service and assign it to the EC2 instance.",
    "isCorrect" : "false"
  }, {
    "id" : "51901d9333f0439cba2ddfea333bf471",
    "option" : "Create a VPC endpoint for S3, use your route tables to control which instances can access resources in Amazon S3 via the endpoint. The traffic to upload and download files will go through the Amazon private network.",
    "isCorrect" : "true"
  }, {
    "id" : "65f4f44a367f482ab56ca4f4de1c7d4f",
    "option" : "A private subnet can always access S3 bucket/ service through the NAT Gateways or NAT instances, so there is no need for additional setup.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect because the S3 service is region-specific, not AZ's specific, and the statement talks about placing the S3 bucket in Public Subnet.\nOption B is incorrect because the VPC endpoint has a policy that controls the use of the endpoint to access Amazon S3 resources.\nThe default policy allows access by any user or service within the VPC, using credentials from any AWS account to any Amazon S3 resource.\nOption C is correct.\nIt can help to access the S3 services in the same region for the EC2 instance.\nYou can create a VPC endpoint and update the route entry of the route table associated with the private subnet.\nThis is a quick solution as well as cost-effective as it will use Amazon's own private network.\nHence, it won't expose the data over the internet.\nOption D is incorrect as this is certainly not a default setup unless we create a NAT Gateway or Instance.\nEven if they are there, it's an expensive solution and exposes the data over the internet.\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html\n\nThe correct answer for this scenario is option C: Create a VPC endpoint for S3, use your route tables to control which instances can access resources in Amazon S3 via the endpoint. The traffic to upload and download files will go through the Amazon private network.\nExplanation: When we place an EC2 instance in a private subnet, it does not have direct access to the internet or other AWS services. In order to allow the EC2 instance to access the S3 bucket, we need to provide a secure and private connection between them.\nOption A: Placing the S3 bucket in another public subnet and creating a VPC peering connection to the private subnet is not a good solution because it involves routing the traffic through the public internet. This can expose data to security risks and may not meet the requirement of the customer who wants to keep their data private.\nOption B: Creating an IAM role and assigning it to the EC2 instance is not a correct solution for this problem. IAM roles provide permissions to access AWS services, but they do not provide a secure connection to access S3 from a private subnet.\nOption C: Creating a VPC endpoint for S3 is a secure and private way to access the S3 bucket from the EC2 instance in the private subnet. The VPC endpoint is a virtual device that allows communication between instances in the VPC and S3 without going over the public internet. With the help of VPC endpoints, the traffic between EC2 instance and S3 bucket will be securely routed through the Amazon private network.\nOption D: It is true that we can use NAT Gateways or NAT instances to allow EC2 instances in a private subnet to access the internet or other AWS services, but this approach will not meet the requirement of the customer who wants to keep their data private.\nIn conclusion, creating a VPC endpoint for S3 is the best solution to provide secure and private access to the S3 bucket from an EC2 instance in a private subnet. It is also cost-effective as it does not require additional resources and incurs minimal data transfer charges.\n\n"
}, {
  "id" : 85,
  "question" : "You are developing an application using AWS SDK to get objects from AWS S3\nThe objects have big sizes.\nSometimes there are failures when getting objects, especially when the network connectivity is poor.\nYou want to get a specific range of bytes in a single GET request and retrieve the whole object in parts.\nWhich method can achieve this?\n",
  "answers" : [ {
    "id" : "2b24ff574c6b425783acbb80c35d9680",
    "option" : "Enable multipart upload in the AWS SDK.",
    "isCorrect" : "false"
  }, {
    "id" : "ec50f720e3fd41a297e7b06f5eda973f",
    "option" : "Use the â€œRangeâ€ HTTP header in a GET request to download the specified range bytes of an object.",
    "isCorrect" : "true"
  }, {
    "id" : "789936fc7e19436788257fa437f52eaa",
    "option" : "Reduce the retry requests and enlarge the retry timeouts through AWS SDK when fetching S3 objects.",
    "isCorrect" : "false"
  }, {
    "id" : "d3ff1a01bd7a4ba1978d99245a642ddb",
    "option" : "Retrieve the whole S3 object through a single GET operation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThrough the â€œRangeâ€ header in the HTTP GET request, a specified portion of the objects can be downloaded instead of the whole objects.\nCheck the explanations in https://docs.aws.amazon.com/AmazonS3/latest/dev/GettingObjectsUsingAPIs.html.\nOption A is incorrect: Because the question asks for multipart download rather than multipart upload.\nOption B is CORRECT: Because with byte-range fetches, users can establish concurrent connections to Amazon S3 to fetch different parts from within the same object.\nOption C is incorrect: Because adjusting retry requests and timeouts cannot download specific parts of an object.\nOption D is incorrect: Because the method to retrieve the entire object does not meet the requirement.\n\nThe correct answer to this question is B. Use the â€œRangeâ€ HTTP header in a GET request to download the specified range bytes of an object.\nWhen retrieving objects from Amazon S3 using the AWS SDK, it is possible to use the \"Range\" HTTP header to specify the byte range of the object to retrieve. This can be useful when dealing with large objects or slow network connectivity, as it allows retrieving only the necessary data.\nUsing the \"Range\" header can also be used in conjunction with downloading an object in parts. This technique is called \"multipart download\" and allows downloading large objects in parallel, thus improving performance. It works by dividing the object into multiple parts and downloading each part individually using separate GET requests.\nOption A is incorrect because enabling multipart upload is used when uploading large objects, not when retrieving them.\nOption C is also incorrect because reducing the retry requests and enlarging the retry timeouts may improve reliability but it does not address the issue of large objects or poor network connectivity.\nOption D is also incorrect because retrieving the whole S3 object through a single GET operation may not be practical when dealing with large objects or poor network connectivity.\n\n"
}, {
  "id" : 86,
  "question" : "An application needs to access resources from another AWS account of another VPC in the same region.\nWhich of the following ensure that the resources can be accessed as required?\n",
  "answers" : [ {
    "id" : "19df565645cb41a49c0516f59c69511a",
    "option" : "Establish a NAT instance between both accounts.",
    "isCorrect" : "false"
  }, {
    "id" : "a72390c0c40f4208baccd043ffb967d7",
    "option" : "Use a VPN between both accounts.",
    "isCorrect" : "false"
  }, {
    "id" : "0d89ab350d9046048ec47e28f0aed19c",
    "option" : "Use a NAT Gateway between both accounts.",
    "isCorrect" : "false"
  }, {
    "id" : "9b4874b1cfac4c178e4217aea04a48d4",
    "option" : "Use VPC Peering between both accounts.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOptions A and C are incorrect because these are used when private resources are required to access the Internet.\nOption B is incorrect because it's used to create a connection between the On-premises and AWS resources.\nAWS Documentation mentions the following about VPC Peering:\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately.\nInstances in either VPC can communicate with each other as if they are within the same network.\nYou can create a VPC Peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.\nFor more information on VPC Peering, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html\n\nTo allow access between resources in different VPCs, or between resources in different AWS accounts, there are several options available. In this scenario, the application needs to access resources from another AWS account of another VPC in the same region.\nOut of the options given, the most suitable solution to ensure that the resources can be accessed as required is to use VPC peering between both accounts.\nVPC peering is a way to connect two VPCs together, which allows resources in one VPC to communicate with resources in another VPC. VPC peering is also available between VPCs in different AWS accounts, as long as both accounts are within the same region. Once VPC peering is established, traffic flows directly between the VPCs without going over the internet.\nOption A, establishing a NAT instance between both accounts, is not the best solution because NAT instances are used to provide internet access to private resources in a VPC. In this scenario, the resources are in different AWS accounts, and there is no mention of providing internet access to private resources.\nOption B, using a VPN between both accounts, is also not the best solution because VPNs are used to establish secure connections between on-premises networks and AWS VPCs. Although it is possible to use VPNs between VPCs, it is not necessary in this scenario because the VPCs are in the same region and there is no mention of on-premises networks.\nOption C, using a NAT Gateway between both accounts, is also not the best solution because NAT gateways are similar to NAT instances, but they are a managed service provided by AWS. Like NAT instances, NAT gateways are used to provide internet access to private resources in a VPC, which is not required in this scenario.\nTherefore, the best option to ensure that the resources can be accessed as required is to use VPC peering between both accounts.\n\n"
}, {
  "id" : 87,
  "question" : "You host a static website in an S3 bucket, and there are global clients from multiple regions.\nYou want to use an AWS service to store cache for frequently accessed content so that the latency is reduced and the data transfer rate increases.\nWhich of the following options would you choose?\n",
  "answers" : [ {
    "id" : "067e4b7992a74d518fc871a6d6066788",
    "option" : "Use AWS SDKs to horizontally scale parallel requests to the Amazon S3 service endpoints.",
    "isCorrect" : "false"
  }, {
    "id" : "ad18703f111145b99c1777a558761841",
    "option" : "Create multiple Amazon S3 buckets and put Amazon EC2 and S3 in the same AWS Region.",
    "isCorrect" : "false"
  }, {
    "id" : "0be791e423b04f2293f74653a53e7633",
    "option" : "Enable Cross-Region Replication to several AWS Regions to serve customers from different locations.",
    "isCorrect" : "false"
  }, {
    "id" : "b71fb0d3172c4e1487d7e2854bfff736",
    "option" : "Configure CloudFront to deliver the content in the S3 bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nCloudFront can store the frequently accessed content as a cache, and the performance is optimized.\nOther options may help with the performance.\nHowever, they do not store cache for the S3 objects.\nOption A is incorrect: This option may increase the throughput.\nHowever, it does not store cache.\nOption B is incorrect: Because this option does not use cache.\nOption C is incorrect: This option creates multiple S3 buckets in different regions.\nIt does not improve the performance using cache.\nOption D is CORRECT: Because CloudFront caches copies the S3 files in its edge locations.\nUsers are routed to the edge location that has the lowest latency.\n\nThe most appropriate option to reduce latency and increase data transfer rate for a static website hosted in an S3 bucket for global clients is to configure CloudFront to deliver content from the S3 bucket. Therefore, the correct answer is D.\nCloudFront is a content delivery network (CDN) service provided by AWS that caches content in edge locations around the world. When a user requests content, CloudFront delivers the cached content from the edge location closest to the user, reducing latency and improving data transfer rates. CloudFront integrates seamlessly with S3, making it easy to distribute content stored in S3 to users around the world.\nOption A, using AWS SDKs to horizontally scale parallel requests to the Amazon S3 service endpoints, is not the best approach for reducing latency and improving data transfer rates. While this approach can increase the number of parallel requests, it may not significantly improve the overall performance, especially for global clients.\nOption B, creating multiple S3 buckets and putting EC2 and S3 in the same AWS Region, is also not an effective approach for serving global clients. While this approach can improve performance for clients in the same region as the resources, it does not help clients in other regions.\nOption C, enabling Cross-Region Replication to several AWS Regions to serve customers from different locations, is not the best approach for reducing latency and improving data transfer rates for a static website hosted in an S3 bucket. While this approach can improve data availability and reduce recovery time objectives (RTOs) for disaster recovery scenarios, it does not address the performance issues of serving content to global clients.\nTherefore, the best option to reduce latency and improve data transfer rates for a static website hosted in an S3 bucket for global clients is to configure CloudFront to deliver content from the S3 bucket.\n\n"
}, {
  "id" : 88,
  "question" : "An application consists of the following architecture: a.\nEC2 Instances in multiple AZ's behind an ELB b.\nEC2 Instances are launched via an Auto Scaling Group. c.There is one NAT instance to download the updates from the Internet. What is a bottleneck in the architecture based on the availability?\n",
  "answers" : [ {
    "id" : "41175bd0f9444dd5b8858b354f89801b",
    "option" : "The EC2 Instances",
    "isCorrect" : "false"
  }, {
    "id" : "c29362f0627e4f1b8343d539e13c7a71",
    "option" : "The ELB",
    "isCorrect" : "false"
  }, {
    "id" : "6a8d40525a5240248562a830d5d2136b",
    "option" : "The NAT Instance",
    "isCorrect" : "true"
  }, {
    "id" : "901ddb6939a44ecab695726ff955ccb7",
    "option" : "The Auto Scaling Group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nSince there is only one NAT instance, this is a bottleneck in the architecture.\nFor high availability, launch NAT instances in multiple Available Zones and make them part of an Auto Scaling Group.\nFor more information on NAT Instances, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html\nNote:\nNAT Gateway is a better choice than NAT instance as NAT Gateway is highly available.\nIf you're already using a NAT instance, you can replace it with a NAT gateway.\n\nBased on the given architecture, the bottleneck in the availability could be the NAT instance. Here's the detailed explanation:\nAn application with EC2 instances in multiple AZs behind an ELB and launched via an Auto Scaling Group is designed to provide high availability and scalability. The EC2 instances are distributed across multiple AZs, which ensures that the application remains available even if one AZ goes down. The Auto Scaling Group ensures that the application can handle varying levels of traffic by launching additional EC2 instances when demand is high and terminating them when demand decreases.\nHowever, there is one NAT instance to download updates from the internet. A NAT instance is used to allow instances in a private subnet to connect to the internet. It acts as a gateway, and all outbound traffic from the private subnet is routed through the NAT instance.\nThe NAT instance can become a bottleneck if it fails or if it cannot handle the traffic volume. If the NAT instance fails, the instances in the private subnet won't be able to connect to the internet, which can impact the availability of the application. Also, if the NAT instance is not appropriately sized to handle the traffic volume, it can become a bottleneck and cause latency issues, impacting the overall performance of the application.\nTherefore, based on the given architecture, the bottleneck in the availability could be the NAT instance.\n\n"
}, {
  "id" : 89,
  "question" : "A company owns an API deployed in EC2 written using Python.\nAll the requests can be finished within 1 second.\nMost of traffic happens during the daytime.\nThe company wants to save the API cost and simplify the maintenance of the server without impacting the performance.\nHow can this be achieved?\n",
  "answers" : [ {
    "id" : "f3ebb136c772415f907d391b0469e7b2",
    "option" : "Use API Gateway with the backend services as it is.",
    "isCorrect" : "false"
  }, {
    "id" : "714808508a3a49798e77ea9d7add9aaf",
    "option" : "Use the API Gateway along with AWS Lambda.",
    "isCorrect" : "true"
  }, {
    "id" : "c98d2f56b65343669cec8d2e135ed4bc",
    "option" : "Use CloudFront along with the API backend service as it is.",
    "isCorrect" : "false"
  }, {
    "id" : "3ba43b8e061e4e728713c9cc70186616",
    "option" : "Use ElastiCache along with the API backend service as it is.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nSince the company has full ownership of the API, the best solution would be to convert the code for the API and use it in a Lambda function.\nThis can help save on cost since, in the case of Lambda, you only pay for the time the function runs and not for the infrastructure.\nThen, you can use the API Gateway along with the AWS Lambda function to scale accordingly.\nFor more information on using API Gateway with AWS Lambda, please visit the following URL:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-with-lambda-integration.html\nNote:With Lambda, you do not have to provision your own instances.\nLambda performs all the administrative activities on your behalf, including capacity provisioning, monitoring fleet health, applying security patches to the underlying compute resources, deploying your code, running a web service front end, and monitoring and logging your code.\nAWS Lambda provides easy scaling and high availability to your code without additional effort on your part.\n\nThe company can save the API cost and simplify the maintenance of the server without impacting the performance by using AWS services like API Gateway, Lambda, CloudFront, and ElastiCache. Each of these services has its own benefits and drawbacks.\nOption A: Use API Gateway with the backend services as it is. API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It can act as a front door for applications to access data, business logic, or functionality from your backend services, such as EC2 instances. With API Gateway, the company can easily manage traffic, handle API requests, and reduce costs by scaling up and down automatically based on traffic patterns. However, using API Gateway with the backend services as it is may not significantly reduce the cost, as EC2 instances will still need to be running.\nOption B: Use the API Gateway along with AWS Lambda. AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the compute resources for you. By using Lambda with API Gateway, the company can create a serverless API that automatically scales up and down based on traffic, reduces costs by only paying for what you use, and simplifies maintenance by eliminating the need to manage servers. With Lambda, the company can run their Python code without managing servers or worrying about infrastructure, which can significantly reduce costs.\nOption C: Use CloudFront along with the API backend service as it is. Amazon CloudFront is a fast content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. By using CloudFront along with the API backend service, the company can improve the performance and availability of the API by caching frequently accessed content closer to the end-user. CloudFront can also reduce costs by minimizing the load on the backend servers and reducing the number of requests that need to be processed. However, using CloudFront alone may not simplify the maintenance of the server.\nOption D: Use ElastiCache along with the API backend service as it is. Amazon ElastiCache is a fully managed in-memory data store and cache service that can improve the performance of web applications by allowing you to retrieve data from fast, managed, in-memory caches instead of relying entirely on slower disk-based databases. By using ElastiCache along with the API backend service, the company can cache frequently accessed data, which can significantly reduce the number of requests that need to be processed, lower latency, and improve performance. This can lead to cost savings by reducing the load on the backend servers. However, using ElastiCache alone may not simplify the maintenance of the server.\nConclusion: Among the options, the best solution for the company depends on their specific needs and requirements. However, considering that the company wants to save API cost and simplify the maintenance of the server without impacting the performance, using AWS Lambda with API Gateway is the most recommended solution, as it allows the company to create a serverless API that automatically scales up and down based on traffic, reduces costs, and simplifies maintenance.\n\n"
}, {
  "id" : 90,
  "question" : "You have an application hosted in an Auto Scaling group, and an application load balancer distributes traffic to the ASG.\nYou want to add a scaling policy that keeps the average aggregate CPU utilization of the Auto Scaling group to be 60 percent.\nThe capacity of the Auto Scaling group should increase or decrease based on this target value.\nWhich scaling policy does it belong to?\n",
  "answers" : [ {
    "id" : "29a79dd775b34de4a8e715d6db4223c8",
    "option" : "Target tracking scaling policy.",
    "isCorrect" : "true"
  }, {
    "id" : "866d3b713c2c4deea9d3d4d04145d042",
    "option" : "Step scaling policy.",
    "isCorrect" : "false"
  }, {
    "id" : "84d1ae0021924aeb80303733fa566395",
    "option" : "Simple scaling policy.",
    "isCorrect" : "false"
  }, {
    "id" : "2abeaccbd172439ebfd8da1e5bc0e34e",
    "option" : "Scheduled scaling policy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nIn ASG, you can add a target tracking scaling policy based on a target.\nFor different scaling policies, please check https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html.\nOption A is CORRECT: Because a target tracking scaling policy can be applied to check the ASGAverageCPUUtilization metric according to https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html.\nOption B is incorrect: Because Step scaling adjusts the capacity based on step adjustments instead of a target.\nOption C is incorrect: Because Simple scaling changes the capacity based on a single adjustment.\nOption D is incorrect: With Scheduled scaling, the capacity is adjusted based on a schedule rather than a target.\n\nThe scaling policy that is suitable for keeping the average aggregate CPU utilization of the Auto Scaling group to 60 percent is a \"Target Tracking Scaling Policy.\"\nTarget Tracking Scaling Policy is a type of scaling policy that allows you to specify a target value for a specific metric of a scalable resource, such as an Amazon EC2 Auto Scaling group or an Application Load Balancer. This policy automatically increases or decreases the capacity of the scalable resource in response to changes in the metric to meet the target value.\nIn this scenario, the metric being tracked is CPU utilization of the Auto Scaling group, and the target value is 60 percent. Therefore, as the CPU utilization of the group increases or decreases, the policy will automatically adjust the capacity of the group to keep the aggregate CPU utilization at 60 percent.\nThe other scaling policies available are:\nSimple Scaling Policy: This policy is based on a fixed value for a metric or a specified number of instances. It increases or decreases the capacity of the Auto Scaling group by a fixed amount when a CloudWatch alarm is triggered. Step Scaling Policy: This policy adjusts the capacity of the Auto Scaling group based on the scale-in and scale-out rules defined by the user. This policy is useful when you have a predictable pattern of usage or when you need to add or remove capacity in large increments. Scheduled Scaling Policy: This policy adjusts the capacity of the Auto Scaling group based on a pre-defined schedule. This policy is useful when you know the exact time and date when you need to increase or decrease the capacity of your resources.\n\n"
}, {
  "id" : 91,
  "question" : "An application sends images to S3\nThe metadata for these images needs to be saved in persistent storage and is required to be indexed.\nWhich one of the following is the best for the underlying metadata storage?\n",
  "answers" : [ {
    "id" : "b2db11fbe52f4bb1ad54a8e50a5e6035",
    "option" : "Amazon Aurora",
    "isCorrect" : "false"
  }, {
    "id" : "8248f1abf22f45e293518afbfbfd40aa",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "d6f4e2cb705f407089e0a60cb582d571",
    "option" : "Amazon DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "a93f8071135e4f9f8f886efbeb95f498",
    "option" : "Amazon RDS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe most efficient storage mechanism for just storing metadata is Amazon DynamoDB.\nAmazon DynamoDB is normally used in conjunction with the Simple Storage service.\nSo, after storing the images in S3, you can store their metadata in DynamoDB.\nYou can also create secondary indexes for DynamoDB Tables.\nFor more information on managing indexes in DynamoDB, please visit the following URL:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.Indexes.html\n\nBased on the requirements provided, the best option for the underlying metadata storage would be Amazon DynamoDB (Option C).\nHere's why:\nAmazon Aurora (Option A) is a relational database service that offers high performance and availability. It is a good option for storing structured data, but it may not be the best choice for unstructured data such as image metadata. Moreover, Aurora is a costlier option as compared to DynamoDB.\nAmazon S3 (Option B) is an object storage service that is designed to store and retrieve large amounts of data. It can also store metadata in key-value pairs, but it is not optimized for indexing or querying this metadata.\nAmazon RDS (Option D) is a managed relational database service that supports multiple database engines like MySQL, PostgreSQL, etc. Similar to Aurora, it may not be the best choice for storing unstructured data such as image metadata.\nOn the other hand, Amazon DynamoDB (Option C) is a NoSQL database service that is optimized for storing and querying unstructured data such as image metadata. DynamoDB offers fast and predictable performance and is designed to scale seamlessly. It also provides flexible indexing options that can help optimize queries for specific access patterns.\nTherefore, based on the requirements mentioned in the question, Amazon DynamoDB is the best option for the underlying metadata storage.\n\n"
}, {
  "id" : 92,
  "question" : "An application hosted on EC2 Instances has its promotional campaign due to start in 2 weeks.\nThe performance team performs some analysis based on the historical data and informs you the number of instances that are required for the campaign.\nYou need to make sure that the Auto Scaling group is properly configured with the provided number of instances.\nWhat should be done to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "c303b2d228664c5aa3cc0d3d7fe19054",
    "option" : "Migrate the application from the Auto Scaling group to a Lambda function so that the application scales automatically by AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "7f9e5dabfffe4f80b35ff71a9109049a",
    "option" : "Configure Scheduled scaling in the Auto Scaling Group.",
    "isCorrect" : "true"
  }, {
    "id" : "95a2894bb6e34b33ab9dac5362eec964",
    "option" : "Configure a Lambda function that scales up the ASG when the activity starts and scales down when the activity ends.",
    "isCorrect" : "false"
  }, {
    "id" : "a2ec69e08f4345b4bfb6dc44fce99f63",
    "option" : "Configure Static scaling for the Auto Scaling Group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nIn the question, the promotional campaign will start in 2 weeks.\nAs you already know how many instances are required, you can configure a scaling schedule in the Auto Scaling group to plan the scaling actions.\nOption A is incorrect because it is unsuitable to migrate the application to a Lambda function since the campaign will start in 2 weeks.\nYou can configure an Auto Scaling policy for the campaign.\nOption C is incorrect because it is easier to use an Auto Scaling policy to achieve this requirement.\nOption D is incorrect because there is no static scaling policy in an Auto Scaling group.\nFor more information on Scheduled Scaling, please visit the following URLs:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\n\nOption A is not a valid solution for this scenario, as migrating an application to a Lambda function means completely changing the architecture of the application, which may not be feasible or practical in this case. Moreover, Lambda functions have different use cases than EC2 instances and may not be the most suitable option for all types of applications.\nOption B, \"Configure Scheduled scaling in the Auto Scaling Group,\" is not the best option because scheduled scaling is useful for predictable traffic patterns. However, in this case, the traffic is not predictable, so scheduled scaling is not the best solution.\nOption C, \"Configure a Lambda function that scales up the ASG when the activity starts and scales down when the activity ends,\" is a possible solution. This approach involves creating a Lambda function that monitors the promotional campaign start and end times and triggers the scaling of the Auto Scaling Group (ASG) accordingly. This Lambda function can be triggered by CloudWatch Events or an API Gateway endpoint.\nOption D, \"Configure Static scaling for the Auto Scaling Group,\" is not the best option for this scenario. Static scaling involves manually setting the number of instances in the Auto Scaling Group, which is not suitable for unpredictable traffic patterns. Static scaling is best suited for applications that have a consistent traffic pattern.\nIn conclusion, option C is the best solution for this scenario. It involves configuring a Lambda function to monitor the start and end times of the promotional campaign and to trigger the scaling of the Auto Scaling Group accordingly. This solution allows the application to automatically scale up and down based on the traffic pattern, which is the most suitable solution for unpredictable traffic.\n\n"
}, {
  "id" : 93,
  "question" : "Currently, a company uses EBS snapshots to back up their EBS Volumes.\nAs a part of the business continuity requirement, these snapshots need to be made available in another region.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "37b2df0ea8df492e815ce06b1de93329",
    "option" : "Directly create the snapshot in the other region.",
    "isCorrect" : "false"
  }, {
    "id" : "810c25a7bb4845d99487476d8a9d0977",
    "option" : "Create Snapshot and copy the snapshot to a new region.",
    "isCorrect" : "true"
  }, {
    "id" : "48658a0024c547f2aab388605b889443",
    "option" : "Copy the snapshot to an S3 bucket and then enable Cross-Region Replication for the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "08c0abe9d9ec48d78e87ca0b668e61b9",
    "option" : "Copy the EBS Snapshot to an EC2 instance in another region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following:\nA snapshot is constrained to the region where it was created.\nAfter you create a snapshot of an EBS volume, you can use it to create new volumes in the same region.\nFor more information, follow the link on Restoring an Amazon EBS Volume from a Snapshot below.\nYou can also copy snapshots across regions, making it possible to use multiple regions for geographical expansion, data center migration, and disaster recovery.\nFor more information on EBS Snapshots, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\nFor more information on Restoring an Amazon EBS Volume from a Snapshot, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html\nOption C is incorrect because the EBS snapshots are stored in S3, which is managed by AWS.\nWe don't have the option to see the snapshots in S3.\n\nThe correct answer is option B: \"Create Snapshot and copy the snapshot to a new region.\"\nExplanation: Amazon Elastic Block Store (EBS) is a storage service provided by AWS for EC2 instances. EBS snapshots are point-in-time backups of EBS volumes. These snapshots can be used to restore data in case of data loss or corruption.\nTo make these snapshots available in another region for business continuity, the snapshots can be copied to a new region using the following steps:\nCreate a snapshot of the EBS volume in the original region. In the AWS Management Console, navigate to the EBS Snapshots page and select the snapshot that needs to be copied. From the Actions drop-down menu, select \"Copy Snapshot.\" In the \"Copy Snapshot\" dialog box, select the destination region where the snapshot needs to be copied. Configure the settings such as the encryption key, if required, and the description for the snapshot. Click \"Copy Snapshot.\"\nThe copied snapshot will now be available in the destination region. This process can be automated using AWS CLI or SDKs.\nOption A is not a viable solution as EBS snapshots cannot be directly created in another region.\nOption C is incorrect because copying snapshots to an S3 bucket and enabling Cross-Region Replication for the bucket is not the recommended method to copy EBS snapshots to another region.\nOption D is also incorrect because EBS snapshots cannot be copied to an EC2 instance directly. However, snapshots can be used to create EBS volumes which can then be attached to an EC2 instance in the destination region.\n\n"
}, {
  "id" : 94,
  "question" : "A company has an application hosted in AWS.\nThis application consists of EC2 Instances that sit behind an ELB.\nThe following are the requirements from an administrative perspective: a) Ensure that notifications are sent when the read requests go beyond 1000 requests per minute. b) Ensure that notifications are sent when the latency goes beyond 10 seconds. c)Monitor all AWS API request activities on the AWS resources. Which of the following can be used to satisfy these requirements? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "3c14616ebe204d14b2ec0c4760156387",
    "option" : "Use CloudTrail to monitor the API Activity.",
    "isCorrect" : "true"
  }, {
    "id" : "095444993c564f11b6ebb74d6e5463a8",
    "option" : "Use CloudWatch Logs to monitor the API Activity.",
    "isCorrect" : "false"
  }, {
    "id" : "9d1bfaf3e6eb4cfeb758825a96b7d211",
    "option" : "Use CloudWatch Metrics for the metrics that need to be monitored as per the requirement and set up an alarm activity to send out notifications when the metric reaches the set threshold limit.",
    "isCorrect" : "true"
  }, {
    "id" : "a3d55a4d787049ada40e42cb456e6271",
    "option" : "Use custom log software to monitor the latency and read requests to the EL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and C.\nOption A is correct.\nCloudTrail is a web service that records AWS API calls for all the resources in your AWS account.\nIt also delivers log files to an Amazon S3 bucket.\nThe recorded information includes the identity of the user, the start time of the AWS API call, the source IP address, the request parameters, and the response elements returned by the service.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/APIReference/Welcome.html\nOption B is incorrect because CloudWatch Logs can be used to monitor log files from other services.\nCloudWatch Logs and CloudWatch are different.\nAmazon CloudWatch Logs are used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources.\nCloudWatch Logs reports the data to a CloudWatch metric.\nRather you can monitor Amazon EC2 API requests using Amazon CloudWatch.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\nOption C is correct.\nUse Cloudwatch Metrics for the metrics that need to be monitored as per the requirement.\nSet up an alarm activity to send out notifications when the metric reaches the set threshold limit.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html\nOption D is incorrect because there is no need to use custom log software as you can set up CloudWatch alarms based on CloudWatch Metrics.\n\nTo meet the administrative requirements mentioned in the question, two options are required from the given choices. Here's a detailed explanation of each choice:\nA. Use CloudTrail to monitor the API Activity. CloudTrail is a service that records and monitors all API activity on AWS resources. It captures all API calls made to AWS resources in the account and saves them in S3 buckets or CloudWatch Logs for auditing and compliance purposes. CloudTrail enables organizations to identify security threats, troubleshoot operational issues, and ensure compliance with regulatory standards. Therefore, it is an ideal choice to meet the requirement of monitoring all AWS API request activities on the AWS resources.\nB. Use CloudWatch Logs to monitor the API Activity. CloudWatch Logs is a log management service that enables customers to collect, monitor, and analyze log data from EC2 instances, AWS services, and custom applications. It provides real-time analysis of log data to monitor and troubleshoot application and system issues. CloudWatch Logs can be used to track API activity by configuring logging for the AWS services that are being used. CloudTrail is a better option for monitoring API activity; however, CloudWatch Logs can be used as a fallback option if CloudTrail is not sufficient.\nC. Use CloudWatch Metrics for the metrics that need to be monitored as per the requirement and set up an alarm activity to send out notifications when the metric reaches the set threshold limit. CloudWatch Metrics is a monitoring service that provides metrics for AWS resources and applications. It can collect metrics from various AWS resources, such as EC2 instances, ELB, and RDS. CloudWatch Metrics can be used to monitor metrics such as read requests per minute and latency. To meet the requirements mentioned in the question, CloudWatch Metrics can be used to monitor these metrics, and an alarm can be set up to send notifications when the metric reaches the set threshold limit.\nD. Use custom log software to monitor the latency and read requests to the EL. Using custom log software can be a valid option to monitor the latency and read requests to the EL. However, it would require additional setup and maintenance efforts. Using CloudWatch Metrics is a more straightforward and efficient solution to monitor these metrics, as it is a built-in AWS service and can easily integrate with ELB.\nTherefore, the two recommended options to meet the requirements mentioned in the question are A. Use CloudTrail to monitor the API Activity and C. Use CloudWatch Metrics for the metrics that need to be monitored as per the requirement and set up an alarm activity to send out notifications when the metric reaches the set threshold limit.\n\n"
}, {
  "id" : 95,
  "question" : "A company has resources hosted in its AWS Account.\nThere is a requirement to monitor API activity for all regions and the audit needs to be applied for future regions as well.\nWhat would fulfill this requirement?\n",
  "answers" : [ {
    "id" : "ce253d5ce52443efb805a8a80859fbf5",
    "option" : "Ensure CloudTrail trail for each region, then enable trail for each future region.",
    "isCorrect" : "false"
  }, {
    "id" : "0ad6fc58a52d4914b6b5c7e726c8961b",
    "option" : "Ensure one CloudTrail trail is enabled for all regions.",
    "isCorrect" : "true"
  }, {
    "id" : "b9400646ca3642978fd34260278bb602",
    "option" : "Create a CloudTrail for each region. Use CloudFormation to enable the trail for all future regions.",
    "isCorrect" : "false"
  }, {
    "id" : "4f3861e6b16449108928b6f653474516",
    "option" : "Create a CloudTrail for each region. Use AWS Config to enable the trail for all future regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following:\nYou can now turn on a trail across all regions for your AWS account.\nCloudTrail will deliver log files from all regions to the Amazon S3 bucket and an optional CloudWatch Logs log group you specified.\nAdditionally, when AWS launches a new region, CloudTrail will create the same trail in the new region.\nAs a result, you will receive log files containing API activity for the new region without taking any action.\nFor more information on this feature, please visit the following URL:\nhttps://aws.amazon.com/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/\n\nThe requirement is to monitor API activity across all regions in the AWS account and to ensure that this monitoring is also applied to future regions. CloudTrail is an AWS service that logs API activity across an AWS account.\nOption A suggests creating a CloudTrail trail for each region and enabling it for each future region. This approach would work, but it would require manual intervention to create and enable the trail for each future region. This could be time-consuming and error-prone, especially if the account has many regions or if new regions are frequently added.\nOption B suggests enabling one CloudTrail trail for all regions. This approach would meet the monitoring requirement but would not address the need to apply the monitoring to future regions.\nOption C suggests creating a CloudTrail trail for each region and using CloudFormation to enable the trail for all future regions. This approach would automate the process of creating and enabling the trail for future regions.\nOption D suggests creating a CloudTrail trail for each region and using AWS Config to enable the trail for all future regions. AWS Config is a service that enables the assessment, audit, and evaluation of the configurations of AWS resources. However, AWS Config does not have a direct feature to enable CloudTrail for future regions. Therefore, Option D is not a viable solution to fulfill the requirement.\nTherefore, the best option to fulfill the requirement would be Option C, where CloudFormation is used to automate the process of creating and enabling CloudTrail trails for future regions.\n\n"
}, {
  "id" : 96,
  "question" : "You are part of the IT sector at the finance department of your country.\nYour organization has implemented AWS Organizations for each internal department, and you have access to the master account.\nYou need to manage Amazon EC2 Dedicated Hosts centrally, and share the host's instance capacity with other AWS accounts in the AWS Organizations.\nHow can you accomplish this in the easiest way?\n",
  "answers" : [ {
    "id" : "4d933e8463e64fd28d6572796fa587c3",
    "option" : "Use AWS Resource Access Manager to manage the EC2 Dedicated Hosts centrally and share them with other member accounts.",
    "isCorrect" : "true"
  }, {
    "id" : "f4c612b4428b4078be05df81eecd6360",
    "option" : "Use service control policies to share the EC2 Dedicated Hosts in the member accounts.",
    "isCorrect" : "false"
  }, {
    "id" : "21dcb2aa7cfa4edaadfb6a37241fc5b9",
    "option" : "Use AWS Control Tower.",
    "isCorrect" : "false"
  }, {
    "id" : "abcafea26c1d43009711dc50c997bcce",
    "option" : "Create IAM policies with conditions and assign them to users in every member account.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT.\nWith AWS Resource manager, you can share resources with other AWS accounts joined to your AWS Organizations reducing the operational overhead.\nMore details: https://docs.aws.amazon.com/ram/latest/userguide/shareable.html#shareable-ec2.\nOption B is incorrect.\nSCP is not a service to share resources such as EC2 Dedicated Hosts with other accounts within the AWS Organizations.\nOption C is incorrect.\nAWS Control Tower is used to do automated deployments in multi-account environments.\nOption D is incorrect.\nCreating IAM policies is not used to manage and share resources within the AWS Organizations.\n\nIn order to manage Amazon EC2 Dedicated Hosts centrally and share the host's instance capacity with other AWS accounts in the AWS Organizations, the easiest and recommended approach is to use AWS Resource Access Manager (RAM).\nAWS RAM is a service that enables resource sharing across AWS accounts within an organization, allowing you to share resources like EC2 Dedicated Hosts, AWS Transit Gateways, and subnets, among others. AWS RAM eliminates the need for manual configuration and sharing of resources between accounts, simplifying the management of resources.\nUsing AWS RAM, you can create a resource share that includes EC2 Dedicated Hosts in the master account, and then share it with the member accounts within the AWS Organization. The member accounts can then launch instances on the shared hosts by specifying the host ID in the instance launch request.\nTo accomplish this, you need to perform the following steps:\nCreate a resource share in the master account for the EC2 Dedicated Hosts that you want to share with the member accounts. You can create the resource share using the AWS RAM console or the AWS RAM API/CLI. Specify the accounts that you want to share the resource with. You can share the resource with all member accounts in the AWS Organization or specific accounts. Configure the resource share settings, including the permissions and the resource utilization. For EC2 Dedicated Hosts, you can specify the number of instances that the member accounts can launch on the shared hosts. Share the resource share with the member accounts. The member accounts will receive an invitation to accept the resource share, and they can accept it using the AWS RAM console or the API/CLI.\nOnce the member accounts accept the resource share, they can launch instances on the shared EC2 Dedicated Hosts by specifying the host ID in the instance launch request. The host ID is available in the master account, where the EC2 Dedicated Hosts are created.\nOption B, using service control policies to share EC2 Dedicated Hosts in the member accounts, is not a recommended approach for managing resources in AWS Organizations. Service control policies are used to manage access and permissions to AWS services across accounts, and they cannot be used to share resources like EC2 Dedicated Hosts.\nOption C, using AWS Control Tower, is a service that provides a centralized view and control of multiple AWS accounts, but it does not provide a mechanism for sharing resources like EC2 Dedicated Hosts.\nOption D, creating IAM policies with conditions and assigning them to users in every member account, is not an effective approach for managing EC2 Dedicated Hosts in AWS Organizations, as it requires manual configuration and management of permissions in each account.\n\n"
}, {
  "id" : 97,
  "question" : "Your company has an online game application deployed in an Auto Scaling group.\nThe traffic of the application is predictable.\nEvery Friday, the traffic starts to increase, remains high on weekends and then drops on Monday.\nYou need to plan the scaling actions for the Auto Scaling group.\nWhich method is the most suitable for the scaling policy?\n",
  "answers" : [ {
    "id" : "626fee2851e44247803bf14312175472",
    "option" : "Configure a scheduled CloudWatch event rule to launch/terminate instances at the specified time every week.",
    "isCorrect" : "false"
  }, {
    "id" : "2020877473144d9a8ec36dc54fe5002f",
    "option" : "Create a predefined target tracking scaling policy based on the average CPU metric and the ASG will scale automatically.",
    "isCorrect" : "false"
  }, {
    "id" : "07347c4f38364795ae5e6c15033da4c6",
    "option" : "Select the ASG and on the Automatic Scaling tab, add a step scaling policy to automatically scale out/in at fixed time every week.",
    "isCorrect" : "false"
  }, {
    "id" : "3f490265fa564ef49f1b8417e2b8a0ac",
    "option" : "Configure a scheduled action in the Auto Scaling group by specifying the recurrence, start/end time, capacities, etc.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThe correct scaling policy should be scheduled scaling as it defines your own scaling schedule.\nRefer to https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html for details.\nOption A is incorrect: This option may work.\nHowever, you have to configure a target such as a Lambda function to perform the scaling actions.\nOption B is incorrect: The target tracking scaling policy defines a target for the ASG.\nThe scaling actions do not happen based on a schedule.\nOption C is incorrect: The step scaling policy does not configure the ASG to scale at a specified time.\nOption D is CORRECT: With scheduled scaling, users define a schedule for the ASG to scale.\nThis option can meet the requirements.\n\nSure, I'd be happy to provide a detailed explanation of the options provided to help you choose the best answer.\nA. Configure a scheduled CloudWatch event rule to launch/terminate instances at the specified time every week.\nThis option suggests using a CloudWatch event rule to schedule the launch and termination of instances at specific times every week. This could work if you know exactly when the traffic will increase and decrease each week, but it's not the most flexible option if the pattern changes in the future. Additionally, if the application experiences unexpected traffic spikes outside of the scheduled times, the Auto Scaling group may not respond quickly enough to handle the increased load.\nB. Create a predefined target tracking scaling policy based on the average CPU metric and the ASG will scale automatically.\nThis option suggests using a predefined target tracking scaling policy to automatically adjust the number of instances in the Auto Scaling group based on the average CPU utilization. This could work if the average CPU utilization is a good indicator of traffic load, but it may not be the most accurate metric for all types of applications. Additionally, this option does not take into account the predictable traffic pattern mentioned in the question, which means that the Auto Scaling group may scale up or down at suboptimal times.\nC. Select the ASG and on the Automatic Scaling tab, add a step scaling policy to automatically scale out/in at fixed time every week.\nThis option suggests using a step scaling policy to automatically adjust the number of instances in the Auto Scaling group at fixed times every week. This option is similar to option A, but it provides more flexibility by allowing you to define scaling thresholds based on metrics like CPU utilization or request count. Additionally, this option allows for more granular control over the scaling actions, as you can define different scaling actions based on the severity of the metric deviation.\nD. Configure a scheduled action in the Auto Scaling group by specifying the recurrence, start/end time, capacities, etc.\nThis option suggests using a scheduled action to adjust the capacity of the Auto Scaling group at specific times. This option is similar to option A, but it allows for more granular control over the scaling actions. For example, you can specify different capacity levels for different times of the day, which can be useful if the traffic patterns vary throughout the day.\nBased on the information provided in the question, option C seems to be the most suitable method for the scaling policy. This option provides the flexibility to adjust the scaling thresholds based on metrics like CPU utilization or request count, while still allowing for automatic scaling at predictable times each week.\n\n"
}, {
  "id" : 98,
  "question" : "There is an application that consists of frontend and backend EC2 Instances behind classic ELBs.\nThe database is deployed in AWS RDS.\nThe application might not be able to scale properly.\nWhat should be used to scale the application appropriately? (SELECT TWO.)\n",
  "answers" : [ {
    "id" : "985834e9f8c34562b728202a96c24d40",
    "option" : "Use Auto Scaling for the frontend EC2 instances.",
    "isCorrect" : "true"
  }, {
    "id" : "e899d957f7ff47eb98536b9b56d78ce2",
    "option" : "Use Auto Scaling for the backend EC2 instances.",
    "isCorrect" : "true"
  }, {
    "id" : "ae8bc771c8a3414ab25c43999a32e68f",
    "option" : "Replace the Classic ELB with Application EL.",
    "isCorrect" : "false"
  }, {
    "id" : "feda63e7429946069a5fedb4f1b1ec96",
    "option" : "Use Network ELB for both the frontend and backend instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and B.\nWhen you see a requirement for scaling, consider the Auto Scaling service provided by AWS.\nThis can be used to scale both the frontend and backend instances.\nFor more information on Auto Scaling, please visit the following URL:\nhttps://docs.aws.amazon.com/autoscaling/plans/userguide/what-is-aws-auto-scaling.html\n\nTo scale the application appropriately, we should consider the following options:\nUse Auto Scaling for the frontend EC2 instances: This option involves using AWS Auto Scaling to automatically add or remove frontend EC2 instances based on traffic or demand. Auto Scaling helps ensure that the application can handle varying levels of traffic, and can help maintain a consistent user experience. This option is recommended if the frontend instances are the bottleneck in the application. Use Auto Scaling for the backend EC2 instances: This option involves using AWS Auto Scaling to automatically add or remove backend EC2 instances based on traffic or demand. Auto Scaling helps ensure that the application can handle varying levels of traffic and can help maintain consistent performance. This option is recommended if the backend instances are the bottleneck in the application. Replace the Classic ELB with Application ELB: The Classic Load Balancer (ELB) operates at Layer 4 and can balance traffic based on IP addresses and ports. On the other hand, the Application Load Balancer (ALB) operates at Layer 7 and can balance traffic based on content, URL, and cookies. ALB can improve the scalability and performance of an application by distributing traffic to healthy instances and providing advanced routing features such as content-based routing. This option is recommended if the application requires advanced routing features or if the Classic ELB is the bottleneck in the application. Use Network ELB for both the frontend and backend instances: The Network Load Balancer (NLB) operates at Layer 4 and can balance traffic based on IP addresses and ports. It is designed to handle high levels of traffic and can scale to millions of requests per second. This option is recommended if the application requires extreme performance and scalability, and if the frontend and backend instances require the same level of balancing.\nTherefore, the correct answers to the question are:\nA. Use Auto Scaling for the frontend EC2 instances. B. Use Auto Scaling for the backend EC2 instances.\n\n"
}, {
  "id" : 99,
  "question" : "There is a multi-region website hosted in AWS EC2 under an ELB.\nRoute 53 is used to manage its DNS record.\nThe website might get a lot of traffic over the next couple of weeks.\nIf the application experiences a natural disaster in the region during the time, what should be used to reduce potential disruption to users?\n",
  "answers" : [ {
    "id" : "8b1cd3e667244b369cc3282f53700a71",
    "option" : "Use an ELB to divert traffic to an Infrastructure hosted in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "d4341c807e834624bc8728e2d32ad021",
    "option" : "Use an ELB to divert traffic to an Infrastructure hosted in another AZ.",
    "isCorrect" : "false"
  }, {
    "id" : "9af3e477b4964bbebf0088a42ba5ac23",
    "option" : "Use CloudFormation to create backup resources in another AZ.",
    "isCorrect" : "false"
  }, {
    "id" : "81295500052a49508985a51da14d2dfc",
    "option" : "Use Route53 to route requests to another instance in a different region.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nIn a disaster recovery scenario, the best choice out of all given options is to divert the traffic to a static website.\nOption A is wrong because ELB can only balance traffic in one region, not across multiple regions.\nOptions B and C are incorrect because using backups across AZs is not enough for disaster recovery purposes.\nFor more information on disaster recovery in AWS, please visit the following URLs:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/fail-over-s3-r53/\nhttps://aws.amazon.com/disaster-recovery/\nThe wording \"to reduce the potential disruption in case of issues\" points to a disaster recovery situation.\nThere is more than one way to manage this situation.\nHowever, we need to choose the best option from the list given here.\nOut of this, the most suitable one is Option.\nD.Most organizations try to implement High Availability (HA) instead of DR to guard them against any downtime of services.\nIn the case of HA, we ensure that there exists a fallback mechanism for our services.\nThe service that runs in HA is handled by hosts running in different availability zones but the same geographical region.\nHowever, this approach does not guarantee that our business will be up and running in case the entire region goes down.\nDR takes things to a completely new level, wherein you need to recover from a different region that is separated by over 250 miles.\nOur DR implementation is an Active/Passive model, meaning that we always have minimum critical services running in different regions.\nBut a major part of the infrastructure is launched and restored when required.\nFor more information on large scale disaster recovery using AWS regions, please visit the following URL:\nhttps://aws.amazon.com/blogs/startups/large-scale-disaster-recovery-using-aws-regions/\nNote:\nUsually, when we discuss a disaster recovery scenario, we assume that the entire region is affected due to some disaster.\nSo we need the service to be provided from yet another region.\nSo, setting up a solution in another AZ will not work as it is in the same region.\nOption A is incorrect though it mentions yet another region because ELBs cannot span across regions.\nSo out of the options provided, Option D is the most suitable solution.\n\nIn order to reduce potential disruption to users in case of a natural disaster affecting the region hosting the multi-region website, one can use the following solutions:\nA. Use an ELB to divert traffic to an infrastructure hosted in another region This solution involves setting up a secondary infrastructure in another region to host the website, and using an Elastic Load Balancer (ELB) to distribute traffic between the two regions. This way, if the primary infrastructure goes down due to a natural disaster, traffic can be automatically redirected to the secondary infrastructure in another region, minimizing downtime and reducing disruption to users.\nB. Use an ELB to divert traffic to an infrastructure hosted in another AZ This solution involves setting up a secondary infrastructure in another availability zone (AZ) within the same region to host the website, and using an Elastic Load Balancer (ELB) to distribute traffic between the two AZs. This way, if the primary infrastructure goes down due to a natural disaster in that AZ, traffic can be automatically redirected to the secondary infrastructure in another AZ within the same region, minimizing downtime and reducing disruption to users.\nC. Use CloudFormation to create backup resources in another AZ This solution involves using AWS CloudFormation to automatically create and manage a secondary infrastructure in another AZ within the same region to host the website. This way, if the primary infrastructure goes down due to a natural disaster in that AZ, CloudFormation can automatically create a new infrastructure in another AZ, minimizing downtime and reducing disruption to users.\nD. Use Route53 to route requests to another instance in a different region This solution involves using AWS Route53 to redirect traffic from the primary infrastructure in the affected region to a secondary infrastructure hosted in another region. This way, if the primary infrastructure goes down due to a natural disaster, Route53 can automatically redirect traffic to the secondary infrastructure in another region, minimizing downtime and reducing disruption to users.\nOverall, solutions A, B, and D are all viable options to reduce potential disruption to users in case of a natural disaster affecting the region hosting the multi-region website. Solution C may also work, but it is more focused on automating the creation of backup resources rather than directing traffic in the event of a disaster.\n\n"
}, {
  "id" : 100,
  "question" : "You have a requirement to host a static website for a domain named mycompany.com in AWS.\nWhich of the listed steps will you follow in order to set this up? (SELECT TWO.)\n",
  "answers" : [ {
    "id" : "73a50ec497f24a77bb210cea70429834",
    "option" : "Host the static site on an EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "bc9a73f7af594d74bde3ab50b4d36b3b",
    "option" : "Use Route53 with static web site in S3.",
    "isCorrect" : "true"
  }, {
    "id" : "a8e9bba794b549bc918bcbf8d8c3872b",
    "option" : "Use Route53 as the domain registrar to register the domain name.",
    "isCorrect" : "true"
  }, {
    "id" : "34d218b0f5744e39a89f8369631bd1f2",
    "option" : "Place the EC2 instance behind the EL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nYou can register the domain name in your domain registrar (Route53) and then configure a record set in Route53 to host the static website in S3.\nFor more information on website hosting in S3, please visit the following URLs:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/RoutingToS3Bucket.html\n\nTo host a static website for a domain named mycompany.com in AWS, you can follow the following steps:\nUse Route53 as the domain registrar to register the domain name: First, you need to register the domain name \"mycompany.com\" with a domain registrar such as Amazon Route53. Route53 is a highly available and scalable Domain Name System (DNS) web service that can be used to register and manage domain names. Once you have registered the domain name, you will be able to manage it in Route53. Use Route53 with static web site in S3: Next, you can create a static website in Amazon S3. S3 is an object storage service that allows you to store and retrieve files. You can create a bucket in S3 and configure it as a static website. When you create a static website, you need to specify the index document and the error document for your website.\nAfter you have created the static website in S3, you can configure Route53 to route traffic to the S3 bucket. You can create a new hosted zone in Route53 and add a record set for the domain name \"mycompany.com\". In the record set, you can specify an alias target for the S3 bucket hosting the static website. This will route traffic from the domain name \"mycompany.com\" to the S3 bucket hosting the static website.\nTherefore, the correct steps to set up a static website for a domain named mycompany.com in AWS are:\nUse Route53 as the domain registrar to register the domain name. Use Route53 with static web site in S3.\nThe other options listed are not recommended because:\nHosting the static site on an EC2 Instance (Option A) is not an ideal solution for hosting a static website, as it requires more maintenance and scalability efforts than using S3. Placing the EC2 instance behind the EL (Option D) also adds complexity and unnecessary cost for hosting a simple static website.\n\n"
}, {
  "id" : 101,
  "question" : "A database, hosted using the Amazon RDS service, is getting many database queries.\nIt has now become a bottleneck for the associating application.\nWhich action would ensure that the database is not a performance bottleneck?\n",
  "answers" : [ {
    "id" : "5dff85c40d2845e4b9f4c55f05ce02bb",
    "option" : "Setup a CloudFront distribution in front of the database.",
    "isCorrect" : "false"
  }, {
    "id" : "8739bb5a2596408e83c1d7897e911c21",
    "option" : "Setup an ELB in front of the database.",
    "isCorrect" : "false"
  }, {
    "id" : "37392a214fa14bd59514694c13327d09",
    "option" : "Setup ElastiCache in front of the database.",
    "isCorrect" : "true"
  }, {
    "id" : "4f79299d8c6241a9bc4b2cc0fc8d914c",
    "option" : "Setup SNS in front of the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nElastiCache is an in-memory solution that can be used in front of a database to cache the common queries issued against the database.\nThis can reduce the overall load on the database.\nOption A is incorrect because this is normally used for content distribution.\nOption B is partially correct.\nBut you need to have one more database as an internal load balancing solution.\nOption D is incorrect because SNS is a simple notification service.\nFor more information on ElastiCache, please visit the following URL:\nhttps://aws.amazon.com/elasticache/\n\nTo ensure that the database is not a performance bottleneck, the best option would be to set up an ElastiCache in front of the database.\nExplanation:\nWhen a database hosted in Amazon RDS is getting many queries, it could become a bottleneck for the associated application. This is because the database may not be able to handle the load of queries that it is receiving, leading to a slowdown in performance. To prevent this from happening, it is essential to optimize the database performance by using caching.\nCaching is a technique that involves storing frequently accessed data in memory, allowing for faster access to the data when it is requested. ElastiCache is a fully managed in-memory caching service provided by AWS that supports popular open-source in-memory engines such as Memcached and Redis. By using ElastiCache, the database queries can be offloaded to the caching layer, thereby reducing the load on the database and improving performance.\nSetting up CloudFront distribution in front of the database (Option A) is not a good choice for improving database performance. CloudFront is a content delivery network (CDN) service that caches static and dynamic web content at edge locations worldwide. However, it is not designed for caching database queries, and using it for this purpose could lead to security concerns.\nSetting up an Elastic Load Balancer (ELB) in front of the database (Option B) can help distribute incoming traffic across multiple database instances, leading to improved performance and availability. However, ELB is not designed for caching, and it does not provide the same level of performance improvement that caching with ElastiCache can offer.\nSetting up SNS in front of the database (Option D) is not an appropriate solution for improving database performance. SNS is a messaging service that allows applications to send and receive messages. It is not designed for caching and does not offer any performance improvement for databases.\nIn conclusion, the best option for ensuring that the database is not a performance bottleneck is to set up ElastiCache in front of the database. This will help offload the database queries to the caching layer, thereby reducing the load on the database and improving performance.\n\n"
}, {
  "id" : 102,
  "question" : "A database is being hosted using the Amazon RDS service.\nThis database will be deployed in production and needs to be highly available.\nWhich of the following could be used to achieve this requirement?\n",
  "answers" : [ {
    "id" : "7759f9a77ff44c9cbced9781c1b0d98a",
    "option" : "Use Multi-AZ for the RDS instance to ensure that a secondary database is created in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "a2f7677149c6434e831e4b9ea6faa373",
    "option" : "Use the Read Replica feature to create another instance of the DB in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "1f2f3396d4fe4b37a81edc15fa764ec6",
    "option" : "Use Multi-AZ for the RDS instance to ensure that a secondary database is created in another Availability Zone.",
    "isCorrect" : "true"
  }, {
    "id" : "70a65c02656049c495b0db9f7d1a8db1",
    "option" : "Use the Read Replica feature to create another instance of the DB in another Availability Zone.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect because the Multi-AZ feature allows high availability across Availability Zones, not regions.\nOptions B and D are incorrect because Read Replicas can be used to offload database reads.\nBut if you want high availability, the Multi-AZ feature is required.\nAWS Documentation mentions the following:\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances.\nWhen you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ).\nFor more information on AWS RDS Multi-AZ, please visit the following URL:\nhttps://aws.amazon.com/rds/details/multi-az/\n\nThe correct answer to this question is option C: Use Multi-AZ for the RDS instance to ensure that a secondary database is created in another Availability Zone.\nExplanation:\nMulti-AZ (Availability Zone) is a feature provided by Amazon RDS that provides high availability and durability for databases. When Multi-AZ is enabled, Amazon RDS automatically creates a primary database instance in one Availability Zone (AZ) and synchronously replicates the data to a standby instance in another AZ within the same region. This ensures that if the primary database instance fails, the standby instance can be promoted to a primary instance and take over its operations without any manual intervention.\nUsing Multi-AZ for the RDS instance ensures that the database is highly available and can withstand failures of the primary instance, network or storage in the primary AZ. This also eliminates the need for any manual intervention or setup for replication.\nOption A: Use Multi-AZ for the RDS instance to ensure that a secondary database is created in another region is incorrect because Multi-AZ creates a standby instance in another AZ within the same region, not another region. Additionally, replicating data across regions may cause additional latency and may not be suitable for certain applications.\nOption B: Use the Read Replica feature to create another instance of the DB in another region is incorrect because Read Replicas are asynchronous copies of the primary database instance and are used for read-intensive workloads. Read Replicas do not provide the same level of availability as Multi-AZ and cannot be promoted to a primary instance in case of failure.\nOption D: Use the Read Replica feature to create another instance of the DB in another Availability Zone is incorrect because Read Replicas are not designed for high availability and cannot be promoted to a primary instance in case of failure.\nTherefore, the correct and optimal solution for ensuring high availability of a database deployed on Amazon RDS is to use Multi-AZ for the RDS instance to ensure that a secondary database is created in another Availability Zone.\n\n"
}, {
  "id" : 103,
  "question" : "A company wants to host a web application and a database layer in AWS.\nThis will be done with the use of subnets in a VPC.\nWhat would be a proper architectural design for supporting the required tiers of the application?\n",
  "answers" : [ {
    "id" : "2c877aaa63574a8cacbb41d656a2925a",
    "option" : "Use a public subnet for the web tier and another public subnet for the database layer.",
    "isCorrect" : "false"
  }, {
    "id" : "9fd2fa05ad2e4d1a9abac53991331c14",
    "option" : "Use a public subnet for the web tier and a private subnet for the database layer.",
    "isCorrect" : "true"
  }, {
    "id" : "96e34daaa9f54f80b94dca1958d8d440",
    "option" : "Use a private subnet for the web tier and another private subnet for the database layer.",
    "isCorrect" : "false"
  }, {
    "id" : "035d5c92ee2c4db4b7f91cace0966a84",
    "option" : "Use a private subnet for the web tier and a public subnet for the database layer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe ideal setup is to ensure that the webserver is hosted in the public subnet so that users on the internet can access it.\nThe database server can be hosted in the private subnet.\nThe below diagram shows how this can be set up:\nFor more information on public and private subnets in AWS, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\n\n\nWhen hosting a web application and database layer in AWS, it is recommended to use a Virtual Private Cloud (VPC) for better security, control, and scalability. The VPC is a virtual network that allows you to create isolated sections (subnets) within it to deploy your application.\nTo properly support the required tiers of the application, it is recommended to separate the web and database layers into different subnets. This provides added security by allowing you to control the traffic flow between the different layers of the application.\nThere are two types of subnets in AWS: public and private. A public subnet has a route to the internet gateway, while a private subnet does not. In general, it is best practice to place resources that need to be accessed from the internet (such as a web server) in a public subnet and resources that do not need to be accessed from the internet (such as a database server) in a private subnet.\nBased on this, the proper architectural design for supporting the required tiers of the application would be to use a public subnet for the web tier and a private subnet for the database layer. This design provides a secure architecture where the web server can interact with the internet, and the database server can only be accessed from within the VPC, thus providing added security to the application.\nTherefore, the correct answer to the question is B. Use a public subnet for the web tier and a private subnet for the database layer.\n\n"
}, {
  "id" : 104,
  "question" : "You need to launch several EC2 instances to run Cassandra.\nThere are large distributed and replicated workloads in Cassandra and you plan to launch instances using EC2 placement groups.\nThe traffic should be distributed evenly across several partitions and each partition should contain multiple instances.\nWhich strategy would you use when launching the placement groups?\n",
  "answers" : [ {
    "id" : "749614add023405888849da71fa3ea86",
    "option" : "Cluster placement strategy.",
    "isCorrect" : "false"
  }, {
    "id" : "eca347eb49754fd59915fc11af180941",
    "option" : "Spread placement strategy.",
    "isCorrect" : "false"
  }, {
    "id" : "7cc2e10228014c2994632539b6d220d0",
    "option" : "Partition placement strategy.",
    "isCorrect" : "true"
  }, {
    "id" : "f1cf837d78a34abeb9912d2118741b9e",
    "option" : "Network placement strategy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nPlacement groups have the placement strategies of Cluster, Partition and Spread.\nWith the Partition placement strategy, instances in one partition do not share the underlying hardware with other partitions.\nThis strategy is suitable for distributed and replicated workloads such as Cassandra.\nFor details, please refer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-limitations-partition.\nOption A is incorrect.\nCluster placement strategy puts instances together in an availability zone.\nThis does not resolve the problem mentioned in the question.\nOption B is incorrect.\nBecause Spread placement strategy puts instances across different racks.\nIt does not group instances in a partition.\nOption C is CORRECT: With the Partition placement strategy, instances in a partition have their own set of racks.\nOption D is incorrect: Because there is no Network placement strategy.\n\nTo achieve an even distribution of traffic and multiple instances per partition when launching EC2 instances to run Cassandra, the most appropriate strategy would be the Partition placement strategy.\nThe Partition placement strategy is designed to distribute instances across logical partitions, where each partition consists of one or more instances that share a common fate. Each partition can span multiple Availability Zones and can be launched in a placement group to ensure that the instances are placed on the same underlying hardware.\nIn Cassandra, partitions are the unit of distribution and replication, and data is distributed across multiple partitions. By using the Partition placement strategy, you can ensure that each partition in Cassandra contains multiple instances that can handle the distributed and replicated workload, which helps to ensure high availability and fault tolerance.\nThe Cluster placement strategy is designed to place instances in a group within a single Availability Zone. The Spread placement strategy is designed to place instances on distinct underlying hardware to reduce the risk of simultaneous failures. The Network placement strategy is designed to place instances in a group that share a network performance characteristic.\nHowever, for the given scenario of launching EC2 instances to run Cassandra, the most appropriate strategy would be the Partition placement strategy to ensure an even distribution of traffic and multiple instances per partition.\n\n"
}, {
  "id" : 105,
  "question" : "A company has an infrastructure that consists of machines that send log information every 5 minutes.\nThe number of these machines can run into thousands.\nIt is required to ensure that the analysis of every log item is completed within 24 hours.\nWhat could help to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "b076e3c2aa894be08077dd0c9514e15c",
    "option" : "Use Kinesis Data Streams to collect the logs and persist the data to S3 using Kinesis Firehose and Lambda.",
    "isCorrect" : "true"
  }, {
    "id" : "fdc024fc0f9a415da10d66818cd6ea56",
    "option" : "Launch an Elastic Beanstalk application to take the processing job of the logs.",
    "isCorrect" : "false"
  }, {
    "id" : "3ec65f2d37b04b139c00bd5ac5004505",
    "option" : "Launch an EC2 instance with enough EBS volumes to consume the logs which can be used for further processing.",
    "isCorrect" : "false"
  }, {
    "id" : "52b8ce8c76774b9fb0c2404ceb889bba",
    "option" : "Use CloudTrail to store all the logs which can be analyzed at a later stage.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following:\nAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.\nKDS can continuously capture gigabytes of data per second from thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\nMake your streaming data available to multiple real-time analytics applications, to Amazon S3 or AWS Lambda within 70 milliseconds of the data being collected.\nFor more information on Amazon Kinesis firehose, please visit the following URL:\nhttps://aws.amazon.com/kinesis/data-streams/\n\nThe requirement is to analyze log information that is sent every 5 minutes by thousands of machines within 24 hours. To fulfill this requirement, we need a solution that can handle high volume, high velocity, and high variety data. Among the options provided, only option A - Use Kinesis Data Streams to collect the logs and persist the data to S3 using Kinesis Firehose and Lambda - can fulfill these requirements.\nKinesis Data Streams is a managed service that can collect and process large volumes of streaming data in real-time. It can handle data from thousands of sources and can scale dynamically to accommodate changes in the volume of data. Kinesis Data Streams can also process the data in real-time, which is important for meeting the requirement of analyzing the logs within 24 hours.\nKinesis Firehose is a managed service that can persist the data to S3, Redshift, or Elasticsearch. It can also transform the data before persisting it using Lambda functions. In this case, we can use Firehose to persist the logs to S3, which is a cost-effective and scalable storage solution.\nLambda is a serverless compute service that can execute code in response to events. In this case, we can use Lambda to transform the data before persisting it to S3 using Firehose. Lambda is also a cost-effective and scalable solution since we only pay for the execution time and don't have to manage any infrastructure.\nOption B - Launch an Elastic Beanstalk application to take the processing job of the logs - may not be suitable for this scenario since it's unclear how the logs would be processed using Elastic Beanstalk. Elastic Beanstalk is a platform-as-a-service (PaaS) solution that can deploy and manage web applications, but it may not be the best fit for processing large volumes of streaming data.\nOption C - Launch an EC2 instance with enough EBS volumes to consume the logs which can be used for further processing - may not be suitable for this scenario since it requires manual scaling and management of the infrastructure. Additionally, it may not be cost-effective since we would have to pay for the EC2 instances and EBS volumes even when they're not being used.\nOption D - Use CloudTrail to store all the logs which can be analyzed at a later stage - is not suitable for this scenario since CloudTrail is a service that logs AWS API calls and events, not application logs. Additionally, CloudTrail may not be able to handle the volume and variety of data in this scenario.\n\n"
}, {
  "id" : 106,
  "question" : "An application hosted in AWS allows users to upload videos to an S3 bucket.\nA user is required to be given access to upload some videos for a week based on the profile.\nHow could this be accomplished in the best way possible?\n",
  "answers" : [ {
    "id" : "a568e07e6d5e4c6cb9c25617e492474a",
    "option" : "Create an IAM bucket policy to provide access for one week.",
    "isCorrect" : "false"
  }, {
    "id" : "e6e74053f45c4590bfdb4478fa6fab0d",
    "option" : "Create a pre-signed URL for each profile which will last for one week.",
    "isCorrect" : "true"
  }, {
    "id" : "496bb1011f3a47008956250cce80fa2e",
    "option" : "Create an S3 bucket policy to provide access for one week.",
    "isCorrect" : "false"
  }, {
    "id" : "d01d8be6b47f4234abec56867cb27dd4",
    "option" : "Create an IAM role to provide access for one week.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nPre-signed URLs are the perfect solution when you want to give temporary access to users for S3 buckets.\nSo, whenever a new profile is created, you can create a pre-signed URL to ensure that the URL lasts for a week and allows users to upload the required objects.\nFor more information on pre-signed URLs, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\nFor Choice D, the maximum time you can set for IAM Role Session is 12 hours.\nSo this doesn't satisfy the 1-week requirement.\nRef: https://docs.aws.amazon.com/singlesignon/latest/userguide/howtosessionduration.html.\n\nThe best way to accomplish providing temporary access to upload videos to an S3 bucket in AWS is by creating a pre-signed URL for each user profile that will last for one week.\nA pre-signed URL is a unique URL that provides temporary access to an S3 object. The pre-signed URL can be used to upload, download, or delete an object without requiring AWS credentials. When a user requests the pre-signed URL, the URL is generated by AWS and includes a signature that verifies the user's credentials and authorizes the user to access the specified S3 object.\nCreating a pre-signed URL for each user profile ensures that each user is given a unique URL that expires after one week. This method provides granular control over access to S3 objects and limits the scope of access to only the necessary resources. Additionally, the use of pre-signed URLs does not require any additional AWS resources, such as IAM roles, policies, or bucket policies.\nCreating an IAM bucket policy, S3 bucket policy, or IAM role to provide access for one week is not the best approach. IAM bucket policies and S3 bucket policies are used to grant or deny access to a bucket or object for specific AWS accounts, IAM users, or roles. They are not suitable for providing temporary access to a specific user for a limited time. IAM roles are intended to grant access to AWS services and resources from within an instance or application running on an EC2 instance or other trusted environment.\nTherefore, the correct answer is B. Create a pre-signed URL for each profile which will last for one week.\n\n"
}, {
  "id" : 107,
  "question" : "You are creating several EC2 instances for a new application.\nThe instances need to communicate with each other.\nFor a better performance of the application, both low network latency and high network throughput are required for the EC2 instances.\nAll instances should be launched in a single availability zone.\nHow would you configure this?\n",
  "answers" : [ {
    "id" : "3a5f350e89c84ce596e2814cea54c404",
    "option" : "Launch all EC2 instances in a placement group using a Cluster placement strategy.",
    "isCorrect" : "true"
  }, {
    "id" : "63f25017e9f944988393f19ea87332b3",
    "option" : "Auto assign a public IP when launching the EC2 instances.",
    "isCorrect" : "false"
  }, {
    "id" : "beae98c3bca04ab086754a5249fe43ef",
    "option" : "Launch EC2 instances in an EC2 placement group and select the Spread placement strategy.",
    "isCorrect" : "false"
  }, {
    "id" : "af608a356863493ea9b69d05906dc472",
    "option" : "When launching the EC2 instances, select an instance type that supports enhanced networking.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThe Cluster placement strategy helps to achieve a low-latency and high throughput network.\nThe reference is in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-limitations-partition.\nOption A is CORRECT: The Cluster placement strategy can improve the network performance among EC2 instances.\nThe strategy can be selected when creating a placement group.\nOption B is incorrect: Because the public IP cannot improve the network performance.\nOption C is incorrect: The Spread placement strategy is recommended when several critical instances should be kept separate from each other.\nThis strategy should not be used in this scenario.\nOption D is incorrect: The description in the option is inaccurate.\nThe correct method is creating a placement group with a suitable placement strategy.\n\n\nTo configure EC2 instances for better performance with low network latency and high network throughput while communicating with each other, there are several factors that need to be considered such as placement group, instance type, and placement strategy.\nA placement group is a logical grouping of instances within a single Availability Zone. By launching instances in a placement group, network traffic can be better optimized and can provide better performance for applications that need low network latency and high network throughput. A placement group can be either a Cluster placement group or a Spread placement group.\nCluster placement group:\nA Cluster placement group is a grouping of instances within a single Availability Zone that are closely placed together and are in the same network segment. When instances are launched in a Cluster placement group, they are placed in a low-latency, high-bandwidth network topology that is optimized for high-performance applications. A Cluster placement group is recommended for applications that need low network latency, high network throughput, and that require a large number of instances to work together.\nSpread placement group:\nA Spread placement group is a grouping of instances within a single Availability Zone that are each placed on distinct underlying hardware. When instances are launched in a Spread placement group, they are isolated from each other and are each placed on different hardware to ensure high availability. A Spread placement group is recommended for applications that require a high level of availability but do not need the low network latency and high network throughput provided by a Cluster placement group.\nInstance types:\nChoosing an instance type that supports enhanced networking is also important for better performance of EC2 instances. Enhanced networking uses advanced network capabilities of the underlying instance and enables higher packet per second (PPS) performance, lower network jitter, and lower latencies. Instance types that support enhanced networking include C5, C5d, M5, M5d, R5, R5d, and T3.\nPublic IP:\nAuto assigning a public IP when launching EC2 instances is not directly related to the performance of the instances communicating with each other within the same network segment. Auto-assigning a public IP is useful when instances need to communicate with the internet, but it is not necessary for instances communicating within the same network segment.\nAnswer:\nGiven the requirement of launching EC2 instances in a single availability zone with low network latency and high network throughput, the best configuration option would be to launch all EC2 instances in a placement group using a Cluster placement strategy. Additionally, selecting an instance type that supports enhanced networking can further enhance the performance of the EC2 instances. Auto-assigning a public IP is not necessary in this scenario since the instances are communicating with each other within the same network segment.\n\n"
}, {
  "id" : 108,
  "question" : "To improve the network performance, you launch a C5 EC2 Amazon Linux instance and enable Enhanced Networking by modifying the instance attribute with â€œaws ec2 modify-instance-attribute --instance-id instance_id --ena-supportâ€\nWhich mechanism does the EC2 instance use to enhance the networking capabilities?\n",
  "answers" : [ {
    "id" : "ac30709d83e44a7c8646af2339e53272",
    "option" : "Intel 82599 Virtual Function (VF) interface.",
    "isCorrect" : "false"
  }, {
    "id" : "c59dcea06597461aa0798fb831b63764",
    "option" : "Transit Virtual Interface",
    "isCorrect" : "false"
  }, {
    "id" : "a357c0bc6fb74b139e3e8def28c19197",
    "option" : "Elastic Network Adapter (ENA).",
    "isCorrect" : "true"
  }, {
    "id" : "d9aa65240bb442c6a266ed33fc476302",
    "option" : "Elastic Network Interface (ENI).",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nEnhanced networking has two mechanisms: Elastic Network Adapter (ENA) and Intel 82599 Virtual Function (VF) interface.\nFor ENA, users can enable it with --ena-support.\nAn EFA is an Elastic Network Adapter (ENA) with added capabilities.\nIt provides all of the functionality of an ENA, with additional OS-bypass functionality.\nOS-bypass is an access model that allows HPC and machine learning applications to communicate directly with the network interface hardware to provide low-latency, reliable transport functionality.\"\nOption A is incorrect: Because the option of â€œ--ena-supportâ€ is not used by Intel 82599 Virtual Function (VF) interface.\nOption B is incorrect: Because a Transit Virtual Interface is used to connect resources hosted in an Amazon VPC (using their private IP addresses) through a transit gateway, not suitable for the above scenario.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-private-interface-dx/\nOption C is CORRECT: In Amazon Linux, users can enable the enhanced networking attribute with the AWS CLI command mentioned in the question.\nOption D is incorrect: In this scenario, the mechanism used for enhanced networking should be Elastic Network Adapter (ENA) instead of Elastic Network Interface (ENI).\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html.\n\nThe correct answer is C. Elastic Network Adapter (ENA).\nWhen you enable Enhanced Networking on an EC2 instance, it leverages the Elastic Network Adapter (ENA) to improve networking performance. The ENA is a custom network interface that provides higher throughput, lower latency, and lower CPU utilization compared to the traditional network interfaces available on EC2 instances.\nThe ENA uses a combination of hardware offloading and software optimizations to achieve its performance benefits. Specifically, it offloads certain network processing tasks from the CPU to specialized hardware, such as the NIC (Network Interface Card), to free up processing resources. Additionally, the ENA uses efficient networking protocols to reduce latency and improve throughput.\nNote that the other options listed in the question are also relevant to EC2 networking, but they are not the mechanisms used when Enhanced Networking is enabled.\nIntel 82599 Virtual Function (VF) interface (Option A): This is a network interface that is used on certain EC2 instance types (such as the C3 and M3 families) to provide high-performance networking capabilities. However, it is not used when Enhanced Networking is enabled on a C5 instance. Transit Virtual Interface (Option B): This is a type of VPN connection that can be used to connect a VPC (Virtual Private Cloud) to an on-premises network. It is not directly related to the networking capabilities of an EC2 instance. Elastic Network Interface (ENI) (Option D): This is a virtual network interface that can be attached to an EC2 instance to provide additional networking capabilities, such as multiple network interfaces or IP addresses. However, it is not the mechanism used when Enhanced Networking is enabled on a C5 instance.\n\n"
}, {
  "id" : 109,
  "question" : "A company hosts 5 web servers in AWS.\nThey want to ensure that multiple values for a DNS query should be returned and traffic routed to multiple IP addresses.\nIn addition, you want to associate your routing records with a Route 53 health check.\nWhich routing policy should be used to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "45b9b5620f624cc5a35b7e6e1149d541",
    "option" : "Simple",
    "isCorrect" : "false"
  }, {
    "id" : "5d3da7a82c5749d285567a80b4fc318d",
    "option" : "Weighted",
    "isCorrect" : "false"
  }, {
    "id" : "d37a16b92f8647209c9f89f0790992a8",
    "option" : "Multivalue Answer",
    "isCorrect" : "true"
  }, {
    "id" : "e481d2cc7db446a5802fc677dda81d66",
    "option" : "Latency.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe AWS Documentation mentions the following to support this:\nIf you want to route traffic randomly to multiple resources such as web servers, you can create one multivalue answer record for each resource and, optionally, associate an Amazon Route 53 health check with each record.\nFor example, suppose you manage an HTTP web service with a dozen web servers where each has its own IP address.\nNo web server could handle all the traffic.\nBut if you create a dozen multivalue answer records, Amazon Route 53 responds to DNS queries with up to eight healthy records in response to each DNS query.\nAmazon Route53 gives different answers to different DNS resolvers.\nIf a web server becomes unavailable after a resolver cache a response, client software can try another IP address in the response.\nSimple routing policy - Use a single resource that performs a given function for your domain, such as a web server that serves the example.com website.\nLatency routing policy - Use when you have resources in multiple locations, and you want to route traffic to the resource that provides the best latency.\nWeighted routing policy - Use to route traffic to multiple resources in proportions that you specify.\nMultivalue answer routing policy - Use when you want Route53 to respond to DNS queries with up to eight healthy records selected randomly.\nFor more information on different routing policies, please visit the following URL:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/multivalue-versus-simple-policies\nhttps://aws.amazon.com/about-aws/whats-new/2017/06/amazon-route-53-announces-support-for-multivalue-answers-in-response-to-dns-queries/\n\nThe routing policy that best fulfills the requirement of returning multiple values for a DNS query and associating the routing records with a Route 53 health check is the \"Multivalue Answer\" routing policy.\nExplanation:\nSimple routing policy: This policy is used to route traffic to a single resource that is specified in the record. It doesn't support multiple IP addresses or health checks. Weighted routing policy: This policy is used to distribute traffic across multiple resources based on a weighted distribution. It supports multiple IP addresses, but it doesn't support health checks. Multivalue Answer routing policy: This policy is used to return multiple values for a DNS query, and Route 53 responds with up to eight healthy records selected at random from the set of eligible resources. It also supports associating routing records with a Route 53 health check. Latency routing policy: This policy is used to route traffic to the resource that has the lowest network latency to the end user. It doesn't support multiple IP addresses or health checks.\nTherefore, the correct answer is C, Multivalue Answer routing policy, as it meets the requirement of returning multiple IP addresses and also allows the association of routing records with a Route 53 health check.\n\n"
}, {
  "id" : 110,
  "question" : "You are working as AWS Solutions Architect for a large banking organization.\nThe requirement is that under normal business hours, there would always be at least 24 web servers up and running in a region (example: US - West (Oregon))\nIt will be a three-tier architecture connecting to the databases.\nThe solution offered should be highly available, secure, and cost-effective.\nIt should respond to the heavy requests during peak hours and fault-tolerate up to one AZ failure. What would be the best solution to meet this requirement?\n",
  "answers" : [ {
    "id" : "7fd5dcae4fdf46fead932ce2c35d2087",
    "option" : "In a given region, use ELB behind two different AZs, each AZ with minimum or desired 24 web servers hosted in a public subnet and Multi-AZ database architecture in a private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "b0b03adfa15e4323b13919e4d886986b",
    "option" : "In a given region, use ELB behind three different AZs, each AZ having ASG, with minimum or desired 12 web servers hosted in a public subnet and Multi-AZ database architecture in a private subnet.",
    "isCorrect" : "true"
  }, {
    "id" : "89da50f7e3dc4f9e84e7550d2b4de94e",
    "option" : "In a given region, use ELB behind two different AZs, each AZ having ASG, with minimum or desired 12 web servers hosted in a public subnet and Multi-AZ database architecture in a private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "d88ca324f9574d1aa3ccf1ddbc0fbd29",
    "option" : "In a given region, use ELB behind three different AZs, each AZ having ASG, with minimum or desired 8 web servers hosted in public subnet and Multi-AZ database architecture in a different public subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect.\nEverything looks good, but the designed architecture does not look cost-effective, as all the time 48 servers will be running.\nIt does not have ASG to cater to additional load on servers.\nHowever, it is fault-tolerant to one AZ failure.\nBesides, it is always a good practice to use multiple AZs to make the application highly available.\nOption B is correct.\nThe solution needs to be tolerant up to one AZ failure.\nIt means there are always 36 web servers to cater to the service requests.\nIf one AZ fails, then there will be 24 servers running all the time.\nIn case two AZs fails, there will be 12 servers running.\nAlso, ASG can be utilized to scale out the required number of servers.\nOption C is incorrect.\nIt will not be a suitable solution.\nIf there is one AZ failure, the other AZ will have only 12 web servers running.\nThe question requires at least 24 web servers are running at all times.\nOption D is incorrect.\nRemember the design principle of keeping the databases in the private subnet.\nAs this solution mentions placing databases in another public subnet, the data can be exposed over the internet, and hence it's an insecure application.\n\nTo meet the requirement of having at least 24 web servers up and running in a region during normal business hours while ensuring high availability, security, and cost-effectiveness, the best solution would be to use ELB behind multiple Availability Zones (AZs), each with its own Auto Scaling Group (ASG) for the web servers and a Multi-AZ database architecture in a private subnet. This would allow for fault tolerance up to one AZ failure, as well as the ability to handle heavy requests during peak hours.\nOption A suggests using ELB behind two different AZs, each with a minimum or desired 24 web servers hosted in a public subnet and Multi-AZ database architecture in a private subnet. While this would meet the requirement for 24 web servers, it does not provide fault tolerance in the event of an AZ failure.\nOption B suggests using ELB behind three different AZs, each with its own ASG and a minimum or desired 12 web servers hosted in a public subnet and Multi-AZ database architecture in a private subnet. This would provide fault tolerance up to one AZ failure and the ability to handle heavy requests during peak hours, but would require a higher number of web servers to be deployed and maintained.\nOption C suggests using ELB behind two different AZs, each with its own ASG and a minimum or desired 12 web servers hosted in a public subnet and Multi-AZ database architecture in a private subnet. This option is similar to option B but with fewer web servers, making it a more cost-effective solution while still providing fault tolerance and the ability to handle heavy requests.\nOption D suggests using ELB behind three different AZs, each with its own ASG and a minimum or desired 8 web servers hosted in a public subnet and Multi-AZ database architecture in a different public subnet. This option would provide fault tolerance up to one AZ failure and the ability to handle heavy requests, but hosting the database in a public subnet may pose security risks.\nTherefore, option C is the best solution as it meets all the requirements while being cost-effective, secure, and highly available with fault tolerance up to one AZ failure.\n\n"
}, {
  "id" : 111,
  "question" : "You need to deploy a high performance computing (HPC) and machine learning application in AWS Linux EC2 instances.\nThe performance of inter-instance communication is very critical for the application.\nYou want to attach a network device to the instance so that the computing performance can be greatly improved.\nWhich of the following options can achieve the best performance?\n",
  "answers" : [ {
    "id" : "bc93e0de1149477d8576c0429464daa0",
    "option" : "Enable enhanced networking feature in the EC2 instance.",
    "isCorrect" : "false"
  }, {
    "id" : "83d550cbfa814229b91197490900aff8",
    "option" : "Configure Elastic Fabric Adapter (EFA) in the instance.",
    "isCorrect" : "true"
  }, {
    "id" : "31c61e31261949c58f21ffd0f801755a",
    "option" : "Attach high speed Elastic Network Interface (ENI) in the instance.",
    "isCorrect" : "false"
  }, {
    "id" : "b544b419364e409198448b08d9c85d31",
    "option" : "Create Elastic File System (EFS) and mount the file system in the instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nWith Elastic Fabric Adapter (EFA), users can perform better than enhanced networking (Elastic Network Adapter) or Elastic Network Interface.\nCheck the differences between EFAs and ENAs in https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html.\nOption A is incorrect: Because with Elastic Fabric Adapter (EFA), users can achieve a better network performance than enhanced networking.\nOption B is CORRECT: Because EFA is the most suitable method for accelerating High Performance Computing (HPC) and machine learning applications.\nOption C is incorrect: Because Elastic Network Interface (ENI) cannot improve the performance as required.\nOption D is incorrect: The Elastic File System (EFS) cannot accelerate the inter-instance communication.\n\nWhen deploying a high-performance computing (HPC) and machine learning application on AWS Linux EC2 instances, the performance of inter-instance communication is critical. One way to improve performance is by attaching a network device to the instance. In this context, the best option to achieve optimal performance is by configuring Elastic Fabric Adapter (EFA) in the instance.\nElastic Fabric Adapter (EFA) is a network interface designed to accelerate HPC and machine learning workloads on EC2 instances. It provides lower and more consistent network latency, higher throughput, and lower jitter compared to other network interfaces, making it an ideal choice for inter-instance communication.\nEnhanced networking feature in EC2 instances improves network performance by offloading the processing of network traffic from the instance's CPU to the network interface card (NIC). It provides better network throughput, lower latency, and lower jitter. However, it does not provide the same level of performance as EFA.\nHigh speed Elastic Network Interface (ENI) also provides better network throughput and lower latency than the default network interface. However, it is not optimized for HPC and machine learning workloads and does not provide the same level of performance as EFA.\nCreating an Elastic File System (EFS) and mounting the file system in the instance does not improve inter-instance communication performance. It provides a shared file system that can be accessed by multiple instances, but it is not designed to improve network performance.\nIn conclusion, to achieve the best performance for inter-instance communication in a high performance computing (HPC) and machine learning application on AWS Linux EC2 instances, configuring Elastic Fabric Adapter (EFA) in the instance is the best option.\n\n"
}, {
  "id" : 112,
  "question" : "You are in charge of migrating your actual On-Premise Postgres database to AWS Cloud.\nThere is a requirement that the database must remain fully operational during the migration.\nWhich of these services is well suited for this task?\n",
  "answers" : [ {
    "id" : "24cbb29c54e547b7a18f3a687efe2673",
    "option" : "AWS DataSync",
    "isCorrect" : "false"
  }, {
    "id" : "76b207993ed147189d19c9ed9df098d1",
    "option" : "AWS Server Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "d253a093ddc541b3a4278b701163cf98",
    "option" : "AWS Database Migration Service",
    "isCorrect" : "true"
  }, {
    "id" : "7d0e9efe0d1a42baae205e33ccd45bfb",
    "option" : "AWS Migration Hub.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect.\nDataSync is a service mainly used to migrate NFS servers to S3, EFS, Fsx, etc.\nIt does not support database migrations.\nMore details here:\nhttps://aws.amazon.com/datasync/faqs/\nOption B is incorrect.\nAWS Server Migration service is used to migrate on-premises workloads to AWS EC2\nIt does not support migrations to databases in AWS.\nMore details:\nhttps://aws.amazon.com/server-migration-service/faqs/\nOption C is CORRECT.\nAWS Database Migration Service helps you to migrate your On-Premise database to AWS, keeping the On-Premise database fully operational during the migration.\nMore details:\nhttps://aws.amazon.com/dms/faqs/\nOption D is incorrect.\nAWS Migration Hub is used to track the progress of migrations in AWS.\nMore details here:\nhttps://aws.amazon.com/migration-hub/\n\nThe AWS service that is best suited for migrating an On-Premise Postgres database to the AWS Cloud while keeping it fully operational is AWS Database Migration Service (AWS DMS).\nAWS Database Migration Service is a fully managed service that enables you to migrate databases to AWS easily and securely. It supports a variety of databases, including PostgreSQL, and allows you to perform both homogenous and heterogeneous migrations. With AWS DMS, you can perform a database migration with minimal downtime.\nHere are some reasons why AWS DMS is well suited for migrating a database while keeping it fully operational:\nReplication: AWS DMS can replicate changes made to the source database to the target database in near real-time. This means that you can keep the source database operational during the migration and minimize downtime. Continuous Data Replication: AWS DMS continuously replicates changes made to the source database to the target database until the migration is complete. This ensures that the target database is up-to-date and that there is minimal data loss. Database consistency: AWS DMS ensures that the data is consistent across both the source and target databases. This is important when migrating a database while keeping it operational. Validation: AWS DMS can validate the data in the target database to ensure that it is accurate and complete.\nIn summary, AWS Database Migration Service is the most suitable service for migrating an On-Premise Postgres database to AWS Cloud while keeping it fully operational because it provides replication, continuous data replication, database consistency, and validation features.\n\n"
}, {
  "id" : 113,
  "question" : "A company is planning on testing a large set of IoT-enabled devices.\nThese devices will generate a large amount of data every second.\nYou need a scalable and durable real-time data streaming service to capture the data generated from these devices.\nWhich AWS service would be the most appropriate for this purpose?\n",
  "answers" : [ {
    "id" : "ad37af96cb474d3cbb43a04bcf3484ad",
    "option" : "AWS EMR.",
    "isCorrect" : "false"
  }, {
    "id" : "c81fe13c1f144f18b47cb4d1d83f879d",
    "option" : "AWS Kinesis Data Streams.",
    "isCorrect" : "true"
  }, {
    "id" : "061f8be3f6904f178f1b80998d929920",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  }, {
    "id" : "d80cbe9a34134aad8cde00fcaa24dc79",
    "option" : "AWS SNS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following on Amazon Kinesis:\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications.\nFor more information on Amazon Kinesis, please refer to the below URL:\nhttps://aws.amazon.com/kinesis/\nOption A is incorrect.\nAmazon EMR can be used to process applications with data-intensive workloads but is not a data streaming service.\nOption B is correct.\nAmazon Kinesis can be used to store, process, and analyze real-time streaming data.\nOption C is incorrect.\nSQS is a fully managed message queuing service.\nBut it cannot be configured as a real-time data streaming service.\nOption D is incorrect.\nSNS is a flexible, fully managed pub/sub messaging and mobile notifications service instead of a data streaming service.\n\nThe most appropriate AWS service for capturing real-time data generated from IoT-enabled devices is AWS Kinesis Data Streams, option B.\nAWS Kinesis Data Streams is a scalable and durable real-time data streaming service that can handle large amounts of data generated by IoT devices. It is designed to capture, store, and process streaming data in real-time. Kinesis Data Streams allows you to collect, process, and analyze data from hundreds of thousands of sources, such as IoT devices, and provides sub-second latency for real-time analytics.\nEMR (Elastic Map Reduce), option A, is a managed Hadoop framework that is used for processing large amounts of data. It is typically used for batch processing of data rather than real-time data streaming.\nSQS (Simple Queue Service), option C, is a fully-managed message queuing service that enables decoupling of components within a distributed application. SQS can be used to decouple the message producer from the message consumer, but it is not a real-time data streaming service.\nSNS (Simple Notification Service), option D, is a fully-managed notification service that can be used to send messages or notifications to multiple recipients, such as email or SMS. SNS is not a real-time data streaming service.\nIn summary, AWS Kinesis Data Streams is the most appropriate service for capturing and processing real-time data generated from IoT-enabled devices, due to its scalability, durability, and sub-second latency capabilities.\n\n"
}, {
  "id" : 114,
  "question" : "Your company currently has a set of non-production EC2 Instances hosted in AWS.\nTo save costs, you want to stop the EC2 instance when the average CPU utilization percentage has been lower than 10 percent for 24 hours, signaling that it is idle and no longer in use.\nWhich step could be helpful to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "c59ac59081ff4928b746a023a7c8718e",
    "option" : "Use CloudWatch Logs to store the state change of the instances.",
    "isCorrect" : "false"
  }, {
    "id" : "d2d362a38ebe4b24b5baf714d2bf47c8",
    "option" : "Create Amazon CloudWatch alarms that monitor the CPU utilization metric and stop the instances when the alarms are triggered.",
    "isCorrect" : "true"
  }, {
    "id" : "7a5c9009fbd945a9b2cd735b677f33f5",
    "option" : "Use SQS to monitor the metric and add the record to a DynamoDB table.",
    "isCorrect" : "false"
  }, {
    "id" : "ab8e40b3dda241069ceafbaa24cc56fe",
    "option" : "Use AWS Lambda to monitor the metric and store the state in a DynamoDB table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nCreate Alarms That Stop, Terminate, Reboot, or Recover an Instance\nUsing Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your instances.\nYou can use the stop or terminate actions to save money when you no longer need an instance.\nYou can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.\nThere are several scenarios in which you might want to stop or terminate your instance automatically.\nFor example, you might have instances dedicated to batch payroll processing jobs or scientific computing tasks that run for a period of time and then complete their work.\nRather than letting those instances sit idle (and accrue charges), you can stop or terminate them, helping you to save money.\nThe main difference between using the stop and the terminate alarm actions is that you can easily restart a stopped instance if you need to rerun it later, and you can keep the same instance ID and root volume.\nHowever, you cannot restart a terminated instance.\nInstead, you must launch a new instance.\nYou can add the stop, terminate, reboot, or recover actions to any alarm that is set on an Amazon EC2 per-instance metric, including basic and detailed monitoring metrics provided by Amazon CloudWatch (in the AWS/EC2 namespace), as well as any custom metrics that include the InstanceId dimension, as long as its value refers to a valid running Amazon EC2 instance.\nFor more information on Amazon EC2, please visit https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingStopActions.\nBreakdown.\nOption A is incorrect because CloudWatch Logs is not suitable to monitor the CloudWatch Metrics.\nOption B is correct.\nCloudWatch alarms are used to trigger notifications for any metric.\nAlarms can trigger auto-scaling, EC2 actions(stop, terminate, recover, or reboot) and SNS notifications.\nOption C is incorrect because SQS cannot modify the EC2 state when a CloudWatch metric breaches a threshold value.\nOption D is incorrect because AWS Lambda is not necessary in this scenario.\nCloudWatch Alarms can directly monitor the metric and trigger actions when needed.\nPlease refer to the below link for more information on Cloudwatch:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/CloudWatch-Events-Monitoring-CloudWatch-Metrics.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/acw-ug.pdf\n\nThe most appropriate option to fulfill the requirement of stopping the EC2 instance when the average CPU utilization percentage has been lower than 10 percent for 24 hours is to create Amazon CloudWatch alarms that monitor the CPU utilization metric and stop the instances when the alarms are triggered (option B).\nAmazon CloudWatch is a monitoring service that provides metrics, logs, and alarms for AWS resources and applications. It allows users to collect and track metrics, collect and monitor log files, and set alarms. Using CloudWatch alarms, users can automatically react to changes in metrics, such as CPU utilization, by triggering actions, such as stopping an EC2 instance.\nTo set up CloudWatch alarms for EC2 instances, the following steps can be taken:\nCreate a CloudWatch alarm: From the CloudWatch console, navigate to Alarms, and click \"Create alarm.\" Select the EC2 metric for CPU utilization, and set the threshold to trigger the alarm when the average CPU utilization percentage has been lower than 10 percent for 24 hours. Define actions: Select the \"Actions\" tab and choose \"Stop this instance\" as the action to take when the alarm is triggered. Add tags: Add any required tags to help organize and manage the alarm. Create the alarm: Click \"Create alarm\" to create the CloudWatch alarm.\nOnce the CloudWatch alarm is set up, it will monitor the CPU utilization metric of the EC2 instance and trigger an action to stop the instance when the average CPU utilization percentage has been lower than 10 percent for 24 hours.\nOption A, using CloudWatch Logs to store the state change of the instances, is not the best option for this use case, as it is primarily used for logging and storing application and system logs, and would not provide the necessary functionality for monitoring CPU utilization and stopping instances.\nOption C, using SQS to monitor the metric and add the record to a DynamoDB table, is not the best option for this use case, as it is primarily used for queueing and processing messages, and would not provide the necessary functionality for monitoring CPU utilization and stopping instances.\nOption D, using AWS Lambda to monitor the metric and store the state in a DynamoDB table, is also not the best option for this use case, as it would require setting up a custom monitoring and automation solution, which would be more complex and time-consuming than setting up CloudWatch alarms.\n\n"
}, {
  "id" : 115,
  "question" : "You have instances hosted in a private subnet in a VPC.\nThere is a need for instances to download updates from the Internet.\nAs an architect, what change would you suggest to the IT Operations team that would also be the most efficient and secure?\n",
  "answers" : [ {
    "id" : "2e6f3518a4154ab484291135f3a8d05d",
    "option" : "Create a new public subnet and move the instance to that subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "791599f09fd142e8bac6a68b14b7c149",
    "option" : "Create a new EC2 Instance to download the updates separately and then push them to the required instance.",
    "isCorrect" : "false"
  }, {
    "id" : "7928babaada14dc783e706e11725e121",
    "option" : "Use a NAT Gateway to allow the instances in the private subnet to download the updates.",
    "isCorrect" : "false"
  }, {
    "id" : "49cc76cdbc6a4e3e949ba7ddaff37d60",
    "option" : "Create a VPC link to the Internet to allow the instances in the private subnet to download the updates.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nAs an architect, the most efficient and secure solution for instances hosted in a private subnet in a VPC to download updates from the Internet would be to use a NAT Gateway.\nA NAT Gateway is a highly available AWS-managed service that allows instances in a private subnet to connect to the Internet, but does not allow the Internet to initiate connections back to the instances. With a NAT Gateway, the instances can access software updates and patches without being exposed to the public Internet.\nOption A, creating a new public subnet and moving the instance to that subnet, is not a secure solution because it exposes the instance to the public Internet, which can increase the risk of security breaches. Additionally, it requires additional management and monitoring to maintain the security of the instance.\nOption B, creating a new EC2 instance to download updates separately and then pushing them to the required instance, is not an efficient solution because it requires additional resources and management to maintain the update process. This option also increases the attack surface of the environment, as the new EC2 instance would also need to be secured and monitored.\nOption D, creating a VPC link to the Internet, is not as secure as using a NAT Gateway because it requires a direct route between the private subnet and the Internet. This option can also be less efficient, as it may require additional configuration and management to ensure the security of the VPC link.\nTherefore, using a NAT Gateway is the most efficient and secure option for instances hosted in a private subnet in a VPC to download updates from the Internet.\n\n"
}, {
  "id" : 116,
  "question" : "You have an S3 bucket that receives photos uploaded by customers.\nWhen an object is uploaded, an event notification is sent to an SQS queue with the object details.\nYou also have an ECS cluster that gets messages from the queue to do the batch processing.\nEach of the batch processing job takes the same amount of time to get executed.\nThe queue size may change greatly depending on the number of incoming messages and backend processing speed.\nWhich metric would you use to scale up/down the ECS cluster capacity?\n",
  "answers" : [ {
    "id" : "8555bac1283c43cbb755d3a244ecbeec",
    "option" : "The number of messages in the SQS queue.",
    "isCorrect" : "true"
  }, {
    "id" : "58eb86abad834b87a0038c5c06083c05",
    "option" : "Memory usage of the ECS cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "c9ef2960c008498e98418c61647d25ed",
    "option" : "Number of objects in the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "d0a3b6ffe5494b56881e3e769b391b67",
    "option" : "Number of containers in the ECS cluster.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nIn this scenario, the SQS queue is used to store the object details, which is a highly scalable and reliable service.\nECS is ideal for performing batch processing, and it should scale up or down based on the number of messages in the queue.\nIf you use a target tracking scaling policy based on a custom Amazon SQS queue metric, dynamic scaling can adjust to the demand curve of your application more effectively.\nOption A is CORRECT: Users can configure a CloudWatch alarm based on the number of messages in the SQS queue and notify the ECS cluster to scale up or down using the alarm.\nOption B is incorrect: Because the memory usage may not be able to reflect the workload.\nOption C is incorrect: Because the number of objects in S3 cannot determine if the ECS cluster should change its capacity.\nOption D is incorrect: Because the number of containers cannot be used as a metric to trigger an auto-scaling event.\nAn Auto Scaling group to manage EC2 instances for the purposes of processing messages from an SQS queue.\nA custom metric to send to Amazon CloudWatch that measures the number of messages in the queue per EC2 instance in the Auto Scaling group.\nA target tracking policy that configures your Auto Scaling group to scale based on the custom metric and a set target value.\nCloudWatch alarms invoke the scaling policy.\nFor details, please check\nhttps://github.com/aws-samples/ecs-refarch-batch-processing.\n\nThe metric that should be used to scale up/down the ECS cluster capacity in this scenario is the number of messages in the SQS queue.\nHere's why:\nWhen an object is uploaded to the S3 bucket, an event notification is sent to an SQS queue with the object details. The ECS cluster then gets messages from the queue to do batch processing. Each of the batch processing jobs takes the same amount of time to execute.\nThe queue size may change greatly depending on the number of incoming messages and backend processing speed. Therefore, to ensure that there are enough resources to handle the incoming messages and process them in a timely manner, the ECS cluster needs to be scaled up or down dynamically based on the queue size.\nThe number of messages in the SQS queue is the most relevant metric to use for scaling the ECS cluster capacity. If the number of messages in the queue increases, it means there is more work to be done, and the ECS cluster should be scaled up to handle the increased workload. If the number of messages in the queue decreases, it means the workload is decreasing, and the ECS cluster should be scaled down to reduce costs.\nThe other metrics provided in the answer options are not relevant to scaling the ECS cluster in this scenario. Memory usage of the ECS cluster is not a good metric to use because it doesn't directly correlate with the workload of processing messages from the SQS queue. The number of objects in the S3 bucket is also not relevant because it doesn't give any indication of the workload of processing messages from the SQS queue. Finally, the number of containers in the ECS cluster is also not relevant because the number of containers doesn't directly correlate with the workload of processing messages from the SQS queue.\n\n"
}, {
  "id" : 117,
  "question" : "You have an EC2 instance in the AWS us-east-1 region.\nThe application in the instance needs to access a DynamoDB table that is located in the AWS us-east-2 region.\nThe connection must be private without leaving the Amazon network.\nThe instance should not use any public IP for communication.\nHow would you configure this?\n",
  "answers" : [ {
    "id" : "b3f8825bee134f01963453fff653de57",
    "option" : "Configure an inter-region VPC endpoint for the DynamoDB service.",
    "isCorrect" : "false"
  }, {
    "id" : "4517cf75770c46da94730947c7200690",
    "option" : "Configure inter-region VPC peering and create a VPC endpoint for DynamoDB in us-east-1.",
    "isCorrect" : "true"
  }, {
    "id" : "52d1580881c5436da13784cfae7b24d3",
    "option" : "Create an inter-region VPC peering connection between us-east-1 and us-east-2.",
    "isCorrect" : "false"
  }, {
    "id" : "d4d93584beab4d0fa6aa89e9a176cb32",
    "option" : "There is no way to setup the private inter-region connections.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nFor the private connections between regions, VPC peering should be used.\nThen VPC endpoint allows users to access the DynamoDB service privately.\nOption A is incorrect because you cannot configure an inter-region VPC endpoint directly.\nOption B is CORRECT because the VPC endpoint should be created in consumer VPC which is in us-east-1\nENI will be created in us-east-1 which will access DynamoDB over VPC peering &amp; AWS PrivateLink.\nOption C is incorrect: This option does not mention the usage of the VPC endpoint.\nOption D is incorrect: Because VPC peering supports the inter-region connections.\nYou can refer to the below URL for inter-region peering &amp; endpoints.\nhttps://d1.awsstatic.com/whitepapers/aws-privatelink.pdf\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service-overview.html\n\nTo enable the EC2 instance in the us-east-1 region to access the DynamoDB table in the us-east-2 region privately without leaving the Amazon network, we need to establish a private network connection between the two regions.\nThere are two possible solutions for this scenario:\nConfigure an inter-region VPC endpoint for the DynamoDB service. Create an inter-region VPC peering connection between us-east-1 and us-east-2.\nOption A: Configure an inter-region VPC endpoint for the DynamoDB service.\nA VPC endpoint enables private communication between instances in a VPC and AWS services. With VPC endpoints, you can access AWS services via private network connections instead of going through the internet. You can create a VPC endpoint for DynamoDB in the us-east-1 region and use it to access the DynamoDB table in the us-east-2 region.\nSteps to configure an inter-region VPC endpoint for the DynamoDB service:\nCreate a VPC in the us-east-1 region and attach the EC2 instance to the VPC. Create a VPC endpoint for DynamoDB in the us-east-1 region and attach it to the VPC. In the VPC endpoint configuration, select the DynamoDB table in the us-east-2 region as the target. Ensure that the security group rules on the EC2 instance allow traffic to the VPC endpoint.\nOption B: Create an inter-region VPC peering connection between us-east-1 and us-east-2.\nVPC peering enables private communication between VPCs in different regions using private IP addresses. You can create a VPC peering connection between the VPC in the us-east-1 region and the VPC in the us-east-2 region, and configure the route tables to allow traffic to flow between the two VPCs.\nSteps to create an inter-region VPC peering connection between us-east-1 and us-east-2:\nCreate a VPC in the us-east-1 region and attach the EC2 instance to the VPC. Create a VPC in the us-east-2 region and create a DynamoDB table in the VPC. Create a VPC peering connection between the VPCs in the us-east-1 and us-east-2 regions. Configure the route tables to allow traffic to flow between the VPCs. Ensure that the security group rules on the EC2 instance allow traffic to the VPC.\nOption C: Incorrect\nAlthough inter-region VPC peering can be used, a VPC endpoint for DynamoDB is not required.\nOption D: Incorrect\nIt is possible to set up private inter-region connections in AWS. Therefore, this option is incorrect.\nIn conclusion, the recommended solution to access a DynamoDB table from an EC2 instance in a different region while maintaining private communication is to use either an inter-region VPC endpoint for DynamoDB or an inter-region VPC peering connection.\n\n"
}, {
  "id" : 118,
  "question" : "You have planned to host a web application on AWS.\nYou create an EC2 Instance in a public subnet that needs to connect to an EC2 Instance that will host an Oracle database.\nWhich steps would ensure a secure setup? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "0a086e284ecc43eead343d8df73fc4db",
    "option" : "Place the EC2 Instance with the Oracle database in the same public subnet as the Webserver for faster communication.",
    "isCorrect" : "false"
  }, {
    "id" : "899522c3ab8744c8826175890e9f4c35",
    "option" : "Place the ec2 instance that will host the Oracle database in a private subnet.",
    "isCorrect" : "true"
  }, {
    "id" : "729c969d2ead42af99066ca5970203dd",
    "option" : "Create a database Security group which allows incoming traffic only from the Web server`s security group.",
    "isCorrect" : "true"
  }, {
    "id" : "cfb77b2e18fc4a1a9bedb0ac358a25d6",
    "option" : "Ensure that the database security group allows incoming traffic from 0.0.0.0/0.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nThe best and most secure option is to place the database in a private subnet.\nThe below diagram from AWS Documentation shows this setup.\nAlso, you ensure that access is not allowed from all sources but only from the web servers.\nOption A is incorrect because DB instances are placed in Private subnets and allowed to communicate with web servers in the public subnet as per the best practice guidelines.\nOption D is incorrect because allowing all incoming traffic from the Internet to the DB instance is a security risk.\nFor more information on this type of setup, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\n\n\nThe most secure way to host a web application on AWS and ensure secure communication between the web server and the database is to place the web server in a public subnet and the database server in a private subnet. This will ensure that the database server is not directly accessible from the internet.\nTherefore, options A and D are not correct.\nThe correct steps to ensure a secure setup are:\nB. Place the EC2 instance that will host the Oracle database in a private subnet: A private subnet is not directly accessible from the internet, which provides an additional layer of security. The web server in the public subnet can still communicate with the database server using private IP addresses, but the database server is not exposed to the public internet.\nC. Create a database Security group which allows incoming traffic only from the Web server's security group: A security group acts as a virtual firewall for your instance to control inbound and outbound traffic. By creating a security group for the database server, you can ensure that only traffic from the web server's security group is allowed to access the database server. This minimizes the attack surface and reduces the risk of unauthorized access to the database.\nTherefore, options B and C are the correct choices.\n\n"
}, {
  "id" : 119,
  "question" : "An EC2 Instance hosts a Java-based application that accesses a DynamoDB table.\nThis EC2 Instance is currently serving production users.\nWhat would be a secure way for the EC2 Instance to access the DynamoDB table?\n",
  "answers" : [ {
    "id" : "c3902957be7347b6939baa73c69af2c6",
    "option" : "Use IAM Roles with permissions to interact with DynamoDB and assign it to the EC2 Instance.",
    "isCorrect" : "true"
  }, {
    "id" : "7b523f7f1ac24c0daec4dc66768b9403",
    "option" : "Use KMS Keys with the right permissions to interact with DynamoDB and assign it to the EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "12540937fee441109af8888be5016172",
    "option" : "Use IAM Access Keys with the right permissions to interact with DynamoDB and assign it to the EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "e37f959d8804426c821ffbaa42f48710",
    "option" : "Use IAM Access Groups with the right permissions to interact with DynamoDB and assign it to the EC2 Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAlways assign a role to the EC2 Instance to ensure secure access to AWS resources from EC2 Instances.\nFor more information on IAM Roles, please refer to the below URL:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\nAn IAM role is similar to a user; it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS.\nHowever, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it.\nAlso, an IAM role does not have standard long-term credentials (password or access keys) associated with it.\nInstead, if a user assumes a role, temporary security credentials are created dynamically and provided to the user.\nYou can use roles to delegate access to users, applications, or services that normally don't have access to your AWS resources.\nNote:\nYou can attach the IAM role to the existing EC2 instance.\nTo know more, please visit the following URL:\nhttps://aws.amazon.com/about-aws/whats-new/2017/02/new-attach-an-iam-role-to-your-existing-amazon-ec2-instance/\n\nThe correct answer for this question is A. Use IAM Roles with permissions to interact with DynamoDB and assign it to the EC2 Instance.\nExplanation:\nAWS Identity and Access Management (IAM) is a service that enables you to manage access to AWS services and resources securely. IAM allows you to create and manage users, groups, and roles to grant permissions for AWS resources.\nIn this scenario, the EC2 Instance is serving production users, and it is accessing a DynamoDB table. To secure the access to DynamoDB, you should use IAM roles instead of IAM access keys or groups.\nIAM roles are secure because they provide temporary security credentials for the application or service to access AWS resources, like DynamoDB, without requiring the use of long-term access keys or passwords. Roles are also easier to manage, and you can define the permissions for the role, and AWS automatically handles the temporary credentials.\nTo set up IAM roles for an EC2 Instance to access DynamoDB, you need to follow these steps:\nCreate an IAM role that has the necessary permissions to access DynamoDB. Attach the role to the EC2 Instance using the AWS Management Console or AWS CLI. Update your Java-based application to use the temporary security credentials provided by IAM roles when accessing DynamoDB.\nUsing KMS keys or IAM access keys is not the best option for securing the access to DynamoDB in this scenario because they require you to manage and protect access keys, which can be challenging and pose security risks if not managed correctly. IAM access groups are not a solution to this problem either because they do not provide temporary security credentials like IAM roles.\n\n"
}, {
  "id" : 120,
  "question" : "You are designing a website for a company that streams anime videos.\nYou serve this content through CloudFront.\nThe company has implemented a section for premium subscribers.\nThis section contains more videos than the free section.\nYou want to ensure that only premium subscribers can access this premium section.\nHow can you achieve this easily?\n",
  "answers" : [ {
    "id" : "145b88ca7463499288fcd296c65ceab3",
    "option" : "Using bucket policies.",
    "isCorrect" : "false"
  }, {
    "id" : "73e0dff9a3ec4e2c977734c1437ce02a",
    "option" : "Requiring HTTPS for communication between users and CloudFront.",
    "isCorrect" : "false"
  }, {
    "id" : "4a29135e6d0c48a7b7933d65e2ebd027",
    "option" : "Using CloudFront origin with signed URLs.",
    "isCorrect" : "false"
  }, {
    "id" : "8580791e94e7448486ec40d14c9a05b9",
    "option" : "Using CloudFront origin with signed cookies.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption A is incorrect.\nBucket policies are mainly used with resources in AWS.\nThey are not intended to restrict content to application users.\nMore details:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/access-policy-language-overview.html\nOption B is incorrect.\nThis option only encrypts the communication between users and CloudFront.\nIt does not restrict access to files.\nMore details:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html\nOption C is incorrect.\nWith a signed URL, you only restrict access to one file.\nIf you implement this, the application would have to generate a new pre-signed URL for every file in the premium section and give this new link to the user.\nMore details:\nhttps://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\nOption D is CORRECT.\nOnce your application allows your user to see this section, it will give a cookie to the user.\nThen CloudFront will use this cookie to access S3 restricted content.\nMore details:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html\n\nThe correct answer is C. Using CloudFront origin with signed URLs.\nExplanation:\nWhen designing a website that offers premium content, it is important to ensure that only users who have paid for access to that content can view it. One way to achieve this is by using CloudFront origin with signed URLs.\nSigned URLs are URLs that have an expiration time and an encrypted signature attached to them. The signature is created using a secret key that is only known to the website's servers. When a user clicks on a signed URL, CloudFront checks the signature to ensure that the URL is valid and has not expired. If the signature is valid and the URL has not expired, CloudFront will grant access to the requested content.\nBy using CloudFront origin with signed URLs, the website can provide access to premium content only to users who have paid for it. When a user pays for access to the premium section, the website's servers generate a signed URL that grants access to that user for a specified period of time. This signed URL is then sent to the user, who can use it to access the premium content.\nBucket policies (option A) are used to define access controls for Amazon S3 buckets. While they can be used to restrict access to content, they are not as flexible as CloudFront signed URLs and are not designed to handle time-limited access.\nRequiring HTTPS for communication between users and CloudFront (option B) is a good security practice, but it does not provide a way to restrict access to premium content.\nUsing CloudFront origin with signed cookies (option D) is another way to restrict access to content, but it requires more configuration than using signed URLs. Signed cookies are more useful for scenarios where access controls need to be applied to specific parts of a website rather than specific content.\nTherefore, the best option for achieving the desired outcome is option C: Using CloudFront origin with signed URLs.\n\n"
}, {
  "id" : 121,
  "question" : "A company has set up an application in AWS that interacts with DynamoDB.\nIt is required that when an item is modified in a DynamoDB table, immediate entry is made to the associating application.\nHow can this be accomplished? (SELECT TWO.)\n",
  "answers" : [ {
    "id" : "31fea26f5be747bc9cc4edb70907c038",
    "option" : "Set up CloudWatch to monitor the DynamoDB table for changes. Then trigger a Lambda function to send the changes to the application.",
    "isCorrect" : "false"
  }, {
    "id" : "b1ece87796ee494bbad11a2deab4bdf8",
    "option" : "Set up CloudWatch logs to monitor the DynamoDB table for changes. Then trigger AWS SQS to send the changes to the application.",
    "isCorrect" : "false"
  }, {
    "id" : "324bf5b42dc64d4c9b8b0104e70f1f3b",
    "option" : "Use DynamoDB streams to monitor the changes to the DynamoDB table.",
    "isCorrect" : "true"
  }, {
    "id" : "37ce1168a2c94837a785276ba8dba534",
    "option" : "Trigger a lambda function to make an associated entry in the application as soon as the DynamoDB streams are modified.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - C and D.\nWhen you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write.\nImmediately after an item in the table is modified, a new record appears in the table's stream.\nAWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.\nSince we require an immediate entry made to an application in case an item in the DynamoDB table is modified, a lambda function is also required.\nLet us try to analyze this with an example:\nConsider a mobile gaming app that writes to a GamesScores table.\nWhenever the top score of the Game Scores table is updated, a corresponding stream record is written to the table's stream.\nThis event could trigger a Lambda function that posts a Congratulatory message on a Social media network handle.\nDynamoDB streams can be used to monitor the changes to a DynamoDB table.\nAWS Documentation mentions the following:\nA DynamoDB stream is an ordered flow of information about changes to items in an Amazon DynamoDB table.\nWhen you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.\nFor more information on DynamoDB streams, please refer to the URL below:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\nNote:\nDynamoDB is integrated with Lambda so that you can create triggers to events in DynamoDB Streams.\nIf you enable DynamoDB Streams on a table, you can associate the stream ARN with a Lambda function that you write.\nImmediately after an item in the table is modified, a new record appears in the table's stream.\nAWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.\nSince our requirement states that an item modified in a DynamoDB table causes an immediate entry to an associating application, a lambda function is also required.\nFor more information on DynamoDB streams Lambda, please refer to the URL below.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html\n\nThe requirement is to have an immediate entry made to the associated application whenever an item is modified in a DynamoDB table. To achieve this, we can use DynamoDB streams and AWS Lambda or CloudWatch Events.\nDynamoDB Streams is a feature of DynamoDB that provides a time-ordered sequence of item-level changes made to a table. It captures the modification events and stores them in a log in near real-time, making it easier to capture, process, and act upon the data.\nHere are the two solutions that fulfill the requirement:\nSet up CloudWatch to monitor the DynamoDB table for changes. Then trigger a Lambda function to send the changes to the application.\nThis solution involves setting up a CloudWatch Events rule that triggers a Lambda function whenever a modification is made to the DynamoDB table. The Lambda function can then send the changes to the associated application using an API call or another method.\nTo implement this solution, follow these steps:\nCreate a new Lambda function that will process the events from the DynamoDB stream and send them to the application. Create a new CloudWatch Events rule that will trigger the Lambda function whenever a modification is made to the DynamoDB table. Configure the Lambda function to process the events from the DynamoDB stream and send them to the application. Use DynamoDB streams to monitor the changes to the DynamoDB table. Trigger a lambda function to make an associated entry in the application as soon as the DynamoDB streams are modified.\nThis solution involves using DynamoDB streams to monitor the changes made to the DynamoDB table. Whenever a modification is made, the stream sends the event to a Lambda function that processes the changes and sends them to the associated application.\nTo implement this solution, follow these steps:\nEnable DynamoDB streams on the table that needs to be monitored. Create a new Lambda function that will process the events from the DynamoDB stream and send them to the application. Configure the Lambda function to process the events from the DynamoDB stream and send them to the application.\nIn conclusion, both solutions use DynamoDB streams and AWS Lambda to capture the modification events and send them to the associated application. The difference is in the trigger mechanism - one uses CloudWatch Events to trigger the Lambda function, while the other uses DynamoDB streams directly to trigger the Lambda function.\n\n"
}, {
  "id" : 122,
  "question" : "You are working as an AWS consultant in an E-Commerce organization.\nYour organization is planning to migrate its database from on-premises data centers to Amazon RDS.\nThe automated backup helps to restore the Database to any specific time during the backup retention period in Amazon RDS.\nWhich of the following actions are performed as a part of the Amazon RDS automated backup process?\n",
  "answers" : [ {
    "id" : "d13fcd85e3ba4d909aa4f8d82cea9dbb",
    "option" : "AWS creates a storage volume snapshot of the database instance during the backup window once a day. AWS RDS also captures transactions logs and uploads them to S3 buckets every 5 minutes.",
    "isCorrect" : "true"
  }, {
    "id" : "4ed57b5c20724893946b5a44c0a70360",
    "option" : "AWS creates a full snapshot of the database every 12 hours during the backup window, captures transactions logs throughout the day, and stores them in S3 buckets.",
    "isCorrect" : "false"
  }, {
    "id" : "df441cc0881b485dae9161db772a05a4",
    "option" : "AWS creates a full daily snapshot during the backup window. With the snapshot, the RDS instance can be restored at any time.",
    "isCorrect" : "false"
  }, {
    "id" : "afaa135d54964aa2a6ef7fd124484f5b",
    "option" : "AWS creates a storage volume snapshot of the database instance every 12 hours during the backup window, captures transactions logs throughout the day, and stores them in S3 buckets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nDuring automated backup, Amazon RDS creates a storage volume snapshot of the entire Database Instance.\nRDS uploads transaction logs for DB instances to Amazon S3 every 5 minutes.\nTo restore a DB instance at a specific point in time, a new DB instance is created using the DB snapshot.\nYou can also use AWS Backup to manage backups of Amazon RDS DB instances.\nIf your DB instance is associated with a backup plan in AWS Backup, that backup plan is used for point-in-time recovery.\nOption B is incorrect as RDS automated backup does not take a full snapshot every 12 hours.\nOption C is incorrect as this option does not mention the transaction logs captured in the automated backup process.\nOption A is more accurate.\nOption D is incorrect as AWS performs storage volume snapshots daily, not every 12 hours.\nFor more information on the Amazon RDS Automated backup process and how to restore a DB instance to a specified time, refer to the following URL:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIT.html\n\nAmazon RDS (Relational Database Service) is a managed database service provided by AWS (Amazon Web Services). It simplifies the process of setting up, operating, and scaling a relational database in the cloud. One of the key features of Amazon RDS is the automated backup process that allows the user to restore the database to any specific time during the backup retention period.\nThe automated backup process of Amazon RDS involves taking periodic snapshots of the database instance and capturing transaction logs. These snapshots and logs are stored in Amazon S3 (Simple Storage Service) buckets for safekeeping.\nNow let's look at the answer options:\nOption A: AWS creates a storage volume snapshot of the database instance during the backup window once a day. AWS RDS also captures transactions logs and uploads them to S3 buckets every 5 minutes.\nThis option is partially correct. Amazon RDS takes a storage volume snapshot of the database instance during the backup window, but it doesn't happen only once a day. By default, Amazon RDS creates a snapshot once a day. However, the user can modify the backup retention period and set the frequency of the snapshots. Additionally, Amazon RDS captures transaction logs every 5 minutes and stores them in Amazon S3.\nOption B: AWS creates a full snapshot of the database every 12 hours during the backup window, captures transactions logs throughout the day, and stores them in S3 buckets.\nThis option is incorrect. Amazon RDS doesn't take a full snapshot of the database every 12 hours. Instead, it takes a storage volume snapshot once a day by default, which can be modified by the user.\nOption C: AWS creates a full daily snapshot during the backup window. With the snapshot, the RDS instance can be restored at any time.\nThis option is partially correct. Amazon RDS takes a storage volume snapshot of the database instance during the backup window, and this snapshot can be used to restore the RDS instance to any specific time during the backup retention period.\nOption D: AWS creates a storage volume snapshot of the database instance every 12 hours during the backup window, captures transactions logs throughout the day, and stores them in S3 buckets.\nThis option is incorrect. Amazon RDS doesn't take a storage volume snapshot every 12 hours. By default, it takes a snapshot once a day, and the user can modify the backup retention period and set the frequency of the snapshots.\nIn conclusion, the correct answer is Option A: AWS creates a storage volume snapshot of the database instance during the backup window once a day. AWS RDS also captures transactions logs and uploads them to S3 buckets every 5 minutes.\n\n"
}, {
  "id" : 123,
  "question" : "You configure an Amazon S3 bucket as the origin for a new CloudFront distribution.\nThe traffic should not hit the S3 URLs directly instead, they should be directed to the CloudFront distribution and the files should be fetched through the CloudFront URL.\nWhich method is the most appropriate?\n",
  "answers" : [ {
    "id" : "65d6c82bc86647dd826f29d83fd8ed20",
    "option" : "Configure Signed URLs to serve private content by using CloudFront.",
    "isCorrect" : "false"
  }, {
    "id" : "30370355059147aaa2447807696ba434",
    "option" : "Configure Signed Cookies to restrict access to S3 files.",
    "isCorrect" : "false"
  }, {
    "id" : "17ac3ff1ca464e548a43a8068b539f07",
    "option" : "Create the origin access identity (OAI) and associate it with the distribution.",
    "isCorrect" : "true"
  }, {
    "id" : "7960738e459f40ae8bcbdff68906c42d",
    "option" : "Configure the CloudFront web distribution to ask viewers to use HTTPS to request S3 objects.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nTo restrict access to content that you serve from Amazon S3 buckets, follow the below steps:\nCreate a special CloudFront user called an origin access identity (OAI) and associate it with your distribution.\nConfigure your S3 bucket permissions so that CloudFront can use the OAI to access the files in your bucket and serve them to your users.\nMake sure that users can't use a direct URL to the S3 bucket to access a file there.\nIf you want to serve private content through CloudFront and you're trying to decide whether to use signed URLs or signed cookies, consider the following.\nUse signed URLs in the following cases:\nYou want to restrict access to individual files, for example, an installation download for your application.\nYour users are using a client (for example, a custom HTTP client) that doesn't support cookies.\nUse signed cookies in the following cases:\nYou want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of the website.\nYou don't want to change your current URLs.\nOption A is incorrect because Signed URLs are used to restrict access to files in CloudFront edge caches.\nIt cannot prevent users from fetching files directly through S3 URLs.\nOption B is incorrect because Signed cookies are also not suited for the above requirements.\nOption C is CORRECT because you can configure the CloudFront origin to restrict bucket access through OAI.\nOption D is incorrect With HTTPS, connections are encrypted between CloudFront and viewers.\nHowever, it does not restrict access to the S3 content.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\n\n\nWhen you use an S3 bucket as the origin for a CloudFront distribution, it is recommended to prevent users from directly accessing the S3 bucket URL, and instead, all requests should be routed through the CloudFront URL. This can be achieved by using the following method:\nC. Create the origin access identity (OAI) and associate it with the distribution.\nExplanation:\nWhen you create an Amazon CloudFront distribution, it uses an Amazon S3 bucket or an HTTP server as the origin. However, if you use an S3 bucket as an origin, you must specify an origin access identity (OAI) to grant CloudFront permission to access the objects in your bucket. This helps to restrict access to your Amazon S3 content and ensures that your users can only access your content through your CloudFront distribution.\nAn origin access identity (OAI) is a special CloudFront user identity that you can associate with your Amazon S3 bucket to restrict access to objects in the bucket. When you create a CloudFront distribution, you can configure it to use an OAI to access the objects in your S3 bucket.\nWhen you associate an OAI with a CloudFront distribution, CloudFront uses the OAI's credentials to access your S3 bucket. Therefore, the users of your application do not need to have permissions to access your S3 bucket, which helps to improve the security of your application.\nTo create an OAI and associate it with a CloudFront distribution:\nOpen the CloudFront console at https://console.aws.amazon.com/cloudfront/. Create a new CloudFront distribution or select an existing one. In the distribution settings, go to the \"Origins and Origin Groups\" section and select the S3 bucket you want to use as the origin. Click on \"Edit\" to edit the origin. In the \"Origin Access Identity\" section, select \"Create a New Identity\" to create a new OAI. Click on \"Yes, Update\" to update the distribution settings with the new OAI. Update your S3 bucket policy to allow the OAI to access the objects in your bucket. Test your CloudFront distribution to ensure that it is working as expected.\nTherefore, option C is the most appropriate method to prevent users from directly accessing the S3 URLs and ensure that the files are fetched through the CloudFront URL. Options A, B, and D are also valid methods for securing your CloudFront distribution, but they do not directly address the issue of preventing direct access to your S3 bucket.\n\n"
}, {
  "id" : 124,
  "question" : "As a Solutions Architect for a multinational organization with more than 150000 employees, management has decided to implement a real-time analysis for their employees' time spent in offices worldwide.\nYou are tasked to design an architecture that will receive the inputs from 10000+ sensors with swipe machine sending in and out data across the globe, each sending 20KB data every 5 Seconds in JSON format.\nThe application will process and analyze the data and upload the results to dashboards in real-time. Other application requirements will include the ability to apply real-time analytics on the captured data.\nProcessing of captured data will be parallel and durable.\nThe application must be scalable as per the requirement as the load varies and new sensors are added or removed at various facilities.\nThe analytic processing results are stored in a persistent data storage for data mining.\nWhat combination of AWS services would be used for the above scenario?\n",
  "answers" : [ {
    "id" : "81f30a0bf39a4013b736e727ef17fee5",
    "option" : "Use EMR to copy the data coming from Swipe machines into DynamoDB and make it available for analytics.",
    "isCorrect" : "false"
  }, {
    "id" : "280b3170f79340f3a1011c0748c51de7",
    "option" : "Use Amazon Kinesis Streams to ingest the Swipe data coming from sensors, Custom Kinesis Streams Applications to analyze the data and then move analytics outcomes to RedShift using AWS EMR.",
    "isCorrect" : "true"
  }, {
    "id" : "5ed4344d294f4034a1456e3736e3bd53",
    "option" : "Use SQS to receive the data coming from sensors, Kinesis Firehose to analyze the data from SQS, then save the results to a Multi-AZ RDS instance.",
    "isCorrect" : "false"
  }, {
    "id" : "aebe7c44df2f4e1389aaa0c76f711b93",
    "option" : "Use Amazon Kinesis Streams to ingest the sensorsâ€™ data, custom Kinesis Streams applications to analyze the data, and move analytics outcomes to RDS using AWS EMR.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect.\nEMR is not for receiving real-time data from thousands of sources.\nEMR is mainly used for Hadoop ecosystem-based data used for Big data analysis.\nOption B is correct as the Amazon Kinesis streams are used to read the data from thousands of sources like social media, survey-based data, etc.\nThe Kinesis streams can be used to analyze the data and feed it using AWS EMR to the analytics-based database like RedShift which works on OLAP.\nOption C is incorrect.\nSQS cannot be used to read the real-time data from thousands of sources.\nThe Kinesis Firehose is used to ship the data to other AWS services, not for the analysis.\nAnd finally, RDS is again an OLTP based database.\nOption D is incorrect as the AWS EMR can read large amounts of data.\nHowever, RDS is a transactional database that works based on the OLTP.\nThus, it cannot store the analytical data.\n\nThe best combination of AWS services to achieve the desired outcome would be option B, which includes the use of Amazon Kinesis Streams, custom Kinesis Streams applications, and AWS EMR, to move the analytics outcomes to RedShift.\nHere is an overview of how this combination of services would work together to meet the application requirements:\nAmazon Kinesis Streams: The first step in the architecture is to ingest the Swipe data coming from sensors. Amazon Kinesis Streams is a fully managed service for real-time data streaming, and it can handle large amounts of data from multiple sources. Kinesis Streams provides the ability to process and analyze data in real-time with low latency. This service will be used to ingest the data coming from the sensors. Custom Kinesis Streams Applications: Once the data is ingested into Kinesis Streams, custom Kinesis Streams applications can be developed to analyze the data in real-time. These applications can be developed using a variety of programming languages, and they can leverage AWS services such as AWS Lambda, Amazon EC2, and AWS EMR to perform real-time data processing and analytics. AWS EMR: AWS EMR is a fully managed service that simplifies big data processing, and it provides a managed Hadoop framework that makes it easy to process large amounts of data using Amazon EC2 instances. In this architecture, AWS EMR will be used to move the analytics outcomes from Kinesis Streams to RedShift, which is a petabyte-scale data warehouse that makes it easy to analyze large amounts of data. RedShift: Once the data has been analyzed by the custom Kinesis Streams applications using AWS EMR, the analytics outcomes can be moved to RedShift. RedShift provides fast querying and data analysis capabilities, and it can easily handle petabyte-scale data sets. RedShift will be used to store the analytics results for data mining purposes.\nOverall, this architecture meets all of the application requirements, including the ability to ingest large amounts of data from multiple sensors, real-time data processing and analytics, durability and scalability, and persistent data storage for data mining.\n\n"
}, {
  "id" : 125,
  "question" : "You have a local data center on-premise that stores archived files.\nThe total amount of the files is about 70TB.\nThe data needs to be transferred to Amazon Simple Storage Service (S3)\nAfter the data transfer is finished, the local data center will not be used.\nWhich solution is the most appropriate?\n",
  "answers" : [ {
    "id" : "b1226a03506b430eacbe9bf092254654",
    "option" : "AWS Direct Connect.",
    "isCorrect" : "false"
  }, {
    "id" : "46db623a4d7e44299e2d2da1d2fb943a",
    "option" : "AWS Snowball.",
    "isCorrect" : "true"
  }, {
    "id" : "04755288c3454fe19fee2b1a77829fcf",
    "option" : "Amazon S3 Transfer Acceleration.",
    "isCorrect" : "false"
  }, {
    "id" : "243f445ec8594bd1a18a719abc5cc273",
    "option" : "AWS Global Accelerator.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Snowball has 80TB and 50TB models.\nThe 80TB model is suitable to transfer 70TB of data to AWS.\nPlease refer to https://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html.\nOption A is incorrect: Because Direct Connect establishes a network connection from on-premises to an AWS Region.\nIt is not suitable to move 70TB of data.\nOption B is CORRECT: Because AWS Snowball is a data transport solution that accelerates moving terabytes to petabytes of data to AWS.\nOption C is incorrect: S3 Transfer Acceleration uses Amazon CloudFront's globally distributed edge locations.\nIt does not help in this scenario.\nOption D is incorrect: Because AWS Global Accelerator improves availability and performance for applications, which does not help on the data transfer.\n\nOption B, AWS Snowball, is the most appropriate solution for transferring 70TB of archived files from a local data center to Amazon S3.\nAWS Direct Connect (option A) is a service that provides dedicated network connections between an on-premises data center and AWS. It is useful for situations where high-bandwidth connectivity is required for frequently accessing data between the on-premises data center and AWS. However, it is not designed for transferring large amounts of data.\nAmazon S3 Transfer Acceleration (option C) is a service that uses Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet to Amazon S3. It can speed up transfers for small to medium-sized files, but it is not designed for transferring large amounts of data, especially in cases where the source location is on-premises.\nAWS Global Accelerator (option D) is a service that improves the availability and performance of applications by providing static IP addresses and routing traffic over the AWS global network. It is useful for situations where users access applications from different regions of the world and need low-latency access to these applications. However, it is not designed for transferring large amounts of data.\nAWS Snowball (option B) is a petabyte-scale data transport solution that uses a physical device to transfer large amounts of data into and out of AWS. It is designed to handle large-scale data transfers quickly and securely, with a data transfer rate of up to 100TB per appliance. In this scenario, the 70TB of archived files can be transferred onto a Snowball appliance, which can then be shipped to AWS for data transfer to S3. Once the data transfer is complete, the local data center can be decommissioned.\n\n"
}, {
  "id" : 126,
  "question" : "You have designed an application that uses AWS resources, such as S3, to operate and store users' documents.\nYou currently use Cognito identity pools and user pools.\nTo increase usage and ease of signing up, you decide that adding social identity federation is the best path forward. How would you differentiate the Cognito identity pool and the federated identity providers (e.g., Google)?\n",
  "answers" : [ {
    "id" : "661b4e089def4485adc83f771103fe8b",
    "option" : "They are the same and just called different things.",
    "isCorrect" : "false"
  }, {
    "id" : "34081b59e62540909500cc86e74aaa0f",
    "option" : "First, you sign-in via Cognito then through a federated site, like Google.",
    "isCorrect" : "false"
  }, {
    "id" : "00ed348cd2c54be5950c7c0f4d639c2b",
    "option" : "Federated identity providers and identity pools are used to authenticate services.",
    "isCorrect" : "false"
  }, {
    "id" : "bae417acd0fd43c49291a5eead28d164",
    "option" : "You can choose a federated identity provider to authenticate users and associate a Cognito identity pool to authorize the users.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption D is correct.\nFederated identity providers are used to authenticate users.\nThen the Cognito identity pool provides the temporary token that authorizes users to access AWS resources.\nOption A is incorrect.\nCognito identity pool and the federated identity providers are separate, independent authentication methods.\nOption B is incorrect.\nOnly one log-in event is needed, not two.\nOption C is incorrect.\nIdentity providers authenticate users, not authenticate services.\nFor more information, refer to the following URLs:\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-identity-federation.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\nhttps://aws.amazon.com/articles/web-identity-federation-with-mobile-applications/\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-getting-started.html\n\nWhen it comes to identity and access management (IAM) in AWS, Cognito is a powerful tool. Cognito allows developers to create and manage user authentication for web and mobile applications.\nCognito User Pools are user directories that provide sign-up and sign-in options for your app users. It is the Cognito User Pool that handles the authentication of users' credentials such as their email and password.\nCognito Identity Pools, on the other hand, allow your users to obtain temporary AWS credentials to access AWS services such as S3, DynamoDB, and more. These temporary credentials are obtained after a user successfully authenticates with a federated identity provider.\nFederated identity providers are third-party services that allow users to authenticate using their existing identities. These could be social identity providers like Google, Facebook, or Amazon, or enterprise identity providers such as Microsoft Active Directory.\nWhen using Cognito with federated identity providers, users can sign in using their Google or Facebook credentials, and then obtain temporary AWS credentials from Cognito Identity Pools. This means that users can access AWS resources without having to create separate AWS credentials.\nTherefore, the correct answer to the question is D. You can choose a federated identity provider to authenticate users and associate a Cognito identity pool to authorize the users.\n\n"
}, {
  "id" : 127,
  "question" : "You have a web application hosted on an EC2 Instance in AWS that users can access across the globe.\nThe Operations team has been receiving support requests about extreme slowness from users in some regions.\nWhat can be done to the architecture to improve the response time for these users?\n",
  "answers" : [ {
    "id" : "025c5651f39443edb26160ddb27dda87",
    "option" : "Add more EC2 Instances to support the load.",
    "isCorrect" : "false"
  }, {
    "id" : "85732749bc544277b691c61fd2fecc7f",
    "option" : "Change the Instance type to a higher instance type.",
    "isCorrect" : "false"
  }, {
    "id" : "ae1b69ad83e04c499dcdba5f92e2f640",
    "option" : "Add Route 53 health checks to improve the performance.",
    "isCorrect" : "false"
  }, {
    "id" : "c0ceaf016dd34dc2aa2a9ee17ccc741e",
    "option" : "Place the EC2 Instance behind CloudFront.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions the following:\nAmazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files to your users.\nCloudFront delivers your content through a worldwide network of data centers called edge locations.\nWhen a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay) so that content is delivered with the best possible performance.\nFor more information on Amazon CloudFront, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\nOption A is incorrect.\nThe latency issue is experienced by people from certain parts of the world only.\nSo, increasing the number of EC2 Instances or increasing the instance size will not make much difference.\nOption B is incorrect.The latency issue is experienced by people from certain parts of the world only.\nSo, changing the Instance type to a higher instance type will not make much difference.\nOption C is incorrect.\nRoute 53 health checks are meant to see whether the instance status is healthy or not.\nSince this case deals with responding to users' requests, we do not have to worry about this.\nHowever, for improving latency issues, CloudFront is a good solution.\n\nThe best answer to improve response time for users across the globe would be to place the EC2 instance behind CloudFront, which is option D.\nCloudFront is a content delivery network (CDN) service provided by AWS. It caches and delivers content from various AWS services, including EC2 instances, to users from a network of global edge locations. When a user requests content, CloudFront delivers the cached content from the edge location closest to the user, reducing the latency and improving the response time.\nAdding more EC2 instances, as suggested in option A, may help to support the load, but it will not necessarily improve the response time for users in different regions. Additionally, it may add unnecessary cost to the architecture.\nChanging the instance type to a higher instance type, as suggested in option B, may help to improve the performance of the EC2 instance, but it will not necessarily address the issue of extreme slowness reported by users in different regions.\nOption C suggests adding Route 53 health checks to improve performance, but Route 53 health checks are used to monitor the health of resources and redirect traffic in case of a failure. They do not necessarily improve the response time for users in different regions.\nIn summary, placing the EC2 instance behind CloudFront is the best option to improve the response time for users across the globe. CloudFront will deliver cached content from the edge location closest to the user, reducing latency and improving the response time.\n\n"
}, {
  "id" : 128,
  "question" : "You currently have your EC2 instances running in multiple availability zones in an AWS region.\nYou need to create NAT gateways for your private instances to access internet.\nHow would you set up the NAT gateways so that they are highly available?\n",
  "answers" : [ {
    "id" : "849fe80f05644d9b9a5143af5741c4d0",
    "option" : "Create two NAT Gateways and place them behind an EL.",
    "isCorrect" : "false"
  }, {
    "id" : "ae95768eeb69410f9f2aa74d5c8502e8",
    "option" : "Create a NAT Gateway in each Availability Zone.",
    "isCorrect" : "true"
  }, {
    "id" : "c4856c3b05504413aee16eadab17ce4d",
    "option" : "Create a NAT Gateway in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "110fc60da1e345c8bbc5f6dec846cafc",
    "option" : "Use Auto Scaling groups to scale the NAT Gateways.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect because you cannot create such configurations.\nOption B is CORRECT because this is recommended by AWS.\nWith this option, if a NAT gateway's Availability Zone is down, resources in other Availability Zones can still access internet.\nOption C is incorrect because the EC2 instances are in one AWS region so there is no need to create a NAT Gateway in another region.\nOption D is incorrect because you cannot create an Auto Scaling group for NAT Gateways.\nFor more information on the NAT Gateway, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html\n\nTo ensure high availability for NAT gateways, you can create NAT gateways in each availability zone where your private instances reside. This is because NAT gateways are region-specific resources and do not span multiple regions.\nTherefore, option B - \"Create a NAT Gateway in each Availability Zone\" - is the correct answer. This approach ensures that if one NAT gateway or an entire availability zone fails, traffic can automatically failover to another NAT gateway in another availability zone.\nOption A, which suggests placing two NAT gateways behind an Elastic Load Balancer (ELB), is not recommended because NAT gateways do not support load balancing. They are not designed to work behind an ELB.\nOption C - \"Create a NAT Gateway in another region\" - is not ideal because it introduces additional latency and potential network issues, and increases data transfer costs since traffic would have to traverse the public internet between regions.\nOption D - \"Use Auto Scaling groups to scale the NAT Gateways\" - is also not recommended as NAT gateways do not support auto-scaling, and it is not necessary to use auto-scaling for high availability since creating NAT gateways in each availability zone provides sufficient redundancy.\n\n"
}, {
  "id" : 129,
  "question" : "A company wants to have a fully managed data store in AWS.\nIt should be a compatible MySQL database, which is an application requirement.\nWhich of the following AWS database engines has the best throughput?\n",
  "answers" : [ {
    "id" : "456514ffd48343a0b08ed97aa7fb31c4",
    "option" : "AWS RDS MySQL",
    "isCorrect" : "false"
  }, {
    "id" : "448e74de19a14c00aa12302b551217bd",
    "option" : "AWS Aurora",
    "isCorrect" : "true"
  }, {
    "id" : "5bd7cf35dea646cd8bff2e5435054f08",
    "option" : "AWS DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "1acafb02f9dc4e09a7887c12d77a9deb",
    "option" : "AWS Redshift.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following:\nAmazon Aurora (Aurora) is a fully managed, MySQL and PostgreSQL compatible, relational database engine.\nIt combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases.\nIt delivers up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.\nFor more information on AWS Aurora, please refer to the URL below.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Overview.html\nNote:\nRDS is a generic service to provide Relational Database service which supports 6 database engines.\nThey are Aurora, MySQL, MariaDB, PostgreSQL, Oracle, and Microsoft SQL Server.\nAmazon Aurora is a MySQL and PostgreSQL compatible enterprise-class database.\nPlease note that with AWS Aurora, you can get 5X the throughput of standard MySQL and 3X the throughput of standard PostgreSQL.\nHence Option B is the correct answer.\n\nThe AWS database engines mentioned in the question have different characteristics, and the best throughput will depend on the specific workload and use case.\nHowever, based on the information provided in the question, we can make some assumptions and recommendations.\nOption A: AWS RDS MySQL AWS RDS MySQL is a fully managed MySQL relational database service that can be used to run, manage, and scale MySQL databases in the cloud. Since it is a MySQL-compatible database, it meets the application requirement stated in the question.\nMySQL is a widely used open-source relational database management system that is known for its performance and scalability. AWS RDS MySQL offers high availability, automated backups, and seamless scaling, making it a popular choice for many applications.\nIn terms of throughput, AWS RDS MySQL can achieve high performance levels when properly configured and optimized. It supports different instance types and can scale up or down based on demand. However, its maximum throughput will depend on factors such as the workload, instance size, and configuration.\nOption B: AWS Aurora AWS Aurora is a MySQL and PostgreSQL-compatible relational database service that is designed for high performance, scalability, and availability. It is a fully managed service that offers up to five times the throughput of standard MySQL and three times the throughput of standard PostgreSQL.\nAWS Aurora uses a distributed architecture that replicates data across multiple availability zones, providing high availability and durability. It also offers automatic scaling, fast failover, and low latency.\nIn terms of throughput, AWS Aurora is designed to deliver high performance and can handle millions of transactions per second. It can automatically scale up or down based on demand and can handle large datasets with ease.\nOption C: AWS DynamoDB AWS DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed for high throughput and low latency, making it ideal for applications that require low latency and high scalability.\nDynamoDB is a non-relational database, which means that it does not support SQL queries or relational data models. Instead, it uses a key-value data model that allows for fast access and retrieval of data.\nIn terms of throughput, DynamoDB is designed for high performance and can handle millions of requests per second. It automatically scales up or down based on demand and provides low latency for read and write operations.\nOption D: AWS Redshift AWS Redshift is a fully managed data warehouse service that is designed for large-scale data analytics. It is a columnar data store that uses parallel processing and compression to deliver fast query performance on large datasets.\nRedshift is not a MySQL-compatible database, so it does not meet the application requirement stated in the question. However, it can provide high throughput for large-scale analytics workloads.\nIn summary, based on the information provided in the question, the best option for a fully managed MySQL-compatible database with high throughput would be either AWS RDS MySQL or AWS Aurora. Both services can deliver high performance and seamless scalability, and can handle millions of transactions per second. The choice between them will depend on factors such as the specific workload, budget, and desired features.\n\n"
}, {
  "id" : 130,
  "question" : "A Solutions Architect is designing an online shopping application running in a VPC on EC2 Instances behind an Elastic Load Balancer.\nThe instances run in an Auto Scaling group across multiple Availability Zones.\nThe application tier must read and write data to a customer-managed database cluster.\nThere should be no access to the database from the Internet.\nBut the cluster must be able to obtain software patches from the Internet.\nWhich VPC design meets these requirements?\n",
  "answers" : [ {
    "id" : "cbde41bba09f428a8372cfe26b88de31",
    "option" : "Create public subnets for the application tier and the database cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "87b72121c0d04cfc92f2d849527f77e4",
    "option" : "Create public subnets for the application tier and private subnets for the database cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "c52f441af12d42b4a0ab44cbc755a349",
    "option" : "Create public subnets for the application tier and NAT Gateway, and private subnets for the database cluster.",
    "isCorrect" : "true"
  }, {
    "id" : "22944ad95e5c4d9890825ec19a75e1c7",
    "option" : "Create private subnets for the application tier, and private subnets for the database cluster and NAT Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe following diagram from AWS Documentation shows the right setup for this scenario:\nWe always need to keep NAT gateway on public Subnet only, because it needs to communicate the Internet.\nAWS says that \"To create a NAT gateway, you must specify the public subnet in which the NAT gateway should reside.\nYou must also specify an Elastic IP address to associate with the NAT gateway when you create it.\nAfter you've created a NAT gateway, you must update the route table associated with one or more of your private subnets to point Internet-bound traffic to the NAT gateway.\nThis enables instances in your private subnets to communicate with the internet.\"\nNOTE:\nHere the requirement is that \"There should be no access to the database from the Internet, but the cluster must be able to obtain software patches from the Internet.\"\n1) There should be no access to the database from the Internet.\nTo achieve this step, we have to launch the database inside the private subnet.\n2) But the cluster must be able to obtain software patches from the Internet.\nFor this, we have to create NAT Gateway inside the Public Subnet.\nBecause the subnet with the internet gateway attached is known as Public Subnet.\nThrough the NAT Gateway, a database inside the Private subnet can access the internet.\nOption D is saying that \"Use private subnet for NAT gateway\".\nOption C includes these discussed Points, and thus, it's a perfect answer.\nFor more information on this setup, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html\n\n\nThe requirement is to design a VPC to host an online shopping application running on EC2 instances behind an Elastic Load Balancer. The application must read and write data to a customer-managed database cluster. The design should ensure that there is no access to the database from the Internet, but the database cluster must be able to obtain software patches from the Internet.\nOption A: Create public subnets for the application tier and the database cluster. This option creates public subnets for both the application tier and the database cluster, which means that both the application and the database would be publicly accessible. This violates the requirement of having no access to the database from the Internet.\nOption B: Create public subnets for the application tier and private subnets for the database cluster. This option creates public subnets for the application tier, allowing access to the Internet, and private subnets for the database cluster, which ensures that the database is not directly accessible from the Internet. However, this design does not allow the database cluster to obtain software patches from the Internet as there is no direct access to the Internet from the private subnets.\nOption C: Create public subnets for the application tier and NAT Gateway, and private subnets for the database cluster. This option creates public subnets for the application tier, allowing access to the Internet, and private subnets for the database cluster, ensuring that the database is not directly accessible from the Internet. A NAT Gateway is also included in the public subnet, which allows the database cluster to obtain software patches from the Internet. This option meets all the requirements and is a suitable design for the given scenario.\nOption D: Create private subnets for the application tier, and private subnets for the database cluster and NAT Gateway. This option creates private subnets for both the application tier and the database cluster, which means that both the application and the database would not be directly accessible from the Internet. However, this design does not allow the database cluster to obtain software patches from the Internet as there is no direct access to the Internet from the private subnets.\nTherefore, option C, i.e., creating public subnets for the application tier and NAT Gateway and private subnets for the database cluster, is the correct design as it meets all the requirements.\n\n"
}, {
  "id" : 131,
  "question" : "It is expected that only certain specified customers can upload images to the S3 bucket for a certain period of time.\nWhat would you suggest as an architect to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "ae1c822527df469a9618fcea31d8a07d",
    "option" : "Create a secondary S3 bucket. Then, use an AWS Lambda to sync the contents to the primary bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "234a0bda316d42efb845a8fd0dd96210",
    "option" : "Use pre-signed URLs for uploading the images.",
    "isCorrect" : "true"
  }, {
    "id" : "384250e46f974787a9d378dca6d8ee90",
    "option" : "Use ECS Containers to upload the images.",
    "isCorrect" : "false"
  }, {
    "id" : "76cc6af006114fd3a342194f7d124a1b",
    "option" : "Upload the images to SQS and then push them to the S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe S3 bucket owner can create Pre-Signed URLs to upload the images to S3.\nFor more information on Pre-Signed URLs, please refer to the URL below.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\nOption A is incorrect.\nAmazon has provided us with an inbuilt function for this requirement.\nThis option is expensive and time-consuming.As a Solution Architect, you are supposed to pick the best and cost-effective solution.\nOption C is incorrect.\nECS is a highly scalable, fast container management service that makes it easy to run, stop, and manage Docker containers on a cluster.\nOption D is incorrect.\nSQS is a message queue service used by distributed applications to exchange messages through a polling model and not through a push mechanism.\nNote:\nThis question is based on the scenario where we can use the pre-signed URL.\nYou need to understand about pre-signed URL - which contains the user login credentials particular resources, such as S3 in this scenario.\nAnd user must have the permission enabled that other applications can use the credential to upload the data (images) in S3 buckets.\nAWS Definition:\n\"A pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permissions to access that object.\nThat is, if you receive a pre-signed URL to upload an object, you can upload the object only if the creator of the pre-signed URL has the necessary permissions to upload that object.\nAll objects and buckets by default are private.\nThe pre-signed URLs are useful if you want your user/customer to upload a specific object to your bucket, but you don't require them to have AWS security credentials or permissions.\nWhen you create a pre-signed URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time.\nThe pre-signed URLs are valid only for the specified duration.\"\nFor more information, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\n\nTo fulfill the requirement of allowing only certain specified customers to upload images to an S3 bucket for a certain period of time, the best solution would be to use pre-signed URLs for uploading the images. Therefore, option B is the correct answer.\nPre-signed URLs are a secure way of granting temporary access to S3 objects. They are generated by the AWS SDK and allow the owner of the bucket to grant time-limited access to a user without sharing their access keys. Pre-signed URLs are signed with an expiration date and time, so they can only be used for a limited period of time.\nTo implement this solution, you can create an S3 bucket and then create a policy that allows access to specific IAM users or roles. Then, you can generate pre-signed URLs for the specific users and time period that they require access to. These pre-signed URLs can then be sent to the users, who can use them to upload the images to the S3 bucket.\nThis solution is secure, as the pre-signed URLs can only be used for the time period specified, and only by the specific users that have been granted access. Additionally, there is no need to create a secondary S3 bucket or use ECS containers, which can add unnecessary complexity to the solution.\nTherefore, using pre-signed URLs for uploading the images is the best solution to fulfill this requirement.\n\n"
}, {
  "id" : 132,
  "question" : "A company requires to use the AWS RDS service to host a MySQL database.\nThis database will be used for production purposes and is expected to experience a high number of read/write activities.\nWhich EBS volume type would be ideal for this database?\n",
  "answers" : [ {
    "id" : "307f7dd495a24e159fc5dfe7c32cdab2",
    "option" : "General Purpose SSD",
    "isCorrect" : "false"
  }, {
    "id" : "67221af66aa14f9ebd78e4545c1badf7",
    "option" : "Provisioned IOPS SSD",
    "isCorrect" : "true"
  }, {
    "id" : "a6a8cfc4301c47dcbdc438936878630f",
    "option" : "Throughput Optimized HDD",
    "isCorrect" : "false"
  }, {
    "id" : "ff9dc39e599c493781860f0fce87afb0",
    "option" : "Cold HDD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe below snapshot from AWS Documentation shows that the ideal storage option in this scenario is the Provisioned IOPS SSD.\nIt provides a high number of IOPS for the underlying database.\nFor more information on EBS volume types, please refer to the URL below.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\n\n\nFor a production MySQL database that experiences high levels of read/write activities, the ideal EBS volume type would be Provisioned IOPS SSD (B).\nExplanation:\nEBS volumes are the virtual disks that provide persistent storage for Amazon EC2 instances. Amazon RDS is a managed relational database service that enables easy setup, operation, and scaling of a relational database in the cloud. Amazon RDS supports various database engines such as MySQL, PostgreSQL, Oracle, SQL Server, and Amazon Aurora.\nEBS volume types available for use with Amazon RDS include General Purpose SSD, Provisioned IOPS SSD, Throughput Optimized HDD, and Cold HDD.\nGeneral Purpose SSD (A) is ideal for small to medium-sized databases with moderate I/O requirements. It offers a balance of price and performance and is designed for a broad range of workloads. Provisioned IOPS SSD (B) is ideal for mission-critical databases that require consistently high performance and low latency. It provides dedicated IOPS, and the performance can be provisioned and scaled according to the workload. Throughput Optimized HDD (C) is ideal for large, sequential workloads such as log processing and data warehousing. It provides low-cost throughput performance and is optimized for large, sequential I/O operations. Cold HDD (D) is ideal for infrequently accessed data and provides the lowest cost per gigabyte of all EBS volumes. However, it has higher latency and lower throughput than other EBS volume types.\nIn the case of a production MySQL database that experiences high levels of read/write activities, Provisioned IOPS SSD is the best choice because it offers dedicated IOPS and consistent performance, ensuring that the database can handle the workload without any performance degradation.\nTherefore, the correct answer is B. Provisioned IOPS SSD.\n\n"
}, {
  "id" : 133,
  "question" : "You own a MySQL RDS instance in AWS Region us-east-1\nThe instance has a Multi-AZ instance in another availability zone for high availability.\nAs business grows, more and more clients come from Europe (eu-west-2), and most of the database workload is read-only.\nWhat is the proper way to reduce the load on the source RDS instance?\n",
  "answers" : [ {
    "id" : "952c38ceb3334f408af7159ff88cd659",
    "option" : "Create a snapshot of the instance and launch a new instance in eu-west-2.",
    "isCorrect" : "false"
  }, {
    "id" : "1083b3bf94964419834120f17acae072",
    "option" : "Promote the Multi-AZ instance to be a Read Replica and move the instance to eu-west-2 region.",
    "isCorrect" : "false"
  }, {
    "id" : "45bcf4284edf400daa7a64a0c09ba3c9",
    "option" : "Configure a read-only Multi-AZ instance in eu-west-2 as Read Replicas cannot span across regions.",
    "isCorrect" : "false"
  }, {
    "id" : "1a43f7189f634ad0a431f501934f8eb7",
    "option" : "Create a Read Replica in the AWS Region eu-west-2.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nRead Replica should be used to share the read workload of the source DB instance.\nRead Replica can also be configured in a different AWS region.\nRefer to this link-https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html.\nOption A is incorrect: Because Read Replica should be configured to share the read traffic.\nYou should not launch a totally new instance.\nOption B is incorrect: Because a Multi-AZ instance cannot be promoted to be a Read Replica.\nOption C is incorrect: Because a Read Replica can be launched in another region for RDS MySQL.\nOption D is CORRECT: Users can quickly configure a Read Replica in another region.\n\n\nThe best approach to reduce the load on the source RDS instance while catering to the growing number of clients from the Europe region (eu-west-2) with a read-heavy workload is to create a Read Replica in the eu-west-2 region. The correct answer is option D.\nA Read Replica is a feature of Amazon RDS that allows you to create one or more read-only copies of your source DB instance in the same or a different AWS region. These replicas can then be used to offload read traffic from the source DB instance, freeing up resources for other tasks.\nOption A (Create a snapshot of the instance and launch a new instance in eu-west-2) is not a recommended solution for reducing the load on the source RDS instance because it involves setting up a new instance from a snapshot, which can take a long time, and any updates made to the source RDS instance will not be reflected in the new instance.\nOption B (Promote the Multi-AZ instance to be a Read Replica and move the instance to eu-west-2 region) is also not a recommended solution because it involves promoting the Multi-AZ instance to be a Read Replica and then moving it to the eu-west-2 region. However, Read Replicas cannot be moved across regions, so this option is not feasible.\nOption C (Configure a read-only Multi-AZ instance in eu-west-2 as Read Replicas cannot span across regions) is also not a recommended solution because Read Replicas cannot be configured to span across regions. Therefore, this option is not feasible.\nIn conclusion, creating a Read Replica in the eu-west-2 region is the most appropriate solution for reducing the load on the source RDS instance while catering to the growing number of clients from the Europe region (eu-west-2) with a read-heavy workload.\n\n"
}, {
  "id" : 134,
  "question" : "A company has a set of web servers.\nIt is required to ensure that all the logs from these web servers can be analyzed in real-time for any sort of threat detection.\nWhat could be the right choice in this regard?\n",
  "answers" : [ {
    "id" : "974f18f9ecff4f8c91e7d2360dddba04",
    "option" : "Upload all the logs to the SQS Service and then use EC2 Instances to scan the logs.",
    "isCorrect" : "false"
  }, {
    "id" : "262e19b4c45c44cda3b319a5deab887e",
    "option" : "Upload the logs to Amazon Kinesis and then analyze the logs accordingly.",
    "isCorrect" : "true"
  }, {
    "id" : "624526665bae40f8b178c7c08613ab42",
    "option" : "Upload the logs to CloudTrail and then analyze the logs accordingly.",
    "isCorrect" : "false"
  }, {
    "id" : "9566af440d1944939a78ccac7de4e565",
    "option" : "Upload the logs to Glacier and then analyze the logs accordingly.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation provides the following information to support this requirement:\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to process streaming data cost-effectively at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications.\nFor more information on Amazon Kinesis, please refer to the below URL:\nhttps://aws.amazon.com/kinesis/\n\nThe best choice for real-time log analysis of web servers is to use Amazon Kinesis, so the correct answer is B.\nHere's a more detailed explanation:\nOption A: Uploading logs to SQS and using EC2 instances to scan the logs is not a viable solution for real-time analysis. SQS is a message queuing service and does not provide real-time processing capabilities, while EC2 instances would require significant setup and maintenance to perform the analysis.\nOption B: Amazon Kinesis is a managed streaming data service that allows real-time processing of data streams. By uploading the logs to Kinesis, they can be analyzed in real-time for threat detection using Kinesis data analytics, or processed by other AWS services like Lambda or Elasticsearch. Kinesis can scale horizontally to handle large volumes of data and offers high throughput and low latency.\nOption C: CloudTrail is an AWS service that logs API calls made within AWS, providing an audit trail of AWS activity. However, it is not designed for real-time log analysis of web servers.\nOption D: Glacier is an archival storage service for long-term data retention, and is not designed for real-time log analysis.\nIn conclusion, the correct option for real-time log analysis of web servers is to upload the logs to Amazon Kinesis and analyze them accordingly.\n\n"
}, {
  "id" : 135,
  "question" : "You currently have the following architecture in AWS: a.\nA couple of EC2 Instances located in us-west-2a b.\nThe EC2 Instances are launched via an Auto Scaling group. c.The EC2 Instances sit behind a Classic ELB.\nWhich additional step would ensure that the above architecture conforms to a well-architected framework?\n",
  "answers" : [ {
    "id" : "038da9782f60410a8c7c85e9b1373492",
    "option" : "Convert the Classic ELB to an Application EL.",
    "isCorrect" : "false"
  }, {
    "id" : "7db4ed68bd7445688f376616c8bc67e7",
    "option" : "Add an additional Auto Scaling Group.",
    "isCorrect" : "false"
  }, {
    "id" : "95c37df19fdc4d33bf3607e8a151653e",
    "option" : "Add additional EC2 Instances to us-west-2a.",
    "isCorrect" : "false"
  }, {
    "id" : "31ca61cc0efe4698945f9c57c895e9e7",
    "option" : "Add or spread existing instances across multiple Availability Zones.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation provides the following information to support this concept:\nBalancing resources across Availability Zones is a best practice for well-architected applications, as this greatly increases aggregate system availability.\nAuto Scaling automatically balances EC2 instances across zones when you configure multiple zones in your Auto Scaling group settings.\nAuto Scaling always launches new instances such that they are balanced between zones as evenly as possible across the entire fleet.\nFor more information on managing resources with Auto Scaling, please refer to the URL below.\nhttps://aws.amazon.com/blogs/compute/fleet-management-made-easy-with-auto-scaling/\n\nThe AWS Well-Architected Framework is a set of best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. The framework provides a consistent approach for customers and partners to evaluate architectures, and it provides guidance to help implement designs that will scale with your application needs over time.\nBased on the current architecture, the additional step that would ensure the architecture conforms to a well-architected framework is D - Add or spread existing instances across multiple Availability Zones.\nAvailability Zones (AZs) are distinct data centers within a region that are isolated from each other in terms of power, cooling, and network connectivity. Deploying instances across multiple AZs helps protect applications against failures in a single location. In the event of a failure, the application can continue to operate from another AZ without interruption.\nBy adding or spreading existing instances across multiple AZs, you can improve the availability and reliability of the architecture. You can achieve this by adding instances to different AZs or by moving existing instances to different AZs. You can also use Auto Scaling groups to automatically launch instances in different AZs.\nOption A, converting the Classic ELB to an Application ELB, is not directly related to improving the availability or reliability of the architecture. It would improve the ability to route traffic based on application-level information, but it would not provide fault tolerance across AZs.\nOption B, adding an additional Auto Scaling group, would not directly improve the availability or reliability of the architecture. Auto Scaling groups are used to scale instances based on demand, but they do not provide fault tolerance across AZs.\nOption C, adding additional EC2 instances to us-west-2a, would not provide fault tolerance across AZs. In fact, it could potentially decrease availability if all instances are in the same AZ and that AZ experiences a failure.\n\n"
}, {
  "id" : 136,
  "question" : "Your company manages an application that currently allows users to upload images to an S3 bucket.\nThese images are picked up by EC2 Instances for processing and then placed in another S3 bucket.\nYou need an area where the metadata for these images can be stored.\nWhat would be an ideal data store for this?\n",
  "answers" : [ {
    "id" : "0c3dcf126aa1406599bfd62bb34e7fa0",
    "option" : "AWS Redshift",
    "isCorrect" : "false"
  }, {
    "id" : "995df1adbdf44d7288700f02648e4dd5",
    "option" : "AWS Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "2d389bb56fa8493a9767f5a634a81806",
    "option" : "AWS DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "ce30c477243b4cb5aa4bacde369a493c",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect because this is normally used for petabyte based storage.\nOption B is incorrect because this is used for archive storage.\nOption C is correct.\nAWS DynamoDB is the best, light-weight and durable storage option for metadata.\nOption D is incorrect because this is used for messaging purposes.\nFor more information on DynamoDB, please refer to the URL below.\nhttps://aws.amazon.com/dynamodb/\n\nThe ideal data store for storing metadata about images in this scenario would be AWS DynamoDB, option C.\nDynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is designed to handle large volumes of data and can easily accommodate the metadata for the uploaded images.\nIn this scenario, DynamoDB could be used to store metadata such as image names, upload times, file sizes, and any other relevant information about the images. The EC2 instances could then access this metadata from DynamoDB during image processing.\nOption A, AWS Redshift, is a data warehouse that is optimized for analyzing large datasets. While Redshift could technically store the metadata, it is overkill for this scenario and would be unnecessarily complex and expensive.\nOption B, AWS Glacier, is an archival storage service that is optimized for data that is rarely accessed. It is not designed to handle frequent read and write operations, making it a poor choice for storing metadata that will be accessed frequently.\nOption D, AWS SQS, is a message queue service that allows decoupling and asynchronous processing between different components of an application. While it could potentially be used to store metadata as messages, it is not an ideal data store for this scenario, as it is primarily designed for messaging and not data storage.\n\n"
}, {
  "id" : 137,
  "question" : "An application team needs to quickly provision a development environment consisting of a web and database layer.\nWhat would be the quickest and most ideal way to get this set up in place?\n",
  "answers" : [ {
    "id" : "d7e17d0ae7ce462ba3ab3b0d8d9ad93b",
    "option" : "Create Spot Instances and install the web and database components.",
    "isCorrect" : "false"
  }, {
    "id" : "2dcbaf24aaf34bca94cb75d7ba7a94ba",
    "option" : "Create Reserved Instances and install the web and database components.",
    "isCorrect" : "false"
  }, {
    "id" : "2e19790539f346e797c90a63edc2d3b7",
    "option" : "Use AWS Lambda to create the web components and AWS RDS for the database layer.",
    "isCorrect" : "false"
  }, {
    "id" : "b16cb16c8b874145a3390f7d3b9ac09d",
    "option" : "Use Elastic Beanstalk to quickly provision the environment.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions the following:\nWith Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the infrastructure that runs those applications.\nAWS Elastic Beanstalk reduces management complexity without restricting choice or control.\nYou simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.\nFor more information on AWS Elastic Beanstalk, please refer to the URL below.\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html\nOption A is incorrect.\nAmazon EC2 Spot instances are spare compute capacity in the AWS cloud available to you at steep discounts compared to On-Demand prices.\nOption B is incorrect.\nA Reserved Instance is a reservation of resources and capacity for either one or three years for a particular Availability Zone within a region.\nOption C is incorrect.\nAWS Lambda is a compute service that makes it easy for you to build applications that respond quickly to new information and not for provisioning a new environment.\nCurrently, the Elastic Beanstalk environment supports the following configurations:\nIt supports RDS.\nDatabase Configuration Setting.\nAWS Elastic Beanstalk provides connection information to your instances by setting environment properties for the database hostname, username, password, table name, and port.\nWhen you add a database to your environment, its lifecycle is tied to your environments.\n\n\nD. Use Elastic Beanstalk to quickly provision the environment.\nElastic Beanstalk is the quickest and most ideal way to set up an environment consisting of a web and database layer. Elastic Beanstalk is a fully-managed service that makes it easy to deploy and run web applications, and it handles the deployment, scaling, and monitoring of the application environment. Elastic Beanstalk supports a variety of programming languages and web frameworks.\nWhen you use Elastic Beanstalk to provision an environment, you can choose from a variety of pre-configured environments that include both the web and database components. You can also customize the environment to meet your specific needs. Elastic Beanstalk can also automatically scale your environment to handle changes in traffic.\nUsing Spot Instances or Reserved Instances may require more manual configuration and setup time, as you would need to install the web and database components yourself. Spot Instances are a good option if you want to save money on your infrastructure costs, but they can be less reliable than On-Demand Instances, which are used by Elastic Beanstalk. Reserved Instances are a good option if you know you will need a specific amount of capacity over a longer period of time.\nUsing AWS Lambda and AWS RDS can be a good option for certain types of applications, but it may not be the most ideal option for a web and database environment that needs to be quickly provisioned. AWS Lambda is best suited for event-driven applications, while RDS is a good option for managing a relational database. However, setting up these services may require more configuration and development effort than using Elastic Beanstalk.\n\n"
}, {
  "id" : 138,
  "question" : "Third-party sign-in (Federation) has been implemented in your web application to allow users who need access to AWS resources.\nUsers have been successfully logging in using Google, Facebook, and other third-party credentials.\nSuddenly, their access to some AWS resources has been restricted.\nWhat is the most likely cause of the restricted use of AWS resources?\n",
  "answers" : [ {
    "id" : "71ec54b9f15a433480db772890148915",
    "option" : "IAM policies for resources were changed, thereby restricting access to AWS resources.",
    "isCorrect" : "true"
  }, {
    "id" : "983da0c4f928429286bed6965177bb5a",
    "option" : "Federation protocols are used to authorize services and need to be updated.",
    "isCorrect" : "false"
  }, {
    "id" : "00bd652e57444582b2240605ad7e1541",
    "option" : "IAM groups for accessing the AWS resources were changed, thereby restricting their access via federated login.",
    "isCorrect" : "false"
  }, {
    "id" : "6488cede168b4957b9d20377f93680d4",
    "option" : "The identity providers no longer allow access to AWS services.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nUsers of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP.\nThey can receive an authentication token, and then exchange that token for temporary security credentials in AWS that map to an IAM role with permissions to use the resources in your AWS account.\nOption A is correct.\nWhen IAM policies are changed, they can impact the user experience and services they can connect to.\nOption B is incorrect.\nFederation is used to authenticate users, not to authorize services.\nOption C is incorrect.\n\"IAM groups for accessing the AWS resources\" is NOT the correct usage, as IAM role(s) must be attached.\nOption D is incorrect.\nThe identity providers don't have the capability to authorize services; they authenticate users.\nReferences:\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-identity-federation.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html\n\nThe most likely cause of the restricted use of AWS resources in this scenario is A. IAM policies for resources were changed, thereby restricting access to AWS resources.\nFederation allows users to access AWS resources using their existing credentials from third-party identity providers such as Google and Facebook. However, once a user has been authenticated, their access to AWS resources is controlled by IAM policies. IAM policies specify which AWS resources a user can access and what actions they can perform on those resources.\nIf the IAM policies for resources have been changed, access to AWS resources could be restricted for the federated users. For example, if the IAM policy for an S3 bucket is updated to deny access to a specific group or user, then that group or user, even if authenticated via federation, will no longer be able to access the S3 bucket.\nOption B is not correct as the federation protocols themselves are not used to authorize services, they are used to authenticate users. Option C is also not the most likely cause of the issue since IAM groups are not directly tied to federated logins. IAM groups are used to manage permissions and access to AWS resources for IAM users. Option D is not correct as it assumes that the identity providers are blocking access to AWS services, which is not likely unless there is an issue with the specific identity provider configuration.\n\n"
}, {
  "id" : 139,
  "question" : "A company has an application that stores images and thumbnails on S3\nThe thumbnail needs to be available for download immediately.\nAdditionally, both the images and thumbnails are not accessed frequently.\nWhat would be the cost-efficient storage option that meets the above-mentioned requirements?\n",
  "answers" : [ {
    "id" : "dd4dd8de41814bcb90e7fe92365706f2",
    "option" : "Amazon Glacier with Expedited Retrievals.",
    "isCorrect" : "false"
  }, {
    "id" : "b3c8e674ab514c97a42920774a5b741f",
    "option" : "Amazon S3 Standard Infrequent Access",
    "isCorrect" : "true"
  }, {
    "id" : "9692647559754d418534a3eafc74749e",
    "option" : "Amazon EFS",
    "isCorrect" : "false"
  }, {
    "id" : "f215c2450a3448fba88b50eee5464d57",
    "option" : "Amazon S3 Standard.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon S3 Infrequent access is perfect if you want to store data that is not frequently accessed.\nIt is more cost-effective than Option D (Amazon S3 Standard)\nIf you choose Amazon Glacier with Expedited Retrievals, you defeat the whole purpose of the requirement because of its increased cost.\nFor more information on AWS Storage Classes, please visit the following URL:\nhttps://aws.amazon.com/s3/storage-classes/\n\nThe most cost-efficient storage option that meets the above-mentioned requirements is Amazon S3 Standard Infrequent Access (IA).\nAmazon S3 Standard IA is designed for infrequently accessed data, but when accessed, it needs to be available immediately. It provides the same low latency and high throughput performance as Amazon S3 Standard, but with lower storage costs and a retrieval fee.\nIn this case, since the images and thumbnails are not accessed frequently but need to be available immediately when accessed, Amazon S3 Standard IA is the best option. This option allows the company to save on storage costs since the data is infrequently accessed, but still provides fast access to the data when needed.\nAmazon Glacier with Expedited Retrievals (Option A) is designed for data that is rarely accessed and can tolerate retrieval times of several minutes to hours. It is a much cheaper option than S3 Standard IA, but it does not meet the requirement of immediate availability when the thumbnail needs to be downloaded.\nAmazon EFS (Option C) is a scalable, highly available, and fully managed file system for use with Amazon EC2 instances. It is not designed for storing objects like images and thumbnails, but rather for storing files that need to be accessed concurrently by multiple instances.\nAmazon S3 Standard (Option D) is designed for frequently accessed data and provides low latency and high throughput performance, but it is not the most cost-efficient option for infrequently accessed data like the images and thumbnails in this scenario.\nIn summary, Amazon S3 Standard Infrequent Access is the most cost-efficient option that meets the company's requirements of immediate availability when the thumbnail needs to be downloaded and infrequent access of both the images and thumbnails.\n\n"
}, {
  "id" : 140,
  "question" : "A security audit discovers that one of your RDS MySQL instances is not encrypted.\nThe instance has a Read Replica in the same AWS region which is also not encrypted.\nYou need to fix this issue as soon as possible.\nWhat is the proper way to add encryption to the instance and its replica?\n",
  "answers" : [ {
    "id" : "460fe9663d9a43dbb2e0e48765be63f5",
    "option" : "Create a DB snapshot from the instance. Copy the DB snapshot with encryption enabled. Restore a new DB instance from the new encrypted snapshot and configure a Read Replica in the new DB instance.",
    "isCorrect" : "true"
  }, {
    "id" : "fb50e8f5dae14acab7b8cc700f1e49a3",
    "option" : "Encrypt the DB instance. Launch a new Read Replica and the replica is encrypted automatically.",
    "isCorrect" : "false"
  }, {
    "id" : "e529545504cf47068afdaca0ca99d49e",
    "option" : "Create a DB snapshot from the RDS instance and encrypt the newly-created snapshot. Launch a new instance and its Read Replica from the snapshot.",
    "isCorrect" : "false"
  }, {
    "id" : "95aa785185744f399948c1e03d1048e0",
    "option" : "Promote the Read Replica to be a standalone instance and encrypt it. Add a new Read Replica to the standalone instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nExisting unencrypted RDS instances and their snapshots cannot be encrypted.\nUsers can only enable encryption for an RDS DB instance when they create it.\nThe limitations can be found in the AWS documentation https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html.\nOption A is CORRECT: Because you can encrypt a copy of an unencrypted DB snapshot.\nThen the new RDS instance launched from the snapshot and the Read Replica of the new DB instance are encrypted.\nOption B is incorrect: Because you cannot encrypt an unencrypted RDS instance directly.\nOption C is incorrect: Because you cannot encrypt an unencrypted DB snapshot directly according to the above reference.\nYou should create a copy of an unencrypted DB snapshot and then encrypt the new snapshot.\nOption D is incorrect: Because an unencrypted DB Read Replica cannot be encrypted.\nThe correct method is to launch a new instance from an encrypted DB snapshot.\n\nThe correct answer is A. Create a DB snapshot from the instance. Copy the DB snapshot with encryption enabled. Restore a new DB instance from the new encrypted snapshot and configure a Read Replica in the new DB instance.\nExplanation: Amazon RDS provides encryption at rest for all database engines. By default, RDS encrypts the data stored in the database instance, including its backups and Read Replicas. However, if the database instance was created without encryption, you can still add encryption to the instance and its Read Replica.\nTo add encryption to the RDS instance and its Read Replica, you should follow these steps:\nCreate a DB snapshot from the non-encrypted RDS instance using the AWS Management Console, AWS CLI, or API. This snapshot will contain a copy of the instance's data. Copy the DB snapshot with encryption enabled using the AWS Management Console, AWS CLI, or API. This will create a new DB snapshot with encryption enabled. Restore a new DB instance from the new encrypted snapshot. The new instance will have the same configuration, security groups, parameter groups, and option groups as the original instance, but with encryption enabled. Once the new instance is available, configure a Read Replica in the new DB instance using the AWS Management Console, AWS CLI, or API. The Read Replica will also have encryption enabled.\nOption B is incorrect because encrypting the instance will not automatically encrypt the Read Replica. You need to launch a new Read Replica from the encrypted DB instance.\nOption C is incorrect because encrypting the DB snapshot does not encrypt the original instance or its Read Replica. You need to restore a new DB instance from the encrypted snapshot to enable encryption.\nOption D is incorrect because promoting the Read Replica to a standalone instance will not encrypt the instance. You need to encrypt the instance before creating a new Read Replica.\n\n"
}, {
  "id" : 141,
  "question" : "You have an application hosted on AWS consisting of EC2 Instances launched via an Auto Scaling Group.\nYou notice that the EC2 Instances are not scaling when the traffic is high.\nWhich check should be done to ensure that the scaling occurs as expected?\n",
  "answers" : [ {
    "id" : "0984166ae3964ff9a3a1891892e6b0f9",
    "option" : "Ensure that the right metrics are being used to trigger the scaling.",
    "isCorrect" : "true"
  }, {
    "id" : "c89daea7e06a4ed78e0bf6a3dad9e80b",
    "option" : "Check your scaling policies to see whether more than one policy is triggered by an event.",
    "isCorrect" : "false"
  }, {
    "id" : "6e21410a00aa41348a711585e7283c37",
    "option" : "Ensure that AutoScaling health checks are being used.",
    "isCorrect" : "false"
  }, {
    "id" : "1a9abe706c79407685d73062ec6f4fbb",
    "option" : "Ensure that you are using Load Balancers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThere could be several reasons mentioned in AWS Documentation.\nBut only options A and B are applicable from the given choices.\nOption A is correct.\nIf your scaling events are not based on the right metrics and do not have the right threshold defined, the scaling will not occur as you want it to happen.\nOption B is incorrect because if two policies are executed at the same time, Amazon EC2 Auto Scaling follows the policy with the greater impact.\nFor example, if you have one policy to add two instances and another policy to add four instances, Amazon EC2 Auto Scaling adds four instances when both policies are triggered simultaneously.\nIn this question, the EC2 instances are not scaling at all so this option is incorrect.\nOption C is incorrect because health checks will help us know the health status of an Auto Scaling instance.\nIt is not a Check if AutoScaling is not working as expected.\nIt is a health check for EC2 instances.\nOption D is incorrect because AutoScaling can be used without Load Balancer also.\nFor more information on Auto Scaling Dynamic Scaling and troubleshooting, please visit the following URLs:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/auto-scaling-troubleshooting/\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ts-as-instancelaunchfailure.html\n\nWhen an application hosted on AWS consists of EC2 instances launched via an Auto Scaling Group, the scaling is triggered by certain metrics that are monitored by AWS services. If the EC2 instances are not scaling when the traffic is high, it could be due to various reasons such as incorrect metrics, incorrect scaling policies, or other configuration issues.\nTo ensure that the scaling occurs as expected, the following check should be performed:\nA. Ensure that the right metrics are being used to trigger the scaling: The metrics that are being used to trigger scaling should be appropriate for the application. These metrics could be related to CPU utilization, network traffic, disk usage, or other performance indicators. It is important to ensure that the right metrics are being monitored and the appropriate thresholds are set to trigger scaling.\nB. Check your scaling policies to see whether more than one policy is triggered by an event: If more than one scaling policy is triggered by an event, it could lead to conflicting actions. For example, if one policy scales up and another policy scales down, the scaling group may not scale at all. Therefore, it is important to ensure that only one policy is triggered by an event.\nC. Ensure that AutoScaling health checks are being used: AutoScaling health checks are used to ensure that only healthy instances are part of the scaling group. If a scaling group contains unhealthy instances, it may not scale up as expected. Therefore, it is important to ensure that AutoScaling health checks are being used.\nD. Ensure that you are using Load Balancers: Load Balancers are used to distribute traffic across multiple instances. If an application is not using Load Balancers, it may not scale up as expected, as the traffic is not distributed across multiple instances.\nIn conclusion, to ensure that the scaling occurs as expected, it is important to monitor the appropriate metrics, ensure that only one scaling policy is triggered by an event, use AutoScaling health checks, and use Load Balancers.\n\n"
}, {
  "id" : 142,
  "question" : "A Media firm Firm_A uses AWS infrastructure and has a global presence for its sports programming &amp; broadcasting network.\nIt uses AWS Organization to manage multiple AWS accounts.\nRecently it was acquired by Firm_B which also uses AWS Infrastructure.\nFirm_B also has its own sets of AWS accounts.\nAfter the merger, AWS Accounts of both organizations need to merge to create &amp; manage policies more effectively. As an AWS Consultant, which of the following steps would you suggest to the client to move the master account of the original media firm to the organization used by the merged entity? (Select THREE.)\n",
  "answers" : [ {
    "id" : "c2548b12244c4df88c96f3c1a3c6a2d5",
    "option" : "Remove all member accounts from the organization in Firm_",
    "isCorrect" : "true"
  }, {
    "id" : "01d11c3e5d784c9c878aa058a6286e02",
    "option" : "Configure another member account as the master account in the Firm_A organization.",
    "isCorrect" : "false"
  }, {
    "id" : "60e056e4c9dc4ea69f2e54d20c0243fd",
    "option" : "Delete the organization in Firm_",
    "isCorrect" : "true"
  }, {
    "id" : "c8e08e91748844aca28cab64a58668da",
    "option" : "Invite the Firm_A master account to join the new organization (Firm_B) as a member account.",
    "isCorrect" : "true"
  }, {
    "id" : "2a6ddaec2b1f44a1b3af0a4ced245026",
    "option" : "Invite the Firm_A master account to join the new organization (Firm_B) as a master account.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A, C, D.\nTo move the master account from one organization to another organization, the following steps need to be implemented:\nÂ· Remove all member accounts from the old organization.\nÂ· Delete the old organization.\nÂ· Invite the master account of the old organization to be a member account of the new organization.\nOption B is incorrect as the master account of an AWS organization cannot be replaced by another member account.\nOption E is incorrect as a master account will be joining as a member account of the new organization, not as a master account.\nFor more information on migrating accounts between AWS organizations, refer to the following URL:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/\n\nThe merger of two organizations with separate AWS accounts requires a consolidation of resources to ensure effective management and administration of the merged infrastructure. To move the master account of the original media firm to the organization used by the merged entity, the following steps need to be taken:\nRemove all member accounts from the organization in Firm_A - This is not a recommended step as it will remove all member accounts and the resources associated with them. It will lead to the loss of access to these resources and can cause business disruptions. Therefore, this option is not suitable. Configure another member account as the master account in the Firm_A organization - This step is also not recommended as it can cause further complications. Configuring another member account as the master account in the Firm_A organization would lead to the loss of all the AWS resources, policies, and account information associated with the original master account. This option is not suitable. Delete the organization in Firm_A - Deleting the organization in Firm_A is not recommended as it will lead to the loss of all member accounts, AWS resources, policies, and account information associated with the organization. This option is not suitable. Invite the Firm_A master account to join the new organization (Firm_B) as a member account - This is a recommended step as it allows the Firm_A master account to join the new organization as a member account. Once the account is added to the new organization, all the resources associated with the account will become part of the new organization, and the policies can be managed more effectively. Invite the Firm_A master account to join the new organization (Firm_B) as a master account - This is also a recommended step as it allows the Firm_A master account to join the new organization as a master account. Once the account is added to the new organization, it can become the master account for the new organization, and all the policies can be managed more effectively.\nTherefore, the recommended steps to move the master account of the original media firm to the organization used by the merged entity are to invite the Firm_A master account to join the new organization (Firm_B) as a member or a master account, depending on the needs of the organization.\n\n"
}, {
  "id" : 143,
  "question" : "Your company has designed an app and requires it to store data in DynamoDB.\nThe company has registered the app with identity providers for users to sign-in using third-parties like Google and Facebook.\nWhat must be in place such that the app can obtain temporary credentials to access DynamoDB?\n",
  "answers" : [ {
    "id" : "57a078bf76bc4fbb805056cbc4d64d65",
    "option" : "Multi-factor authentication must be used to access DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "4675eef795b84591a4264b0a95c7b258",
    "option" : "AWS CloudTrail needs to be enabled to audit usage.",
    "isCorrect" : "false"
  }, {
    "id" : "16c72fe3bff74bb6aaf06d2241697497",
    "option" : "An IAM role allowing the app to have access to DynamoD.",
    "isCorrect" : "true"
  }, {
    "id" : "fa0adf20b5a147f19b841471b078cb43",
    "option" : "The user must additionally log into the AWS console to gain database access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption C is correct.\nThe user will have to assume a role that has the permissions to interact with DynamoDB.\nOption A is incorrect.\nMulti-factor authentication is available but not required.\nOption B is incorrect.\nCloudTrail is recommended for auditing but is not required.\nOption D is incorrect.\nA second log-in event to the management console is not required.\nReferences:\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-identity-federation.html\nttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc.html.\nhttps://aws.amazon.com/articles/web-identity-federation-with-mobile-applications/\n\nThe correct answer to this question is C - An IAM role allowing the app to have access to DynamoDB.\nExplanation:\nDynamoDB is a fully managed NoSQL database service provided by AWS that supports document and key-value data models. It is designed to provide fast and predictable performance with seamless scalability. When building applications that require access to DynamoDB, it is important to ensure that access is secured and properly authenticated.\nIn this scenario, the company has registered the app with identity providers for users to sign-in using third-party services like Google and Facebook. This means that the app is using an identity federation solution to authenticate users, which allows users to sign in to the app using their existing social media or email accounts.\nTo enable the app to access DynamoDB, an IAM role must be created that allows the app to obtain temporary security credentials. The role should be configured to trust the identity providers that are used to authenticate users. This will allow the app to obtain temporary security credentials that can be used to access DynamoDB without requiring users to sign in to the AWS Management Console.\nOption A, Multi-factor authentication (MFA), is a method of authentication that requires users to provide two or more forms of identification. While MFA can enhance security, it is not a requirement for accessing DynamoDB.\nOption B, AWS CloudTrail, is a service that provides visibility into user activity by recording AWS API calls. While CloudTrail can be used to monitor usage of DynamoDB, it is not a requirement for accessing DynamoDB.\nOption D, requiring users to log in to the AWS Management Console to access DynamoDB, is not ideal as it adds complexity for end users and could lead to potential security vulnerabilities.\nTherefore, the correct answer is C - An IAM role allowing the app to have access to DynamoDB.\n\n"
}, {
  "id" : 144,
  "question" : "A company has an entire infrastructure hosted on AWS.\nIt requires to create code templates used to provide the same set of resources in another region in case of a disaster in the primary region.\nWhich AWS service can be helpful in this regard?\n",
  "answers" : [ {
    "id" : "87119fbfff1041ae952f7b92361f5850",
    "option" : "AWS Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "c846eec431a54b4296639c4fb16d04fa",
    "option" : "AWS CloudFormation",
    "isCorrect" : "true"
  }, {
    "id" : "aac8e93d5eae4e29ba9a1c0d3c8a7fee",
    "option" : "AWS CodeBuild",
    "isCorrect" : "false"
  }, {
    "id" : "6244121c138146499721f91b70bc16db",
    "option" : "AWS CodeDeploy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation provides the following information to support this requirement:\nAWS CloudFormation provisions your resources in a safe and repeatable manner, allowing you to build and rebuild your infrastructure and applications, without having to perform manual actions or write custom scripts.\nCloudFormation takes care of determining the right operations to perform while managing your stack and rolls back changes automatically if errors are detected.\nFor more information on AWS CloudFormation, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\nAWS Beanstalk is an orchestration service for deploying applications that orchestrate various AWS Services, including EC2, S3, SNS, CloudWatch, AutoScaling, and ELB.https://aws.amazon.com/elasticbeanstalk/\nAWS CodeBuild is a fully managed continuous integration(CI) service that compiles source code, run tests, and produces software packages that are ready to deploy.\nUsing it, you don't need to provision, manage, and scale your own build servers.\nhttps://aws.amazon.com/codebuild/\nAWS CodeDeploy is a service that automates application deployments to various computing services, including EC2, Fargate, Lambda, and on-premises instances.\nIt protects your application from downtime during deployments through rolling updates and deployment health tracking.\nhttps://aws.amazon.com/codedeploy/\n\nThe AWS service that can be helpful in this scenario is AWS CloudFormation (Option B).\nAWS CloudFormation is an Infrastructure as Code (IaC) service that allows developers to create and manage a collection of AWS resources as code templates. CloudFormation templates are written in either JSON or YAML format and can be version controlled just like any other code.\nIn this scenario, the company can create a CloudFormation template for their entire infrastructure in the primary region, including EC2 instances, RDS databases, load balancers, security groups, etc. This CloudFormation template can then be used to create an identical infrastructure in another region, providing redundancy and disaster recovery capabilities.\nIf a disaster occurs in the primary region, the company can simply run the CloudFormation template in the secondary region to recreate the entire infrastructure. This saves time and effort compared to manually creating the infrastructure in another region and ensures that the infrastructure is consistent across both regions.\nAWS Beanstalk (Option A) is a service that simplifies the deployment and management of applications in the AWS Cloud. It is not suitable for creating and managing an entire infrastructure.\nAWS CodeBuild (Option C) is a fully-managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. It is not relevant to this scenario as it does not create or manage AWS resources.\nAWS CodeDeploy (Option D) is a service that automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. It is not relevant to this scenario as it does not create or manage AWS resources at scale.\n\n"
}, {
  "id" : 145,
  "question" : "You are working as an AWS Architect for a start-up company.\nThe company has web-servers deployed in all AZ's in the AWS eu-central-1 (Frankfurt) region.\nThese web servers provide news for users from Germany.\nThe application is deployed in multiple EC2 servers with several static IP addresses and you need to create a record set for the application.\nHow would you configure the record set in Route 53?\n",
  "answers" : [ {
    "id" : "3e35bd1dcdba47d898c8b38afe5482ad",
    "option" : "Latency routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "409e6de827174fe58e0bf9fa0618833b",
    "option" : "Weighted routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "b942d23d25ad4ce3801e71ea473b7fb7",
    "option" : "Multivalue answer routing policy",
    "isCorrect" : "true"
  }, {
    "id" : "1ae84e4ac17a4f658d8ca30bad16d39b",
    "option" : "Geolocation routing policy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nWhen Route 53 is configured with Multi-value answer routing, it returns multiple values for web-servers.\nRoute 53 responds to DNS queries with up to eight healthy records.\nTraffic is approximately load-balanced between these multiple web-servers.\nOption A is incorrect.\nLatency routing policy is used when multiple resources are mapped with a single domain &amp; the resource with the best latency to the resource is provided.\nSince most of the time, these servers will be accessing locally from the German region, latency to the web servers will be approximately the same.\nOption B is incorrect.\nWeighted routing policy is used when multiple resources are mapped with a single domain &amp; you need to route traffic in a weighted proportionate to each resource.\nAs in this case, the requirement is to use all web servers randomly, the weighted routing policy will not be an ideal option.\nOption D is incorrect as the Geolocation routing policy is used to choose resources based upon the user's location.\nIn this case, all users will be Germany-based.\nSo there would not be random selection on the resource.\nFor more information on using Multi-value Answer Routing for Route 53, refer to the following URL:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-multivalue\n\nGiven the scenario, the objective is to create a record set in Route 53 for an application that is deployed in multiple EC2 servers with several static IP addresses, which provides news for users from Germany in all availability zones of the eu-central-1 (Frankfurt) region.\nThe correct routing policy to choose in this scenario would be the Geolocation routing policy, which allows routing traffic based on the geographic location of the end-user. This policy can be configured to route traffic based on the country, continent, or state/region of the user.\nHere, since the target audience is in Germany, the Geolocation routing policy would be the most appropriate option to route traffic to the web servers deployed in the eu-central-1 (Frankfurt) region. The policy can be configured to route traffic to the nearest available web server to provide the best user experience.\nThe other routing policies available in Route 53 are:\nA. Latency routing policy: This policy routes traffic to the AWS region that provides the lowest latency to the end-user. This policy is useful when the application needs to be highly responsive and latency is a critical factor.\nB. Weighted routing policy: This policy is used to distribute traffic across multiple resources in proportion to their respective weights. This policy is useful when you want to distribute traffic evenly across different resources.\nC. Multivalue answer routing policy: This policy returns multiple IP addresses for a single DNS name. It can be used to spread traffic across multiple resources and provide fault tolerance.\nIn conclusion, the correct routing policy to configure the record set in Route 53 for the scenario described would be the Geolocation routing policy, which allows routing traffic based on the geographic location of the end-user, providing the best user experience.\n\n"
}, {
  "id" : 146,
  "question" : "Your recent security review revealed a large spike in attempted logins to your AWS account.\nFor sensitive data stored in encryption enabled S3, the data has not been encrypted and is susceptible to fraud if it was to be stolen.\nYou've recommended AWS Key Management Service as a solution.\nWhich of the following is true regarding the server-side encryption of KMS?\n",
  "answers" : [ {
    "id" : "fdfefb9d4aa54c98ada2b86523f78a46",
    "option" : "Only KMS generated keys can be used to encrypt or decrypt data.",
    "isCorrect" : "false"
  }, {
    "id" : "4f122591278f4a77ae535a873521d13d",
    "option" : "Data is encrypted at rest with KMS.",
    "isCorrect" : "true"
  }, {
    "id" : "6593a630d46949dea750d2892dfab0a3",
    "option" : "KMS allows all users and roles to use the keys by default.",
    "isCorrect" : "false"
  }, {
    "id" : "77d33056a650426db581b5ee302458e8",
    "option" : "Data is encrypted in transit with the KMS key.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption B is correct.\nData is encrypted at rest once uploaded to S3.\nOption A is incorrect.\nData can be encrypted/decrypted using AWS keys or keys provided by your company.\nOption C is incorrect.\nUsers are granted permissions explicitly, not by default by KMS.\nOption D is incorrect.\nKMS is used for the encryption at rest instead of in transit.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\nttps://d1.awsstatic.com/whitepapers/AWS_Securing_Data_at_Rest_with_Encryption.pdf.\nhttps://aws.amazon.com/kms/faqs/\nhttps://docs.aws.amazon.com/general/latest/gr/rande.html#kms_region\nhttps://www.slideshare.net/AmazonWebServices/encryption-and-key-management-in-aws\n\nSure, I'd be happy to help!\nRegarding the server-side encryption of KMS, let's take a closer look at each of the options:\nA. Only KMS generated keys can be used to encrypt or decrypt data.\nThis statement is true. KMS allows users to create and manage customer master keys (CMKs), which are used to encrypt and decrypt data. When using KMS to encrypt S3 data, only KMS-generated keys can be used.\nB. Data is encrypted at rest with KMS.\nThis statement is partially true. While KMS does not directly encrypt data at rest, it allows users to encrypt data with KMS-generated keys. The data itself is encrypted by the service that stores the data, such as S3.\nC. KMS allows all users and roles to use the keys by default.\nThis statement is false. KMS provides fine-grained access control to CMKs, allowing users and roles to be granted specific permissions to use the keys. By default, no one has access to a CMK.\nD. Data is encrypted in transit with the KMS key.\nThis statement is false. KMS is not involved in encrypting data in transit. Instead, SSL/TLS is used to encrypt data as it travels over the network.\nIn summary, the correct answer is A, which states that only KMS-generated keys can be used to encrypt or decrypt data.\n\n"
}, {
  "id" : 147,
  "question" : "Your company has a set of EC2 Instances hosted in AWS.\nIt is mandatory to prepare for regional disasters and come up with the necessary disaster recovery procedures.\nWhat would help to mitigate the effects of a disaster for the EC2 Instances?\n",
  "answers" : [ {
    "id" : "9ee837206cea4dd9b6f8f241ca26a919",
    "option" : "Place an ELB in front of the EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "6cee0ca0ce8e4c2a859bdc600dbd5718",
    "option" : "Use Auto Scaling to ensure that the minimum number of instances are always running.",
    "isCorrect" : "false"
  }, {
    "id" : "b883c484fe1843a8bff740cc72d5468c",
    "option" : "Use CloudFront in front of the EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "e13e685c3f4c451f8d1ef45eee6a3c77",
    "option" : "Create AMIs from the EC2 Instances. Use them to recreate the EC2 Instances in another region.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nYou can create an AMI from the EC2 Instances and then copy them to another region.\nIn case of a disaster, an EC2 Instance can be created from the AMI.\nOptions A and B are good for fault tolerance.\nBut they cannot help completely in disaster recovery for the EC2 Instances.\nOption C is incorrect because we cannot determine if CloudFront would be helpful in this scenario or not without knowing what is hosted on the EC2 Instance.\nFor disaster recovery, we have to make sure that we can launch instances in another region when required.\nFor more information on AWS AMIs, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nIn the event of a regional disaster, the availability of your EC2 instances hosted in that region may be affected, causing downtime or even data loss. To mitigate the impact of such an event, you need to prepare a disaster recovery plan that includes replicating your data and infrastructure to a different region.\nThe most effective way to mitigate the effects of a disaster for your EC2 instances is to create Amazon Machine Images (AMIs) from them and use them to recreate the EC2 instances in another region. This ensures that you have a backup of your infrastructure, and can quickly restore it in a different region in the event of a disaster.\nOption A, placing an Elastic Load Balancer (ELB) in front of the EC2 instances, would help distribute traffic across multiple instances, but it would not necessarily mitigate the effects of a disaster.\nOption B, using Auto Scaling to ensure that the minimum number of instances are always running, would help maintain availability but would not help in case of a disaster, as the instances would still be located in the same region.\nOption C, using CloudFront in front of the EC2 instances, would help cache content and improve performance but would not mitigate the effects of a disaster.\nTherefore, the correct answer is D, creating AMIs from the EC2 instances and using them to recreate the instances in another region.\n\n"
}, {
  "id" : 148,
  "question" : "A company currently hosts a Redshift cluster in AWS.\nIt should ensure that all traffic from and to the Redshift cluster does not go through the Internet for security reasons.\nWhich features can be used to fulfill this requirement in an efficient manner?\n",
  "answers" : [ {
    "id" : "0f3f21d236524c1681dcfec1437c8445",
    "option" : "Enable Amazon Redshift Enhanced VPC Routing.",
    "isCorrect" : "true"
  }, {
    "id" : "05ef5993ba9f4cdb9c010579bf96cc2b",
    "option" : "Create a NAT Gateway to route the traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "fa595a8ae7c74c66a24ac69bb7d51b00",
    "option" : "Create a NAT Instance to route the traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "1ea9163dcfe04789817516abac21059c",
    "option" : "Create a VPN Connection to ensure traffic does not flow through the Internet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following:\nWhen you use Amazon Redshift Enhanced VPC Routing, Amazon Redshift forces all COPY and UNLOAD traffic between your cluster and your data repositories through your Amazon VPC.If Enhanced VPC Routing is not enabled, Amazon Redshift routes traffic through the Internet, including traffic to other services within the AWS network.\nFor more information on Redshift Enhanced Routing, please visit the following URL:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/enhanced-vpc-routing.html\n\nThe most efficient and recommended solution to ensure that all traffic from and to the Redshift cluster does not go through the Internet for security reasons is to enable Amazon Redshift Enhanced VPC Routing. Therefore, the correct answer is A.\nHere's an explanation for each of the options:\nA. Enable Amazon Redshift Enhanced VPC Routing: Amazon Redshift Enhanced VPC Routing is a feature that allows Amazon Redshift clusters to use resources in an Amazon Virtual Private Cloud (VPC). This feature uses VPC endpoints to route traffic between Amazon Redshift and other services in the VPC, without traversing the Internet. This ensures that traffic between Amazon Redshift and other services in the VPC is secure and private.\nB. Create a NAT Gateway to route the traffic: A NAT Gateway is a highly available, managed network address translation (NAT) service that enables instances in a private subnet to connect to the Internet, but prevents the Internet from initiating connections with the instances. While a NAT Gateway can be used to route traffic from a private subnet to the Internet, it does not prevent traffic from traversing the Internet to reach the Redshift cluster.\nC. Create a NAT Instance to route the traffic: A NAT Instance is similar to a NAT Gateway in that it allows instances in a private subnet to connect to the Internet, but it requires manual configuration and maintenance. Like a NAT Gateway, it does not prevent traffic from traversing the Internet to reach the Redshift cluster.\nD. Create a VPN Connection to ensure traffic does not flow through the Internet: A VPN connection can be used to securely connect the VPC to an on-premises data center or other remote site. While this can provide a secure connection, it does not prevent traffic from traversing the Internet to reach the Redshift cluster.\nIn summary, Amazon Redshift Enhanced VPC Routing is the most efficient and recommended solution for ensuring that all traffic from and to the Redshift cluster does not go through the Internet for security reasons.\n\n"
}, {
  "id" : 149,
  "question" : "A company has a set of Hyper-V machines and VMware virtual machines.\nThey are now planning to migrate these resources to the AWS Cloud.\nWhat should they use to move these resources to the AWS Cloud?\n",
  "answers" : [ {
    "id" : "04999758a4be40b79b7c4866eb45f396",
    "option" : "DB Migration utility",
    "isCorrect" : "false"
  }, {
    "id" : "c32e9e6d61eb4653b49257dbbe74bdbe",
    "option" : "AWS Server Migration Service",
    "isCorrect" : "true"
  }, {
    "id" : "e2b9c7cb71ad4512a9b4664cb52b1fc9",
    "option" : "Use AWS Migration Tools.",
    "isCorrect" : "false"
  }, {
    "id" : "7447518b7fa84c67b564881311cabc18",
    "option" : "Use AWS Config Tools.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Server Migration Service (SMS) is an agentless service that makes it easier and faster for you to migrate thousands of on-premises workloads to AWS.\nAWS SMS allows you to automate, schedule, and track incremental replications of live server volumes, making it easier for you to coordinate large-scale server migrations.\nFor more information on AWS Server Migration Service, please visit the following URL:\nhttps://aws.amazon.com/server-migration-service/\n\nThe recommended service for migrating Hyper-V machines and VMware virtual machines to the AWS Cloud is AWS Server Migration Service (SMS), which is option B.\nAWS Server Migration Service (SMS) is an agentless service that makes it easier and faster for you to migrate your on-premises workloads to AWS. With SMS, you can automate, schedule, and track incremental replications of live server volumes, making it easier for you to coordinate large-scale server migrations. SMS supports both Windows and Linux operating systems and allows you to replicate your server images from your on-premises environment to the AWS Cloud, and automate the creation of Amazon Machine Images (AMIs) and launch instances in AWS.\nOption A, DB Migration utility, is not the recommended service for migrating Hyper-V machines and VMware virtual machines to AWS. The DB Migration utility is primarily used for migrating database workloads to AWS.\nOption C, Use AWS Migration Tools, is too broad and not specific enough. AWS Migration Tools is a suite of services and tools that are designed to help you migrate your data, applications, and other resources to AWS. While AWS Server Migration Service is included in AWS Migration Tools, it is the recommended service for migrating Hyper-V machines and VMware virtual machines.\nOption D, Use AWS Config Tools, is also not the recommended service for migrating Hyper-V machines and VMware virtual machines to AWS. AWS Config Tools is a service that allows you to monitor and track changes to your AWS resources, ensuring that your configurations comply with your policies and standards. It is not used for migrating workloads to AWS.\n\n"
}, {
  "id" : 150,
  "question" : "You've implemented AWS Key Management Service to protect your data in your applications and other AWS services.\nYour global headquarters is in Northern Virginia (US East (N.\nVirginia) where you created your keys and have provided the appropriate permissions to designated users and specific roles within your organization.\nWhile the N.\nAmerican users are not having issues, German and Japanese users cannot get KMS to function.\nWhat is the most likely cause of it?\n",
  "answers" : [ {
    "id" : "b6908013bf214c39bb2e6655111b8b75",
    "option" : "KMS is only offered in North America.",
    "isCorrect" : "false"
  }, {
    "id" : "7b73079d05e34d03a32d56a401ee9a8f",
    "option" : "AWS CloudTrail has not been enabled to log events.",
    "isCorrect" : "false"
  }, {
    "id" : "e032e7518f194b35af993734b8520372",
    "option" : "KMS master keys are region-specific and the applications are hitting the wrong API endpoints.",
    "isCorrect" : "true"
  }, {
    "id" : "e6dffb11ae3b4d2f9a417bc211d40d03",
    "option" : "The master keys have been disabled.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption C is correct.\nThis is the most likely cause as the application should be sure to hit the correct region endpoint.\nOption A is incorrect.\nKMS is offered in several regions, but keys are not transferrable out of the region they were created in.\nOption B is incorrect.\nCloudTrail is recommended for auditing but is not required.\nOption D is incorrect.\nThe keys are working as expected where they were created; keys are region-specific.\nReferences:\nhttps://aws.amazon.com/kms/faqs/\nttps://docs.aws.amazon.com/general/latest/gr/rande.html#kms_region.\nhttps://www.slideshare.net/AmazonWebServices/encryption-and-key-management-in-aws\n\nThe most likely cause of German and Japanese users being unable to use KMS is option C: KMS master keys are region-specific and the applications are hitting the wrong API endpoints.\nAWS Key Management Service (KMS) is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data, and uses Hardware Security Modules (HSMs) to protect the security of your keys. KMS is a global service, meaning that it can be used in any AWS region.\nHowever, KMS master keys are region-specific. When you create a master key in a specific region, you can only use that key to encrypt and decrypt data in that region. If an application in a different region tries to use that key, it will hit the wrong API endpoint and will not function correctly.\nIn this case, since the global headquarters is in Northern Virginia, it is likely that the KMS master keys were created in that region. When German and Japanese users try to use KMS, their applications are hitting the wrong API endpoints because they are not in the same region as the KMS master keys.\nOption A, KMS is only offered in North America, is incorrect because KMS is a global service that can be used in any AWS region.\nOption B, AWS CloudTrail has not been enabled to log events, is not directly related to the issue of KMS not functioning for German and Japanese users. However, enabling AWS CloudTrail can provide additional visibility into KMS usage and can help with auditing and compliance.\nOption D, the master keys have been disabled, is unlikely to be the cause because if the master keys were disabled, then no users, including those in Northern Virginia, would be able to use KMS.\n\n"
}, {
  "id" : 151,
  "question" : "A company with a set of admin jobs (.NET core) in the C# programming language is moving its infrastructure to AWS.\nWhat would be an efficient mean of hosting the admin related jobs in AWS?\n",
  "answers" : [ {
    "id" : "b1fd6e050f0f4ea6bf0afbf5b3c7c2ec",
    "option" : "Use AWS DynamoDB to store the jobs and then run them on demand.",
    "isCorrect" : "false"
  }, {
    "id" : "d2ee135c6dbf4e5586c63eb27e13dfa8",
    "option" : "Use AWS Lambda functions with C# for the Admin jobs.",
    "isCorrect" : "true"
  }, {
    "id" : "8e0dae9e55f04628ad4fda90301bb5b8",
    "option" : "Use AWS S3 to store the jobs and then run them on demand.",
    "isCorrect" : "false"
  }, {
    "id" : "2f4f3325679e40c5bf3a697096d3c248",
    "option" : "Use AWS Config functions with C# for the Admin jobs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe best and most efficient option is to host the jobs using AWS Lambda.\nThis service has the facility to have the code run in the C# programming language.\nAWS Documentation mentions the following on AWS Lambda:\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers.\nAWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume - there is no charge when your code is not running.\nWith AWS Lambda, you can run code virtually for any type of application or backend service - all with zero administration.\nFor more information on AWS Lambda, please visit the following URL:\nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html\n\nOf the options presented, the most efficient means of hosting the admin related jobs in AWS is B. Use AWS Lambda functions with C# for the Admin jobs.\nAWS Lambda is a serverless computing platform that allows users to run code without having to manage servers. This means that the admin jobs can be executed without the need for the company to provision or manage any infrastructure. With AWS Lambda, the company can write the admin jobs in C# and deploy them as functions. The Lambda function can be triggered by events, such as changes to data in AWS S3 or a message arriving in an Amazon Simple Notification Service (SNS) topic.\nAWS Lambda is also highly scalable, which makes it ideal for processing jobs that can be executed in parallel. The company can set the number of concurrent executions for the Lambda function, and AWS will automatically scale up or down based on demand.\nAnother advantage of using AWS Lambda for the admin jobs is the pay-per-use pricing model. The company will only be charged for the number of times the function is executed, and the amount of compute time used.\nIn contrast, using AWS DynamoDB or AWS S3 to store the admin jobs and then run them on demand would require the company to manage infrastructure and provision resources to run the jobs. AWS Config, while useful for managing resources, is not designed for running code.\nIn summary, AWS Lambda with C# is the most efficient means of hosting admin related jobs in AWS, as it offers serverless computing, scalability, and cost-effective pay-per-use pricing.\n\n"
}, {
  "id" : 152,
  "question" : "Your company currently has setup their data store on AWS DynamoDB.\nOne of your main revenue generating applications uses the tables in this service.\nYour application is now expanding to 2 different other locations and you want to ensure that the latency for data retrieval is the least from the new regions.\nWhich of the following can help accomplish this?\n",
  "answers" : [ {
    "id" : "2250e92c2e0a45b58d8d1999efb663af",
    "option" : "Place a cloudfront distribution in front of the database",
    "isCorrect" : "false"
  }, {
    "id" : "31f76cdfa7a74d71ae1f9b5bf5a6ba16",
    "option" : "Enable Multi-AZ for DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "7b9716b143ae41c2adbabac9cd8b47c6",
    "option" : "Place an ElastiCache in front of DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "b1c9e2d45bd440f2b3127074b7a596ee",
    "option" : "Enable global tables for DynamoDB.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nThe AWS Documentation mentions the following.\nTo illustrate one use case for a global table, suppose that you have a large customer base spread across three geographic areas-the US east coast, the US west coast, and western Europe.\nCustomers would need to update their profile information while using your application.\nTo address these requirements, you could create three identical DynamoDB tables named CustomerProfiles, in three different AWS regions.\nThese three tables would be entirely separate from each other, and changes to the data in one table would not be reflected in the other tables.\nWithout a managed replication solution, you could write code to replicate data changes among these tables; however, this would be a time-consuming and labor-intensive effort.\nOption A is incorrect since cloudfront should ideally be used in front of web distributions.\nOption B is incorrect since this is not an option for DynamoDB.\nOption C is incorrect since would not be affective for multiple regions.\nFor more information on AWS DynamoDB global tables, please visit the below URL.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html\n\nTo minimize the latency for data retrieval from DynamoDB for the new locations, you can enable Global Tables for DynamoDB.\nOption A: Placing a CloudFront distribution in front of the database is not a recommended solution as it will not reduce the latency for data retrieval from DynamoDB. CloudFront is a content delivery network (CDN) that caches data at the edge locations to minimize the latency for content delivery, but it cannot cache data from a database.\nOption B: Enabling Multi-AZ for DynamoDB ensures high availability of your DynamoDB data by replicating it across multiple availability zones within a single region. However, this will not help in reducing the latency for data retrieval from the new regions.\nOption C: Placing an ElastiCache in front of DynamoDB can reduce the read latency for frequently accessed data. ElastiCache is an in-memory caching service that can store frequently accessed data in memory, which can reduce the number of requests to DynamoDB. However, this solution is only effective for read-intensive workloads and will not help in reducing the latency for write operations.\nOption D: Enabling Global Tables for DynamoDB can replicate your DynamoDB tables automatically across multiple AWS regions, which can help reduce the latency for data retrieval from the new regions. Global Tables can also provide disaster recovery and high availability. With Global Tables, you can read and write to the replicated tables with low latency, which can help improve the performance of your application for the new regions.\nTherefore, the correct answer is D. Enable global tables for DynamoDB.\n\n"
}, {
  "id" : 153,
  "question" : "You got hired to implement a system for a movie theater company.\nThis system will allow the user to rent movies for a limited period of time.\nIn order to do this, the user will log in to an application and pay to rent the movie.\nThen a link will be sent to the user through email.\nThe requisite here is only to allow the user to see this file through CloudFront for a limited period of time.\nWhat is the easiest way to achieve this?\n",
  "answers" : [ {
    "id" : "75a7d7ec93d44500ab0b4ff93c89d937",
    "option" : "Use AWS Lambda authorizer and CloudFront.",
    "isCorrect" : "false"
  }, {
    "id" : "6671a1854f1d41f4b2e388d59dab5ed8",
    "option" : "Use Lambda, Cognito and CloudFront.",
    "isCorrect" : "false"
  }, {
    "id" : "a6dea6472b8e407780d872f2a6acf66b",
    "option" : "Using CloudFront origin access identify (OAI).",
    "isCorrect" : "false"
  }, {
    "id" : "98ecdce4b9ff4ae88c210acc26978939",
    "option" : "Using CloudFront with signed URL.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption A is incorrect because a Lambda authorizer just controls the access to an API.\nIt does not restrict content to CloudFront.\nMore details please check https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html.\nOption B is incorrect because with this combination of services, you can authenticate users in your application.\nBut none of these services restrict content to users.\nOption C is incorrect because CloudFront OAI cannot provide access for a limited period of time.\nOption D is CORRECT because with this option, you allow a user to access indivdual files in S3 through CloudFront for a limited period of time.\nThere is an expiration date and time in a signed URL.\nCloudFront checks the expiration date and time in the URL at the time of the HTTP request.\nUsing this method, you can control the time when the user can see the file.\nFor more details, please refer to https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html.\n\nThe easiest way to allow the user to see the file through CloudFront for a limited period of time is by using CloudFront with signed URLs. Therefore, the correct answer is option D.\nCloudFront signed URLs allow you to restrict access to specific content on your website, such as files, images, and videos. A signed URL contains information that CloudFront uses to authorize the requester to access the specific file for a limited period of time, typically a few hours.\nTo implement this solution, the movie theater company would need to generate a signed URL for each file that they want to make available to users. The signed URL would include an expiration time, after which the URL would no longer be valid, and the file would be inaccessible.\nThe signed URL can be generated using one of the following methods:\nUsing a custom application - The company can create a custom application that generates signed URLs for each file. The application can be hosted on AWS or on the company's own servers. Using the AWS SDK - The company can use the AWS SDK to generate signed URLs programmatically. This method is easier than creating a custom application, as it requires less coding.\nOnce the signed URL is generated, it can be sent to the user through email or any other means. When the user clicks on the link, CloudFront will verify the validity of the signed URL and grant access to the file if it's still within the time window.\nTherefore, option D is the easiest and most appropriate solution to achieve this requisite.\n\n"
}, {
  "id" : 154,
  "question" : "A start-up firm is using an AWS Organization for managing policies across its Development and Production accounts.\nThe development account needs an EC2 dedicated host.\nThe Production account has subscribed to an EC2 dedicated host for its application but is not currently using it.\nSharing has NOT been enabled with the AWS Organization in AWS RAM.\nWhich of the following can be done to share the Amazon EC2 dedicated host from the Production account to the Development account?\n",
  "answers" : [ {
    "id" : "2bc6ea733c1d47248a1f97f689f4daed",
    "option" : "Remove both Development &amp; Production Accounts from Organisation &amp; then share resources between them.",
    "isCorrect" : "false"
  }, {
    "id" : "cc16110448bf4b3abd76b324cc551cad",
    "option" : "Resources in the same organization are automatically shared without the need to accept the invitation of sharing resources.",
    "isCorrect" : "false"
  }, {
    "id" : "19203eefb0a040dd86bdfa93fd10871e",
    "option" : "Create a resource share in the production account and accept the invitation in the development account.",
    "isCorrect" : "true"
  }, {
    "id" : "b2f5335deb73457d8ef1ddecb2b90476",
    "option" : "Remove the destination Development account from an Organisation &amp; then share resources with it.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nFor accounts that are part of the AWS Organization, Resource sharing can be done on an individual account basis if resource sharing is not enabled at the AWS Organisation level.\nWith this, resources are shared within accounts as external accounts &amp; an invitation needs to be accepted between these accounts to start resource sharing.\nOption A is incorrect as removing both accounts from AWS Organization for Resource sharing is not a valid option.\nOption B is incorrect because when sharing is not enabled in the Organization, resources are not automatically shared.\nOption D is incorrect as removing a destination account from AWS Organisation is not required for resource sharing.\nFor more information on using AWS Resource Access Manager, refer to the following URL:\nhttps://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html\n\nThe correct answer is C. Create a resource share in the production account and accept the invitation in the development account.\nExplanation: AWS Organizations provides a way for customers to consolidate multiple AWS accounts into an organization that can be centrally managed. It allows an organization to manage policies for multiple AWS accounts from a single master account.\nIn this scenario, the start-up firm is using an AWS Organization to manage policies across its Development and Production accounts. The Development account needs an EC2 dedicated host, while the Production account has subscribed to an EC2 dedicated host but is not currently using it.\nTo share the Amazon EC2 dedicated host from the Production account to the Development account, we need to create a resource share in the Production account and accept the invitation in the Development account. Resource Access Manager (RAM) is the service used to share resources across multiple AWS accounts.\nTo create a resource share in the Production account, follow these steps:\nOpen the RAM console at https://console.aws.amazon.com/ram/. In the navigation pane, choose Resource shares, and then choose Create resource share. For Resource type, choose EC2 Dedicated Hosts. For Select the resources to share, choose the EC2 dedicated host that needs to be shared. For Sharing settings, choose Specific AWS account(s), and then enter the AWS account ID for the Development account. Choose Create resource share.\nOnce the resource share is created in the Production account, the Development account will receive an invitation to accept the share. To accept the share in the Development account, follow these steps:\nOpen the RAM console at https://console.aws.amazon.com/ram/. In the navigation pane, choose Resource shares, and then choose the Pending tab. Choose the resource share invitation for the Production account, and then choose Accept.\nOption A is incorrect because removing both accounts from the organization is not a viable solution. It would break the organization's management structure and policies.\nOption B is incorrect because resources in the same organization are not automatically shared without explicit sharing or acceptance of sharing invitations.\nOption D is incorrect because removing the destination Development account from the organization is not necessary, and it would break the organization's management structure and policies.\n\n"
}, {
  "id" : 155,
  "question" : "A global company has an Amazon Aurora database to store a large amount of customer data.\nThe database is deployed in an AWS account owned by the development team, and the AWS account is within the AWS Organization A.\nNow the database needs to be shared with AWS accounts in another AWS Organization B.\nWhich of the following can be done to achieve the requirement?\n",
  "answers" : [ {
    "id" : "1333cee440b048dca7a7c67e6507bf0b",
    "option" : "In the Management AWS account of Organization A, share the database to the AWS Organization B in Resource Access Manager.",
    "isCorrect" : "false"
  }, {
    "id" : "93cc0e57d40343dabf8991733db2b0b2",
    "option" : "In AWS Aurora Console, select the database and share it to all the AWS accounts of AWS Organization",
    "isCorrect" : "false"
  }, {
    "id" : "9f6fcd453b4a488483743f9fc1889aaf",
    "option" : "In AWS Organization console, select the database resource and share it to the AWS Organization",
    "isCorrect" : "false"
  }, {
    "id" : "67e3b1c9828b4415b3ee2d8a8fc2834e",
    "option" : "In the Management AWS account of Organization A, share the database to the AWS accounts of AWS Organization B in Resource Access Manager.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect because you cannot share the resource with another AWS Organization rather than sharing with all the AWS accounts in RAM.\nOption B is incorrect because you cannot directly share the Aurora database with another AWS Organization in the AWS Aurora console.\nOption C is incorrect because you cannot enable the sharing through the AWS Organization console.\nOption D is CORRECT because you can share the resource with AWS accounts of another Organization in RAM.\nFor more information on using AWS Resource Access Manager, refer to the AWS documentation: https://aws.amazon.com/blogs/aws/new-aws-resource-access-manager-cross-account-resource-sharing/\nhttps://docs.aws.amazon.com/ram/latest/userguide/getting-started-sharing.html#getting-started-sharing-orgs\n\nTo share the Amazon Aurora database with AWS accounts in another AWS Organization, you need to use AWS Resource Access Manager (RAM). RAM allows resource sharing across accounts and enables resource access without resource duplication.\nIn this case, there are two AWS Organizations, A and B. The database is deployed in an AWS account owned by the development team, which is part of AWS Organization A. The goal is to share the database with AWS accounts in AWS Organization B.\nOption A: In the Management AWS account of Organization A, share the database to the AWS Organization B in Resource Access Manager.\nThis is the correct option. To share the database, you need to go to the AWS RAM console and create a resource share for the database. In the resource share, you can specify which accounts in AWS Organization B can access the database. Once the resource share is created, the specified AWS accounts in AWS Organization B can access the database without needing to create a copy of the database.\nOption B: In AWS Aurora Console, select the database and share it to all the AWS accounts of AWS Organization.\nThis option is incorrect. While you can share resources from the AWS Aurora console, this option does not allow you to share resources with accounts in another AWS Organization. This option also does not allow you to specify which AWS accounts can access the database.\nOption C: In AWS Organization console, select the database resource and share it to the AWS Organization.\nThis option is also incorrect. While you can share resources from the AWS Organization console, this option does not allow you to share resources with accounts in another AWS Organization. This option also does not allow you to specify which AWS accounts can access the database.\nOption D: In the Management AWS account of Organization A, share the database to the AWS accounts of AWS Organization B in Resource Access Manager.\nThis option is incorrect because it does not specify which AWS accounts in AWS Organization B can access the database. Without specifying the accounts, anyone in AWS Organization B could potentially access the database, which may not be desirable.\nTherefore, the correct option to achieve the requirement is to create a resource share for the Amazon Aurora database in the Management AWS account of Organization A and share it with the specified AWS accounts of AWS Organization B in AWS Resource Access Manager.\n\n"
}, {
  "id" : 156,
  "question" : "An application consists of the following architecture: a.\nEC2 Instances in a single AZ behind an ELB b.\nA NAT Instance which is used to ensure that instances can download updates from the Internet What could be done to ensure better fault tolerance in this setup? (SELECT TWO.)\n",
  "answers" : [ {
    "id" : "60512505c4a14fc3948e047e90e0f91a",
    "option" : "Add more instances in the existing Availability Zone.",
    "isCorrect" : "false"
  }, {
    "id" : "bbfb082337e14235928f38a83ec7f694",
    "option" : "Add an Auto Scaling Group to the setup.",
    "isCorrect" : "true"
  }, {
    "id" : "1ecd44c840f34b6fae6683323032fe52",
    "option" : "Add more instances in another Availability Zone.",
    "isCorrect" : "true"
  }, {
    "id" : "e796b242165345b29ef1360e1319c372",
    "option" : "Add another ELB for more fault tolerance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nAWS Documentation mentions the following:\nAdding Auto Scaling to your application architecture is one way to maximize the benefits of the AWS Cloud.\nWhen you use Auto Scaling, your applications gain the following benefits:\nBetter fault tolerance.\nAuto Scaling can detect when an instance is unhealthy.\nThen it terminates that instance, and launches an instance to replace it.\nYou can also configure Auto Scaling to use multiple Availability Zones.\nIf one Availability Zone becomes unavailable, Auto Scaling can launch instances in another one to compensate.\nBetter availability.\nAuto Scaling can help you ensure that your application always has the right amount of capacity to handle the current traffic demands.\nFor more information on the benefits of Auto Scaling, please visit the following URL:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-benefits.html\n\nIn this architecture, the application is running on EC2 instances behind an Elastic Load Balancer (ELB) in a single Availability Zone (AZ). Additionally, a NAT instance is used to allow instances to download updates from the Internet. The challenge with this setup is that if the Availability Zone goes down, the application will become unavailable. To improve fault tolerance, two potential solutions can be implemented:\nAdd more instances in another Availability Zone: One approach to improve fault tolerance is to launch more EC2 instances in another Availability Zone. This ensures that if the primary Availability Zone goes down, the application can still function from the secondary Availability Zone. In addition, by distributing instances across multiple AZs, you can increase application availability and reduce the risk of a single point of failure. Add an Auto Scaling Group to the setup: Another solution is to use an Auto Scaling group to launch additional EC2 instances automatically based on predefined policies. Auto Scaling can help maintain the desired capacity by automatically launching new instances in response to changes in demand, ensuring the application is always available. By using an Auto Scaling group, you can ensure that the number of instances is sufficient to handle traffic and that new instances are launched when needed, and that instances are terminated when demand decreases.\nTherefore, the correct answers are:\nA. Add more instances in another Availability Zone. B. Add an Auto Scaling Group to the setup.\n\n"
}, {
  "id" : 157,
  "question" : "A company has a lot of data hosted on their On-premises infrastructure.\nRunning out of storage space, the company wants a quick win solution using AWS.\nThere should be low latency for the frequently accessed data.\nWhich of the following would allow the easy extension of their data infrastructure to AWS?\n",
  "answers" : [ {
    "id" : "a28d309c48664a34b685ebaab4f17fec",
    "option" : "The company could start using Gateway Cached Volumes.",
    "isCorrect" : "true"
  }, {
    "id" : "2f72a631a7ac4a3485a3044c37e1db3e",
    "option" : "The company could start using Gateway Stored Volumes.",
    "isCorrect" : "false"
  }, {
    "id" : "85e96c66701e4965aa2da382e4cbe800",
    "option" : "The company could start using the DEEP_ARCHIVE storage class.",
    "isCorrect" : "false"
  }, {
    "id" : "86f1c9a66c464ea482d918a42b1099b3",
    "option" : "The company could start using Amazon Glacier.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nVolume Gateways and Cached Volumes can be used to start storing data in S3.\nAWS Documentation mentions the following:\nYou store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally.\nCached volumes offer substantial cost savings on primary storage and minimize the need to scale your storage on-premises.\nYou also retain low-latency access to your frequently accessed data.\nFor more information on Storage Gateways, please visit the following URL:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\nNote:\nThe question states that they are running out of storage space and they need a solution to store data with AWS rather than a backup.\nFor this purpose, gateway-cached volumes are appropriate, which will help them to avoid scaling their on-premises data center and store on AWS storage service while having the most recent files available for them at low latency.\nThis is the difference between Cached and stored volumes:\nCached volumes - You store your data in S3 and retain a copy of frequently accessed data subsets locally.\nCached volumes offer substantial cost savings on primary storage and \"minimize the need to scale your storage on-premises.\nYou also retain low-latency access to your frequently accessed data.\"\nStored volumes - If you need low-latency access to your entire data set, first configure your on-premises gateway to store all your data locally.\nThen asynchronously back up point-in-time snapshots of this data to Amazon S3\n\"This configuration provides durable and inexpensive off-site backups that you can recover to your local data center or Amazon EC2.\" For example, if you need replacement capacity for disaster recovery, you can recover the backups to Amazon EC2.\nAs described in the answer: The company wants a quick win solution to store data with AWS, avoiding scaling the on-premise setup rather than backing up the data.\nIn the question, they mentioned that \"A company has a lot of data hosted on their On-premises infrastructure.\" From On-premises to cloud infrastructure, you can use AWS storage gateways.\nOption C is talking about the storage class.\nBut here, the requirement is (How) to transfer or migrate your data from On-premises to Cloud infrastructure.\nSo there is no clear process mentioned in Option.\nC.\n\nThe best option for the company in this scenario would be to use AWS Storage Gateway. The Storage Gateway service allows on-premises applications to use cloud storage through a virtual machine or hardware gateway appliance. The service provides various types of gateways, including File Gateway, Volume Gateway, and Tape Gateway, that can be used depending on the type of workload.\nIn this case, the company is running out of storage space, and there is frequently accessed data that needs low latency. Therefore, the company should use Gateway Cached Volumes. This option provides low-latency access to frequently accessed data, as it caches data on the on-premises gateway appliance. As data is accessed, it is stored in the local cache, allowing for quick access. Additionally, the data is also stored in Amazon S3, providing scalable and durable storage.\nGateway Stored Volumes, on the other hand, provide a complete copy of the data on the on-premises gateway appliance, allowing for low-latency access to the data. However, this option may not be suitable for the company as it requires more on-premises storage space to store a complete copy of the data.\nThe DEEP_ARCHIVE storage class and Amazon Glacier are both used for long-term archiving and backup of data, and are not suitable for frequently accessed data due to their high retrieval latency. These options would not provide the low-latency access required by the company for their frequently accessed data.\nIn summary, the company should start using Gateway Cached Volumes to extend their data infrastructure to AWS, as it provides low-latency access to frequently accessed data while also providing scalable and durable storage in Amazon S3.\n\n"
}, {
  "id" : 158,
  "question" : "A Large Medical Institute is using a legacy database for saving all its patient details.\nDue to compatibility issues with the latest software, they plan to migrate this database to AWS cloud infrastructure.\nThis large size database will be using a NoSQL database Amazon DynamoDB in AWS.\nAs an AWS consultant, you need to ensure that all the current legacy database tables are migrated without a glitch to Amazon DynamoDB.\nWhich of the following is the most cost-effective way of transferring legacy databases to Amazon DynamoDB?\n",
  "answers" : [ {
    "id" : "cbebb92c7454496cb6aef793e4095ada",
    "option" : "Use AWS DMS with AWS Schema Conversion Tool to save data to Amazon S3 bucket &amp; then upload all data to Amazon DynamoD.",
    "isCorrect" : "true"
  }, {
    "id" : "a28a749cf22a4ca991924407039bac95",
    "option" : "Use AWS DMS with engine conversion tool to save data to Amazon S3 bucket &amp; then upload all data to Amazon DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "96b8d452ef664309a106610d02943e1b",
    "option" : "Use AWS DMS with engine conversion tool to save data to Amazon EC2 &amp; then upload all data to Amazon DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "063ae0d4fb684365adfda81ca75a00d3",
    "option" : "Use AWS DMS with AWS Schema Conversion Tool to save data to Amazon EC2 instance &amp; then upload all data to Amazon DynamoD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nIn this case, Legacy Database will be converted to Amazon DynamoDB, which will be a heterogenous conversion.\nUsing AWS Schema Conversion Tool is best suited for such conversion along with AWS DMS to transfer data from on-premise to AWS.\nUsing Amazon S3 bucket will help to save any amount of data most cost-effectively before uploading the data to Amazon DynamoDB.Option B is incorrect as the engine conversion tool is best suited for homogeneous database migration.\nIn this case, it's a heterogeneous database.\nSo using AWS SCT along with AWS DMS is the best option.\nOption C &amp; D are incorrect as using the Amazon S3 bucket is a more cost-effective option than using the Amazon EC2 instance.\nFor more information on using AWS Database Migration Service with AWS SCT, refer to the following URL:\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_BestPractices.html#CHAP_BestPractices.SchemaConversion\n\nThe most cost-effective way of transferring a legacy database to Amazon DynamoDB is option B: Use AWS DMS with engine conversion tool to save data to Amazon S3 bucket & then upload all data to Amazon DynamoDB.\nAmazon Database Migration Service (AWS DMS) is a managed service that can help migrate databases to AWS quickly and securely. It can migrate data from various sources to various target databases. In this case, it will migrate data from a legacy database to Amazon DynamoDB.\nAmazon DynamoDB is a NoSQL database that is designed to handle large amounts of data with low latency and high performance. It is a fully managed database service, which means that AWS takes care of the infrastructure and maintenance of the database.\nThe engine conversion tool is a feature of AWS DMS that helps convert data from one database engine to another. In this case, it will convert the data from the legacy database engine to the Amazon DynamoDB engine. This tool is helpful when migrating from one database engine to another and ensures that data is migrated correctly.\nThe steps involved in the migration process are:\nSet up an Amazon S3 bucket to store the data temporarily during migration. Create an AWS DMS migration task and select the legacy database as the source and Amazon DynamoDB as the target. Use the engine conversion tool to convert the data from the legacy database engine to the Amazon DynamoDB engine. Migrate the data to the Amazon S3 bucket using AWS DMS. Finally, upload the data from the S3 bucket to Amazon DynamoDB.\nThis method is cost-effective because Amazon S3 and Amazon DynamoDB are both highly scalable and pay-as-you-go services, which means that you only pay for what you use. Also, using the engine conversion tool reduces the manual effort involved in the migration process, which saves time and money.\nOption A, using AWS DMS with AWS Schema Conversion Tool to save data to Amazon S3 bucket & then upload all data to Amazon DynamoDB, is not as cost-effective as option B because the schema conversion tool is not necessary for migrating data from a legacy database to Amazon DynamoDB.\nOption C, using AWS DMS with engine conversion tool to save data to Amazon EC2 & then upload all data to Amazon DynamoDB, is not as cost-effective as option B because it involves the additional cost of an Amazon EC2 instance, which is not necessary for the migration process.\nOption D, using AWS DMS with AWS Schema Conversion Tool to save data to Amazon EC2 instance & then upload all data to Amazon DynamoDB, is not as cost-effective as option B because it involves the additional cost of an Amazon EC2 instance and the unnecessary use of the schema conversion tool.\n\n"
}, {
  "id" : 159,
  "question" : "A Financial firm is planning to build a highly resilient application with primary database servers located at on-premises data centers while maintaining its DB snapshots in an S3 bucket.\nThe IT Team is looking for a cost-effective and secure way of transferring the large customer financial databases from on-premises servers to the Amazon S3 bucket with no impact on the client usage of these applications.\nAlso, post this data transfer, the on-premises application will be fetching data from the Amazon S3 bucket in case of a primary database failure. So, your solution should ensure that the Amazon S3 data is fully synced with the on-premises database.\nWhich of the following can be used to meet this requirement?\n",
  "answers" : [ {
    "id" : "187bac140a1c42e7bb092d7a73a4a485",
    "option" : "Use Amazon S3 Transfer Acceleration for transferring data between the on-premises &amp; Amazon S3 bucket while using AWS Data Sync for accessing these S3 bucket data from the on-premises application.",
    "isCorrect" : "false"
  }, {
    "id" : "be8fd2e57ae542c99a94388f7163aa73",
    "option" : "Use AWS Data Sync for transferring data between the on-premises &amp; Amazon S3 bucket while using AWS Storage Gateway for accessing these S3 bucket data from the on-premises application.",
    "isCorrect" : "true"
  }, {
    "id" : "7c1f7b39727f49e4bf962d2383992ba7",
    "option" : "Use AWS Snowball Edge for transferring data between the on-premises &amp; Amazon S3 bucket while using AWS Storage Gateway for accessing these S3 bucket data from the on-premises application.",
    "isCorrect" : "false"
  }, {
    "id" : "da849909a8b74d6f87a675a1ae763c45",
    "option" : "Use AWS Transfer for transferring data between the on-premises &amp; Amazon S3 bucket while using AWS Data Sync for accessing these S3 bucket data from the on-premises application.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS DataSync can be used for huge amounts of data transfer between on-premises &amp; AWS.\nAWS Data Sync is a secure way of online data transfer.\nOnce data is transferred to the AWS S3 bucket, AWS Storage Gateway can have data synced between on-premises servers &amp; AWS S3 buckets.\nOption A is incorrect as Amazon S3 Transfer Acceleration can be used for applications that have already integrated with Amazon S3 API.\nOption C is incorrect as AWS Snowball Edge can be used for offline data transfer between on-premises &amp; AWS S3 bucket.\nOption D is incorrect as AWS Transfer is a better choice for transferring SFTP data between on-premises &amp; Amazon S3.\nFor more information on using AWS DataSync, refer to the following URLs:\nhttps://aws.amazon.com/datasync/faqs/\nhttps://aws.amazon.com/blogs/storage/migrating-hundreds-of-tb-of-data-to-amazon-s3-with-aws-datasync/\n\nThe best option to meet the requirements of the Financial firm is to use AWS Data Sync for transferring data between the on-premises and Amazon S3 bucket, while using AWS Storage Gateway for accessing the S3 bucket data from the on-premises application. Therefore, the correct answer is B.\nHere's why:\nOption A, which suggests using Amazon S3 Transfer Acceleration for transferring data between on-premises and Amazon S3 bucket, is not a suitable choice for this use case. Amazon S3 Transfer Acceleration is designed to accelerate transfers over the internet, and it's not optimized for use within private networks. Furthermore, this option suggests using AWS Data Sync for accessing S3 bucket data from the on-premises application, which isn't optimal. While AWS Data Sync is excellent for transferring data between on-premises storage and AWS storage services, it doesn't provide a seamless way to access the data from on-premises applications.\nOption C, which recommends using AWS Snowball Edge for transferring data, is an excellent option for large data transfers, but it may not be cost-effective for this particular use case, considering that the Financial firm plans to transfer data frequently. Additionally, using AWS Storage Gateway for accessing S3 bucket data from the on-premises application is still the most suitable option.\nOption D, which suggests using AWS Transfer for transferring data, may not be the best option for this use case since AWS Transfer is optimized for transferring files to and from Amazon S3 over the internet. Therefore, using it to transfer data between on-premises and Amazon S3 bucket may not be the most efficient option.\nThat leaves us with option B, which is the best choice for the Financial firm's requirements. AWS Data Sync is an excellent solution for synchronizing data between on-premises storage and AWS storage services, including S3 buckets. AWS Storage Gateway, on the other hand, is designed to provide a seamless way to access AWS storage services from on-premises applications, including S3 bucket data. Therefore, using AWS Data Sync for transferring data and AWS Storage Gateway for accessing S3 bucket data from the on-premises application is the most cost-effective, efficient, and secure way to meet the requirements of the Financial firm.\n\n"
}, {
  "id" : 160,
  "question" : "A company has an application that delivers objects from S3 to global users.\nOf late, some users have been complaining of slow response times.\nWhich additional step would help to build a cost-effective solution and ensure that the users get an optimal response to objects from S3?\n",
  "answers" : [ {
    "id" : "1916ef031d6d45d28ca70b9be95b5c33",
    "option" : "Use S3 Replication to replicate the objects to regions closest to the users.",
    "isCorrect" : "false"
  }, {
    "id" : "80900b9d66c54fa39de271bed87aa39f",
    "option" : "Ensure S3 Transfer Acceleration is enabled to ensure that all users get the desired response times.",
    "isCorrect" : "false"
  }, {
    "id" : "42cb6d3a596742cc9722229dc1d33fad",
    "option" : "Place an ELB in front of S3 to distribute the load across S3.",
    "isCorrect" : "false"
  }, {
    "id" : "727a47577e354fea988c1bcd62a2dd46",
    "option" : "Place the S3 bucket behind a CloudFront distribution.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions the following:\nIf your workload is mainly sending GET requests, in addition to the preceding guidelines, you should consider using Amazon CloudFront for performance optimization.\nIntegrating Amazon CloudFront with Amazon S3, you can distribute content to your users with low latency and a high data transfer rate.\nYou will also send fewer direct requests to Amazon S3, which will reduce your costs.\nFor example, suppose that you have a few very popular objects.\nAmazon CloudFront fetches those objects from Amazon S3 and caches them.\nAmazon CloudFront can then serve future requests for the objects from its cache, reducing the number of GET requests it sends to Amazon S3.\nFor more information on performance considerations in S3, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/request-rate-perf-considerations.html\nOption A is incorrect.\nS3 Cross-Region Replication is not the correct answer for this business scenario.You are asked how to provide easier &amp; faster access to data in the S3 bucket, and this option is used to replicate S3 bucket data across regions.\nOption B is incorrect.\nS3 TA is used for fast, easy, and secure file transfer over long distances between your client and your Amazon S3 bucket.\nS3 Transfer Acceleration does leverage Amazon CloudFront's globally distributed AWS Edge Locations.\nBut it would be too costly for this situation.\nOption C is incorrect.\nELB is used to distribute traffic on to EC2 Instances.\n\nThe best option to address the slow response times for global users accessing objects from S3 while also being cost-effective would be to use Amazon CloudFront.\nCloudFront is a content delivery network (CDN) that distributes content to various locations globally using edge locations. By placing an S3 bucket behind a CloudFront distribution, the user requests can be routed to the edge location that is closest to the user, resulting in lower latency and faster response times.\nOption A, using S3 replication to replicate objects to regions closest to the users, is a valid solution but may not be as cost-effective as CloudFront. Replicating objects to multiple regions would incur additional costs and would require more time and effort to set up and manage.\nOption B, enabling S3 Transfer Acceleration, is useful for speeding up transfers over long distances but does not address the issue of slow response times for users accessing objects from S3.\nOption C, placing an Elastic Load Balancer (ELB) in front of S3, would not be useful since S3 is not an application server and cannot be load-balanced by ELB.\nTherefore, the best option to ensure optimal response times for users accessing objects from S3 globally while also being cost-effective is to place the S3 bucket behind a CloudFront distribution.\n\n"
}, {
  "id" : 161,
  "question" : "A popular blogging site plans to use AWS DataSync to migrate all the data from the on-premises network file system (NFS) server to AWS EFS.\nThe blogging application constantly updates its on-premises dataset.\nYou need to ensure that all the files saved at EFS are validated for data integrity.\nHow would you set up the data verification in AWS DataSync?\n",
  "answers" : [ {
    "id" : "814250613c3d48c19e93a58ecc4b52d8",
    "option" : "Enable verification all the time.",
    "isCorrect" : "false"
  }, {
    "id" : "a1521cc086ee4bbeb30624ea69e28542",
    "option" : "Enable verification during initial file transfers and disable it during the final cut-over.",
    "isCorrect" : "false"
  }, {
    "id" : "0d4b0bdf00974dbaa38fbbaf95926988",
    "option" : "Disable verification during initial file transfers and enable it during the final cut-over.",
    "isCorrect" : "true"
  }, {
    "id" : "03df9ab516b44555b3795273188a20d0",
    "option" : "Disable verification all the time.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nWhile transferring a constantly changing dataset between on-premise servers &amp; EFS using AWS DataSync, you could initially uncheck Enable verification, because files at the source are slightly different from files at the destination.\nYou can enable the verification during the final cut-over from on-premises to AWS.\nOption A is incorrect as enabling data verification for a constantly changing dataset will lead to a slow data transfer.\nOption B is incorrect as data verification needs to be performed during the final cut-over to ensure that all data is properly copied to EFS.\nOption D is incorrect as disabling verification will not perform the data integrity check on the transferred data between on-premise servers and EFS.\nFor more information on using AWS DataSync, refer to the following URL:\nhttps://aws.amazon.com/blogs/storage/migrating-storage-with-aws-datasync/\n\nTo ensure data integrity during migration of data from an on-premises network file system (NFS) server to AWS EFS using AWS DataSync, verification must be enabled.\nDataSync provides data validation mechanisms to verify that the data transferred is the same as the source. This helps to ensure that the data is not corrupted during transfer and that the destination data is an exact copy of the source data.\nIn this case, since the blogging application constantly updates its on-premises dataset, it is important to ensure that all the data transferred is valid and accurate.\nOption A - Enable verification all the time: This option would enable data verification during the entire transfer process, providing constant validation of data, which is ideal for ensuring data integrity. Therefore, this option is a valid choice.\nOption B - Enable verification during initial file transfers and disable it during the final cut-over: This option would enable verification only during the initial transfer, which is not enough to ensure data integrity as the data is updated constantly. Therefore, this option is not suitable.\nOption C - Disable verification during initial file transfers and enable it during the final cut-over: This option would not provide enough data validation during the initial transfer, which is when most of the data is transferred. Therefore, this option is not ideal.\nOption D - Disable verification all the time: This option would not validate the data transferred at any point, which would result in potential data loss or corruption. Therefore, this option is not suitable.\nIn summary, Option A is the best choice as it provides constant data validation, which ensures that all data transferred is valid and accurate.\n\n"
}, {
  "id" : 162,
  "question" : "A company is planning to build an application using the services available on AWS.\nThis application will be stateless in nature, and the service must have the ability to scale according to the demand.\nWhich compute service should be used in this scenario?\n",
  "answers" : [ {
    "id" : "a9d30f4c43dc44f0a63d71670f8fad73",
    "option" : "AWS DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "06964d602e614dc0a1e8a759f6c9d398",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "51a7a87fdc644ccb88cb7e7142b48c0d",
    "option" : "AWS S3",
    "isCorrect" : "false"
  }, {
    "id" : "9d628c46536748c2aeb9ea92a949b8ac",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe following content from an AWS Whitepaper supports the usage of AWS Lambda for this requirement:\nA stateless application is an application that needs no knowledge of previous interactions and stores no session information.\nSuch an example could be an application that, given the same input, provides the same response to any end-user.\nA stateless application can scale horizontally since any request can be serviced by any of the available compute resources (e.g., EC2 instances, AWS Lambda functions).\nFor more information on AWS Cloud best practices, please visit the following URL:\nhttps://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf\n\nThe correct answer for this scenario is B. AWS Lambda.\nExplanation: In this scenario, the application is stateless, meaning that it doesn't require any persistence of data or information. This implies that the application can run without maintaining any internal state, and any instance of the application can handle a request without depending on the state of another instance.\nAlso, the service must have the ability to scale according to the demand. This means that the compute service must be able to dynamically adjust the resources allocated to it to meet the increasing or decreasing demand for the application.\nAWS Lambda is a serverless computing service that allows developers to run their code without managing any servers or infrastructure. It provides an event-driven architecture that automatically scales to handle the incoming requests, making it an ideal choice for stateless applications that require on-demand scaling.\nWhen a request is made to AWS Lambda, it creates a new instance of the function to handle the request. As the number of requests increases, AWS Lambda automatically creates new instances to handle the load, and as the load decreases, it scales down the number of instances accordingly. This ensures that the application always has the required compute resources to handle the requests.\nOption A (AWS DynamoDB) is a NoSQL database service that can store and retrieve data, but it doesn't provide compute resources to run the application code.\nOption C (AWS S3) is an object storage service that can store and retrieve data, but it doesn't provide any compute resources to run the application code.\nOption D (AWS SQS) is a message queue service that can store and process messages, but it doesn't provide any compute resources to run the application code.\nTherefore, the best option for this scenario is AWS Lambda, which can provide the required compute resources to run the stateless application code and scale dynamically to meet the demand.\n\n"
}, {
  "id" : 163,
  "question" : "A large IT company is using Amazon CloudFront for its web application.\nStatic Content for this application is saved in the Amazon S3 bucket.\nAmazon CloudFront is configured for this application to provide faster access to these files for global users. IT Team is concerned about some critical files that need to be accessed only by users from certain white-list countries that you have defined in Amazon CloudFront geo-restriction.\nThere is a requirement that no users should access these files directly using the Amazon S3 URL.\nWhich of the following is the best way to achieve the given requirement?\n",
  "answers" : [ {
    "id" : "48284f1d3a5346edb101440663ac6018",
    "option" : "Create an OAI user to associate with distribution &amp; modify permission on Amazon S3 bucket using bucket policy.",
    "isCorrect" : "false"
  }, {
    "id" : "ace0d71b1cee469cb0dfca43c1369255",
    "option" : "Create Amazon CloudFront Signed URLs to limit access to these files &amp; modify permission on Amazon S3 bucket using bucket policy.",
    "isCorrect" : "false"
  }, {
    "id" : "e01e8169451e4fae8c2a94982b6a31cb",
    "option" : "Create an OAI user to associate with distribution &amp; modify permission on Amazon S3 bucket using object ACLâ€™s.",
    "isCorrect" : "true"
  }, {
    "id" : "416ad62be1eb4f378e07e4d29a2a6cec",
    "option" : "Create Amazon CloudFront Signed URLs to limit access to these files &amp; modify permission on Amazon S3 bucket using object ACLâ€™s.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon CloudFront Origin Access Identity is a special user that can control access to content in the Amazon S3 bucket.\nUsing Object ACLs provides a granular control on each file in the Amazon S3 bucket.\nAssociating Amazon CloudFront OAI to distribution &amp; modifying permission on the S3 bucket allows access only to OAI.\nWhen you create or update a distribution, you can add an origin access identity (OAI) and automatically update the Amazon S3 bucket policy to give the OAI permission to access your bucket.\nAlternatively, you can choose to manually create or update the bucket policy, or use object ACLs that control access to individual files in the bucket.\nUsing CloudFront geo-restriction:\nWhen a user requests your content, CloudFront typically serves the requested content regardless of where the user is located.\nIf you need to prevent users in specific countries from accessing your content, you can use the CloudFront geo restriction feature to do one of the following:\nAllow your users to access your content only if they're in one of the countries on a whitelist of approved countries.\nPrevent your users from accessing your content if they're in one of the countries on a blacklist of banned countries.\nUsing a third-party geolocation service:\nWhen you're using a third-party geolocation service, we recommend that you use CloudFront signed URLs, which let you specify an expiration date and time after which the URL is no longer valid.\nIn addition, you use an Amazon S3 bucket as your origin because you can then use a CloudFront origin access identity to prevent users from accessing your content directly from the origin.\nOption A is incorrect as modifying permission in the Amazon S3 bucket using bucket policy will not provide granular control on access to each file in a bucket.\nOptions B and D are incorrect as Amazon CloudFront Signed URLs will provide access only to authorized users for a specified time period.\nSigned URLs are mainly used with CloudFront third-party geolocation services.\nFor more information on using restricting access using Amazon CloudFront OAI, refer to the following URL:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/georestrictions.html#georestrictions-cloudfront\n\nThe best way to achieve the given requirement is to use Amazon CloudFront Signed URLs to limit access to the files and modify permissions on the Amazon S3 bucket using a bucket policy. Therefore, the correct answer is B.\nExplanation: Amazon CloudFront is a content delivery network (CDN) service that speeds up the distribution of your static and dynamic web content, such as HTML, CSS, JavaScript, and images, to users worldwide. CloudFront has several features, including Geo-Restriction and Signed URLs, that can help restrict access to content based on location and expiration time.\nIn this case, the IT team has concerns about some critical files that should only be accessed by users from certain white-list countries that have been defined in Amazon CloudFront geo-restriction. To achieve this requirement, we need to prevent users from accessing these files directly using the Amazon S3 URL. Therefore, we need to create signed URLs for these files that are only accessible through CloudFront.\nCreating a signed URL involves adding query string parameters to the URL that provide authentication information, such as an expiration time and a private key. The user must have access to the private key to generate a signed URL, which limits access to the content to a specific time period and can restrict access to specific IP addresses, HTTP methods, and other parameters.\nTo implement this solution, we also need to modify the permissions on the Amazon S3 bucket to prevent direct access to the files using the Amazon S3 URL. We can do this by creating a bucket policy that denies access to the bucket from all IP addresses except for those coming from the CloudFront distribution.\nTherefore, the correct solution is to create Amazon CloudFront Signed URLs to limit access to the files and modify permissions on the Amazon S3 bucket using a bucket policy. Option B is the correct answer.\n\n"
}, {
  "id" : 164,
  "question" : "You use a CloudFront distribution for a website and the origin is an Amazon S3 bucket that supports HTTPS communication.\nYou require HTTPS for the communication between CloudFront and Amazon S3\nWhen viewers access the content in the CloudFront edge locations using HTTP, you want the HTTP requests to be automatically redirected to HTTPS requests.\nHow would you achieve this requirement?\n",
  "answers" : [ {
    "id" : "e018e6a694a34c9db0c849177d8aae89",
    "option" : "No actions are needed as it is the default behavior.",
    "isCorrect" : "false"
  }, {
    "id" : "be91233faa994074923a243bec01c415",
    "option" : "Configure the Viewer Protocol Policy of the CloudFront distribution to be \"HTTPS only\".",
    "isCorrect" : "false"
  }, {
    "id" : "66fe40a153464714bc770faf24976892",
    "option" : "Configure the Origin Protocol Policy of the CloudFront distribution to be \"HTTPS\".",
    "isCorrect" : "false"
  }, {
    "id" : "76fdfdd1886a46f0b2e7a72f62c2984a",
    "option" : "Configure the Viewer Protocol Policy of the CloudFront distribution to be \"Redirect HTTP to HTTPS\".",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nWhen your origin is an Amazon S3 bucket that supports HTTPS communication, CloudFront always forwards requests to S3 by using the protocol that viewers used to submit the requests.\nThe default setting for the Origin Protocol Policy setting is Match Viewer and can't be changed.\nIf you want to require HTTPS for communication between CloudFront and Amazon S3, you must change the value of Viewer Protocol Policy to Redirect HTTP to HTTPS or HTTPS Only.\nThe procedure later in this section explains how to use the CloudFront console to change the Viewer Protocol Policy.\nFor information about using the CloudFront API to update the\nViewerProtocolPolicy.\nelement for a distribution, see UpdateDistribution in the Amazon CloudFront API Reference.\nOption A is incorrect because this is not the default behaviour to redirect HTTP to HTTPS for CloudFront.\nOption B is incorrect because using this option user cannot redirect HTTP requests o HTTPS requests automatically.\nIt does not achieve the requirement.\nViewers can access your content only if they're using HTTPS.\nIf a viewer sends an HTTP request instead of an HTTPS request, CloudFront returns HTTP status code 403 (Forbidden) and does not return the object.\nOption C is incorrect because the Origin Protocol Policy setting is \"Match Viewer\" and can't be changed.\nDetails please check the following reference.\nOption D is CORRECT because this configuration in Viewer Protocol Policy helps to automatically redirect HTTP requests to HTTPS requests.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-s3-origin.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-web-values-specify.html\n\nThe correct answer is D. Configure the Viewer Protocol Policy of the CloudFront distribution to be \"Redirect HTTP to HTTPS\".\nExplanation:\nCloudFront is a content delivery network (CDN) that can be used to distribute content to edge locations for faster access to viewers. When creating a CloudFront distribution, you can specify an origin where the content is stored. In this case, the origin is an Amazon S3 bucket that supports HTTPS communication.\nTo ensure that all communication between CloudFront and Amazon S3 is encrypted, you can configure the Origin Protocol Policy of the CloudFront distribution to be \"HTTPS\". This means that CloudFront will only communicate with the origin using HTTPS.\nHowever, this does not automatically redirect viewers who access the content using HTTP to HTTPS. To achieve this requirement, you need to configure the Viewer Protocol Policy of the CloudFront distribution to be \"Redirect HTTP to HTTPS\". This means that when viewers access the content using HTTP, CloudFront will automatically redirect them to the HTTPS version of the content.\nOption A is incorrect because the default behavior is not to automatically redirect HTTP requests to HTTPS requests.\nOption B is incorrect because it only specifies the allowed viewer protocols, not how to handle HTTP requests.\nOption C is incorrect because it only specifies the allowed origin protocol, not how to handle HTTP requests from the viewers.\n\n"
}, {
  "id" : 165,
  "question" : "Which of the following is the correct way to load streaming data into Amazon OpenSearch Service from different sources?\n",
  "answers" : [ {
    "id" : "da100436eac34cd3ab117d61f5441f31",
    "option" : "Load streaming data from Amazon Kinesis Data Firehose using AWS Lambda functions as event handlers.",
    "isCorrect" : "false"
  }, {
    "id" : "9b23c090c0a74b2fa2492e83013537ba",
    "option" : "Load streaming data from Amazon S3 using AWS Lambda functions as event handlers.",
    "isCorrect" : "true"
  }, {
    "id" : "f6e10c0f169c4abf94ad06293ec13251",
    "option" : "Load streaming data from Amazon CloudWatch Logs using AWS Lambda functions as event handlers.",
    "isCorrect" : "false"
  }, {
    "id" : "b0e3a7e2465a42e4b06415182447a56e",
    "option" : "Load streaming data from AWS IoT using AWS Lambda functions as event handlers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nYou can load streaming data into your Amazon OpenSearch Service domain from many different sources.\nSome sources, like Amazon Kinesis Data Firehose and Amazon CloudWatch Logs, have built-in support for OpenSearch Service.\nOthers, like Amazon S3, Amazon Kinesis Data Streams, and Amazon DynamoDB, use AWS Lambda functions as event handlers.\nThe Lambda functions respond to new data by processing it and streaming it to your domain.\nYou can use Lambda to send data to your OpenSearch Service domain from Amazon S3\nNew data that arrives in an S3 bucket triggers an event notification to Lambda, which then runs your custom code to perform the indexing.\nOptions A and C are incorrect because they don't require lambda functions as event handlers to load streaming data.\nKinesis Data Firehose supports OpenSearch Service as a delivery destination.\nAnd you can load streaming data from CloudWatch Logs to your OpenSearch Service domain by using a CloudWatch Logs subscription.\nOption D is incorrect because rules are required instead of lambda functions to load streaming data from AWS IoT.\nRules give your devices the ability to interact with AWS services.\nRules are analyzed and actions are performed based on the MQTT topic stream.\nFor more information on Amazon Athena, please refer to the following URLs:\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html#integrations-s3-lambda\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/integrations.html\n\nAmazon OpenSearch Service is a fully managed search and analytics service that makes it easy to search, analyze, and visualize data in real-time. When it comes to loading streaming data into Amazon OpenSearch Service, there are several options available, including Amazon Kinesis Data Firehose, Amazon S3, Amazon CloudWatch Logs, and AWS IoT.\nOption A: Load streaming data from Amazon Kinesis Data Firehose using AWS Lambda functions as event handlers. Amazon Kinesis Data Firehose is a fully managed service that enables you to deliver real-time streaming data to destinations such as Amazon S3, Amazon Redshift, and Amazon OpenSearch Service. With Kinesis Data Firehose, you can easily load and transform streaming data from various sources, including web applications, IoT devices, and logs. To load streaming data from Kinesis Data Firehose into Amazon OpenSearch Service, you can use AWS Lambda functions as event handlers. The Lambda function will receive the data from Kinesis Data Firehose and transform it before sending it to OpenSearch Service.\nOption B: Load streaming data from Amazon S3 using AWS Lambda functions as event handlers. Amazon S3 is a highly scalable and durable object storage service that can be used to store and retrieve any amount of data from anywhere. To load streaming data from S3 into Amazon OpenSearch Service, you can use AWS Lambda functions as event handlers. The Lambda function will be triggered by a new object being created in the S3 bucket and will transform the data before sending it to OpenSearch Service.\nOption C: Load streaming data from Amazon CloudWatch Logs using AWS Lambda functions as event handlers. Amazon CloudWatch Logs is a fully managed service that enables you to monitor, store, and access log files from EC2 instances, AWS CloudTrail, and other sources. To load streaming data from CloudWatch Logs into Amazon OpenSearch Service, you can use AWS Lambda functions as event handlers. The Lambda function will receive the log data from CloudWatch Logs and transform it before sending it to OpenSearch Service.\nOption D: Load streaming data from AWS IoT using AWS Lambda functions as event handlers. AWS IoT is a fully managed service that enables you to connect IoT devices to the cloud and securely interact with them. To load streaming data from AWS IoT into Amazon OpenSearch Service, you can use AWS Lambda functions as event handlers. The Lambda function will be triggered by a new message being published to an IoT topic and will transform the data before sending it to OpenSearch Service.\nIn summary, all of the options listed are valid ways to load streaming data into Amazon OpenSearch Service from different sources. The choice of the most appropriate option will depend on the specific use case and the source of the streaming data.\n\n"
}, {
  "id" : 166,
  "question" : "A company plans to deploy a batch processing application using docker containers.\nWhich of the following would ideally help to host this application? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "60e613f27e26494984db47782387dd89",
    "option" : "Copy the batch processing application to an ECS Container.",
    "isCorrect" : "false"
  }, {
    "id" : "f059ed055ddb409488c9b59f0c7a12e9",
    "option" : "Create a docker image of your batch processing application.",
    "isCorrect" : "true"
  }, {
    "id" : "a9ac635725da447eb5d8addc69d1ef47",
    "option" : "Deploy the image as an Amazon ECS task.",
    "isCorrect" : "true"
  }, {
    "id" : "5c2195e9b9c44bfdb5441d9afe7e9d8d",
    "option" : "Deploy the container behind the EL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nAWS Documentation mentions the following:\nDocker containers are particularly suited for batch job workloads.\nBatch jobs are often short-lived and embarrassingly parallel.\nYou can package your batch processing application into a Docker image so that you can deploy it anywhere, such as in an Amazon ECS task.\nReference:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/common_use_cases.html\n\nSure, I can explain the options for hosting a batch processing application using Docker containers.\nA. Copy the batch processing application to an ECS Container: Amazon Elastic Container Service (ECS) is a fully-managed container orchestration service provided by AWS. In this option, you can copy the batch processing application to an ECS container, which can run on either EC2 or Fargate instances. However, simply copying the application to the container won't be sufficient. You will also need to create a Docker image of the application and deploy it as an ECS task, which brings us to the next option.\nB. Create a Docker image of your batch processing application: To run a batch processing application using Docker containers, you will first need to create a Docker image of your application. A Docker image is a lightweight, standalone, executable package that contains everything needed to run the application, including the application code, runtime, system tools, libraries, and settings. You can use a Dockerfile to define the application's environment and dependencies, and then build the image using the Docker build command.\nC. Deploy the image as an Amazon ECS task: Once you have created the Docker image of your batch processing application, you can deploy it as an ECS task. An ECS task is a unit of work that runs on an ECS cluster, which is a group of EC2 instances or Fargate tasks that host the containers. You can define the task's configuration, including the number of containers, CPU and memory requirements, and networking settings, using an ECS task definition. You can then deploy the task using the ECS service, which manages the lifecycle of the task, including scaling, updating, and monitoring.\nD. Deploy the container behind the EL: Elastic Load Balancer (ELB) is a service that automatically distributes incoming traffic across multiple EC2 instances or containers. However, it is not directly related to hosting a batch processing application using Docker containers. ELB is primarily used for load balancing web applications or microservices that receive incoming requests from clients. Batch processing applications, on the other hand, typically run as batch jobs that process large volumes of data offline, without any incoming requests from clients.\nIn summary, the ideal options for hosting a batch processing application using Docker containers would be to create a Docker image of the application and deploy it as an ECS task. Therefore, options B and C are the correct answers.\n\n"
}, {
  "id" : 167,
  "question" : "A start-up firm has a corporate office in New York &amp; a regional office in Washington &amp; Chicago.\nThese offices are interconnected over Internet links.\nRecently they have migrated a few application servers to EC2 instance launched in the AWS US-east-1 region.\nThe Developer Team located at the corporate office requires secure access to these servers for initial testing &amp; performance checks before go-live of the new application.\nSince the go-live date is approaching soon, the IT team is looking for quick connectivity to be established.\nAs an AWS consultant, which link option will you suggest as a cost-effective &amp; quick way to establish secure connectivity from on-premise to servers launched in AWS?\n",
  "answers" : [ {
    "id" : "fda8256fdb0b4dea95b584f7e935433c",
    "option" : "Use AWS Direct Connect to establish IPSEC connectivity from On-premise to VGW.",
    "isCorrect" : "false"
  }, {
    "id" : "94ca7d10125e42d19657907442d88720",
    "option" : "Install a third party software VPN appliance from AWS Marketplace in the EC2 instance to create a VPN connection to the on-premises network.",
    "isCorrect" : "false"
  }, {
    "id" : "13b3d473513045d7b67bf3d0a1e46a29",
    "option" : "Use Hardware VPN over AWS Direct Connect to establish IPSEC connectivity from On-premise to VGW.",
    "isCorrect" : "false"
  }, {
    "id" : "4421a0b8366649239fcca712425116a3",
    "option" : "Use AWS Site-to-Site VPN to establish IPSEC VPN connectivity between VPC and the on-premises network.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nUsing AWS VPN is the fastest &amp; cost-effective way of establishing IPSEC connectivity from on-premise to AWS.\nIT teams can quickly set up a VPN connection with VGW in the US-east-1 region so that internal users can seamlessly connect to resources hosted on AWS.\nOption A is incorrect as AWS Direct Connect does not provide IPSEC connectivity.\nIt is not a quick way to establish connectivity.\nOption B is incorrect as you need to look for a third party solution from AWS Marketplace.\nAnd it may not be as cost-efficient as option.\nD.Option C is incorrect as although this will provide a high performance secure IPSEC connectivity from On-premise to AWS, it is not a quick way to establish connectivity.\nIt may take weeks or months to configure the AWS Direct Connect connection.\nAWS Direct Connect is also not cost-effective.\nFor more information on using AWS Direct Connect &amp; VPN, refer to the following URL:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/network-to-amazon-vpc-connectivity-options.html\n\nThe requirement is to establish secure connectivity from the corporate office to the EC2 instances launched in the AWS US-east-1 region.\nOption A suggests using AWS Direct Connect to establish IPSEC connectivity from on-premise to Virtual Private Gateway (VGW). AWS Direct Connect is a dedicated network connection from on-premises infrastructure to AWS. This option requires the purchase of dedicated network equipment and a Direct Connect port, which could take some time to set up. It is also the most expensive option among the four. Therefore, this option might not be the most appropriate solution for the situation.\nOption B suggests installing a third-party software VPN appliance from AWS Marketplace in the EC2 instance to create a VPN connection to the on-premises network. This option is relatively quick and easy to set up. It allows secure access to the EC2 instances from the corporate office without requiring dedicated network equipment. The cost is only the hourly fee for running the VPN instance. However, the VPN connection may become a bottleneck for traffic if many users need to access the EC2 instances simultaneously.\nOption C suggests using Hardware VPN over AWS Direct Connect to establish IPSEC connectivity from on-premise to VGW. This option is similar to option A, but it uses a Hardware VPN instead of a Direct Connect port. This option is more cost-effective than option A, but still requires the purchase of dedicated network equipment. Additionally, it might take some time to set up.\nOption D suggests using AWS Site-to-Site VPN to establish IPSEC VPN connectivity between the VPC and the on-premises network. This option is quick and easy to set up, and it does not require any dedicated network equipment. It is a cost-effective solution for the given situation. It allows secure access to the EC2 instances from the corporate office without any additional costs, except for the hourly fee for running the VPN instance.\nTherefore, option D is the most suitable and cost-effective solution for the situation.\n\n"
}, {
  "id" : 168,
  "question" : "Your company uses KMS to fully manage the master keys and perform encryption and decryption operations on your data and applications.As an additional level of security, you now recommend AWS rotate your keys.\nWhat would happen after enabling this additional feature?\n",
  "answers" : [ {
    "id" : "4cea0a24433a42c5a964ec5ea955d524",
    "option" : "Nothing needs to be done. KMS will manage all encrypt/decrypt actions using the appropriate keys.",
    "isCorrect" : "true"
  }, {
    "id" : "1407b31723aa4ddf8eab0a34d8ffd69a",
    "option" : "Your company must instruct KMS to re-encrypt all data in all services each time a new key is created.",
    "isCorrect" : "false"
  }, {
    "id" : "16e9a902301a495ba58c6b15d4737107",
    "option" : "You have 30 days to delete old keys after a new one is rotated in.",
    "isCorrect" : "false"
  }, {
    "id" : "cbeeb1c759574082aa2f41988010bcec",
    "option" : "Your company must create its own keys and import them to KMS to enable key rotation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is correct.\nKMS will rotate keys annually and use the appropriate keys to perform cryptographic operations.\nOption B is incorrect.\nThis is not necessary.\nKMS, as a managed service, will keep old keys and perform operations based on the appropriate key.\nOption C is incorrect.\nThis is not a requirement of KMS.\nOption D is incorrect.\nThis is not a requirement of KMS.\nReferences:\nhttps://aws.amazon.com/kms/faqs/\nttps://docs.aws.amazon.com/general/latest/gr/rande.html#kms_region.\nhttps://www.slideshare.net/AmazonWebServices/encryption-and-key-management-in-aws\n\nAWS Key Management Service (KMS) is a managed service that allows you to create and control the encryption keys used to encrypt your data. Key rotation is a feature provided by KMS that allows you to periodically create new cryptographic keys and replace older ones. By rotating keys, you add an additional layer of security to your data by reducing the amount of time a key is in use.\nEnabling key rotation means that KMS will automatically create new cryptographic keys periodically and then use the new key for encrypting data, while the old key will remain in use for decrypting data that was encrypted using the previous version of the key. This process ensures that even if a key is compromised, the amount of data that can be decrypted is limited to the data that was encrypted using the compromised key.\nNow, let's go through each answer option:\nA. Nothing needs to be done. KMS will manage all encrypt/decrypt actions using the appropriate keys.\nThis is partially correct. Enabling key rotation means that KMS will automatically create new cryptographic keys periodically, and manage all encrypt/decrypt actions using the appropriate keys. However, some additional actions may be required by your company.\nB. Your company must instruct KMS to re-encrypt all data in all services each time a new key is created.\nThis answer is incorrect. When you enable key rotation, KMS will automatically create new cryptographic keys and use the new key for encrypting data. There is no need to re-encrypt all data in all services each time a new key is created.\nC. You have 30 days to delete old keys after a new one is rotated in.\nThis answer is partially correct. After you enable key rotation, KMS will automatically create new cryptographic keys and use the new key for encrypting data. You can configure the key rotation period and the number of keys to retain, but there is no fixed 30-day limit for deleting old keys.\nD. Your company must create its own keys and import them to KMS to enable key rotation.\nThis answer is incorrect. When you enable key rotation, KMS will automatically create new cryptographic keys and use them for encrypting data. There is no need to create your own keys or import them to KMS to enable key rotation.\nIn summary, enabling key rotation in KMS is a recommended security measure that can improve the protection of your data. Once enabled, KMS will automatically create new cryptographic keys periodically and use the new key for encrypting data, while the old key will remain in use for decrypting data that was encrypted using the previous version of the key.\n\n"
}, {
  "id" : 169,
  "question" : "You are a Solutions Architect in a startup company that is releasing the first iteration of its app.\nYour company doesn't have a directory service for its intended users but wants the users to sign in and use the app.\nWhich of the following solutions is the most cost-efficient?\n",
  "answers" : [ {
    "id" : "84790f469b8c4a30a7d3b26f81f6722f",
    "option" : "Create an IAM role for each end user and the user will assume the IAM role when he signs in the APP.",
    "isCorrect" : "false"
  }, {
    "id" : "f411610c3ecd45dcb9878f206cd8d4f0",
    "option" : "Create an AWS user account for each customer.",
    "isCorrect" : "false"
  }, {
    "id" : "7f904c73c22c417b8bf4548181be4337",
    "option" : "Invest heavily in Microsoft Active Directory as itâ€™s the industry standard.",
    "isCorrect" : "false"
  }, {
    "id" : "6d463ddb3a6f46d0b5e5687f079e38ee",
    "option" : "Use Cognito Identity along with a User Pool to securely save usersâ€™ profile attributes.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption A is incorrect.\nIt is improper to assign an IAM role for each end-user.\nIAM role is not a directory service.\nOption B is incorrect.\nAWS account cannot be configured as a directory service.\nOption C is incorrect.\nThis isn't the most efficient means to authenticate and save user information.\nOption D is correct.\nCognito is a managed service that can be used for this app and scale quickly as usage grows.\nReferences:\nhttps://aws.amazon.com/cognito/\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\nhttps://aws.amazon.com/cognito/getting-started/\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/concepts.html\n\nThe most cost-efficient solution for a startup company that wants its users to sign in and use the app without a directory service is to use Amazon Cognito Identity along with a User Pool to securely save users' profile attributes.\nAmazon Cognito Identity is a fully managed identity service that makes it easy for developers to add user sign-up, sign-in, and access control to mobile and web apps. It provides a User Pool, which is a user directory that enables users to sign in to their apps using their preferred social identity provider or their own user name and password. Cognito also provides access to AWS Identity and Access Management (IAM) roles, allowing fine-grained access control to AWS resources.\nOption A, creating an IAM role for each end user, is not feasible as IAM roles are designed for granting access to AWS resources, not for managing user sign-in and access control to apps.\nOption B, creating an AWS user account for each customer, is not cost-efficient because it requires setting up and managing an AWS account for each user. This approach may also raise security and compliance concerns.\nOption C, investing heavily in Microsoft Active Directory, is not cost-efficient for a startup company as it requires a significant upfront investment and ongoing maintenance costs.\nTherefore, the most cost-efficient and effective solution is to use Amazon Cognito Identity along with a User Pool to manage user sign-in and access control to the app.\n\n"
}, {
  "id" : 170,
  "question" : "A company is migrating an on-premises 5TB MySQL database to AWS and expects its database size to increase steadily.\nWhich Amazon RDS engine would meet these requirements?\n",
  "answers" : [ {
    "id" : "74653477110e4c4580cf08b11793f88e",
    "option" : "MySQL",
    "isCorrect" : "false"
  }, {
    "id" : "54609a42e17d4c15b663074bf397e2fe",
    "option" : "Microsoft SQL Server",
    "isCorrect" : "false"
  }, {
    "id" : "d2fb4cfae01e47afb1e137a44a5ca49e",
    "option" : "Oracle",
    "isCorrect" : "false"
  }, {
    "id" : "896c3640fa874a798153a8529c061364",
    "option" : "Amazon Aurora.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation supports the above requirements concerning AWS Aurora.\nAmazon Aurora (Aurora) is a fully managed, MySQL and PostgreSQL compatible, relational database engine.\nIt combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases.\nIt delivers up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.\nAll Aurora Replicas return the same data for query results with minimal replica lag-usually, much lesser than 100 milliseconds after the primary instance has written an update.\nFor more information on AWS Aurora, please visit the following URL:\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Overview.html\nNOTE:\nOn a MySQL DB instance, avoid tables in your database growing too large.\nProvisioned storage limits restrict the maximum size of a MySQL table file to 16 TB.\nHowever, based on database usage, your Amazon Aurora storage will automatically grow, from the minimum of 10 GB up to 64 TB, in 10 GB increments, with no impact on database performance.\nHence, the best answer would be option.\nD.\n\nThe best option for the company to meet their requirements of migrating a 5TB MySQL database to AWS and expecting the database size to increase steadily would be Amazon Aurora (option D).\nAmazon Aurora is a MySQL and PostgreSQL compatible relational database that is designed for the cloud. It is a fully managed database engine, meaning that Amazon takes care of the management and maintenance of the database. Aurora is also highly scalable and can automatically grow or shrink the storage capacity based on the needs of the application.\nAurora is designed for high availability and durability, with data replicated across multiple availability zones in a region. This provides better protection against data loss in case of a failure or disaster.\nIn terms of performance, Aurora is known for its fast and consistent performance due to its unique architecture, which separates the storage layer from the compute layer. This allows Aurora to scale out read operations, which can help with performance as the database grows.\nMicrosoft SQL Server (option B) and Oracle (option C) are not suitable options for a MySQL database migration. Both of these database engines are proprietary and have different syntax and features compared to MySQL. While they can be used on AWS, they would require significant modifications to the existing application code to work with the new database engine.\nFinally, option A (MySQL) is a possible option, but Amazon Aurora provides better scalability, availability, and performance capabilities. Aurora is also compatible with MySQL, meaning that the migration process would be simpler and require fewer modifications to the existing code.\n\n"
}, {
  "id" : 171,
  "question" : "You have implemented AWS Cognito services to require users to sign in and sign up to your app through social identity providers like Facebook, Google, etc.\nYour marketing department wants users to anonymously try out the app because the current log-in requirement is excessive, which may reduce the demand for products and services offered through the app.\nWhat would you suggest to the marketing department in this regard?\n",
  "answers" : [ {
    "id" : "25922806f188496c8dfcb3b3ed4d1b83",
    "option" : "Itâ€™s too much of a security risk to allow unauthenticated users access to the app.",
    "isCorrect" : "false"
  }, {
    "id" : "eef388852acc4bb2a9af76fa7fc770cb",
    "option" : "Cognito Identity supports guest users for the ability to enter the app and have limited access.",
    "isCorrect" : "true"
  }, {
    "id" : "ee154e5fbdf049ea93d32bb61c006555",
    "option" : "A second version of the app will need to be offered for unauthenticated users.",
    "isCorrect" : "false"
  }, {
    "id" : "39ef882400c5426e9e80789c11b85a7e",
    "option" : "This is possible only if we remove the authentication from everywhere.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption B is correct.\nAmazon Cognito Identity Pools can support unauthenticated identities by providing a unique identifier and AWS credentials for users who do not authenticate with an identity provider.\nUnauthenticated users can be associated with a role with limited access to resources compared to a role for authenticated users.\nOption A is incorrect.\nCognito will allow unauthenticated users without being a security risk.\nOption C is incorrect.\nCognito supports both authenticated and unauthenticated users.\nReferences:\nhttps://aws.amazon.com/cognito/\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.html\nhttps://aws.amazon.com/cognito/getting-started/\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/concepts.html\n\nThe marketing department wants users to anonymously try out the app without requiring them to sign in or sign up. This could potentially increase demand for the products and services offered through the app. However, it's important to keep in mind that user authentication provides security for the app and its users.\nOption A suggests that allowing unauthenticated users access to the app would be a security risk. This is true to some extent because unauthenticated users would not have their identities verified, and could potentially cause security breaches or harm to the app or its users.\nOption B suggests that Cognito Identity supports guest users who have limited access to the app. This option is a good compromise because it allows users to access the app without requiring them to sign in or sign up. However, guest users have limited access, which means that they can only perform certain tasks and activities within the app. This ensures that the app remains secure and that user data is protected.\nOption C suggests that a second version of the app needs to be offered for unauthenticated users. This option is not practical because it requires the development of an entirely new version of the app, which can be costly and time-consuming. Moreover, this option still poses a security risk because unauthenticated users may still be able to access certain parts of the app that could potentially cause harm.\nOption D suggests that the only way to allow unauthenticated users access to the app is by removing authentication altogether. This option is not recommended because it completely removes the security measures in place to protect the app and its users. Removing authentication would make the app vulnerable to security breaches, and could potentially lead to data theft or loss.\nIn summary, the best option to suggest to the marketing department is B. Cognito Identity supports guest users who have limited access to the app. This option allows users to access the app without requiring them to sign in or sign up, while still ensuring that the app remains secure and that user data is protected.\n\n"
}, {
  "id" : 172,
  "question" : "Your app uses AWS Cognito Identity for authentication and stores user profiles in a User Pool.\nTo expand the availability and ease of signing in to the app, your team is requesting advice on allowing the use of OpenID Connect (OIDC) identity providers as additional means of authenticating users and saving the user profile information.\nWhat is your recommendation on OIDC identity providers?\n",
  "answers" : [ {
    "id" : "0ce244057f4344f7a9c272d4b7ae8635",
    "option" : "This is supported, along with social and SAML based identity providers.",
    "isCorrect" : "true"
  }, {
    "id" : "0713bb98dffc447789d1c22fc929364e",
    "option" : "This is not supported, only social identity providers can be integrated into User Pools",
    "isCorrect" : "false"
  }, {
    "id" : "291de50af6e741a4b3b5029ac40b3ca3",
    "option" : "If you want OIDC identity providers, then you must include SAML and social-based support as well",
    "isCorrect" : "false"
  }, {
    "id" : "fe2f86beefa144d6b1ffc378ee05272c",
    "option" : "Itâ€™s too much effort to add non-Cognito authenticated user information to a User Pool.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is correct.\nOpenID Connect (OIDC) identity providers (IdPs) (like Salesforce or Ping Identity) are supported in Cognito, along with social and SAML based identity providers.\nYou can add an OIDC IdP to your user pool in the AWS Management Console, with the AWS CLI, or with the user pool API method CreateIdentityProvider.\nOption B is incorrect.\nCognito supports more than just social identity providers, including OIDC, SAML, and its own identity pools.\nOption C is incorrect.\nYou can add any combination of federated types.\nYou don't have to add them all.\nOption D is incorrect.\nWhile there is additional coding to develop this, the effort is most likely not too great to add the feature.\nReferences:\nhttps://aws.amazon.com/cognito/\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-oidc-idp.html\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.html\nhttps://aws.amazon.com/cognito/getting-started/\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/concepts.html\n\nThe correct answer to this question is A. This is supported, along with social and SAML based identity providers.\nAWS Cognito Identity is a service that provides user authentication, authorization, and user management. It allows developers to add user sign-up, sign-in, and access control to their web and mobile apps quickly and easily. Cognito User Pools, a component of AWS Cognito Identity, is a user directory that allows developers to add user registration and sign-in to their web and mobile apps.\nOpenID Connect (OIDC) is a simple identity layer on top of the OAuth 2.0 protocol. It allows clients to verify the identity of the end-user based on the authentication performed by an authorization server, as well as to obtain basic profile information about the end-user in an interoperable and REST-like manner. In simpler terms, OIDC is a protocol that allows third-party identity providers to authenticate users for a given application.\nAWS Cognito Identity supports the integration of OIDC identity providers as well as social and SAML-based identity providers. This means that developers can use Cognito User Pools to store user profiles and manage authentication for all these different types of identity providers. By supporting multiple identity providers, developers can offer their users more options for signing in to their application.\nTo summarize, the recommendation on OIDC identity providers is that they are supported, along with social and SAML based identity providers, when using AWS Cognito Identity for authentication and user profile management.\n\n"
}, {
  "id" : 173,
  "question" : "A company is building a two-tier web application to serve dynamic transaction-based content.\nWhich services would you leverage to enable an elastic and scalable WEB TIER?\n",
  "answers" : [ {
    "id" : "933a963ba2fb43049b7bca6a96047612",
    "option" : "Elastic Load Balancing, Amazon EC2, and Auto Scaling",
    "isCorrect" : "true"
  }, {
    "id" : "426372492268477abdd6a431f8004615",
    "option" : "Elastic Load Balancing, Amazon RDS with Multi-AZ, and Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "7abf4f5fbadb4294916db1a319ced36a",
    "option" : "Amazon RDS with Multi-AZ and Auto Scaling",
    "isCorrect" : "false"
  }, {
    "id" : "877b7aec68a5497a8b4647fe3d86f322",
    "option" : "Amazon EC2, Amazon Dynamo DB, and Amazon S3",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThe question mentions a scalable Web Tier.\nSo Option B, C, and D can be eliminated since they are database related options.\nThe below example ( this is a general depiction giving the deployment design of standby architecture having a two-tier in them ) shows an Elastic Load Balancer connected to 2 EC2 instances via Auto Scaling.\nThis is an example of an elastic and scalable Web Tier.\nBy scalable, we mean that the Auto Scaling process can increase or decrease the number of EC2 Instances as required.\nFor more information on the Elastic Load Balancer, please refer to the URL below.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/introduction.html\n\n\nThe most appropriate answer is A. Elastic Load Balancing, Amazon EC2, and Auto Scaling.\nExplanation:\nA two-tier web application usually consists of a web tier and a database tier. The web tier is responsible for serving dynamic content and handling user requests, while the database tier stores and retrieves data. To ensure the application is elastic and scalable, the company can use the following services:\nElastic Load Balancing: It distributes incoming traffic across multiple Amazon Elastic Compute Cloud (EC2) instances to improve the application's availability and fault tolerance. It also automatically scales the number of instances to handle the varying amount of incoming traffic. Amazon EC2: It provides scalable computing capacity in the cloud. It allows the company to launch and terminate instances on demand based on the traffic volume and user requests. Auto Scaling: It automatically adjusts the number of EC2 instances in response to changes in demand. It enables the company to maintain optimal performance and cost-effectiveness by increasing the number of instances during high traffic periods and reducing them during low traffic periods.\nOption B includes Amazon RDS with Multi-AZ and Amazon S3, which are not necessary for the web tier's elasticity and scalability. Amazon RDS with Multi-AZ provides high availability and fault tolerance for the database tier, while Amazon S3 is an object storage service for storing and retrieving data.\nOption C includes Amazon RDS with Multi-AZ and Auto Scaling, which provides high availability and automatic scaling for the database tier but does not address the web tier's scalability.\nOption D includes Amazon EC2, Amazon DynamoDB, and Amazon S3. While Amazon DynamoDB is a NoSQL database service, it is not necessary for a two-tier web application, as it is not part of the web tier's infrastructure.\nTherefore, option A is the most appropriate answer to enable an elastic and scalable web tier for a two-tier web application.\n\n"
}, {
  "id" : 174,
  "question" : "An instance is launched into a VPC subnet with the network ACL configured to allow all outbound traffic and deny all inbound traffic.\nThe security group of the instance is configured to allow SSH from any IP address.\nWhat changes are required to allow SSH access to the instance?\n",
  "answers" : [ {
    "id" : "f5578dabce09452e981f92f33d55fd1e",
    "option" : "The Outbound Security Group needs to be modified to allow outbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "640e9119f8824249963d7e7e0ca35934",
    "option" : "The Inbound Network ACL needs to be modified to allow inbound traffic",
    "isCorrect" : "true"
  }, {
    "id" : "0d9835d25f0a4b70827b38892fc5d94b",
    "option" : "Nothing, it can be accessed from any IP address using SSH.",
    "isCorrect" : "false"
  }, {
    "id" : "a94c0d5b7ca443daa1b38305a29343f8",
    "option" : "Both the Outbound Security Group and Outbound Network ACL need to be modified to allow outbound traffic.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nFor an EC2 Instance to allow SSH, you can have the below configurations for the Security and Network ACL for Inbound and Outbound Traffic.\nNetwork ACL has to have both an Allow for Inbound and Outbound because network ACLs are stateless.\nResponses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)\nWhereas for Security groups, responses are stateful.\nSo if an incoming request is granted, by default, an outgoing request will also be granted.\nOptions A and D are invalid because Security Groups are stateful.\nHere, any traffic allowed in the Inbound rule is allowed in the Outbound rule too.\nOption C is also incorrect.\nFor more information on Network ACLs, please refer to the URL below.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\n\n\nThe given scenario is that an EC2 instance is launched into a subnet of a VPC with the network ACL configured to allow all outbound traffic and deny all inbound traffic. The security group of the instance is configured to allow SSH from any IP address.\nTo allow SSH access to the instance, we need to make some changes.\nNetwork ACLs and Security groups are the two main components of VPC networking that control inbound and outbound traffic to and from the instances in the VPC.\nNetwork Access Control Lists (NACLs) operate at the subnet level, whereas security groups operate at the instance level.\nNACLs are stateless, meaning that if you allow inbound traffic for a particular port, you must also allow outbound traffic for the same port. NACLs are evaluated in a sequential manner, which means that you need to take care while modifying them because a deny rule can block all subsequent traffic even if there are allow rules that follow.\nSecurity groups are stateful, meaning that if you allow inbound traffic for a particular port, the return traffic is automatically allowed, even if there is no explicit rule that permits it. Security groups are evaluated at the instance level and are independent of each other.\nIn the given scenario, the security group is already configured to allow SSH from any IP address, so we don't need to make any changes to it.\nHowever, the network ACL is configured to deny all inbound traffic, so we need to modify it to allow SSH traffic on port 22. To do this, we can add an inbound rule to the NACL that allows traffic on port 22 from any IP address.\nTherefore, the correct answer is B. The Inbound Network ACL needs to be modified to allow inbound traffic.\nOption A is incorrect because the outbound security group is already configured to allow all outbound traffic, so there is no need to modify it.\nOption C is incorrect because although the security group is configured to allow SSH traffic, the inbound traffic is blocked by the network ACL.\nOption D is incorrect because we only need to modify the inbound traffic rules of the network ACL to allow SSH traffic, and there is no need to modify the outbound rules.\n\n"
}, {
  "id" : 175,
  "question" : "Your company currently has a web distribution hosted using the AWS CloudFront service.\nThe IT Security department has confirmed that the application using this web distribution now falls under the scope of PCI (Payment Card Industry) compliance.\nWhat are the necessary steps to be followed before auditing? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "f80b062d37d7468c8ff010c9dce07661",
    "option" : "Enable CloudFront access logs.",
    "isCorrect" : "true"
  }, {
    "id" : "074a012cb9554622802810db55670782",
    "option" : "Enable Cache in CloudFront.",
    "isCorrect" : "false"
  }, {
    "id" : "92871c3d1410470bbf07808b23ecb00e",
    "option" : "Capture requests that are sent to the CloudFront API.",
    "isCorrect" : "true"
  }, {
    "id" : "56e66d95d48f41e1b8036a6253f67faf",
    "option" : "Enable VPC Flow Logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and C.\nAWS Documentation mentions the following:\nIf you run PCI or HIPAA-compliant workloads based on the AWS Shared Responsibility Model, we recommend that you log your CloudFront usage data for the last 365 days for future auditing purposes.\nThird-party auditors assess the security and compliance of Amazon CloudFront as part of multiple AWS compliance programs.\nOption B is incorrect.\nIt helps to reduce latency.\nOption D is incorrect.\nVPC flow logs capture information about the IP traffic going to and from network interfaces in a VPC but not for CloudFront.\nFor more information on compliance with CloudFront, please visit the following URLs:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html\nhttps://aws.amazon.com/blogs/aws/pci-compliance-for-amazon-cloudfront/\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SERVICENAME-compliance.html\n\nThe Payment Card Industry Data Security Standard (PCI DSS) is a set of security standards designed to ensure that all companies that accept, process, store, or transmit credit card information maintain a secure environment. If an application falls under the scope of PCI compliance, there are certain steps that need to be followed to ensure that the environment is secure. In this case, since the company is using AWS CloudFront for hosting its web distribution, the following two steps need to be taken:\nEnable CloudFront access logs: Enabling access logs in CloudFront allows you to track all requests made to your distribution, including information about the source IP address, the user agent, and the response status. This is an important step in ensuring that you can monitor and audit all activity on your web distribution, which is necessary for PCI compliance. Enable VPC Flow Logs: VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. This feature can be used to monitor and audit traffic to and from your web distribution, which is necessary for PCI compliance.\nOption B, enabling cache in CloudFront, is not related to PCI compliance. Option C, capturing requests that are sent to the CloudFront API, is also not necessary for PCI compliance as the API is not relevant to the web distribution being hosted. Therefore, the correct answers are A and D.\n\n"
}, {
  "id" : 176,
  "question" : "A start-up firm has created a cloud storage application that gives users the ability to store any amount of personal data &amp; share them with their contacts.\nFor this, they are using Amazon S3 buckets to store user data.\nDuring the last quarter, the costing team has observed a surge in storage costs for the S3 bucket.\nFurther checking observed that many 100 GB files are uploaded by users &amp; are in a partially completed state.\nAs an AWS consultant, the IT Team requests you to prevent this from happening again by stopping incomplete multipart uploads and deleting all parts associated with the multipart upload if they do not complete within a specified number of days after initiation.\nWhich of the following actions can be taken to meet this requirement cost-effectively with the least effort?\n",
  "answers" : [ {
    "id" : "87d0be0d1c2840af97034a83198a2398",
    "option" : "Create an S3 lifecycle Configuration to abort incomplete multipart uploads.",
    "isCorrect" : "true"
  }, {
    "id" : "d1df8f99c0ad4d8aa291ecd8ece2135f",
    "option" : "Manually delete incomplete multipart uploads from the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "51dc6e14261e4fd6a560ec12186bb0f7",
    "option" : "Use Cron tool to identify incomplete uploads &amp; delete those files.",
    "isCorrect" : "false"
  }, {
    "id" : "303f500b769546629b97cebc989103c5",
    "option" : "All Incomplete uploads are automatically deleted every three months by Amazon S3.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon S3 Lifecycle rules can be configured to abort all multipart uploads which are failing to complete in a specific time period.\nFor all files from size 5 MB to 5GB, the multipart upload can be used.\nOption B is incorrect as this will need additional manual efforts.\nOption C is incorrect as this incurs additional cost &amp; admin work to use Cron tools.\nOption D is incorrect as incomplete uploads are not automatically deleted.\nFor more information on using S3 Lifecycle policies, refer to the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-configuration-examples.html\n\nThe most cost-effective and efficient way to prevent incomplete multipart uploads from increasing storage costs on an Amazon S3 bucket is to create an S3 lifecycle configuration to abort incomplete multipart uploads.\nOption A is the correct answer.\nAn S3 lifecycle configuration enables the automation of the transition and deletion of objects in an S3 bucket. An S3 lifecycle policy can be used to automatically delete incomplete multipart uploads after a specified number of days since initiation if they do not complete. This will help to reduce the storage costs for the S3 bucket and prevent the accumulation of partially uploaded files.\nOption B is not an efficient method for handling incomplete multipart uploads as it involves manually deleting the files, which is time-consuming and prone to human error.\nOption C is not recommended because using a Cron tool would require scripting to identify and delete the incomplete uploads, which can be complex, and there is a risk of deleting necessary files accidentally.\nOption D is incorrect. Amazon S3 does not automatically delete incomplete uploads every three months. Incomplete multipart uploads remain in the S3 bucket until they are completed or explicitly deleted.\nTherefore, the most effective and cost-efficient solution to prevent incomplete multipart uploads from increasing storage costs on an Amazon S3 bucket is to create an S3 lifecycle configuration to abort incomplete multipart uploads.\n\n"
}, {
  "id" : 177,
  "question" : "You are developing a new mobile application which is expected to be used by thousands of customers.\nYou are considering to store user preferences in AWS and need a non-relational data store to save the same.\nEach data item is expected to be 20KB in size.\nThe solution needs to be cost-effective, highly available, scalable, and secure.\nWhich of the following designs is the most suitable?\n",
  "answers" : [ {
    "id" : "170c95a268c147b088bfdb94a99da44b",
    "option" : "Create a new Amazon RDS instance and store the user data there.",
    "isCorrect" : "false"
  }, {
    "id" : "292bbaa19c654dcf80f19fdd54956788",
    "option" : "Create a Amazon DynamoDB table with the required Read and Write capacity and use it as the data layer.",
    "isCorrect" : "true"
  }, {
    "id" : "8ce3f8a66c1a4194924766c69ac6f05d",
    "option" : "Use Amazon Glacier to store the user data.",
    "isCorrect" : "false"
  }, {
    "id" : "96bb82f7eb874c1e8c8cadd08ebaa0ac",
    "option" : "Use an Amazon Redshift Cluster for managing the user preferences.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nIn this case, since each data item is 20KB and given the fact that DynamoDB is an ideal data layer for storing user preferences, this would be the ideal choice.\nAlso, DynamoDB is a highly scalable and available service.\nFor more information on AWS DynamoDB, please refer to the URL given below:\nhttps://aws.amazon.com/dynamodb/\n\nOut of the given options, option B is the most suitable choice for storing user preferences in a non-relational data store for a mobile application.\nAmazon DynamoDB is a fully managed NoSQL database service that provides low-latency access to data and can handle millions of requests per second. DynamoDB automatically scales up and down to accommodate traffic fluctuations and supports data encryption at rest and in transit, making it a highly available and secure solution. DynamoDB can also be cost-effective since it only charges for the provisioned capacity and the actual usage.\nIn this case, the expected size of each data item is 20KB, which is well within the limit of DynamoDB's maximum item size of 400KB. Since the application is expected to be used by thousands of customers, using DynamoDB as the data layer can ensure that it can handle the expected traffic and scaling needs.\nOption A, creating a new Amazon RDS instance, is not suitable since RDS is a relational database service and may not provide the necessary scalability and cost-effectiveness required for this use case.\nOption C, using Amazon Glacier to store the user data, is not suitable since Glacier is designed for long-term data archiving and retrieval, and may not provide the necessary low-latency access required for a mobile application.\nOption D, using Amazon Redshift cluster, is not suitable for storing user preferences since Redshift is a data warehousing solution designed for analytics and reporting and may not provide the necessary low-latency access and cost-effectiveness required for this use case.\nTherefore, option B, creating an Amazon DynamoDB table with the required Read and Write capacity and using it as the data layer, is the most suitable solution for storing user preferences in a cost-effective, highly available, scalable, and secure manner.\n\n"
}, {
  "id" : 178,
  "question" : "Your Operations department is using an incident-based application hosted on a set of EC2 Instances.\nThese instances are placed behind an Auto Scaling Group to ensure that the right number of instances are in place to support the application.\nThe Operations department has expressed dissatisfaction concerning poor application performance every day at 9:00 AM.\nHowever, it is also noted that the system performance returns to optimal at 9:45 AM. What could be done to fix this issue?\n",
  "answers" : [ {
    "id" : "3b3d46f6463a45c8b754fc432a832a7e",
    "option" : "Create another Dynamic Scaling Policy to ensure that the scaling happens at 9:00 AM.",
    "isCorrect" : "false"
  }, {
    "id" : "98cf6a2a781c4b57b4e25202245788d3",
    "option" : "Add another Auto Scaling group to support the current one.",
    "isCorrect" : "false"
  }, {
    "id" : "f6a1004f561d48c7926345a34cf51234",
    "option" : "Change the Cool Down Timers for the existing Auto Scaling Group.",
    "isCorrect" : "false"
  }, {
    "id" : "fea062f975c040ed965cbfe971bf7e83",
    "option" : "Add a Scheduled Scaling Policy at 8:30 AM.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nScheduled Scaling can be used to ensure that the capacity is peaked before 9:00 AM every day.\nAWS Documentation further mentions the following on Scheduled Scaling:\nScaling based on a schedule allows you to scale your application in response to predictable load changes.\nFor example, every week the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday.\nYou can plan your scaling activities based on the predictable traffic patterns of your web application.\nOption A is incorrect because a scheduled scaling should be used as per the requirements of the question instead of dynamic scaling.\nOption B is incorrect because adding another autoscaling group will not solve the problem.\nOption C is incorrect because changing the cooldown timers of the existing autoscaling group will not meet the requirements of the question.\nFor more information on Scheduled Scaling, please refer to the URL below:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\n\nThe issue described in the question involves poor application performance every day at 9:00 AM, which then returns to optimal at 9:45 AM. This suggests that there is a period of high demand on the application during that time, causing performance issues. To resolve this issue, we need to determine the root cause of the performance issue and identify the appropriate solution.\nOption A suggests creating another dynamic scaling policy to ensure that the scaling happens at 9:00 AM. While this may increase the number of instances during the period of high demand, it does not address the root cause of the performance issue. Additionally, it may lead to unnecessary scaling during other periods of low demand, increasing costs.\nOption B suggests adding another Auto Scaling group to support the current one. This solution may also increase the number of instances during the period of high demand, but it does not address the root cause of the performance issue. Additionally, it increases the complexity of the deployment, making it more difficult to manage.\nOption C suggests changing the Cool Down Timers for the existing Auto Scaling Group. This solution involves adjusting the period of time between scaling events. While this may be a viable solution, it requires knowledge of the specific scaling policies in use and the performance characteristics of the application. If the current cool-down timers are too long, it may take too long to scale up during periods of high demand. If they are too short, it may result in unnecessary scaling during periods of low demand.\nOption D suggests adding a Scheduled Scaling Policy at 8:30 AM. This solution involves scheduling a scaling event before the period of high demand, allowing the number of instances to increase before the performance issue occurs. This solution addresses the root cause of the performance issue by proactively increasing the capacity of the application during periods of high demand, rather than reacting to the issue after it occurs. Additionally, it minimizes unnecessary scaling during other periods of low demand.\nIn conclusion, the most appropriate solution to fix the performance issue is Option D - adding a Scheduled Scaling Policy at 8:30 AM. This solution addresses the root cause of the issue, minimizes unnecessary scaling, and simplifies the deployment by avoiding the need for additional Auto Scaling groups or complex scaling policies.\n\n"
}, {
  "id" : 179,
  "question" : "As an AWS Solutions Architect, you are designing a new Java application in EC2 with a relational database deployed in RDS.\nDuring proof of concept, it has been found that the database encounters an extended number of write operations at certain times and cannot handle the load.\nWhich of the following options is appropriate to improve the write performance?\n",
  "answers" : [ {
    "id" : "2bd0f1ab4c0044b7b4f940b6f3ebdf4b",
    "option" : "Add a read replica to the RDS DB instance.",
    "isCorrect" : "false"
  }, {
    "id" : "f6e31e0e3728463b8bd42558a008990b",
    "option" : "Replace AWS RDS with DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "efa33c1e32074c63a64272654eef0356",
    "option" : "Use SQS to queue the database writes.",
    "isCorrect" : "true"
  }, {
    "id" : "0606a32e18bf485b859d403b465defc6",
    "option" : "Use SNS to send notification for the missed database writes and then add them manually at a later stage.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nSQS Queues can be used to cache the pending database writes so that the database can handle the load properly.\nFor more information on AWS SQS, please refer to the URL below:\nhttps://aws.amazon.com/sqs/faqs/\nOption A is incorrect because read replica cannot help on the write requests.\nOption B is incorrect because DynamoDB is non-relational however the application needs a relational database.\nOption D is incorrect because adding the missed write operation back manually is not a suitable approach.\n\nThe correct option to improve write performance for an RDS database that encounters an extended number of write operations at certain times is:\nC. Use SQS to queue the database writes.\nExplanation:\nOption A, adding a read replica to the RDS DB instance, won't necessarily improve write performance since the write operations will still hit the primary RDS instance, and then be replicated to the read replica(s). This will reduce read latency, but not write latency.\nOption B, replacing AWS RDS with DynamoDB, is a viable option for improving write performance since DynamoDB is designed to handle high write throughput. However, it may require significant application changes since DynamoDB is a NoSQL database and has a different data model than a relational database like RDS.\nOption D, using SNS to send notifications for missed database writes and then adding them manually at a later stage, is not a scalable solution for handling high write throughput since it requires manual intervention.\nOption C, using SQS to queue the database writes, is the most appropriate option since it can handle high write throughput and provides scalable and reliable message queuing. The Java application can send messages to an SQS queue instead of writing directly to the RDS database. The SQS queue will then handle the writes in a more scalable and efficient way by buffering and batching the writes to the RDS instance, thereby reducing the load on the RDS database. The SQS queue can also be configured to automatically scale to handle higher write throughput.\n\n"
}, {
  "id" : 180,
  "question" : "You have created an AWS Lambda function that will write data to a DynamoDB table.\nWhich of the following must be in place to ensure that the Lambda function can interact with the DynamoDB table?\n",
  "answers" : [ {
    "id" : "5d294a925a3747e890f0afd9f1bf0ca5",
    "option" : "Ensure an IAM Role is attached to the Lambda function which has the required DynamoDB privileges.",
    "isCorrect" : "true"
  }, {
    "id" : "5165b4b3ef35450da6c7f36ed73d5af7",
    "option" : "Ensure an IAM User is attached to the Lambda function which has the required DynamoDB privileges.",
    "isCorrect" : "false"
  }, {
    "id" : "5d429a2c323b46918cd4c12adc6b1b50",
    "option" : "Ensure the Access keys are embedded in the AWS Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "6a9fe4258014440fa0f6ebfea90e0768",
    "option" : "Ensure the IAM user password is embedded in the AWS Lambda function.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following to support this requirement:\nEach Lambda function has an IAM role (execution role) associated with it.\nYou specify the IAM role when you create your Lambda function.\nPermissions you grant to this role determine what AWS Lambda can do when it assumes the role.\nThere are two types of permissions that you grant to the IAM role:\nIf your Lambda function code accesses other AWS resources, such as reading an object from an S3 bucket or writing logs to CloudWatch Logs, you need to grant permissions for relevant Amazon S3 and CloudWatch actions to the role.\nIf the event source is stream-based (Amazon Kinesis Data Streams and DynamoDB streams), AWS Lambda polls these streams on your behalf.\nAWS Lambda needs permissions to poll the stream and read new records on the stream.\nSo you need to grant the relevant permissions to this role.\nFor more information on the Permission Role model for AWS Lambda, please refer to the URL below:\nhttps://docs.aws.amazon.com/lambda/latest/dg/intro-permission-model.html\n\nThe correct answer is A: Ensure an IAM Role is attached to the Lambda function which has the required DynamoDB privileges.\nExplanation:\nAWS Lambda functions can interact with various AWS services, including DynamoDB, to perform tasks such as reading and writing data. However, to enable this interaction, the Lambda function needs to have appropriate permissions.\nTo give permissions to an AWS Lambda function to interact with DynamoDB, an IAM role should be created with the necessary permissions and then attached to the Lambda function. An IAM role is an AWS Identity and Access Management (IAM) entity that defines a set of permissions for making AWS service requests. When a Lambda function is executed, it assumes the IAM role and can perform actions allowed by the role.\nUsing an IAM role has several benefits:\nIt provides a more secure approach to granting permissions to Lambda functions, as credentials such as access keys or passwords are not embedded in the function code. It allows for easier management of permissions and access control, as permissions can be updated for the role without needing to modify the Lambda function code. It simplifies the process of granting the same set of permissions to multiple Lambda functions, as the same role can be attached to multiple functions.\nOption B is incorrect because IAM users are used to grant permissions to people or services outside of AWS Lambda functions, and IAM users cannot be attached to a Lambda function.\nOption C is incorrect because embedding access keys in the Lambda function code is not recommended, as it can expose the keys to potential security risks.\nOption D is also incorrect because embedding passwords in the Lambda function code is not recommended, as it can expose the password to potential security risks.\n\n"
}, {
  "id" : 181,
  "question" : "A Media firm is saving all its old videos in S3 Glacier Deep Archive.\nDue to the shortage of new video footage, the channel has decided to reuse all these old videos.\nSince these are old videos, the channel is not sure of its popularity &amp; response from users.\nChannel Head wants to make sure that these huge size files do not shoot up their budget.\nFor this, as an AWS consultant, you advise them to use the S3 intelligent storage class.\nThe Operations Team is concerned about moving these files to the S3 Intelligent-Tiering storage class.\nWhich of the following actions can be taken to move objects in Amazon S3 Glacier Deep Archive to S3 Intelligent-Tiering storage class?\n",
  "answers" : [ {
    "id" : "7465c172b36346c9ad55a820cf392ff9",
    "option" : "Use Amazon S3 Console to copy these objects from S3 Glacier Deep Archive to required S3 Intelligent-Tiering storage class.",
    "isCorrect" : "false"
  }, {
    "id" : "db76392cbed74dcbb401bbb8416b4553",
    "option" : "Use Amazon S3 Glacier Console to restore objects from S3 Glacier Deep Archive &amp; then copy these objects to required S3 Intelligent-Tiering storage class.",
    "isCorrect" : "false"
  }, {
    "id" : "c66540cc006c4cee9d9c97528c689ea3",
    "option" : "Use Amazon S3 console to restore objects from S3 Glacier Deep Archive &amp; then copy these objects to required S3 Intelligent-Tiering storage class.",
    "isCorrect" : "true"
  }, {
    "id" : "0e05f78276824600ab1da8f0b678e449",
    "option" : "Use Amazon S3 Glacier console to copy these objects to the required S3 Intelligent-Tiering storage class.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nTo move objects from Glacier Deep Archive to different storage classes, first need to restore to original locations using Amazon S3 console &amp; then use lifecycle policy to move objects to required S3 Intelligent-Tiering storage class.\nOption A &amp; D are incorrect as Objects in Glacier Deep Archive cannot be directly moved to another storage class.\nThese need to be restored first &amp; then copied to the desired storage class.\nOption B is incorrect as the Amazon S3 Glacier console can be used to access the vaults and objects in them.\nBut it cannot be used to restore the objects.\nFor more information on moving objects between S3 storage classes, refer to the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/lifecycle-transition-general-considerations.html\n\nIn order to move objects from Amazon S3 Glacier Deep Archive to S3 Intelligent-Tiering storage class, we need to first understand the differences between the two storage classes and their properties.\nAmazon S3 Glacier Deep Archive is a very low-cost, long-term archival storage class that provides secure, durable object storage. The cost of storing data in S3 Glacier Deep Archive is much lower than S3 Intelligent-Tiering, but the retrieval time can take several hours. This storage class is suitable for data that is rarely accessed and has a retrieval time of 12 hours or more.\nOn the other hand, Amazon S3 Intelligent-Tiering storage class is designed to optimize costs by automatically moving objects between two access tiers: frequent access and infrequent access. This storage class is suitable for data with unknown or changing access patterns, where it automatically moves data between access tiers based on changing access patterns.\nGiven the above differences, it is clear that S3 Intelligent-Tiering storage class is better suited for data with unknown access patterns, as is the case with the media firm's old videos. Therefore, as an AWS consultant, we can advise the firm to use the S3 Intelligent-Tiering storage class.\nTo move objects from S3 Glacier Deep Archive to S3 Intelligent-Tiering storage class, we have four options:\nOption A: Use Amazon S3 Console to copy these objects from S3 Glacier Deep Archive to required S3 Intelligent-Tiering storage class. This option involves using the S3 console to copy the objects from S3 Glacier Deep Archive to the required S3 Intelligent-Tiering storage class. This option is valid, as we can use the S3 console to copy objects between different storage classes.\nOption B: Use Amazon S3 Glacier Console to restore objects from S3 Glacier Deep Archive and then copy these objects to the required S3 Intelligent-Tiering storage class. This option involves using the S3 Glacier console to restore the objects from S3 Glacier Deep Archive and then using the S3 console to copy the objects to the required S3 Intelligent-Tiering storage class. This option is also valid, as we can restore the objects from S3 Glacier Deep Archive using the S3 Glacier console.\nOption C: Use Amazon S3 console to restore objects from S3 Glacier Deep Archive and then copy these objects to the required S3 Intelligent-Tiering storage class. This option is the same as option B, except that we are using the S3 console to restore the objects from S3 Glacier Deep Archive. This option is also valid, as we can restore the objects from S3 Glacier Deep Archive using the S3 console.\nOption D: Use Amazon S3 Glacier console to copy these objects to the required S3 Intelligent-Tiering storage class. This option is not valid, as the S3 Glacier console does not have the capability to copy objects to S3 Intelligent-Tiering storage class.\nTherefore, options A, B, and C are all valid ways to move objects from S3 Glacier Deep Archive to S3 Intelligent-Tiering storage class.\n\n"
}, {
  "id" : 182,
  "question" : "A large retail firm saves its global sales reports in the S3 bucket &amp; uses S3 Lifecycle rules to move this data from Standard_IA storage class to AWS S3 Glacier post 180 days.\nDue to the financial year-end, the Finance team is looking for a sales report for only the Europe region where a mismatch is reported in the sales figure.\nWhich of the following is a recommended way to fetch this data with the least effort?\n",
  "answers" : [ {
    "id" : "539b99e5528a464aba93f728ea17470b",
    "option" : "Retrieve this data from Amazon Glacier to S3 bucket &amp; use Amazon S3 select to query specific continent data using simple SQL.",
    "isCorrect" : "false"
  }, {
    "id" : "383e55513eb04ef29199f6dcf170a0ad",
    "option" : "Retrieve this data from Amazon Glacier to S3 bucket &amp; use Amazon Athena to query specific continent data using SQL.",
    "isCorrect" : "false"
  }, {
    "id" : "eead94774120424b8acdbe009a499dcd",
    "option" : "Use Amazon S3 Glacier Select to query specific continent data which is restored to S3 bucket from AWS S3 Glacier.",
    "isCorrect" : "false"
  }, {
    "id" : "1847261f9c24416eb4d00d1468619886",
    "option" : "Use Amazon S3 Glacier Select to query specific continent data directly from Amazon S3 Glacier using simple SQL.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAmazon S3 Glacier Select can be used to query specific data from Amazon S3 Glacier instead of querying whole data.\nAmazon S3 Glacier Select can directly query data from Amazon S3 Glacier &amp; restoration of data to the S3 bucket is not required for querying this data.\nOption A is incorrect because for data stored in Amazon S3 Glacier, it's not necessary to restore data to the S3 bucket.\nOption B is incorrect because for data stored in Amazon S3 Glacier, it's not necessary to restore data to the S3 bucket.\nAlso, Amazon Athena is an interactive query tool to analyze data with an S3 bucket.\nOption C is incorrect because with Amazon S3 Glacier Select, you do not need to restore data from Glacier to S3.\nFor more information on using Amazon S3 Glacier Select, refer to the following URL:\nhttps://aws.amazon.com/glacier/features/#amazon-glacier-select\n\nThe recommended way to fetch the required data with the least effort is Option B: Retrieve this data from Amazon Glacier to S3 bucket and use Amazon Athena to query specific continent data using SQL.\nExplanation:\nThe S3 bucket used by the retail firm has a lifecycle rule that moves the data from the Standard_IA storage class to the Glacier storage class after 180 days. Glacier is an extremely low-cost storage option for long-term data retention, but accessing data from Glacier can take several hours or even days due to its retrieval process. Therefore, it is not recommended to query data directly from Glacier using simple SQL.\nOption A suggests retrieving the required data from Glacier to the S3 bucket and using Amazon S3 select to query specific continent data using simple SQL. However, this approach involves data retrieval from Glacier, which can take several hours or even days. Moreover, using Amazon S3 select to query data can be time-consuming and requires knowledge of SQL.\nOption C suggests using Amazon S3 Glacier Select to query specific continent data that is restored to the S3 bucket from AWS S3 Glacier. However, restoring data from Glacier can take several hours or even days, and using Glacier Select requires knowledge of its specific syntax.\nOption B suggests retrieving the required data from Glacier to the S3 bucket and using Amazon Athena to query specific continent data using SQL. Amazon Athena is an interactive query service that can easily and quickly analyze data directly from S3 using SQL. It can also query data in any format, including CSV, JSON, and Parquet. Since the required data is already stored in the S3 bucket, it can be queried directly using Amazon Athena, without having to wait for data retrieval or restoration from Glacier. Moreover, the use of SQL makes the querying process simpler and more intuitive.\nTherefore, Option B is the recommended way to fetch the required data with the least effort.\n\n"
}, {
  "id" : 183,
  "question" : "A large educational institute is using Amazon S3 buckets to save data for all graduation streams.\nDuring annual external audits from local government bodies, institutes need to fetch data of specific streams to get it audited from auditors.\nA large amount of data is saved in these S3 buckets, making it cumbersome to download whole data &amp; retrieve only a small amount of information from it.\nThe IT Team is looking for your consultation for this issue without incurring additional costs or compromising security.\nWhich of the following actions is recommended for resolution?\n",
  "answers" : [ {
    "id" : "f53ce811d5d94c93ad0246613724d2c8",
    "option" : "Store objects in CSV format compressing it with Snappy using server-side encryption. Use Amazon S3 Select to retrieve a subset of data.",
    "isCorrect" : "false"
  }, {
    "id" : "dd345ed457c6416c884288ccb026f8da",
    "option" : "Store objects in JSON format compressing it with GZIP using server-side encryption. Use Amazon S3 Select to retrieve a subset of data.",
    "isCorrect" : "true"
  }, {
    "id" : "8b9176ec2cbe4964a496067dcd936945",
    "option" : "Store objects in Apache Parquet format compressing the whole object with GZIP using server-side encryption. Use Amazon S3 Select to retrieve a subset of data.",
    "isCorrect" : "false"
  }, {
    "id" : "3d1ae6884ff541e9afcc8afbb95e2d7d",
    "option" : "Store objects in CSV format compressing it with BZIP2 without any encryption. Use Amazon S3 Select to retrieve a subset of data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon S3 Select can be used to query a subset of data from the objects stored in the S3 bucket using simple SQL.\nFor using this, objects need to be stored in an S3 bucket with CSV, JSON, or Apache Parquet format.\nGZIP &amp; BZIP2 compression is supported with CSV or JSON format with server-side encryption.\nOption A is incorrect as with Amazon S3 Select, only GZIP &amp; BZIP2 compression is supported with CSV format.\nOption C is incorrect as Whole Object compression is not supported for Parquet using GZIP.\n(Column compression is supported).\nOption D is incorrect as although this will work, saving objects in S3 without encryption will risk the security of objects.\nFor more information on using S3 Select, refer to the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/selecting-content-from-objects.html\n\nThe best option for the educational institute in this scenario is to store objects in Apache Parquet format compressing the whole object with GZIP using server-side encryption and using Amazon S3 Select to retrieve a subset of data. Therefore, the correct option is C.\nStoring objects in CSV format (option A and D) can lead to performance issues when retrieving data because it requires scanning the entire object to retrieve a subset of data. Additionally, compressing CSV with Snappy or BZIP2 might not yield optimal results for data retrieval.\nStoring objects in JSON format (option B) can also lead to performance issues as it requires parsing the entire object to retrieve a subset of data. Compressing JSON with GZIP might not yield optimal results for data retrieval, as GZIP can only compress one file at a time, which can increase the overhead in downloading and decompressing files.\nApache Parquet format (option C) is a columnar storage format that can help optimize data retrieval as it stores data in columns rather than rows. This makes it more efficient to retrieve a subset of data as it only needs to read the relevant columns. Additionally, compressing the whole object with GZIP can help reduce storage costs and download times. Server-side encryption ensures that the data is secure while being stored in S3 buckets.\nAmazon S3 Select is a service that allows you to retrieve a subset of data from an object stored in S3, without having to download the entire object. This can help optimize performance and reduce costs as it reduces the amount of data that needs to be retrieved.\nTherefore, option C is the recommended solution for the educational institute, as it provides optimal performance, reduces costs, and maintains data security.\n\n"
}, {
  "id" : 184,
  "question" : "A legal consultant firm is using versioning enabled S3 buckets to save all its legal documents.\nTo avoid any deletion/ modification of these documents, they have locked these files with a retention period of 6 months.\nIn some cases, these legal documents are getting updated with new information that the firm requires to set a different retention period than the original object.\nWhich of the following actions will meet this requirement with the least efforts?\n",
  "answers" : [ {
    "id" : "bca9ba0863ec4b53bed3fba5d574affc",
    "option" : "Create another version of the object with the same name and have a separate retention period than the current object.",
    "isCorrect" : "true"
  }, {
    "id" : "f3f4a8c5f5e04572b0131bb5905cdc4e",
    "option" : "Create another bucket &amp; place new objects with different retention periods.",
    "isCorrect" : "false"
  }, {
    "id" : "f458f9dc0d474a0b82c9522aaa4f98b5",
    "option" : "Overwrite the current object and then place the object in the same bucket with different retention periods.",
    "isCorrect" : "false"
  }, {
    "id" : "083c2461f4a6466a98b509abd4b7b9ef",
    "option" : "Modify name &amp; version of object &amp; have separate retention period than the current object.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nWith version-enabled S3 buckets, each version of an object can have a different retention period.\nOption B is incorrect because although creating different buckets and placing an object in that bucket will work, this is not required as the same can be done using the existing S3 bucket.\nWe have to select the option with the least effort.\nOption C is incorrect because if we overwrite the object then there is no use of versioning-enabled bucket.\nIt will create more complexity within the bucket to identify the actual content modified in the object.\nOption D is incorrect because modifying the name of the same object will increase additional complexity in the bucket.\nFor more information on using Amazon S3 object lock, refer to the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-overview.html#object-lock-retention-periods\n\nThe best option to meet the requirement with the least effort would be A. Create another version of the object with the same name and have a separate retention period than the current object.\nExplanation: The legal consultant firm is using versioning-enabled S3 buckets to store all its legal documents. Versioning-enabled S3 buckets help to keep multiple versions of the same object in the same bucket, with unique version IDs. With versioning, it is possible to recover any previous version of an object that has been overwritten or deleted.\nTo avoid any deletion/modification of these documents, the firm has locked these files with a retention period of 6 months. This retention policy ensures that the objects in the bucket cannot be deleted or modified for a period of 6 months.\nIn some cases, these legal documents are getting updated with new information that the firm requires to set a different retention period than the original object. This means that the retention period needs to be changed for the new version of the object.\nTo meet this requirement with the least effort, the firm can create another version of the object with the same name and have a separate retention period than the current object. This is possible because versioning allows multiple versions of the same object to be stored in the same bucket. By creating a new version of the object, the firm can set a different retention period for that specific version, without affecting the previous versions. This approach is also cost-effective because it does not require the creation of a new bucket or the modification of the existing object.\nOption B, creating another bucket, and placing new objects with different retention periods is not the best approach because it is not necessary to create a new bucket for this scenario. It is also not the most cost-effective option.\nOption C, overwriting the current object and then placing the object in the same bucket with different retention periods, is not recommended because it would modify the original object, which would result in the loss of the original information. This option does not meet the requirement of keeping the original object intact.\nOption D, modifying the name and version of the object and having a separate retention period than the current object, is not necessary because versioning already provides the functionality to keep multiple versions of the same object in the same bucket. This option is also not the most cost-effective approach.\n\n"
}, {
  "id" : 185,
  "question" : "You manage the IT users for a large organization that is moving many services to AWS.\nYou want a seamless way for your employees to log in and use cloud services.\nYou also want to use AWS Managed Microsoft AD and have been asked if users can use it to access services in the on-premises environment.\nWhat would you respond?\n",
  "answers" : [ {
    "id" : "de968aa67c7745f2a60a681febf34aa7",
    "option" : "AWS Managed Microsoft AD requires data synchronization and replication to work properly.",
    "isCorrect" : "false"
  }, {
    "id" : "9b3155e03f814459b373f2b9aa1ca235",
    "option" : "AWS Managed Microsoft AD can only be used for cloud or on-premises environments, not both.",
    "isCorrect" : "false"
  }, {
    "id" : "a8f1107f1f0346f095b4b8a370be16a5",
    "option" : "AWS Managed Microsoft AD can be used as the Active Directory over VPN or Direct Connect.",
    "isCorrect" : "true"
  }, {
    "id" : "8b89a5317c4d4626afd8d1e49d630332",
    "option" : "AWS Managed Microsoft AD is 100% the same as Active Directory running on separate EC2 instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption C is correct.\nBecause you want to use AWS Managed Microsoft AD, you want to be certain that your users can use the AWS cloud resources and services in your on-premise environment.\nTo make your company have connectivity for AWS services, once you implement VPN or Direct Connect, your AWS Managed Microsoft AD can be used for both cloud services and on-premises services.\nOption A is incorrect.\nTo access on-premises resources through AWS Managed Microsoft AD, synchronization or replication is not needed.\nOption B is incorrect.\nAWS Managed Microsoft AD can be used for both.\nIt's not one or the other.\nOption D is incorrect.\nAWS Managed Microsoft AD being a managed service limits some capabilities versus running Active Directory by itself on EC2 instances.\nFor more information, please visit the URLs below:\nhttps://aws.amazon.com/directoryservice/\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\n\nAs the IT manager for a large organization moving many services to AWS, you want a seamless way for employees to log in and use cloud services. You also want to use AWS Managed Microsoft AD and have been asked if users can use it to access services in the on-premises environment.\nAWS Managed Microsoft AD is a fully managed Active Directory service that enables you to connect AWS resources with your on-premises network. It provides managed Active Directory in the AWS Cloud, allowing you to manage users, groups, and computers from a single location. AWS Managed Microsoft AD is compatible with Microsoft AD, so you can use it to manage your Windows workloads in the cloud.\nTo answer the question, the correct answer is C. AWS Managed Microsoft AD can be used as the Active Directory over VPN or Direct Connect. This means that users can use AWS Managed Microsoft AD to access services in the on-premises environment, as long as they are connected to the on-premises network through a VPN or Direct Connect connection.\nOption A is incorrect because although data synchronization and replication are required for AWS Managed Microsoft AD to work properly, it is not the answer to the question being asked.\nOption B is incorrect because AWS Managed Microsoft AD can be used for both cloud and on-premises environments, and it is designed to provide a seamless experience for users regardless of where they are accessing the resources from.\nOption D is incorrect because AWS Managed Microsoft AD is not exactly the same as Active Directory running on a separate EC2 instance. While it is compatible with Microsoft AD, it is a managed service that provides additional features and benefits such as automatic backups, automatic software updates, and built-in security features.\n\n"
}, {
  "id" : 186,
  "question" : "Your company is planning to use Route 53 as the DNS provider.\nThere is a need to ensure that the company's domain name points to an existing CloudFront distribution.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "04bbd98961524573a93a7a6cf4f7a82b",
    "option" : "Create an Alias record which points to the CloudFront distribution.",
    "isCorrect" : "true"
  }, {
    "id" : "a87ed90e069340d1b6d337137e217a26",
    "option" : "Create a host record which points to the CloudFront distribution.",
    "isCorrect" : "false"
  }, {
    "id" : "382a6ba3b9cc49769123bec220168003",
    "option" : "Create a CNAME record which points to the CloudFront distribution.",
    "isCorrect" : "false"
  }, {
    "id" : "bf9a86700f9644cfa7efc10f84daf1fa",
    "option" : "Create a Non-Alias Record which points to the CloudFront distribution.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following.\nWhile ordinary Amazon Route 53 records are standard DNS records, alias records provide a Route 53-specific extension to the DNS functionality.\nInstead of an IP address or a domain name, an alias record contains a pointer to a CloudFront distribution, an Elastic Beanstalk environment, an ELB Classic, Application, or Network Load Balancer, an Amazon S3 bucket that is configured as a static website, or another Route 53 record in the same hosted zone.\nWhen Route 53 receives a DNS query that matches the name and type in an alias record, Route 53 follows the pointer and responds with the applicable value.\nFor more information on Route 53 Alias records, please visit the following URL-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\nNote:\nRoute 53 uses \"Alias Name\" to connect to the CloudFront as Alias Record is a Route 53 extension to DNS.Also, an Alias record is similar to a CNAME record, but the main difference is - you can create an Alias record for both root domain &amp; subdomain.\nIn contrast, a CNAME record can be created only to subdomain.\nCheck the below link to get more information:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-cloudfront-distribution.html\n\nThe correct answer is A: Create an Alias record which points to the CloudFront distribution.\nExplanation: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service that helps developers and businesses to route end users to Internet applications. It can be used to route users to Amazon Web Services (AWS) resources like EC2 instances, S3 buckets, or CloudFront distributions.\nCloudFront is AWS's content delivery network (CDN) service that speeds up the delivery of static and dynamic web content like HTML, CSS, JavaScript, and images, videos, and other files to end users. It has a global network of edge locations that cache the content and serve it from the nearest location to the end user.\nTo ensure that your domain name points to an existing CloudFront distribution, you can create an Alias record in Route 53. An Alias record is a Route 53 extension to DNS that lets you map a DNS name (example.com) to an AWS resource (such as a CloudFront distribution) that is specified in another AWS service (such as CloudFront).\nAn Alias record is like a CNAME record, but with some differences:\nAn Alias record can map your domain name apex (example.com) to a CloudFront distribution, whereas a CNAME record cannot. An Alias record is not a redirect; it results in a DNS response with the IP address of the AWS resource, whereas a redirect returns an HTTP 301 or 302 response code and a new URL.\nTo create an Alias record for your domain name in Route 53, follow these steps:\nIn the AWS Management Console, open the Route 53 console at https://console.aws.amazon.com/route53/. In the left navigation pane, choose Hosted zones. Choose the name of the hosted zone for your domain name. Choose Create Record Set. In the Name box, enter the domain name that you want to route traffic to (for example, www.example.com). In the Type box, choose A - IPv4 address. In the Alias box, choose Yes. In the Alias Target box, choose the CloudFront distribution that you want to route traffic to (for example, d111111abcdef8.cloudfront.net). Choose Create.\nRoute 53 propagates your changes to the DNS servers that are authoritative for your domain name, which typically takes only a few minutes.\nIn summary, to point your domain name to a CloudFront distribution using Route 53, you need to create an Alias record that maps your domain name to the CloudFront distribution. This allows you to take advantage of the performance and scalability benefits of CloudFront and Route 53 together.\n\n"
}, {
  "id" : 187,
  "question" : "A company needs to extend its storage infrastructure to the AWS Cloud.\nThe storage needs to be available as iSCSI devices for on-premises application servers.\nWhat should be done to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "f05a3d324d644d49a136f7ab9f5a8a56",
    "option" : "Create a Glacier vault. Use a Glacier Connector and mount it as an iSCSI device.",
    "isCorrect" : "false"
  }, {
    "id" : "4db89e16c38b46499fb54fd4d51ce182",
    "option" : "Create an S3 bucket. Use an S3 Connector and mount it as an iSCSI device.",
    "isCorrect" : "false"
  }, {
    "id" : "adfa000cd3674b4fb01ac299f82c068a",
    "option" : "Use the EFS file service and mount the different file systems to the on-premises servers.",
    "isCorrect" : "false"
  }, {
    "id" : "e83d821025d8429ead7539f4fd917d4c",
    "option" : "Use the AWS Storage Gateway-cached volumes service.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions the following:\nBy using cached volumes, you can use Amazon S3 as your primary data storage, while retaining frequently accessed data locally in your storage gateway.\nCached volumes minimize the need to scale your on-premises storage infrastructure while still providing your applications with low-latency access to their frequently accessed data.\nYou can create storage volumes up to 32 TB in size and attach iSCSI devices to them from your on-premises application servers.\nYour gateway stores data that you write to these volumes in Amazon S3, retains recently read data in your on-premises storage gateway's cache, and upload buffer storage.\nOptions A, B and C are incorrect because they cannot be mounted as iSCSI devices to the on-premises application servers.\nOption D is CORRECT because AWS Storage Gateway connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure.\nThe storage can also be mounted as iSCSI devices to the on-premises application servers.\nFor more information on AWS Storage Gateways, please visit the following URL:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\n\nThe correct answer for this scenario is D. Use the AWS Storage Gateway-cached volumes service.\nExplanation:\nThe AWS Storage Gateway service is a hybrid storage service that allows on-premises applications to use AWS cloud storage. It supports several types of storage protocols, including iSCSI.\nThe Storage Gateway service provides three types of storage volumes - cached volumes, stored volumes, and tape gateway. In this scenario, the appropriate service to fulfill the requirement is the Storage Gateway-cached volumes service.\nThe Storage Gateway-cached volumes service stores frequently accessed data on-premises while keeping a copy of all the data securely in Amazon S3. This service allows customers to use AWS storage with their existing applications and provides low-latency access to frequently accessed data while still benefiting from the durability and scalability of Amazon S3.\nHere's how to set up the Storage Gateway-cached volumes service:\nLaunch the AWS Storage Gateway service from the AWS Management Console. Select the \"cached volumes\" option and create a gateway. Install the Storage Gateway virtual machine or software appliance on-premises and connect it to the gateway. Configure the iSCSI target on the Storage Gateway service. Mount the iSCSI device on the on-premises application servers.\nWith these steps, the iSCSI devices are now available for the on-premises application servers.\nOption A is incorrect because Glacier is an archival storage service designed for infrequently accessed data, and Glacier Connector is not used to mount data as iSCSI devices.\nOption B is incorrect because S3 is an object storage service and cannot be mounted as an iSCSI device directly.\nOption C is incorrect because EFS is a file storage service, and while it can be mounted to on-premises servers, it does not support iSCSI.\n\n"
}, {
  "id" : 188,
  "question" : "You have several instances doing machine learning to compute.\nYou have all the data required for the machine learning in an S3 bucket.\nYou need to find a high-performance storage in which all the instances can read and write data simultaneously.\nWhich of the following options is the best suited solution for this?\n",
  "answers" : [ {
    "id" : "1f48df41c97c40e28f29691de01b4afc",
    "option" : "FSx for Windows File Server.",
    "isCorrect" : "false"
  }, {
    "id" : "412245657bbc4483b47cbe0657b64990",
    "option" : "EFS.",
    "isCorrect" : "false"
  }, {
    "id" : "ae4b0182dd1a44fbbea459ad3adc51b4",
    "option" : "FSx for Lustre.",
    "isCorrect" : "true"
  }, {
    "id" : "b47888c190dd46409ed1ecd094ceaf1c",
    "option" : "DynamoDB Accelerator.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect.\nFSx for Windows File Server is well suited to have a shared storage for your Windows instances.\nBut it does not read data from S3, and it isn't a high-performance storage.\nMore details:\nhttps://aws.amazon.com/fsx/windows/\nOption B is incorrect.\nEFS is a shared storage for Linux instances.\nBut it does not read data from S3, and it is not a high-performance storage.\nMore details:\nhttps://aws.amazon.com/efs/\nOption C is CORRECT.\nFSx for Lustre is a high-performance storage.\nIt can read data from S3 and connect to multiple instances at the same time.\nMore details:\nhttps://aws.amazon.com/fsx/lustre/faqs/\nOption D is incorrect.\nDynamoDB accelerator is an in-memory cache used to increase the performance of DynamoDB.\nMore details:\nhttps://aws.amazon.com/dynamodb/dax/\n\nBased on the scenario described, the best option among the given choices is C. FSx for Lustre.\nFSx for Lustre is a highly scalable and high-performance file system designed for processing large-scale workloads, such as machine learning, high-performance computing, and video processing. It provides sub-millisecond latencies, high throughput, and consistent performance.\nHere are some reasons why FSx for Lustre is the best option:\nHigh performance: FSx for Lustre can deliver throughput up to hundreds of gigabytes per second, which is suitable for data-intensive workloads. The performance is consistent, regardless of the file size, number of files, or number of clients accessing the file system. Scalability: FSx for Lustre can scale up or down to meet the workload demands. You can start with a small file system and add more capacity as needed without impacting performance. Simultaneous access: FSx for Lustre supports concurrent read and write access from multiple instances, which is critical for machine learning workloads. All the instances can access the same file system and data simultaneously. S3 integration: FSx for Lustre can seamlessly integrate with S3, which is where all the data required for machine learning is stored. This allows the instances to read and write data directly to/from S3 without any data movement or replication.\nIn contrast, let's briefly examine the other options:\nA. FSx for Windows File Server: It provides a fully managed Windows file server that is accessible over the industry-standard Server Message Block (SMB) protocol. It is more suitable for Windows-based workloads and not ideal for machine learning workloads.\nB. EFS: It is a fully managed, highly available, and scalable file system for Linux-based workloads. While it can be used for machine learning workloads, it may not provide the required performance and scalability as FSx for Lustre.\nD. DynamoDB Accelerator: It is an in-memory cache for DynamoDB that can improve performance and reduce latency for read-intensive workloads. It is not suitable for machine learning workloads that require high throughput and simultaneous read/write access.\nIn summary, FSx for Lustre is the best option among the given choices for machine learning workloads that require high-performance storage with simultaneous read and write access.\n\n"
}, {
  "id" : 189,
  "question" : "Your current setup in AWS consists of the following architecture: 2 public subnets, one subnet with web servers accessed by users across the Internet and another subnet for the database server.\nWhich of the following changes to the architecture would add a better security boundary to the resources hosted in this setup?\n",
  "answers" : [ {
    "id" : "c5107669f76543399cb09a29a16852d3",
    "option" : "Consider moving the web server to a private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "8ebb3c9b7a2946e5be84c46d02ed9cff",
    "option" : "Create a private subnet and move the database server to a private subnet.",
    "isCorrect" : "true"
  }, {
    "id" : "6493ccf491cc403680a8007c3bdeb097",
    "option" : "Consider moving both the web and database servers to a private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "f4c8d64316f24028b07a6b0c4b11fb57",
    "option" : "Consider creating a private subnet and adding a NAT Instance to that subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe ideal setup is to host the web server in the public subnet so that users on the Internet can access it.\nThe database server can be hosted in the private subnet.\nThe below diagram from AWS Documentation shows the set up:\n\n\nThe current architecture has two public subnets, one containing web servers that are accessed by users across the internet and the other containing the database server. The architecture can be improved by adding a better security boundary to the resources hosted in this setup.\nOption A - Consider moving the web server to a private subnet. Moving the web server to a private subnet will help to increase the security of the web server by limiting access to the internet. The private subnet can be configured to allow access only from the necessary IP addresses or CIDR blocks. This will ensure that only authorized requests are allowed to the web server.\nOption B - Create a private subnet and move the database server to a private subnet. Creating a private subnet and moving the database server to it will provide a higher level of security for the database. This will help to reduce the risk of unauthorized access to the database server, as it will only be accessible from within the private subnet.\nOption C - Consider moving both the web and database servers to a private subnet. Moving both the web and database servers to a private subnet will provide a higher level of security for both servers. This will reduce the risk of unauthorized access to the web server and database server.\nOption D - Consider creating a private subnet and adding a NAT Instance to that subnet. Creating a private subnet and adding a NAT instance to it will provide access to the internet for resources within the private subnet. This option can be useful when resources within the private subnet need to access the internet, but the subnet needs to be protected from inbound traffic from the internet.\nIn summary, options A, B, and C all add a better security boundary to the resources hosted in this setup, while option D provides a way for resources within a private subnet to access the internet while being protected from inbound traffic from the internet.\n\n"
}, {
  "id" : 190,
  "question" : "You have a cluster of windows instances joined to an AWS Managed Active Directory.\nYou want to have a shared storage for all these instances and control this storage access with the Managed Active Directory.\nWhat service allows you to achieve this?\n",
  "answers" : [ {
    "id" : "c12f2d55f4944a41b9f0c35b17511d3c",
    "option" : "FSx for Lustre.",
    "isCorrect" : "false"
  }, {
    "id" : "3239099af21a42c5b4c01750797d80f3",
    "option" : "FSx for Windows File Server.",
    "isCorrect" : "true"
  }, {
    "id" : "279ec73610eb42979f10d05c37acf6e1",
    "option" : "EFS.",
    "isCorrect" : "false"
  }, {
    "id" : "68ee02cf63fb465d957b470a7736f677",
    "option" : "S3 and AD Connector.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect.\nFSx for Lustre is mainly used to do high-performance compute, read and write data into S3, and only works with Linux instances.\nmore details:\nhttps://aws.amazon.com/fsx/lustre/\nOption B is CORRECT.\nFSx for windows can connect to Manage Active Directory and to Windows instances.\nInside managed Active Directory, you can manage the permissions for it.\nMore details:\nhttps://aws.amazon.com/fsx/windows/\nOption C is incorrect.\nEFS is an NFS storage system to work with Linux instances.\nMore details:\nhttps://aws.amazon.com/efs/\nOption D is incorrect.\nS3 and AD Connector are not related.\nAnd are not designed for this purpose.\n\nThe service that allows you to have a shared storage for all these instances and control this storage access with the Managed Active Directory is FSx for Windows File Server (option B).\nFSx for Windows File Server is a fully managed file storage service that is built on Windows Server and integrates with AWS Managed Active Directory to provide an enterprise-ready shared file storage solution. With FSx for Windows File Server, you can create file shares that are accessible from multiple instances, including Windows instances, using the SMB protocol.\nFSx for Windows File Server integrates with AWS Managed Active Directory, which is a managed Microsoft Active Directory service that allows you to use existing Windows-based applications and tools while managing users and groups in the cloud. This integration enables you to control access to the file shares using the security groups and permissions you have defined in your AWS Managed Active Directory.\nOption A (FSx for Lustre) is a high-performance file system service that is optimized for processing large data sets with high throughput and low latency, and it is not optimized for Windows environments. Therefore, it is not the best option for this scenario.\nOption C (EFS) is a fully managed, scalable, and highly available file storage service that is designed for use with Linux-based systems. While it can be accessed from Windows instances, it does not integrate with AWS Managed Active Directory, so it does not provide the level of integration required for this scenario.\nOption D (S3 and AD Connector) is not a direct solution to create shared storage for Windows instances. S3 is an object storage service, and AD Connector is a service that allows you to connect your on-premises Microsoft Active Directory to AWS. While it can be used to manage access to S3 buckets, it does not provide the level of integration required for this scenario, and it does not provide a shared file system for Windows instances.\n\n"
}, {
  "id" : 191,
  "question" : "Instances hosted in the private subnet of your VPC need to access some important documents from the S3 bucket which is outside the VPC.\nDue to the confidential nature of these documents, you have to ensure that the traffic does not traverse through the internet.\nAs an architect, how would you implement this solution?\n",
  "answers" : [ {
    "id" : "eb808cea1a504f5bae8224b359261965",
    "option" : "Consider using a VPC Endpoint.",
    "isCorrect" : "true"
  }, {
    "id" : "d5151a625d774767b6d5bbfd184837e0",
    "option" : "Consider using an EC2 Endpoint.",
    "isCorrect" : "false"
  }, {
    "id" : "81e98ad226ab4b4abc5f42247eee90f8",
    "option" : "Move the instances to a public subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "4677bf11d1ee4d4daacdb831dd9d6ec0",
    "option" : "Create a VPN connection and access the S3 resources from the EC2 Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS documentation mentions the following:\nA VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection or AWS Direct Connect connection.\nInstances in your VPC do not require public IP addresses to communicate with resources in the service.\nTraffic between your VPC and the other services does not leave the Amazon network.\nFor more information on VPC Endpoints, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html\n\nTo provide secure access to S3 resources from instances hosted in a private subnet of a VPC, without the traffic traversing the internet, the recommended solution is to use a VPC endpoint. Therefore, the correct answer is A.\nA VPC endpoint is a secure and reliable way to access S3 resources from within a VPC without going through the internet. A VPC endpoint provides a direct connection between the VPC and S3, so the traffic never leaves the Amazon network. This ensures that the communication between the private subnet and the S3 bucket remains private and secure.\nUsing an EC2 endpoint (B) is not a valid solution for accessing S3 resources since EC2 endpoints are used to access services hosted within the same VPC as the EC2 instance, and not external services like S3.\nMoving the instances to a public subnet (C) is not a valid solution because a public subnet is connected to the internet, and traffic from instances in a public subnet to S3 would go through the internet, which does not meet the requirement of keeping the traffic private and secure.\nCreating a VPN connection (D) to access S3 resources is possible, but it requires additional resources and setup, and it is not as efficient and secure as using a VPC endpoint. A VPN connection involves creating a virtual private gateway in the VPC and a customer gateway outside the VPC, which can be time-consuming and complex to configure. Additionally, the traffic will still have to traverse the public internet, which is not as secure as using a VPC endpoint.\nTherefore, the best solution for providing secure access to S3 resources from instances hosted in a private subnet of a VPC without going through the internet is to use a VPC endpoint.\n\n"
}, {
  "id" : 192,
  "question" : "The development team is working on a new application for which they will be launching an EC2 Instance.\nTo decrease time in launching the EC2 instance, they want you to pre-warm the instance &amp; keep it ready for launching with all required patches &amp; software.\nWhich of the following can be done to meet this requirement?\n",
  "answers" : [ {
    "id" : "56ef6ce904ad477ab1f0bd95b151af30",
    "option" : "Launch the Amazon EC2 instance within an Auto-Scaling group and enable Hibernate on the instance in the Auto-Scaling group.",
    "isCorrect" : "false"
  }, {
    "id" : "946a2c67f3f842f3a6cb3c852eb6c872",
    "option" : "Launch the Amazon EC2 instance with an instance root volume and enable Hibernate.",
    "isCorrect" : "false"
  }, {
    "id" : "7becd016a0be429b87a3e4904a1f1e87",
    "option" : "Launch the Amazon EC2 instance with an Amazon EBS root volume and enable Hibernate.",
    "isCorrect" : "true"
  }, {
    "id" : "b51751c179de4e6faf913a7cae84f0eb",
    "option" : "Launch the Amazon EC2 instance within an Auto-Scaling group and enable Hibernate only on the EC2 instance that needs to be hibernated.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nTo pre-warm EC2 instance, EC2 Hibernate can be used.\nThe instance needs to be launched with an Amazon EBS root volume.\nBesides, you cannot hibernate an instance in an Auto Scaling group or used by Amazon ECS.\nOptions A &amp; D are incorrect as EC2 hibernate is not supported if the EC2 instance is in an Auto-Scaling group.\nOption B is incorrect as EC2 hibernate is not supported on Instance store volume.\nAn EBS root volume is required.\nFor more information on hibernating EC2 instances, refer to the following URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html\n\nSure, I'd be happy to explain in detail.\nTo meet the requirement of pre-warming an EC2 instance and keeping it ready with all required patches and software, the best option would be to launch the instance within an Auto-Scaling group and enable hibernate on the instance.\nOption A - Launch the Amazon EC2 instance within an Auto-Scaling group and enable Hibernate on the instance in the Auto-Scaling group - This option is correct because launching the instance within an Auto-Scaling group allows you to pre-warm the instance by launching it and keeping it running with the required software and patches. Enabling Hibernate on the instance in the Auto-Scaling group allows you to pause the instance while still retaining its state and then quickly resume it when needed. This reduces the launch time of the instance, as the instance is already pre-warmed and ready for use.\nOption B - Launch the Amazon EC2 instance with an instance root volume and enable Hibernate - This option is not recommended because hibernation is only supported for instances with an EBS root volume, and not for instances with an instance store root volume.\nOption C - Launch the Amazon EC2 instance with an Amazon EBS root volume and enable Hibernate - This option is correct because hibernation is only supported for instances with an EBS root volume. However, launching the instance within an Auto-Scaling group is a better option as it allows you to pre-warm the instance and keep it ready for use.\nOption D - Launch the Amazon EC2 instance within an Auto-Scaling group and enable Hibernate only on the EC2 instance that needs to be hibernated - This option is not recommended because if you enable hibernate only on a specific instance, you will not be able to take advantage of the pre-warming benefits provided by launching the instance within an Auto-Scaling group. Additionally, this approach could lead to inconsistencies in the state of the instance if it needs to be terminated and relaunched in the future.\nIn summary, option A - Launch the Amazon EC2 instance within an Auto-Scaling group and enable Hibernate on the instance in the Auto-Scaling group - is the best option to pre-warm an EC2 instance and keep it ready for launching with all required patches and software.\n\n"
}, {
  "id" : 193,
  "question" : "A company has a workflow that sends video files from their on-premise system to AWS for transcoding.\nThey use EC2 worker instances to pull transcoding jobs from SQS.\nWhy is SQS an appropriate service for this scenario?\n",
  "answers" : [ {
    "id" : "bb91e9325920467ca211834490c6a9f5",
    "option" : "SQS guarantees the order of the messages.",
    "isCorrect" : "false"
  }, {
    "id" : "45f963b0a2984770982488fb0be5e225",
    "option" : "SQS synchronously provides transcoding output.",
    "isCorrect" : "false"
  }, {
    "id" : "fb72d88c2f9c416dbf19696f41ec69d3",
    "option" : "SQS checks the health of the worker instances.",
    "isCorrect" : "false"
  }, {
    "id" : "ca5176baf24f4e7398f55a0cf57fea12",
    "option" : "SQS helps to facilitate horizontal scaling of EC2 worker instances when the queue grows.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nEven though SQS guarantees the order of messages for FIFO queues, the main reason for using it is that it helps in horizontal scaling of AWS resources and is used for decoupling systems.\nSQS can neither be used for transcoding output nor for checking the health of worker instances.\nThe health of worker instances can be checked via ELB or CloudWatch.\nFor more information on SQS, please visit the following URL-\nhttps://aws.amazon.com/sqs/faqs/\n\nSQS (Simple Queue Service) is a fully managed message queuing service provided by AWS that enables decoupling of components of cloud-based applications, by allowing them to communicate asynchronously with each other.\nIn the scenario given, the company sends video files from their on-premise system to AWS for transcoding, and uses EC2 worker instances to pull transcoding jobs from SQS. This suggests that the workflow needs to be scalable and flexible enough to handle varying workloads, without overwhelming the on-premise system or the EC2 instances.\nSQS is an appropriate service for this scenario for the following reasons:\nD. SQS helps to facilitate horizontal scaling of EC2 worker instances when the queue grows. By using SQS, the company can ensure that the workloads are distributed evenly among multiple EC2 instances, which can be easily scaled horizontally as the workload increases. When the number of messages in the queue grows, SQS can trigger the scaling of EC2 instances, and balance the workload to ensure efficient processing of messages. This helps to improve the scalability and fault tolerance of the workflow, as the workload can be easily distributed and processed by multiple EC2 instances.\nA. SQS guarantees the order of the messages. SQS ensures that messages are processed in the order in which they were added to the queue, which is important for video transcoding workflows, as it ensures that the output of the transcoding jobs is consistent and reliable. This ensures that the workflow remains consistent and avoids any issues with job dependencies.\nC. SQS checks the health of the worker instances. SQS provides a health check feature, which can be used to monitor the health of the worker instances, and to ensure that any unhealthy instances are taken out of the rotation. This helps to ensure that the workflow remains reliable and consistent, and that any issues are quickly detected and resolved.\nB. SQS synchronously provides transcoding output. This option is incorrect, as SQS is an asynchronous messaging service and does not provide synchronous output.\nIn summary, SQS is an appropriate service for the given scenario as it helps to ensure the scalability, reliability, and consistency of the video transcoding workflow by distributing the workload evenly among multiple EC2 instances, ensuring the order of messages, monitoring the health of worker instances, and detecting any issues quickly.\n\n"
}, {
  "id" : 194,
  "question" : "You run an ad-supported photo sharing website using S3 to serve photos to visitors of your site.\nAt some point, you find out that other sites have been linking to the photos on your site, causing loss to your business.\nWhat would be an effective method to mitigate this?\n",
  "answers" : [ {
    "id" : "ca3b6b177b734e8890c84b5640346854",
    "option" : "Remove public read access and use presigned URL with expiry dates.",
    "isCorrect" : "true"
  }, {
    "id" : "a4ef371804964747b6fff08f5fc8769d",
    "option" : "Use CloudFront distributions for static content.",
    "isCorrect" : "false"
  }, {
    "id" : "2f415a89dcaa4dc9858bd7c669917f86",
    "option" : "Block the IPs of the offending websites in Security Groups.",
    "isCorrect" : "false"
  }, {
    "id" : "32b4054793d444f4a319e12b0c29dfdb",
    "option" : "Store photos on an EBS Volume of the web server.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nWhen you create a presigned URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time.\nThe presigned URLs are valid only for the specified duration.\nOption B is incorrect because CloudFront is used to distribute content across edge or region locations, distribution is not the requirement above.\nOption C is not feasible.\nBecause of their dynamic nature, blocking IPs is challenging, and you will not know which sites are accessing your main site.\nOption D is incorrect since storing photos on an EBS Volume is neither good practice nor an ideal architectural approach for an AWS Solutions Architect.\nFor more information on signed cookies, please visit the following URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/PresignedUrlUploadObject.html\n\nThe most effective method to mitigate the issue of other sites linking to photos on your website and causing loss to your business would be option A: Remove public read access and use presigned URL with expiry dates.\nOption A involves removing public read access from S3 buckets and using presigned URLs with expiry dates to allow access to the photos. Public read access allows anyone to access the photos in the S3 bucket, which can lead to other websites linking to the photos and causing loss of business. By removing public read access, only authorized users or applications can access the photos. Presigned URLs with expiry dates can be used to grant temporary access to the photos, which means that the URL will only work for a specified period of time. This ensures that unauthorized users cannot access the photos even if they somehow obtain the URL.\nOption B, using CloudFront distributions for static content, is a good method to improve the performance and availability of your website. CloudFront can cache content at edge locations, reducing the load on your origin server and improving the speed at which content is delivered to users. However, it does not address the issue of other sites linking to your photos.\nOption C, blocking the IPs of the offending websites in Security Groups, is not an effective method because IP addresses can easily be changed or spoofed. Moreover, it does not address the root cause of the problem.\nOption D, storing photos on an EBS volume of the web server, is not a recommended approach for serving photos because EBS volumes are not designed for high throughput, low latency, or high durability storage of large files like photos. Additionally, storing photos on the web server can lead to scalability issues as the traffic to the website increases.\nIn summary, the most effective method to mitigate the issue of other sites linking to photos on your website and causing loss to your business would be to remove public read access and use presigned URLs with expiry dates.\n\n"
}, {
  "id" : 195,
  "question" : "You are part of the IT team of an insurance company.\nYou have 4 M5.large EC2 instances used to compute some data of your core services.\nThe amount of usage of these instances has been very consistent.\nSo you predict that it will not increase in the next two or three years.\nHowever, your CFO is asking if there is a way to reduce costs in the EC2 instances.\nThe only requirement will be to be able to change the size of the instances.\nWhat do you suggest to get the maximum cost reduction?\n",
  "answers" : [ {
    "id" : "1d3d38f8760342e888676310e793936f",
    "option" : "Purchase a Compute Saving plan.",
    "isCorrect" : "false"
  }, {
    "id" : "0ddf5ac67737422188af53b7346cd6f5",
    "option" : "Purchase an EC2 instance saving plan.",
    "isCorrect" : "true"
  }, {
    "id" : "1bafde5d316f46c8a9a20a6733734696",
    "option" : "Purchase a Convertible Reserved Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "515da0ea5af94d2e99a0961ed70e5989",
    "option" : "Purchase a Dedicated Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect because Compute Saving plans also offer flexibility, but the maximum cost reduction is 66%\nOption B is CORRECT because, with an EC2 saving plan, you can save up to 72% (just like Standard RIs)\nThis plan applies to all of your EC2 instances using the same instance family.\nOption C is incorrect because Compute Savings Plans, which provides savings up to 66% (just like Convertible RIs), automatically reduce your cost on any EC2 instance usage regardless of region, instance family, size, OS, tenancy, and even on AWS Fargate or AWS Lambda.\nOption D is incorrect because Dedicated Instances are Amazon EC2 instances that run in a virtual private cloud (VPC) on hardware that's dedicated to a single customer.\nThey cannot reduce the costs.\nReferences:\nhttps://aws.amazon.com/savingsplans/faq/\n\nBased on the given scenario, where the amount of usage of the 4 M5.large EC2 instances has been consistent and not expected to increase for the next two or three years, the CFO is asking for a way to reduce costs while still being able to change the instance size. There are a few different options available for cost reduction, but the best option would depend on the specific requirements and usage patterns of the instances.\nOption A: Purchase a Compute Saving Plan A Compute Saving Plan is a type of AWS Reserved Instance that provides a discount of up to 66% compared to On-Demand pricing. The discount is based on the upfront payment and the commitment to use the instance for a one or three-year term. Compute Saving Plans provide flexibility in terms of instance size and region, so they can be a good option for cost reduction if the usage is consistent over a longer period.\nHowever, since the usage has been consistent and not expected to increase, purchasing a Compute Saving Plan may not provide significant cost savings, as the discount is based on the commitment to use the instance for a longer period.\nOption B: Purchase an EC2 Instance Saving Plan An EC2 Instance Saving Plan is another type of AWS Reserved Instance that provides a discount of up to 72% compared to On-Demand pricing. The discount is based on the upfront payment and the commitment to use a specific instance family and size, but it provides a higher discount compared to Compute Saving Plans. However, EC2 Instance Saving Plans require a specific instance family and size commitment, which means they are less flexible compared to Compute Saving Plans.\nSince the requirement is to change the size of the instances, purchasing an EC2 Instance Saving Plan may not be the best option for cost reduction.\nOption C: Purchase a Convertible Reserved Instance A Convertible Reserved Instance is a type of AWS Reserved Instance that provides a discount of up to 54% compared to On-Demand pricing. The discount is based on the upfront payment and the commitment to use the instance for a one or three-year term, but it provides more flexibility compared to a standard Reserved Instance. Convertible Reserved Instances allow users to modify the instance family, operating system, and tenancy, which means they can be a good option for cost reduction if there is a need to change instance types in the future.\nSince the requirement is to change the size of the instances, a Convertible Reserved Instance can be a good option for cost reduction while still providing flexibility to change the instance type if needed.\nOption D: Purchase a Dedicated Instance A Dedicated Instance is an EC2 instance that is dedicated to a single customer, meaning that the underlying hardware is not shared with other customers. Dedicated Instances can provide additional security and compliance benefits, but they are more expensive compared to regular instances.\nSince the requirement is to reduce costs, purchasing a Dedicated Instance may not be the best option for cost reduction.\nIn conclusion, the best option for cost reduction in this scenario would be to purchase a Convertible Reserved Instance. It provides a good balance between cost savings and flexibility to change instance types in the future.\n\n"
}, {
  "id" : 196,
  "question" : "A company currently hosts its architecture in the us-east-1 region.\nThey now need to duplicate this architecture to the eu-west-1 region and extend the application hosted on this architecture to the new AWS Region.\nIn order to ensure that users in both AWS Regions get the same seamless experience, what should be done?\n",
  "answers" : [ {
    "id" : "a826b48abadd42db99259ab4b6c77472",
    "option" : "Create a Classic Elastic Load Balancer setup to route traffic to both locations.",
    "isCorrect" : "false"
  }, {
    "id" : "994e383aa8f94bee861fc2c05d1e6dd5",
    "option" : "Create a weighted Route 53 policy to route the policy based on the weightage for each location.",
    "isCorrect" : "false"
  }, {
    "id" : "6d0dca9f35464e968569a96c021aab99",
    "option" : "Create an Application Elastic Load Balancer setup to route traffic to both locations.",
    "isCorrect" : "false"
  }, {
    "id" : "08b689ac29fb4b068bc31545296fecb9",
    "option" : "Create a Geolocation Route 53 Policy to route the traffic based on the location.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions the following with respect to this requirement:\nGeolocation routing lets you choose the resources that serve your traffic based on the geographic location of your users, which means the location that DNS queries originate from.\nFor more information on AWS Route 53 Routing Policies, please visit the following URL:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nTo ensure that users in both AWS Regions get the same seamless experience, we need to create a high availability and scalable architecture with minimal latency.\nOption A - Create a Classic Elastic Load Balancer setup to route traffic to both locations: A Classic Elastic Load Balancer (ELB) setup is used for distributing traffic to EC2 instances in multiple Availability Zones within a single region. However, it cannot route traffic between regions. Therefore, this option is not suitable for this scenario.\nOption B - Create a weighted Route 53 policy to route the policy based on the weightage for each location: A weighted Route 53 policy is used to distribute traffic between different AWS resources based on assigned weights. We can create an AWS Route 53 weighted policy to distribute traffic between the two regions based on a percentage weightage. However, it doesn't take into account the geographic distance between the user and the resources, which may cause latency issues.\nOption C - Create an Application Elastic Load Balancer setup to route traffic to both locations: An Application Elastic Load Balancer (ALB) can route traffic to different regions based on various routing algorithms such as round-robin, least connections, and IP-based routing. ALB can be used to route traffic between regions, and it also supports various other features like SSL termination, sticky sessions, and URL-based routing. Therefore, this option is a good choice for this scenario.\nOption D - Create a Geolocation Route 53 Policy to route the traffic based on the location: A Geolocation Route 53 policy is used to direct traffic to different resources based on the geographic location of the user. However, this option is not suitable for this scenario, as we want to ensure that users in both regions get the same seamless experience.\nTherefore, option C - Create an Application Elastic Load Balancer setup to route traffic to both locations is the best choice for this scenario, as it can route traffic between regions and support various other features that can ensure high availability, scalability, and low latency.\n\n"
}, {
  "id" : 197,
  "question" : "You have a set of EC2 Instances that support an application.\nThey are currently hosted in the US Region.\nThe EBS volume type of the EC2 instances is General Purpose SSD.\nIn the event of a disaster, you need a way to ensure that you can quickly provision the resources in another region.\nHow could this be accomplished? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "67f7575c2b1c4330a8d7ad2c09ab1e3d",
    "option" : "Copy the underlying EBS Volumes to the destination region.",
    "isCorrect" : "false"
  }, {
    "id" : "7766743c463e4a658ab8e1123c4ad639",
    "option" : "Create EBS Snapshots and then copy them to the destination region.",
    "isCorrect" : "true"
  }, {
    "id" : "5a1f2386b99a4305ba455ace4c122811",
    "option" : "Create AMIs for the underlying instances and copy them to the destination region.",
    "isCorrect" : "true"
  }, {
    "id" : "76057a6238ae40fca383c1df4b1d2c62",
    "option" : "Copy the metadata for the EC2 Instances to S3.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - B and C.\nSnapshots can be used to create an AMI or template of the underlying instance.\nYou can then copy the AMI to another region.\nYou can also take snapshots of the volumes and then copy them to the destination region.\nFor more information on AMIs and EBS Snapshots, please visit the following URLs:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nTo ensure that you can quickly provision the resources in another region in the event of a disaster, you can perform the following actions:\nA. Copy the underlying EBS Volumes to the destination region: This involves copying the entire EBS volume to the destination region. You can use the AWS CLI or the AWS Management Console to copy the volume. However, this can be a time-consuming process, depending on the size of the volume.\nB. Create EBS Snapshots and then copy them to the destination region: This involves creating a snapshot of the EBS volume and then copying the snapshot to the destination region. This method is generally faster than copying the entire volume. You can use the AWS CLI or the AWS Management Console to create the snapshot and copy it to the destination region.\nC. Create AMIs for the underlying instances and copy them to the destination region: This involves creating an Amazon Machine Image (AMI) of the EC2 instances and then copying the AMI to the destination region. An AMI is a pre-configured virtual machine image that is used to create EC2 instances. This method is useful if you want to create new instances quickly in the destination region. You can use the AWS CLI or the AWS Management Console to create the AMI and copy it to the destination region.\nD. Copy the metadata for the EC2 Instances to S3: This involves copying the metadata for the EC2 instances, such as instance type, security groups, and network settings, to Amazon S3. This method is not very useful for disaster recovery purposes, as it does not include the actual data on the instances.\nTherefore, options A and B are the correct answers as they both involve copying the data from the current region to the destination region, which can be used to quickly provision the resources in the event of a disaster. Option C is also a viable option for creating new instances quickly, but it involves creating an AMI, which may take more time than copying a snapshot or volume. Option D is not useful for disaster recovery purposes.\n\n"
}, {
  "id" : 198,
  "question" : "You have an application hosted in EC2\nThis application is accessed by users in different countries and needs high performance.\nThe application also needs static IPs that should not be changed.\nWhat service could be used to achieve these requirements?\n",
  "answers" : [ {
    "id" : "6808ef9bcde442e78f9822efed00a935",
    "option" : "AWS Global Accelerator.",
    "isCorrect" : "true"
  }, {
    "id" : "2edc007aa618482bbbdd039bcf0cff86",
    "option" : "Place an ALB before the instances and assign the ALB a public IP.",
    "isCorrect" : "false"
  }, {
    "id" : "35562802ec6445879fa880e74b8fad1d",
    "option" : "AWS CloudFront.",
    "isCorrect" : "false"
  }, {
    "id" : "65be6143a0794152ad01d23b78e7d212",
    "option" : "AWS Direct Connect.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT.\nAWS Global Accelerator provides two static IPs as endpoints.\nAWS Global Accelerator also improves the application's performance by directing traffic to endpoints over the AWS global network.\nMore details:\nhttps://aws.amazon.com/global-accelerator/faqs/\nOption B is incorrect.\nALB cannot provide static IPs for the application.\nOption C is incorrect.\nAWS CloudFront does not provide static IPs for the application.\nOption D is incorrect.\nAWS Direct Connect is used to provide a private network connection from the On-Premise datacenter to AWS.\nIt does not improve the performance of the application hosted in EC2.\n\nOption A: AWS Global Accelerator AWS Global Accelerator is a service that is designed to improve the availability and performance of applications for local and global users. It works by routing traffic to optimal AWS endpoints based on geographic proximity, health, and routing policies. It can also provide a static IP address that is mapped to multiple AWS resources, including EC2 instances, AWS Elastic IP addresses, and ALBs. Therefore, this option could be a viable solution for meeting the requirements of high performance and static IPs.\nOption B: Place an ALB before the instances and assign the ALB a public IP Amazon Elastic Load Balancing (ELB) is a service that distributes incoming traffic across multiple EC2 instances. By placing an Application Load Balancer (ALB) in front of the instances and assigning it a public IP, traffic can be load-balanced across the instances to improve performance. However, this option does not provide static IPs, and the IP address of the ALB could change over time.\nOption C: AWS CloudFront AWS CloudFront is a content delivery network (CDN) service that delivers content, including webpages, videos, and images, to users around the world with low latency and high data transfer speeds. It can also provide static IP addresses for the distribution, which could meet the requirement for static IPs. However, this option may not be the best fit for an application that requires dynamic content, as it is more geared towards static content delivery.\nOption D: AWS Direct Connect AWS Direct Connect is a service that enables customers to establish a dedicated network connection from their premises to AWS. This can improve performance by providing a dedicated network connection with lower latency and higher bandwidth. However, this option does not provide static IPs and may not be necessary for an application hosted entirely in EC2.\nBased on the requirements specified in the question, Option A (AWS Global Accelerator) seems to be the best fit, as it can provide both high performance and static IPs that should not be changed. However, it is always important to consider other factors such as cost, complexity, and security when selecting a solution.\n\n"
}, {
  "id" : 199,
  "question" : "You are building an automated transcription service where Amazon EC2 worker instances process an uploaded audio file and generate a text file.\nYou must store both of these files in the same durable storage until the text file is retrieved.\nCustomers fetch the text files frequently.\nYou do not know about the storage capacity requirements.\nWhich storage option would be both cost-efficient and highly available in this situation?\n",
  "answers" : [ {
    "id" : "c6585d1416bc4003b234b38c7aa40ac5",
    "option" : "Multiple Amazon EBS Volume with snapshots",
    "isCorrect" : "false"
  }, {
    "id" : "c893b8dc5a0a45c5a15372d501eb9c09",
    "option" : "A single Amazon Glacier Vault",
    "isCorrect" : "false"
  }, {
    "id" : "e6d7ef4f44954a6fafada08f1a3bf28d",
    "option" : "A single Amazon S3 bucket",
    "isCorrect" : "true"
  }, {
    "id" : "22e308c209554bdfae32115afb265454",
    "option" : "Multiple instance stores.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon S3 is the perfect storage solution for audio and text files.\nIt is a highly available and durable storage device.\nOption A is incorrect because storing files in EBS is not cost-efficient.\nOption B is incorrect because files need to be retrieved frequently so Glacier is not suitable.\nOption D is incorrect because instance store is not highly available compared with S3.\nFor more information on Amazon S3, please visit the following URL:\nhttps://aws.amazon.com/s3/\n\nThe best option for this scenario would be to use a single Amazon S3 bucket as it provides durable storage with high availability, scalability, and low cost. Amazon S3 is designed to offer 99.999999999% durability and 99.99% availability, making it a highly reliable option for storing files. It is also scalable, meaning that it can handle an increase in storage capacity as needed without any disruption to the service.\nMultiple Amazon EBS volumes with snapshots are not suitable for this scenario as they require you to manage multiple volumes, and it is challenging to predict the exact storage requirements. Additionally, EBS volumes are not designed for frequent access, which can lead to higher costs.\nA single Amazon Glacier Vault is not the best option for this scenario as it is designed for long-term storage of infrequently accessed data. Retrieval times can take several hours or even days, making it unsuitable for customers who need to fetch the text files frequently.\nMultiple instance stores are not recommended for this scenario because they are not durable and are volatile storage that is tied to the life of the EC2 instance. This means that if the instance fails, the data stored on the instance store will also be lost, which is not acceptable for a transcription service where data loss is not an option.\nTherefore, the best option for this scenario is a single Amazon S3 bucket, which provides cost-efficient, highly available, and durable storage that can handle unpredictable storage requirements and frequent access by customers.\n\n"
}, {
  "id" : 200,
  "question" : "A customer has an instance hosted in the public subnet of the default VPC.\nThe subnet has the default settings for the Network Access Control List.\nAn IT Administrator needs to be provided SSH access to the underlying instance.\nHow could this be accomplished?\n",
  "answers" : [ {
    "id" : "ace1aa17aa3f4d2180ab96cf3a733b60",
    "option" : "Ensure the Network Access Control Lists allow Inbound SSH traffic from the IT Administratorâ€™s Workstation.",
    "isCorrect" : "false"
  }, {
    "id" : "ea4b5f700b474dbcb054a29ff8256ad3",
    "option" : "Ensure the Network Access Control Lists allow Outbound SSH traffic from the IT Administratorâ€™s Workstation.",
    "isCorrect" : "false"
  }, {
    "id" : "18bcd6b739fe4a38be93b6933331adc2",
    "option" : "Ensure that the Security group allows Inbound SSH traffic from the IT Administratorâ€™s Workstation.",
    "isCorrect" : "true"
  }, {
    "id" : "e86f2e170a5747059efca586015b1e73",
    "option" : "Ensure that the Security group allows Outbound SSH traffic from the IT Administratorâ€™s Workstation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nEnsure that the security group allows Inbound SSH traffic from the IT Administrator's Workstation.\nSince Security groups are stateful, we do not have to configure outbound traffic.\nWhat enters the inbound traffic is allowed in the outbound traffic too.\nNote: The default network ACL is configured to allow all traffic to flow in and out of the subnets to which it is associated.\nBased on this, Option C is the correct answer.\nSince the IT administrator needs to be provided SSH access to the instance, the traffic would be inbound to the instance.\nSecurity group being stateful means that return response to the allowed inbound request will be allowed and vice-versa.\nReference:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html#DefaultSecurityGroup\n\nThe correct answer for this question is C. Ensure that the Security group allows Inbound SSH traffic from the IT Administrator's Workstation.\nExplanation: The scenario describes a customer instance in a public subnet of a default VPC. By default, instances launched in a public subnet are accessible from the internet, but they have no public IP addresses assigned. Thus, the instance can be accessed using an Elastic IP address, a VPN connection, or a bastion host.\nIn order to provide SSH access to the IT administrator, the security group associated with the instance must be updated to allow inbound SSH traffic from the IT administrator's workstation. A security group acts as a virtual firewall that controls the traffic allowed to reach the instance. By default, all inbound traffic is denied, so a new rule must be added to allow SSH traffic from the IT administrator's workstation.\nOn the other hand, Network Access Control Lists (NACLs) are stateless, meaning that you must explicitly allow both inbound and outbound traffic separately. By default, NACLs allow all inbound and outbound traffic, so they do not need to be modified to allow SSH access to the instance.\nTherefore, option A is incorrect. Options B and D are also incorrect since they refer to NACLs instead of security groups.\nIn conclusion, the correct way to provide SSH access to the instance is to update the security group associated with the instance to allow inbound SSH traffic from the IT administrator's workstation.\n\n"
}, {
  "id" : 201,
  "question" : "Your Organization is planning to move its on-premise databases to the AWS Cloud.\nYou have been selected to migrate the main production database, and there are some requirements.\nThe production database should remain active during the migration.\nYou need to monitor the progress of the migration.\nThe database is an SQL Server Database.\nYou need to find an easy way to convert the actual schemas to MySQL schemas.\nWhat services could help to achieve this? (Select two)\n",
  "answers" : [ {
    "id" : "a4a3217a467747fc888a1d677d004e3d",
    "option" : "AWS DataSync.",
    "isCorrect" : "false"
  }, {
    "id" : "fda4f53fcf97446f950f82cc1b2e671f",
    "option" : "AWS Server Migration Service.",
    "isCorrect" : "false"
  }, {
    "id" : "4515c47d4f69483da7764e5ae37ac03b",
    "option" : "AWS Database Migration Service.",
    "isCorrect" : "false"
  }, {
    "id" : "d3cabf44e1cf4a19a11dd151d41c1f23",
    "option" : "AWS Migration Hub.",
    "isCorrect" : "false"
  }, {
    "id" : "0a4b041f5bce452aba631c844b46c855",
    "option" : "AWS Server Migration Service Connector.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: C &amp;\nD.Option A is incorrect.\nDataSync is a service mainly used to migrate NFS servers to S3, EFS, Fsx, etc.\nIt does not support database migrations.\nMore details here:\nhttps://aws.amazon.com/datasync/faqs/\nOption B is incorrect.\nAWS Server Migration service is used to migrate on-premises workloads to AWS EC2\nIt does not support migrations to databases in AWS.\nMore details:\nhttps://aws.amazon.com/server-migration-service/faqs/\nOption C is CORRECT.\nAWS Database Migration Service helps you to migrate your On-Premise database to AWS, keeping the On-Premise database fully operational during the migration.\nIt also converts PL/SQL and SQL Server to MySQL or Postgres schemas with the help of the AWS Schema Conversion Tool included in this service.More details:\nhttps://aws.amazon.com/dms/faqs/\nOption D is CORRECT.\nAWS Migration Hub is used to track the progress of migrations in AWS.\nMore details here:\nhttps://aws.amazon.com/migration-hub/\nOption E is incorrect.\nAWS Server Migration Service Connector is a FreeBSD VM that needs to be installed in the on-premises virtualization environment to work with the Server Migration Service and AWS resources.\nMore details:\nhttps://aws.amazon.com/server-migration-service/faqs/\n\nThe two services that could help achieve the requirements are C. AWS Database Migration Service and D. AWS Migration Hub.\nAWS Database Migration Service (DMS) is a fully managed service that allows for database migration to and from various database engines such as SQL Server, MySQL, Oracle, and others. It can be used to migrate data between on-premises databases and AWS, or between different AWS database services. During the migration, the source database remains operational, and changes made to the source database during the migration are continuously replicated to the target database.\nTo use DMS to convert SQL Server schemas to MySQL schemas, you would first create a replication instance in AWS, which will act as a migration server. You would then create a migration task, specifying the source database endpoint (in this case, the SQL Server database) and the target database endpoint (in this case, the MySQL database). DMS will then use schema conversion tools to convert the SQL Server schema to a MySQL schema, and replicate the data to the target database.\nAWS Migration Hub (MH) is a service that provides a central location to track and manage your application migrations across multiple AWS and partner solutions. It helps monitor the progress of the migration, providing visibility into the status of each migration task and identifying any issues that may need to be resolved.\nMH integrates with various migration tools and services, including DMS, to provide a centralized view of all migration activities. It can help you assess your migration readiness, plan and execute your migration, and track your progress. By using MH, you can easily track the progress of your database migration task and monitor any issues that may arise during the migration.\nTherefore, AWS Database Migration Service and AWS Migration Hub are two services that can be used to migrate an SQL Server database to MySQL while keeping the source database operational during the migration, and to monitor the progress of the migration.\n\n"
}, {
  "id" : 202,
  "question" : "A company wants to build a brand new application on the AWS Cloud.\nThey want to ensure that this application follows the Microservices architecture.\nWhich of the following services can be used to build this type of architecture? (SELECT THREE)\n",
  "answers" : [ {
    "id" : "f7d0cad345754243b897385e66dc268c",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "887d3ef1606b4c6ba54c25b148105f64",
    "option" : "AWS ECS",
    "isCorrect" : "true"
  }, {
    "id" : "8e2b4ad494c84206b1d28a3da83a96ce",
    "option" : "AWS API Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "51b0a0c7ad704ebba06f59c89aaad508",
    "option" : "AWS Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - A, B, and C.\nAWS Lambda is a serverless compute service that allows you to build independent services.\nThe Elastic Container Service (ECS) can be used to manage containers.\nThe API Gateway is a serverless component for managing access to APIs.\nFor more information about Microservices on AWS, please visit the following URL:\nhttps://aws.amazon.com/microservices/\n\nMicroservices is an architecture pattern that structures an application as a collection of small, independent, and loosely coupled services. Each service is self-contained and can be developed, deployed, and scaled independently of other services. AWS offers several services that can be used to build a microservices-based application.\nThe correct answers are: A. AWS Lambda, B. AWS ECS, and C. AWS API Gateway.\nA. AWS Lambda: AWS Lambda is a compute service that allows you to run code without provisioning or managing servers. You can write your microservices as Lambda functions and then deploy and run them in response to events. This way, you only pay for the compute time that your code actually uses. AWS Lambda supports multiple programming languages such as Node.js, Python, Java, Go, and Ruby.\nB. AWS ECS: AWS ECS (Elastic Container Service) is a fully managed container orchestration service that enables you to run, manage, and scale Docker containers on AWS. You can use ECS to deploy and run your microservices as containers. ECS integrates with other AWS services such as EC2, ECR, and IAM to provide a secure and scalable platform for running containers.\nC. AWS API Gateway: AWS API Gateway is a fully managed service that makes it easy for developers to create, publish, and manage APIs. You can use API Gateway to expose your microservices as RESTful APIs, which can then be consumed by other applications or services. API Gateway provides features such as authentication, authorization, rate limiting, and caching, which help you to build secure and scalable APIs.\nD. AWS Config: AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It does not directly relate to building microservices architecture.\nIn summary, AWS Lambda, AWS ECS, and AWS API Gateway are the correct answers for building a microservices-based application on the AWS Cloud.\n\n"
}, {
  "id" : 203,
  "question" : "You are deploying an application to track the GPS coordinates of delivery trucks in the United States.\nCoordinates are transmitted from each delivery truck once every three seconds.\nYou need to design an architecture that will enable real-time processing of these coordinates from multiple consumers.\nWhich service should you use to implement data ingestion?\n",
  "answers" : [ {
    "id" : "baca532760604be1adc0d9c03009c624",
    "option" : "Amazon Kinesis",
    "isCorrect" : "true"
  }, {
    "id" : "88c88743ba5a4efea135e282b052e9dc",
    "option" : "AWS Data Pipeline",
    "isCorrect" : "false"
  }, {
    "id" : "e6f545c2ab954ee49a9a5826c8cb8175",
    "option" : "Amazon AppStream",
    "isCorrect" : "false"
  }, {
    "id" : "b00d0e54916c47efa8f3161258ac96cf",
    "option" : "Amazon Simple Queue Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS documentation mentions the following:\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to process streaming data cost-effectively at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications.\nAmazon Kinesis enables you to process and analyze data as it arrives and responds instantly instead of waiting until all your data is collected before the processing can begin.\nFor more information on Amazon Kinesis, please visit the following URL:\nhttps://aws.amazon.com/kinesis/\n\nFor real-time processing of streaming data from multiple sources, Amazon Kinesis is the most suitable service.\nAmazon Kinesis is a fully-managed service for real-time processing of streaming data at a large scale. It allows you to ingest and process large amounts of data in real-time from various sources such as websites, social media, sensors, logs, and other applications.\nIn this case, the GPS coordinates from the delivery trucks can be transmitted to Amazon Kinesis using the Kinesis Producer Library or Kinesis Agent. Kinesis can then receive and process the data in real-time using Kinesis Streams or Kinesis Firehose.\nKinesis Streams is a scalable and durable real-time streaming data processing service that enables multiple consumers to read data from the same stream simultaneously. It is designed for low-latency, high-throughput, and continuous processing of data. Multiple consumers can read the same stream simultaneously, enabling real-time processing by multiple applications.\nKinesis Firehose is a fully-managed service that can receive streaming data and deliver it to other AWS services such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch Service. It can also transform and compress the data before delivering it to the destination.\nAWS Data Pipeline is a batch-processing service for data transformation and movement between different AWS services. It is not suitable for real-time processing of streaming data.\nAmazon AppStream is a fully-managed desktop streaming service that enables you to stream desktop applications to users. It is not related to data ingestion or real-time processing.\nAmazon Simple Queue Service (SQS) is a message queuing service that enables decoupling of components and asynchronous communication between microservices. It is not designed for real-time data processing and would not be suitable for this use case.\nIn conclusion, Amazon Kinesis is the most suitable service for data ingestion and real-time processing of streaming data in this scenario.\n\n"
}, {
  "id" : 204,
  "question" : "With its own Active Directory, your company authenticates users for different applications.\nYou have been assigned the task of consolidating and migrating services to the cloud and using the same credentials if possible.\nWhat would you recommend?\n",
  "answers" : [ {
    "id" : "6ae3b7f530c7416a986fb24fae220fa8",
    "option" : "Use AWS Directory Service that allows users to sign in with their existing corporate credentials.",
    "isCorrect" : "true"
  }, {
    "id" : "5b3e811345a84a47a81fdda5dc394c52",
    "option" : "Create two Active Directories â€“ one for the cloud and one for on-premises â€“ reducing username/password combinations to two.",
    "isCorrect" : "false"
  }, {
    "id" : "e7b6c222ed624d3c8de47212d0a931f2",
    "option" : "Require users to use third-party identity providers to log-in for all services.",
    "isCorrect" : "false"
  }, {
    "id" : "d2a624aea592454fb7b6e26bca30f62a",
    "option" : "Build out Active Directory on EC2 instances to gain more control over user profiles.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is correct.\nAWS Directory Service enables your end-users to use their existing corporate credentials while accessing AWS applications.\nOnce you've been able to consolidate services to AWS, you won't have to create new credentials.\nInstead, you'll be able to allow the users to use their existing username/password.\nOption B is incorrect.\nOne Active Directory can be used for both on-premises and the cloud.\nThis isn't the best option provided.\nOption C is incorrect.\nThis won't always reduce the number of username/password combinations.\nOption D is incorrect.\nThis requires more effort and additional management than using a managed service.\nFor more information, please refer to the URLs below:\nhttps://aws.amazon.com/directoryservice/faqs\nhttps://aws.amazon.com/directoryservice/\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\n\nAs the company is currently using Active Directory for user authentication, it would be best to look for a solution that can integrate with the existing Active Directory. Out of the given options, the best choice would be A: Use AWS Directory Service that allows users to sign in with their existing corporate credentials.\nAWS Directory Service provides multiple options for integrating with existing Active Directory environments. These options include:\nAWS Managed Microsoft AD: It is a fully managed Active Directory in the AWS cloud. It can be integrated with an existing on-premises Active Directory using AD Connector, allowing users to access resources in both environments using their existing credentials. AD Connector: It allows the AWS resources to authenticate and authorize users against the existing on-premises Active Directory. It acts as a proxy between the AWS applications and the on-premises directory. Simple AD: It is a standalone directory that is compatible with Microsoft Active Directory. It can be used as a standalone directory or can be integrated with an existing on-premises Active Directory using AD Connector.\nBy using AWS Directory Service, the company can consolidate its user authentication and management, which can simplify administration and reduce costs. Additionally, it can provide a more seamless experience for users by allowing them to use their existing corporate credentials to access cloud-based applications.\nOption B is not a recommended approach as it would create two separate directories, which could lead to more complexity and potential security issues. Option C is not a good choice as it would require users to log in using different credentials for different services, leading to confusion and more administrative overhead. Option D is not recommended as it would require managing and securing Active Directory on EC2 instances, which could be complex and costly.\n\n"
}, {
  "id" : 205,
  "question" : "A company is planning to use the AWS Redshift service.\nThe Redshift service and data on it would be used continuously for the next 3 years as per the current business plan.\nWhat would be the most cost-effective solution in this scenario?\n",
  "answers" : [ {
    "id" : "91d698551501494aa9ba311237622cf4",
    "option" : "Consider using On-demand instances for the Redshift Cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "15cc3ca3ba244af6b7c21e7b1a05bc49",
    "option" : "Enable Automated backup.",
    "isCorrect" : "false"
  }, {
    "id" : "014d9fe5e37b440998446aaa659f9810",
    "option" : "Consider using Reserved Instances for the Redshift Cluster.",
    "isCorrect" : "true"
  }, {
    "id" : "5cab700289e74578bcb6d7e9fd0d8944",
    "option" : "Consider not using a cluster for the Redshift nodes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS documentation mentions the following:\nIf you intend to keep your Amazon Redshift cluster running continuously for a prolonged period, you should consider purchasing reserved node offerings.\nThese offerings provide significant savings over on-demand pricing, but they require you to reserve compute nodes and commit to paying for those nodes for either a one-year or three-year duration.\nFor more information on Reserved Nodes in Redshift, please visit the following URL:\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/purchase-reserved-node-instance.html\n\nIn this scenario, the company plans to use AWS Redshift continuously for the next 3 years. The most cost-effective solution would be to consider using Reserved Instances for the Redshift Cluster.\nReserved Instances provide a significant discount compared to On-Demand instances, which are charged at a higher rate. By purchasing a Reserved Instance, the company can reserve capacity for the Redshift cluster for a specific period, typically for 1 or 3 years, at a lower hourly rate compared to On-Demand instances. This means that the company can save up to 75% on the hourly rate by using Reserved Instances.\nEnabling automated backup is a best practice to ensure data durability and recoverability, but it does not directly impact the cost of using Redshift.\nConsidering not using a cluster for the Redshift nodes would not be practical, as Redshift is a columnar data store designed to work with large data sets, and a cluster provides scalability and fault tolerance.\nTherefore, the most cost-effective solution for a company planning to use AWS Redshift continuously for the next 3 years is to consider using Reserved Instances for the Redshift Cluster.\n\n"
}, {
  "id" : 206,
  "question" : "A company has recently started using AWS Cloud services and needs to transfer a large set of data online from on-prem Windows servers to AWS Storage Services S3, EFS, and FSx.\nThis transfer should be a bi-directional, reliable, and incremental based on schedules. How would a Solution Architect design this solution?\n",
  "answers" : [ {
    "id" : "3ff03518a61740d1aef46fcbac281086",
    "option" : "Use Snowball devices to transfer data to S3, EFS and FSx.",
    "isCorrect" : "false"
  }, {
    "id" : "db66c2c274fd4b708233cbc97e5543b8",
    "option" : "Use AWS DataSync service to transfer data to AWS Services.",
    "isCorrect" : "true"
  }, {
    "id" : "54f1864cf64148e3b575b098dbb62312",
    "option" : "AWS Database Migration Service to transfer data to AWS services.",
    "isCorrect" : "false"
  }, {
    "id" : "086d68784aac4eb9b4b45b3c93adcea4",
    "option" : "Use AWS S3 Transfer acceleration to transfer a large set of data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect because snowball devices are used to transfer a large set of data electronically but do not meet the bi-directional and incremental transfer requirements.\nOption B is CORRECT because it is a more reliable service to transfer data bi-directionally, supporting S3, EFS and FSx.\nDataSync helps to move data over the network between on-premise File Systems storage and AWS Storage services.\nDataSync automates both the management of data transfer processes and the infrastructure required for high-performance, secure data transfer.\nOption C is incorrect because the Data migration service is used to transfer databases to AWS RDS.\nOption D is incorrect because S3 Transfer acceleration is used to accelerate data transfer to S3 and doesn't support bi-directional and the fie systems.\nReferences:\nhttps://aws.amazon.com/datasync/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc\n\n<h2>Option A: Use Snowball devices to transfer data to S3, EFS, and FSx</h2><p>Snowball is a physical data transfer service that enables businesses to transfer large amounts of data into and out of AWS cloud environments. A Snowball device can be shipped to the customer's on-premises location, where data is loaded onto it and then shipped back to AWS to be transferred to S3, EFS, and FSx.</p><p>This option can be considered if the data set is quite large, and transferring data over the network would be time-consuming or expensive. Snowball devices are suitable for data sets ranging from a few terabytes to dozens of petabytes. However, this option is not incremental and may require manual scheduling for data transfer.</p><h2>Option B: Use AWS DataSync service to transfer data to AWS Services</h2><p>AWS DataSync is a data transfer service that makes it easy for businesses to automate and accelerate copying large amounts of data to and from AWS services over the network. DataSync provides a reliable and secure transfer of data with features such as incremental transfer, checksum verification, and scheduling capabilities.</p><p>This option is a good choice for transferring data over the network and provides a bi-directional and incremental transfer. DataSync also allows you to transfer data from on-premises storage systems to S3, EFS, and FSx. DataSync is also suitable for real-time replication of data to cloud services.</p><h2>Option C: Use AWS Database Migration Service to transfer data to AWS services</h2><p>The AWS Database Migration Service (DMS) is a managed service that makes it easy to migrate data from one database to another. DMS supports a variety of database sources, including on-premises databases, and can transfer data to a variety of database targets, including Amazon S3, EFS, and FSx.</p><p>This option is a good choice if the data set is stored in a database, and the customer wants to migrate the database to AWS. AWS DMS provides incremental migration and scheduling capabilities. However, if the data set is not stored in a database, this option is not suitable.</p><h2>Option D: Use AWS S3 Transfer acceleration to transfer a large set of data</h2><p>AWS S3 Transfer Acceleration is a data transfer acceleration service that enables faster transfer of data over the internet to and from Amazon S3. S3 Transfer Acceleration uses Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet.</p><p>This option is a good choice if the customer wants to transfer a large amount of data to S3 and wants to accelerate the transfer over the internet. However, this option is not bi-directional, and it does not support transfer to EFS or FSx.</p><h2>Conclusion</h2><p>Based on the requirements of bi-directional, reliable, and incremental transfer based on schedules, the best option for this scenario is B. Use AWS DataSync service to transfer data to AWS services. This option can provide reliable and secure transfer of data with features such as incremental transfer, checksum verification, and scheduling capabilities. DataSync also allows transferring data from on-premises storage systems to S3, EFS, and FSx.</p>\n"
}, {
  "id" : 207,
  "question" : "Your organization is using a CloudFront distribution to distribute content from an S3 bucket.\nIt is required that only a particular set of users get access to certain objects respectively.\nHow could this be accomplished?\n",
  "answers" : [ {
    "id" : "f19c39006a6c4d34a7ec1b500bd20a63",
    "option" : "Create IAM Users for each user and then provide access to the S3 bucket content.",
    "isCorrect" : "false"
  }, {
    "id" : "dc682902bd4f4c37ab56383e2cba3ac7",
    "option" : "Create an IAM Group for each set of users and then provide each Group access to the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "d884076acf984f53b1b961552ac796c5",
    "option" : "Create CloudFront signed URLs and then distribute these URLs to the users.",
    "isCorrect" : "true"
  }, {
    "id" : "46f6d8c198a243159b576877d3649335",
    "option" : "Use IAM Polices for the underlying S3 buckets to restrict content.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the following:\nCloudFront signed URLs and signed cookies provide the same basic functionality: they allow you to control who can access your content.\nUse signed URLs in the following cases:\nYou want to restrict access to individual files, for example, an installation download for your application.\nYour users are using a client (for example, a custom HTTP client) that doesn't support cookies.\nUse signed cookies in the following cases:\nYou want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or all of the files in the subscribers' area of the website.\nYou don't want to change your current URLs.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\n\nSure! Let me explain each option and provide more details:\nA. Create IAM Users for each user and then provide access to the S3 bucket content.\nThis option involves creating an IAM User for each user who needs access to specific objects in the S3 bucket. Then, you would need to set up S3 bucket policies to grant the appropriate permissions for each IAM User to access specific objects in the S3 bucket. However, this option could be complex and time-consuming if you have a large number of users and objects, and it could be challenging to maintain.\nB. Create an IAM Group for each set of users and then provide each Group access to the S3 bucket.\nThis option involves creating an IAM Group for each set of users who need access to specific objects in the S3 bucket. Then, you would need to add the IAM Users to the appropriate IAM Group and set up S3 bucket policies to grant the appropriate permissions for each IAM Group to access specific objects in the S3 bucket. This option could be more manageable than option A, as you could add and remove users from the IAM Groups to manage access more efficiently.\nC. Create CloudFront signed URLs and then distribute these URLs to the users.\nThis option involves using CloudFront signed URLs to grant access to specific objects in the S3 bucket. You would create a CloudFront distribution and configure it to serve content from the S3 bucket. Then, you could create signed URLs that provide time-limited access to specific objects in the S3 bucket, and distribute those URLs to the authorized users. This option provides more flexibility and control than options A and B, as you could create and revoke signed URLs as needed.\nD. Use IAM Polices for the underlying S3 buckets to restrict content.\nThis option involves using IAM policies to restrict access to specific objects in the S3 bucket. You would need to create IAM policies that allow access to the S3 bucket and objects for only the authorized users. This option provides more control than option A, but it could be challenging to manage and maintain if you have a large number of objects and users.\nIn conclusion, option C (Create CloudFront signed URLs and then distribute these URLs to the users) provides the most flexibility and control for granting access to specific objects in the S3 bucket, while option B (Create an IAM Group for each set of users and then provide each Group access to the S3 bucket) provides a more manageable option for managing access to the S3 bucket.\n\n"
}, {
  "id" : 208,
  "question" : "You are working in a financial company.\nThe company needs to create a serverless application in AWS, including the serverless resources such as Lambda functions and API gateways.\nYou prefer using a framework to build the application on AWS as quickly as possible.\nWhich of the following frameworks is built for serverless applications?\n",
  "answers" : [ {
    "id" : "cabf6cdd55524a80839f1137a045102b",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "2941d96d6eb44b669ec187b815598086",
    "option" : "AWS Serverless Application Model (AWS SAM)",
    "isCorrect" : "true"
  }, {
    "id" : "23350b563d31453e8559e6bc86056c48",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "36f50053fa424df680245d75ea4001ba",
    "option" : "Terraform.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless applications in AWS.\nIt provides you with a simple and clean syntax to describe the functions, APIs, permissions, configurations, and events that make up a serverless application.\nReference can be found in https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html.\nOption A is incorrect because AWS CloudFormation is a tool to manage all the AWS resources using a template.\nHowever, it is not a tool dedicated for serverless applications.\nOption C is incorrect because Elastic Beanstalk can help users to quickly launch a web application in AWS.\nIt is not a tool for serverless applications.\nOption D is incorrect because Terraform is an open-source infrastructure as code software tool to configure the infrastructure.\nIt is not a dedicated tool for serverless applications.\n\nThe correct answer is B. AWS Serverless Application Model (AWS SAM).\nExplanation:\nWhen building serverless applications on AWS, you can use different frameworks to facilitate the process of creating and deploying serverless resources such as Lambda functions, API gateways, and more.\nAmong the options provided, AWS Serverless Application Model (AWS SAM) is the framework specifically designed for building serverless applications on AWS. It is an open-source framework that extends AWS CloudFormation to provide a simplified way of defining the Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables needed by your serverless application.\nAWS SAM uses AWS CloudFormation templates, which are YAML or JSON files that describe the infrastructure resources required for your application. You can define the AWS Lambda functions, API Gateway APIs, and other resources required by your application using a simple syntax in the AWS SAM template. AWS SAM also includes additional resources and convenience methods that make it easier to work with serverless applications.\nAWS CloudFormation is also an option for building serverless applications on AWS. However, AWS CloudFormation is a general-purpose infrastructure as code (IaC) tool that can be used to define and deploy any AWS resource, not just serverless resources. While it provides more flexibility, it can be more complex to use.\nAWS Elastic Beanstalk is a platform as a service (PaaS) offering from AWS that can deploy and manage applications of different types, including serverless applications. However, Elastic Beanstalk is not specifically designed for serverless applications, and it requires more configuration and management overhead than AWS SAM.\nTerraform is another infrastructure as code (IaC) tool that can be used to define and manage resources on various cloud providers, including AWS. However, like AWS CloudFormation, it is a general-purpose tool and does not provide the same level of abstraction and convenience as AWS SAM when building serverless applications on AWS.\nTherefore, among the options provided, AWS SAM is the most suitable framework for building serverless applications on AWS, as it is specifically designed for this purpose and provides a simplified way of defining and deploying serverless resources.\n\n"
}, {
  "id" : 209,
  "question" : "You have a small company, running on Windows OS, that leverages cloud resources like AWS Workspaces and AWS Workmail.\nYou want a fully managed solution to set policies and provide user management.\nWhich of the minimum required AWS Directory Service would you recommend?\n",
  "answers" : [ {
    "id" : "b742afc6fb9c4ee1952d96cd4fc48656",
    "option" : "AWS Managed Microsoft AD for its full-blown AD features and capabilities",
    "isCorrect" : "false"
  }, {
    "id" : "991517d877ea4277ac9c792474964300",
    "option" : "AD Connector to be used with on-premises applications",
    "isCorrect" : "false"
  }, {
    "id" : "12d613fe7afe4b04aacc29b149a0ecd2",
    "option" : "AWS Cognito for its scalability and customization",
    "isCorrect" : "false"
  }, {
    "id" : "4f4478fc7ef440fd944c68191156bbd2",
    "option" : "Simple AD for limited functionality and compatibility with desired applications.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption D - Simple AD for limited functionality and compatibility with desired applications is the correct answer.\nSimple AD is a Microsoft Active Directory-compatible directory from AWS Directory Service.\nYou can use Simple AD as a standalone directory in the cloud to support Windows workloads that need basic AD features or compatible AWS applications.\nIt can also be used to support Linux workloads that need LDAP service.\nOption A is incorrect.\nThis is more functionality and feature-rich than you need, given the desired applications.\nOption B is incorrect.\nYou don't have on-premises applications, so AD Connector is not needed.\nOption C is incorrect.\nThis is more functionality and feature-rich than you need, given the desired applications.\nFor more information, please check the URLs below:\nhttps://aws.amazon.com/directoryservice/\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\n\nBased on the scenario provided, the recommended minimum required AWS Directory Service for a small company that runs on Windows OS and leverages AWS Workspaces and AWS Workmail, and requires a fully managed solution to set policies and provide user management is the AWS Managed Microsoft AD.\nOption A: AWS Managed Microsoft AD provides a fully managed Active Directory (AD) in the AWS Cloud, which enables the company to manage users and groups, join Amazon EC2 instances to a domain, and use Group Policies to configure and lock down the environment. This option offers the most comprehensive features and capabilities for Windows-based workloads and is the recommended solution for managing and securing Windows-based resources in AWS.\nOption B: AD Connector is a proxy service that allows on-premises directory services to be used with AWS applications such as Amazon WorkSpaces and Amazon WorkMail. While it is a useful option for customers who want to extend their on-premises directory to AWS, it is not a fully managed solution for user management in the AWS Cloud.\nOption C: AWS Cognito is a user sign-up, sign-in, and access control service that is commonly used in mobile and web applications. It provides scalable and customizable user authentication and authorization solutions and is not designed for managing Windows-based resources like AWS Workspaces.\nOption D: Simple AD is a low-cost directory service that provides a subset of AD features, including user and group management and authentication for Amazon EC2 instances running Linux or Windows. However, it has limited functionality and compatibility with the desired applications, which may result in management and compatibility issues for Windows-based workloads.\nIn conclusion, based on the requirements stated in the scenario, the most suitable option for the minimum required AWS Directory Service is AWS Managed Microsoft AD, as it provides a fully managed solution for Windows-based workloads and offers the most comprehensive features and capabilities for user management in the AWS Cloud.\n\n"
}, {
  "id" : 210,
  "question" : "A solutions architect is designing the cloud architecture of a new application being deployed onto AWS Cloud.\nThis application has a set of instances that are required to be placed in a placements group.\nThe instances should be placed on distinct racks, with each rack having its own network and power source. Which of the following designs provides the highest availability for the application?\n",
  "answers" : [ {
    "id" : "6953ccdb22db4c3e99e0593631c3b611",
    "option" : "Place the instances into a cluster placement group that can span across multiple Availability Zones.",
    "isCorrect" : "false"
  }, {
    "id" : "55768d585f0d418eb1c0e724603be3ca",
    "option" : "Place the instances into a partitions placement group to evenly distribute the instances across partitions in an availability zone.",
    "isCorrect" : "false"
  }, {
    "id" : "2f481802229c4a8ca36f4114bf7311a4",
    "option" : "Place the instances into a spread placement group that can span multiple Availability Zones in the same Region.",
    "isCorrect" : "true"
  }, {
    "id" : "a7c49bf5509a41cf93d9b5ae9a174292",
    "option" : "Place the instances into a spread placement group that can span multiple Availability Zones in the multiple Regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nCluster.\nPartition.\nSpread.\nOption A is incorrect because a cluster placement group cannot span across multiple Availability Zones, doesn't meet HA requirements.\nOption B is incorrect because the instances are placed in an availability zone.\nIf this particular availability zone has an outage, the application will be impacted.\nAs this question asks for the design with the highest availability, this option is not suitable.\nOption C is CORRECT as this meets the HA requirement.\nSee above diagrams.\nA spread placement group supports a maximum of seven running instances per Availability Zone.\nFor example, in a Region with three Availability Zones, a total of 21 instances ( 7 x 3)in the group (seven per zone)\nIf any plans to start the eighth instance in the same AZ and the same spread placement group, the instance will not launch.\nOption D is incorrect because the spread placement group cannot span across multiple regions.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#concepts-placement-groups\n\n\nThe requirement is to place the instances of an application in a placements group such that each instance is on a distinct rack with its own network and power source to ensure the highest availability. The options given are:\nA. Place the instances into a cluster placement group that can span across multiple Availability Zones. B. Place the instances into a partitions placement group to evenly distribute the instances across partitions in an availability zone. C. Place the instances into a spread placement group that can span multiple Availability Zones in the same Region. D. Place the instances into a spread placement group that can span multiple Availability Zones in the multiple Regions.\nLet's discuss each option and its suitability for the given requirement:\nOption A: Place the instances into a cluster placement group that can span across multiple Availability Zones.\nA cluster placement group is a logical grouping of instances within a single availability zone, and it can span across multiple instances. This placement group type is best suited for applications that require low network latency, high network throughput, or both. However, this placement group does not meet the requirement of placing instances on distinct racks with their own network and power source, as the instances within the group are placed in close physical proximity to each other.\nTherefore, Option A is not suitable for this requirement.\nOption B: Place the instances into a partitions placement group to evenly distribute the instances across partitions in an availability zone.\nA partitions placement group is a logical grouping of instances within a single availability zone, and it evenly distributes instances across partitions to provide high availability. This placement group type is best suited for applications that have a large number of instances and require high availability within a single availability zone. However, like a cluster placement group, it does not meet the requirement of placing instances on distinct racks with their own network and power source.\nTherefore, Option B is not suitable for this requirement.\nOption C: Place the instances into a spread placement group that can span multiple Availability Zones in the same Region.\nA spread placement group is a logical grouping of instances that are placed on distinct underlying hardware to reduce the risk of simultaneous failure. This placement group type is best suited for applications that require high availability and isolation from other instances within the same availability zone. The spread placement group ensures that each instance is placed on a distinct rack with its own network and power source, which meets the requirement. Additionally, the spread placement group can span multiple availability zones in the same region, providing additional redundancy.\nTherefore, Option C is the most suitable for this requirement.\nOption D: Place the instances into a spread placement group that can span multiple Availability Zones in the multiple Regions.\nThis option provides the highest level of availability by placing instances in multiple regions, but it is not required by the given requirement. Additionally, this option increases the complexity of the deployment and may have higher costs associated with it. Therefore, it is not the most suitable option for this requirement.\nIn conclusion, the correct answer is option C: Place the instances into a spread placement group that can span multiple Availability Zones in the same Region.\n\n"
}, {
  "id" : 211,
  "question" : "Your company requires a Stack-based model for its resources in AWS.\nThere is a need to have different stacks for the Development and Production environments.\nWhich of the following can be used for this?\n",
  "answers" : [ {
    "id" : "8d9948318b904cb08416119016a15494",
    "option" : "Use EC2 tags to define different stack layers for your resources.",
    "isCorrect" : "false"
  }, {
    "id" : "370c9952c622433f94c39e4470a391bb",
    "option" : "Define the metadata for the different layers in DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "be71dd0feec44eebbc8aee969288f5ea",
    "option" : "Use AWS OpsWorks to define the different layers for your application.",
    "isCorrect" : "true"
  }, {
    "id" : "09fa785e91c64687b24c640b6e20f53d",
    "option" : "Use AWS Config to define the different layers for your application.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe requirement can be fulfilled via the OpsWorks service.\nThe AWS Documentation given below supports this requirement:\nAWS OpsWorks Stacks lets you manage applications and servers on AWS and on-premises.\nWith OpsWorks Stacks, you can model your application as a stack containing different layers, such as load balancing, database, and application server.\nYou can deploy and configure Amazon EC2 instances in each layer or connect other resources such as Amazon RDS databases.\nFor more information on OpsWorks stacks, please visit the following URL:\nhttps://aws.amazon.com/opsworks/stacks/\nA stack is basically a collection of instances that are managed together for serving a common task.\nConsider a sample stack whose purpose is to serve web applications.\nIt will be comprised of the following instances.\nA set of application server instances, each of which handles a portion of the incoming traffic.\nA load balancer instance, which takes incoming traffic and distributes it across the application servers.\nA database instance, which serves as a back-end data store for the application servers.\nA common practice is to have multiple stacks that represent different environments.\nA typical set of stacks consists of:\nA development stack to be used by developers to add features, fix bugs, and perform other development and maintenance tasks.\nA staging stack to verify updates or fixes before exposing them publicly.\nA production stack, which is the public-facing version that handles incoming requests from users.\nFor more information, please see the link given below:\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingstacks.html\n\nThe answer is C. Use AWS OpsWorks to define the different layers for your application.\nAWS OpsWorks is a configuration management service that uses Chef, an automation platform, to manage and automate infrastructure deployment. OpsWorks provides a way to define a stack, which is a collection of AWS resources that are managed together as a single unit. By using OpsWorks, you can define different stacks for different environments, such as development and production, and configure them independently.\nEC2 tags (Option A) can be used to categorize EC2 instances but are not specific to creating stack-based models for resources.\nDynamoDB (Option B) is a NoSQL database service, and it is not related to creating stack-based models for resources.\nAWS Config (Option D) is a service that provides a detailed view of the configuration of AWS resources in an account. While it can help with tracking and managing changes to resources, it is not specifically designed to create a stack-based model for resources.\nIn summary, AWS OpsWorks is the best option for creating a stack-based model for resources in AWS, where different stacks can be defined for different environments.\n\n"
}, {
  "id" : 212,
  "question" : "Your company has been hosting a static website in an S3 bucket for several months and gets a fair amount of traffic.\nNow you want your website to make requests to another website (example.com) with a different domain name.\nThe requests are unsuccessful in the browser.\nHowever, the website (example.com) works properly when the requests are not from your website.\nWhat could be the most likely cause of this disruption?\n",
  "answers" : [ {
    "id" : "77f21aaa3b32479dbd07569c2b97320c",
    "option" : "The new domain name is not registered in CloudWatch monitoring.",
    "isCorrect" : "false"
  }, {
    "id" : "971e9f8c64fb44a8876b0273c0c790ce",
    "option" : "The S3 bucket has not been configured to allow Cross-Origin Resource Sharing (CORS).",
    "isCorrect" : "true"
  }, {
    "id" : "9ff2d35980714ab488e9e482ee65c1d7",
    "option" : "The S3 bucket was not created in the correct region.",
    "isCorrect" : "false"
  }, {
    "id" : "072645f7b9e742f08c1ec36e8f09d23d",
    "option" : "The domain name wasnâ€™t registered with AWS Route 53 and therefore wonâ€™t work.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption B is correct.\nThe S3 bucket has not been configured to allow Cross-Origin Resource Sharing (CORS)\nTo keep your content safe, your web browser implements something called the same-origin policy.\nThe default policy ensures that scripts and other active content loaded from one site or domain cannot interfere or interact with content from another location without an explicit indication that this is the desired behavior.\nOption A is incorrect.\nEnabling Cloudwatch doesn't affect Cross-Origin Resource Sharing (CORS).\nOption C is incorrect.\nThe S3 region does not cause the issue.\nOption D is incorrect.\nThe domain can be registered with any online registrar, not just AWS Route53\nThe question also mentions that the website (example.com) works properly when the requests are not from your website.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\nhttps://aws.amazon.com/blogs/aws/amazon-S3-cross-origin-resource-sharing/\n\nThe most likely cause of this disruption is that the S3 bucket has not been configured to allow Cross-Origin Resource Sharing (CORS) (Option B).\nCORS is a security feature implemented in web browsers to prevent websites from making requests to a different domain than the one that served the original content. This is a common technique used to prevent cross-site scripting (XSS) attacks.\nWhen the website hosted in the S3 bucket tries to make a request to example.com, the browser blocks the request by default because it is a cross-origin request. To allow the website to make requests to example.com, the server hosting the website must include specific HTTP headers in its responses that tell the browser it's safe to make cross-origin requests. These headers are collectively referred to as CORS headers.\nTo enable CORS on an S3 bucket, you must configure the bucket's CORS policy. This policy specifies which origins are allowed to make cross-origin requests to the bucket and which HTTP methods are allowed in these requests. You can define a CORS policy for an S3 bucket using the AWS Management Console or AWS CLI.\nOption A (The new domain name is not registered in CloudWatch monitoring) is not relevant to this issue since CloudWatch is a monitoring service and does not affect the functionality of the website or its ability to make requests.\nOption C (The S3 bucket was not created in the correct region) is also not relevant to this issue since the region in which the bucket is created does not affect its ability to make cross-origin requests.\nOption D (The domain name wasn't registered with AWS Route 53 and therefore won't work) is also not relevant to this issue since registering a domain name with Route 53 does not affect the ability of a website to make cross-origin requests. However, if the website's domain name was not correctly configured in DNS, it could potentially cause issues with accessing the website in general.\n\n"
}, {
  "id" : 213,
  "question" : "An AWS Organization has below the hierarchy of Organizational Units (OUs): Root -&gt; Project_OU -&gt; Dev_OU The Root is attached to the default Service Control Policy (SCP). Project_OU is attached to an SCP that prevents users from deleting VPC Flow Logs. Dev_OU has an SCP that allows the action of \"ec2: DeleteFlowLogs\". Are the IAM users/roles in Dev_OU AWS accounts allowed to delete VPC Flow Logs?\n",
  "answers" : [ {
    "id" : "a95a4a53f2c84d98a37f95e49b1d6153",
    "option" : "It is permitted because the SCP in Dev_OU allows it.",
    "isCorrect" : "false"
  }, {
    "id" : "5e1d458b9e4c4e5c85ede57441ae9a71",
    "option" : "It is allowed because the Root has the default SCP that allows all actions.",
    "isCorrect" : "false"
  }, {
    "id" : "557200f7b7364b78a3fac95245648dc5",
    "option" : "It is not allowed as the SCP in Project_OU restricts the action.",
    "isCorrect" : "true"
  }, {
    "id" : "e792a4e8f42647379623c6cfdcef69f0",
    "option" : "It is not allowed as the default SCP in Root denies the action.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nCheck the AWS documentation https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_about-scps.html for how SCPs work in an AWS Organization.\nOption A is incorrect: Because if any parent OU has an SCP to deny the action, the final result is Deny.\nOption B is incorrect: Although the default SCP allows the action, the parent OU (Project_OU) denies it.\nOption C is CORRECT: Because an explicit Deny statement in Project_OU overrides any Allow.\nOption D is incorrect: Because the default SCP is FullAWSAccess which allows all actions and all services.\n\nThe correct answer is D. It is not allowed as the default SCP in Root denies the action.\nHere's why:\nFirst, let's understand what Service Control Policies (SCPs) do. SCPs are a type of AWS Identity and Access Management (IAM) policy that allow you to set permissions on the maximum permissions that can be granted to accounts in your organization. SCPs can be attached to either the root of the organization or to individual organizational units (OUs) within the organization. SCPs are designed to limit the actions that can be performed on AWS services, APIs, and resources.\nIn this scenario, the Root of the organization has an attached default SCP. This means that all OUs within the organization inherit the permissions defined in the default SCP, unless a more restrictive policy is attached to an individual OU.\nThe Project_OU is attached to an SCP that prevents users from deleting VPC Flow Logs. This means that IAM users/roles in the Project_OU cannot delete VPC Flow Logs.\nThe Dev_OU has an SCP that allows the action of \"ec2: DeleteFlowLogs\". This means that IAM users/roles in the Dev_OU are allowed to delete VPC Flow Logs.\nHowever, since the Root has the default SCP that denies the action of deleting VPC Flow Logs, IAM users/roles in the Dev_OU are not allowed to delete VPC Flow Logs.\nTherefore, the correct answer is D. It is not allowed as the default SCP in Root denies the action.\n\n"
}, {
  "id" : 214,
  "question" : "One AWS Organization owns several AWS accounts.\nRecently, due to a change of company organizations, one member account needs to be moved from this AWS Organization to another one.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "0edc7420b7654cdc8a2b68a56002b7c1",
    "option" : "In the AWS console, drag and drop this account from one Organization to another.",
    "isCorrect" : "false"
  }, {
    "id" : "10410627dd45491bbd7689c378bec969",
    "option" : "In the AWS console, select the member account and migrate it to the destination AWS Organization.",
    "isCorrect" : "false"
  }, {
    "id" : "649e3aa72cb94a2dba0aeec23a491400",
    "option" : "Delete the old AWS Organization. Send an invite from the new Organization and accept the invite for the member account.",
    "isCorrect" : "false"
  }, {
    "id" : "eb609684e47f4aafb92dfa69f0d06fd6",
    "option" : "Remove the member account from the old Organization. Send an invite from the new Organization to the member account and accept the invite.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect: This operation cannot be performed.\nOption B is incorrect: Because a member account cannot be migrated to another AWS Organization directly.\nOption C is incorrect: Because there is no need to delete the old Organization.\nOption D is CORRECT: The account needs to be removed from the old Organization and then accept the invitation from the new Organization.\nThe option describes the correct method.\nPlanning the migration process:\nIf you have only a few accounts to migrate, you can use the Organizations console.\nRemove the member account from the old Organization.\nSend an invite to the member account from the new Organization.\nAccept the invite to the new Organization from the member account.\nReference:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/\n\nTo move a member account from one AWS Organization to another, you can follow the below steps:\nLog in to the AWS Management Console using the credentials of the AWS Organizations management account. Navigate to the AWS Organizations console and select the source Organization from which you want to move the member account. In the left navigation pane, click on \"Accounts.\" Select the account you want to move to a new Organization. Choose \"Move account\" from the \"Actions\" dropdown menu. In the \"Move account\" dialog box, select the destination AWS Organization that you want to move the account to. Review the information and ensure that the correct account is selected for the move. Click \"Move account.\"\nAWS Organizations will initiate the account move process, and it may take some time to complete the move. The status of the move will be displayed in the AWS Organizations console.\nOption A is not a valid approach to move a member account from one Organization to another. Drag and drop is only available to move accounts within the same Organization.\nOption B is not a valid approach because there is no option available to \"migrate\" an account to a destination AWS Organization.\nOption C is not a valid approach because deleting an old AWS Organization is not required to move a member account to a new Organization.\nOption D is not a complete solution, as sending an invite from the new Organization and accepting the invite is only one step in the process. The account must first be removed from the old Organization before accepting the new invite.\n\n"
}, {
  "id" : 215,
  "question" : "While managing permissions for the API Gateway, what could be used to ensure that the right level of permissions is given to Developers, IT Admins, and users? Also, the permissions should be easily managed.\n",
  "answers" : [ {
    "id" : "731e652ef63f4bddbc5eba7394219d28",
    "option" : "Use the secure token service to manage the permissions for different users.",
    "isCorrect" : "false"
  }, {
    "id" : "f7ae8680f6f64bcab50fad7f7524d93f",
    "option" : "Use IAM Policies to create different policies for different types of users.",
    "isCorrect" : "true"
  }, {
    "id" : "21ae075ec68345908c526806bd71fd04",
    "option" : "Use the AWS Config tool to manage the permissions for different users.",
    "isCorrect" : "false"
  }, {
    "id" : "050c961e67494b07a31c59904adff4d6",
    "option" : "Use IAM Access Keys to create sets of keys for different types of users.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following:\nYou control access to Amazon API Gateway with IAM permissions by controlling access to the following two API Gateway component processes.\nTo create, deploy, and manage an API in API Gateway, you must grant the API developer permissions to perform the required actions supported by the API management component of API Gateway.\nTo call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway.\nFor more information on permissions with the API Gateway, please visit the following URL:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html\n\nWhen managing permissions for the API Gateway, the best approach is to use AWS Identity and Access Management (IAM) policies. IAM policies are used to grant permissions to AWS services and resources, including API Gateway.\nIAM policies provide fine-grained control over permissions by allowing you to define permissions for specific API Gateway resources, HTTP methods, and stages. This makes it easy to grant the appropriate level of permissions to developers, IT admins, and users.\nTo create IAM policies, you can use the IAM console, CLI, or SDKs. You can also use the policy generator in the IAM console to create policies without having to write the JSON syntax.\nAdditionally, IAM policies can be easily managed and modified. You can attach, detach, and delete policies from IAM roles, users, and groups at any time.\nSecure token service (STS) is a service that provides temporary security credentials for users to access AWS resources. While STS can be used to manage permissions, it is not the best option for managing permissions for the API Gateway.\nAWS Config is a service that provides a detailed inventory of AWS resources and configuration history. It is not used to manage permissions for API Gateway.\nIAM access keys are used to authenticate AWS API requests made by users or applications. Access keys are not used to manage permissions for API Gateway.\nTherefore, option B is the correct answer.\n\n"
}, {
  "id" : 216,
  "question" : "Your Development team wants to use EC2 Instances to host their Application and Web servers.\nIn the automation space, they want the Instances always to download the latest version of the Web and Application servers when launched.\nAs an architect, what would you recommend for this scenario?\n",
  "answers" : [ {
    "id" : "f7b8986098f843d7a8d2981fa6a67d7f",
    "option" : "Ask the Development team to create scripts which can be added to the User Data section when the instance is launched.",
    "isCorrect" : "true"
  }, {
    "id" : "385301cc1af440429e7a4231af17c33a",
    "option" : "Ask the Development team to create scripts which can be added to the Meta Data section when the instance is launched.",
    "isCorrect" : "false"
  }, {
    "id" : "ba862ce2549541e3922fd7be05c275a6",
    "option" : "Use Auto Scaling Groups to install the Web and Application servers when the instances are launched.",
    "isCorrect" : "false"
  }, {
    "id" : "d5022312d922461bb1ab2932db40ffce",
    "option" : "Use EC2 Config to install the Web and Application servers when the instances are launched.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following:\nWhen you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common automated configuration tasks and even run scripts after the instance starts.\nYou can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.\nYou can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the command line tools) or as base64-encoded text (for API calls).\nFor more information on User Data, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\n\nTo ensure that EC2 instances always download the latest version of web and application servers when launched, there are different options available. Let's review the options provided in the exam question:\nA. Ask the Development team to create scripts which can be added to the User Data section when the instance is launched:\nThis option allows the Development team to provide scripts that can be executed when the instance is launched. The user data can be passed as a string when launching the instance, and EC2 will automatically execute it. The scripts can be used to download and install the latest version of the web and application servers.\nB. Ask the Development team to create scripts which can be added to the Meta Data section when the instance is launched:\nThe metadata section provides information about the instance, such as the instance ID, instance type, and network interfaces. It is not used to execute scripts, so this option is not suitable for downloading and installing web and application servers.\nC. Use Auto Scaling Groups to install the Web and Application servers when the instances are launched:\nAuto Scaling Groups (ASG) can be used to automatically launch instances and scale the number of instances based on demand. An ASG can be configured to use a launch configuration or launch template, which includes user data. The user data can be used to execute scripts that download and install the latest version of the web and application servers when the instance is launched.\nD. Use EC2 Config to install the Web and Application servers when the instances are launched:\nEC2 Config is an agent that can be installed on an EC2 instance to automatically configure the instance after launch. It can be used to execute scripts and run commands on the instance, but it is not designed to download and install software.\nBased on the options provided, the recommended solution is A: Ask the Development team to create scripts which can be added to the User Data section when the instance is launched. This option provides the Development team with the flexibility to create and manage the scripts that are executed when the instance is launched. It also allows the team to easily update the scripts to download and install the latest version of the web and application servers.\n\n"
}, {
  "id" : 217,
  "question" : "Your company has an application that takes care of uploading, processing, and publishing videos posted by users.\nThe current architecture for this application includes the following: a) A set of EC2 Instances to transfer user-uploaded videos to S3 buckets. b) A set of EC2 worker processes to process and publish the videos. c) An Auto Scaling Group for the EC2 worker processes. Which of the following AWS Services can be added to the architecture to make it more integrated and decoupled?\n",
  "answers" : [ {
    "id" : "2ed8dcd2fa73483c95066fdcc6b09880",
    "option" : "Amazon SQS",
    "isCorrect" : "true"
  }, {
    "id" : "fa4e2a7ea85d4b60af6a4cd2223effe1",
    "option" : "Amazon SNS",
    "isCorrect" : "false"
  }, {
    "id" : "50607b4af136471f8c07eb644fc0c03a",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "512459f07c0441fca03c76a750fe0f24",
    "option" : "Amazon SES.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon SQS is used to decouple systems.\nIt can store requests to process videos, to be picked up by the worker processes.\nAWS Documentation mentions the following:\nAmazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable, hosted queue for storing messages as they travel between applications or microservices.\nIt moves data between distributed application components and helps you decouple these components.\nOption B is incorrect because it is not used for decoupling.\nOption C is incorrect because it does not use EC2 worker processes to process anything.\nIt does not relate to the requirements.\nOption D is incorrect because it is an email service.\nFor more information on AWS SQS, please visit the following URL:\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/Welcome.html\nhttps://aws.amazon.com/blogs/compute/running-cost-effective-queue-workers-with-amazon-sqs-and-amazon-ec2-spot-instances/\n\nTo make the architecture more integrated and decoupled, the following AWS services can be added:\nAmazon Simple Queue Service (SQS) SQS is a fully-managed message queuing service that enables decoupling and scalability of microservices, distributed systems, and serverless applications. By using SQS, messages can be sent between different components of the application, allowing for asynchronous communication between services. In this case, SQS can be used to transfer messages between the EC2 instances and the worker processes. When a user uploads a video, the EC2 instances can send a message to SQS, which can then be processed by the worker processes. SQS ensures that messages are delivered once and in the order in which they are sent. Amazon Simple Notification Service (SNS) SNS is a fully-managed pub/sub messaging service that enables the distribution of messages to multiple subscribers. It can be used to send notifications or alerts to multiple endpoints, such as email, SMS, or HTTP/HTTPS. In this case, SNS can be used to send notifications to subscribers when videos are processed and published. For example, users can receive an email or SMS notification when their video is ready to be viewed. Amazon CloudFront CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. It can be used to accelerate the delivery of video content to users. In this case, CloudFront can be used to distribute the videos to users, reducing the load on the EC2 instances and improving the performance of the application. Amazon Simple Email Service (SES) SES is a flexible and scalable email service that enables the sending and receiving of email using a simple API. It can be used to send notifications, alerts, or marketing emails to users. In this case, SES can be used to send email notifications to users when their video is processed and published, or to send marketing emails promoting new features or content on the application.\nTherefore, in this scenario, the most suitable AWS services to make the architecture more integrated and decoupled are Amazon Simple Queue Service (SQS) and Amazon Simple Notification Service (SNS).\n\n"
}, {
  "id" : 218,
  "question" : "There is an urgent requirement to monitor some database metrics for a database hosted on AWS and send notifications.\nWhich AWS services can accomplish this? (Select TWO.)\n",
  "answers" : [ {
    "id" : "cbf7674eb88143cb8380c2a66e7a3c96",
    "option" : "Amazon Simple Email Service",
    "isCorrect" : "false"
  }, {
    "id" : "38498802528d40acb49c93a5648ce3f3",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "true"
  }, {
    "id" : "265798cf29514742ab8661814059843b",
    "option" : "Amazon Simple Queue Service",
    "isCorrect" : "false"
  }, {
    "id" : "23191aba776c4d768f591368f1f8c511",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "7c694db67bb0437c973046f00ab08b0e",
    "option" : "Amazon Simple Notification Service.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - B and E.\nAmazon CloudWatch will be used to monitor the IOPS metrics from the RDS Instance.\nAmazon Simple Notification Service will be used to send the notification if an alarm is triggered.\nFor more information on CloudWatch and SNS, please visit the URLs below:\nhttps://aws.amazon.com/cloudwatch/\nhttps://aws.amazon.com/sns/\n\nThe correct answers for this question are B. Amazon CloudWatch and E. Amazon Simple Notification Service (SNS).\nHere's a detailed explanation for each option:\nA. Amazon Simple Email Service (SES): Amazon SES is an email service that allows you to send and receive emails using your own email addresses and domains. It is not directly related to database monitoring and notifications.\nB. Amazon CloudWatch: Amazon CloudWatch is a monitoring service that allows you to collect and track metrics, collect and monitor log files, and set alarms. With CloudWatch, you can monitor your AWS resources, including Amazon RDS databases, and set up alarms to notify you of specific events. You can also use CloudWatch to monitor custom metrics and integrate with other AWS services for more detailed monitoring.\nC. Amazon Simple Queue Service (SQS): Amazon SQS is a managed message queue service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It is not directly related to database monitoring and notifications.\nD. Amazon Route 53: Amazon Route 53 is a highly available and scalable DNS service that allows you to route internet traffic to your AWS resources. It is not directly related to database monitoring and notifications.\nE. Amazon Simple Notification Service (SNS): Amazon SNS is a messaging service that allows you to send notifications and messages to multiple recipients or subscribers. With SNS, you can create topics and publish messages to those topics, and subscribers can receive notifications through various supported protocols, such as email, SMS, HTTP, and more. You can use SNS to send notifications for specific events or alarms triggered by CloudWatch.\nTherefore, B. Amazon CloudWatch and E. Amazon Simple Notification Service (SNS) are the AWS services that can accomplish database monitoring and notifications.\n\n"
}, {
  "id" : 219,
  "question" : "You have a business-critical two-tier web application, currently deployed in 2 Availability Zones in the US East region, using Elastic Load Balancing and Auto Scaling.\nThe app depends on synchronous replication at the database layer.\nThe application needs to remain fully available even if one application AZ suddenly goes offline and Auto Scaling cannot launch new instances in the remaining AZ.\nHow could the current Elastic Load Balancing be enhanced to ensure this?\n",
  "answers" : [ {
    "id" : "5c9b839415de4a33a4f580e85ab290e5",
    "option" : "Deploy in 2 regions using Weighted Round Robin with Auto Scaling set at a minimum 50% peak load per region.",
    "isCorrect" : "false"
  }, {
    "id" : "c1659f436d6c4ef191f0726392280ca8",
    "option" : "Deploy in 3 AZs with Auto Scaling, set to handle a minimum of 33 percent peak load per zone.",
    "isCorrect" : "false"
  }, {
    "id" : "07f0dcb639124cb19eb2360e886e757a",
    "option" : "Deploy in 3 AZs with Auto Scaling, set to handle a minimum 50 percent peak load per zone.",
    "isCorrect" : "true"
  }, {
    "id" : "f9f2cbe6374b479c9d84d263d414ea71",
    "option" : "Deploy in 2 regions using Weighted Round Robin with Auto Scaling, set at minimum 100% peak load per region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nSince the requirement states that the application should never go down even if an AZ is not available, we need to maintain 100% availability.\nOptions A and D are incorrect because region deployment is not possible for ELB.\nELBs can manage traffic within a region, not between regions.\nOption B is incorrect because when one AZ goes down, we would be operating at only 66% and not the required 100%.\nNOTE:\nIn the question, it is clearly mentioned that \"The application needs to remain fully available even if one application AZ goes offline and if Auto Scaling cannot launch new instances in the remaining AZ.\"\nHere you need to maintain 100% availability.\nIn option B, when you create 3 AZs with a minimum 33% load on each, if any failure occurs in one AZ, then\n33% + 33% = 66% .\nHere you can handle only 66% and the remaining 34% of the load is not handled.\nBut when you select option C, when you create 3 AZs with a minimum 50% load on each, if any failure occurs in one AZ, then\n50% + 50% =100% .\nHere you can handle a full load, i.e., 100%.\nReference:\nhttps://aws.amazon.com/autoscaling/\nhttps://aws.amazon.com/about-aws/global-infrastructure/regions_az/\n\nThe correct answer is B. Deploy in 3 AZs with Auto Scaling, set to handle a minimum of 33 percent peak load per zone.\nExplanation: The current setup has the web application deployed in 2 Availability Zones in the US East region using Elastic Load Balancing and Auto Scaling. The application also depends on synchronous replication at the database layer.\nThe requirement is to ensure that the application remains fully available even if one application AZ suddenly goes offline, and Auto Scaling cannot launch new instances in the remaining AZ.\nTo achieve high availability, the application needs to be deployed in at least 3 AZs to ensure that there is redundancy in case one AZ goes down.\nOption A is incorrect because deploying in 2 regions using Weighted Round Robin with Auto Scaling set at a minimum 50% peak load per region does not guarantee high availability since an entire region can still go down.\nOption C is incorrect because it recommends setting Auto Scaling to handle a minimum of 50% peak load per zone, which means that if an AZ goes down, the remaining AZs may not be able to handle the increased load, leading to service disruption.\nOption D is incorrect because it recommends setting Auto Scaling to handle a minimum of 100% peak load per region, which does not provide redundancy in case one region goes down.\nOption B is the correct answer because it recommends deploying the application in 3 AZs with Auto Scaling, set to handle a minimum of 33% peak load per zone. This ensures redundancy in case one AZ goes down, and Auto Scaling can handle the increased load on the remaining AZs.\nTherefore, option B is the best solution for ensuring high availability of the application.\n\n"
}, {
  "id" : 220,
  "question" : "You have been asked to create a VPC network topology for your company.\nThe VPC network must support both internet-facing and intranet applications, accessed only over VPN.\nBoth internet-facing and intranet applications must be able to leverage at least 3 AZs each for high availability.\nHow many subnets must you create within your VPC to accommodate these requirements?\n",
  "answers" : [ {
    "id" : "e6be6a44c0d34e60a43a502cfb072703",
    "option" : "2",
    "isCorrect" : "false"
  }, {
    "id" : "5e95542f31c54c278d91427d9ec32e39",
    "option" : "3",
    "isCorrect" : "false"
  }, {
    "id" : "47e960de0bb040c4ba27543bee2273df",
    "option" : "4",
    "isCorrect" : "false"
  }, {
    "id" : "6b38da571ceb4e0a8a6bbc94228a07b1",
    "option" : "6",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nSince each subnet corresponds to one Availability Zone and you need 3 AZs for both the internet and intranet applications, you will need 6 subnets.\nFor internet-facing and intranet applications, they should be put at different subnets.\nFor more information on VPC and subnets, please visit the below URL:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html\n\nTo accommodate both internet-facing and intranet applications and support high availability, the VPC network should have multiple subnets distributed across multiple Availability Zones (AZs).\nTo calculate the number of subnets required, we need to consider the following points:\nBoth internet-facing and intranet applications are accessed only over VPN. Each of the internet-facing and intranet applications should be able to leverage at least 3 AZs for high availability.\nTo support both types of applications, we need to create subnets for each type of application separately. Each subnet should span across at least 3 AZs for high availability.\nConsidering the above points, we need to create:\n3 subnets for the internet-facing application (1 subnet per AZ) 3 subnets for the intranet application (1 subnet per AZ)\nTherefore, we need a total of 6 subnets for the VPC network topology to accommodate the given requirements.\nSo the correct answer is (D) 6.\n\n"
}, {
  "id" : 221,
  "question" : "You have the following architecture deployed in AWS. a) A set of EC2 Instances which sit behind an ELB b) A database hosted in Amazon RDS Of late, the performance on the database has been slacking due to a high number of read requests.\nWhich of the following can be added to the architecture to alleviate the performance issue? (Select TWO.)\n",
  "answers" : [ {
    "id" : "090db11e8fbc4335adacd275f56b615a",
    "option" : "Add read replica to the primary database to offload read traffic.",
    "isCorrect" : "true"
  }, {
    "id" : "000794561fdd43e990fffb800ea7f63c",
    "option" : "Use ElastiCache in front of the database.",
    "isCorrect" : "true"
  }, {
    "id" : "3bf556dd75a342de80f22042437cb2d2",
    "option" : "Use AWS CloudFront in front of the database.",
    "isCorrect" : "false"
  }, {
    "id" : "dbb15cada3ff43dfa5e50e4d9ae05514",
    "option" : "Use Amazon DynamoDB to offload all the reads. Populate the common read items in a separate table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and B.\nOption A is correct.\nAWS says, \"Amazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances.\nThis feature makes it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\nYou can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput\"\nAmazon ElastiCache is an in-memory cache that can be used to cache common read requests.\nThe below diagram shows how caching can be added to an existing architecture:\nFor more information on database caching, please visit the URL below:\nhttps://aws.amazon.com/caching/database-caching/\nNote:\nOption C is incorrect because CloudFront is a valuable component of scaling a website, especially for geo-location workloads and queries, more advanced for the given architecture.\nOption D is incorrect because it will have latency and additional changes as well.\n\n\nThe scenario describes an architecture in which a set of EC2 instances are sitting behind an Elastic Load Balancer (ELB), while a database is hosted in Amazon RDS. The database is experiencing performance issues due to a high volume of read requests. To address this issue, two possible solutions can be implemented:\nAdd read replica to the primary database to offload read traffic: A read replica is a copy of a primary database instance that is kept in sync with the primary database. The read replica can be used to offload read traffic from the primary database, as read requests can be directed to the read replica instead of the primary database. This can help to reduce the load on the primary database and improve its performance. Use ElastiCache in front of the database: Amazon ElastiCache is a managed in-memory data store that can be used to improve the performance of read-heavy applications. By caching frequently accessed data in memory, ElastiCache can help to reduce the number of read requests that need to be sent to the database. This can help to improve the performance of the database by reducing the load on it.\nOption C, using AWS CloudFront in front of the database, is not a recommended solution for improving database performance as CloudFront is a content delivery network (CDN) that is used to improve the delivery of static content, such as images, videos, and other files, to end-users. CloudFront is not designed to be used as a caching layer for a database.\nOption D, using Amazon DynamoDB to offload all the reads and populating the common read items in a separate table, is not a recommended solution in this scenario. While DynamoDB can be used as a high-performance NoSQL database that can handle large volumes of read and write requests, it is not an appropriate solution for this scenario. DynamoDB is a separate database service that requires data to be migrated from the existing RDS database to DynamoDB, which can be time-consuming and complex.\nTherefore, the two recommended solutions for improving database performance in this scenario are to add read replicas to the primary database and to use ElastiCache in front of the database.\n\n"
}, {
  "id" : 222,
  "question" : "You have two AWS Organizations.\nAll the AWS accounts in Organization A need to be moved to Organization B.\nYou have already moved all the member accounts, and now you need to migrate the master account.\nWhich of the following options should you choose?\n",
  "answers" : [ {
    "id" : "099393c8d41e4e29aa31c7b6ff895c17",
    "option" : "Delete Organization A and invite the master account to join Organization",
    "isCorrect" : "true"
  }, {
    "id" : "31a23590a4df42569f2e0222edc72c16",
    "option" : "Remove the master account from Organization A and send an invitation to the account to join Organization",
    "isCorrect" : "false"
  }, {
    "id" : "cc734723dd4349879a3bbfa9c6d28170",
    "option" : "Send an invitation to the master account. Accept the invitation to move the account from Organization A to Organization",
    "isCorrect" : "false"
  }, {
    "id" : "d97af9769ae0408994628784af488b9e",
    "option" : "The master account in one Organization cannot join another one.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThe reference can be found in this link-https://aws.amazon.com/premiumsupport/knowledge-center/organizations-move-accounts/.\nOption A is CORRECT: For the master account, the Organization needs to be deleted before the account accepts an invitation to join another one.\nOption B is incorrect: Because the master account cannot be removed from an AWS Organization.\nOption C is incorrect: Because the master account in one AWS Organization cannot accept an invitation to join another Organization.\nOption D is incorrect: Check the above explanations in option A.\n\nOrganizations in AWS allow you to centrally manage and govern multiple AWS accounts as a single entity. AWS Organizations helps you to simplify the management of multiple AWS accounts, including billing and access control. It also provides a consolidated view of AWS resources across multiple accounts.\nTo migrate all the member accounts from Organization A to Organization B, you need to move the master account to Organization B. The master account is the account that created the organization and has full administrative control over all member accounts in the organization.\nTo move the master account, you need to follow the below steps:\nOption A - Delete Organization A and invite the master account to join Organization B: This option is not recommended as it will delete the entire Organization A, which may result in the loss of data, configuration, and other settings. It also requires you to create a new Organization, which can be time-consuming and may require significant effort to recreate the entire organization from scratch.\nOption B - Remove the master account from Organization A and send an invitation to the account to join Organization B: This option is also not recommended as it will require you to remove the master account from Organization A, which will result in the loss of administrative control over the member accounts in that organization. It also requires you to send an invitation to the account to join Organization B, which may result in a delay in migrating the account.\nOption C - Send an invitation to the master account. Accept the invitation to move the account from Organization A to Organization B: This is the recommended option to migrate the master account from Organization A to Organization B. This option allows you to move the master account to Organization B while retaining full administrative control over the member accounts in both organizations. You can send an invitation to the master account from the AWS Organizations console or the AWS CLI. After the invitation is accepted, the account will be moved to Organization B, and you can manage both organizations from a single location.\nOption D - The master account in one Organization cannot join another one: This option is incorrect as it is not possible to have a single master account in multiple organizations. However, you can move the master account from one organization to another as described in option C.\nIn summary, the recommended option to migrate the master account from Organization A to Organization B is to send an invitation to the master account and accept the invitation to move the account from Organization A to Organization B.\n\n"
}, {
  "id" : 223,
  "question" : "A customer wants to import the existing virtual machines to the cloud.\nWhich service should they use for this purpose?\n",
  "answers" : [ {
    "id" : "771d760df7394d9c9a479f3b571c7b39",
    "option" : "VM Import/Export",
    "isCorrect" : "true"
  }, {
    "id" : "5da6c4c64afa4f5ebc715d42cbe46f73",
    "option" : "AWS Import/Export",
    "isCorrect" : "false"
  }, {
    "id" : "020f887156db45d9813fdadf53f6d2e8",
    "option" : "AWS Storage Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "ba0bf936030b454c911c6a4ca4004267",
    "option" : "DB Migration Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nVM Import/Export enables customers to import Virtual Machine (VM) images to create Amazon EC2 instances.\nCustomers can also export previously imported EC2 instances to create VMs.\nCustomers can use VM Import/Export to leverage their previous investments in building VMs by migrating their VMs to Amazon EC2.\nFor more information on AWS VM Import, please visit the URL below:\nhttps://aws.amazon.com/ec2/vm-import/\nFew strategies used for migration are:\n1\nForklift migration strategy.\n2\nHybrid migration strategy.\n3\nCreating AMIs.\nAWS Import/Export -It is a data transport service used to move large amounts of data in and out of the Amazon Web Services public cloud using portable storage devices for transport.\nhttps://aws.amazon.com/about-aws/whats-new/2009/05/20/AWS-Import-Export/\nAWS Storage Gateway -It connects an on-premises software appliance with cloud-based storage to provide seamless integration with data security features between your on-premises IT environment and the AWS storage infrastructure.\nThe gateway provides access to objects in S3 as files or file share mount points.\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html.\nDB Migration Service - It can migrate your data to and from the most widely used commercial and open-source databases.\nIt supports homogeneous migrations such as Oracle to Oracle, as well as heterogeneous migrations between different database platforms, such as Oracle to Amazon Aurora.\nFor more information, please check the URL below:\nhttps://aws.amazon.com/dms/\n\nThe correct answer for this question is A. VM Import/Export.\nVM Import/Export is a service offered by Amazon Web Services (AWS) that allows customers to import existing virtual machine images into AWS, and export them back to their on-premises environments. This service supports a wide range of virtualization platforms, including VMware vSphere, Microsoft Hyper-V, and Citrix Xen.\nVM Import/Export enables customers to import virtual machines as Amazon Machine Images (AMIs), which can be used to launch new instances in Amazon Elastic Compute Cloud (Amazon EC2). Customers can also export their existing Amazon EC2 instances as AMIs, which can be used to launch instances in their on-premises environments.\nAWS Import/Export, on the other hand, is a discontinued service that allowed customers to physically transfer large amounts of data between their on-premises environments and AWS using portable storage devices, such as hard drives and USB drives.\nAWS Storage Gateway is a hybrid cloud storage service that enables customers to securely store data in AWS for scalable and cost-effective storage solutions. This service supports a wide range of storage protocols, including NFS, SMB, and iSCSI.\nDB Migration Service is a service that enables customers to migrate their on-premises databases to AWS with minimal downtime. This service supports a wide range of database platforms, including Oracle, SQL Server, MySQL, PostgreSQL, and Amazon Aurora.\n\n"
}, {
  "id" : 224,
  "question" : "A company website is set to launch in the upcoming weeks.\nThere is a probability that the traffic will be quite high during the initial weeks.\nHow is it possible to set up DNS failover to a static website in the event of a load failure?\n",
  "answers" : [ {
    "id" : "0ddb19e472ec4e048c8588379722d712",
    "option" : "Duplicate the exact application architecture in another region and configure DNS Weight-based routing.",
    "isCorrect" : "false"
  }, {
    "id" : "17f61d76a493419ebc4fb1a20285a5e5",
    "option" : "Enable failover to an on-premises data center to the application hosted there.",
    "isCorrect" : "false"
  }, {
    "id" : "459d1c7033b54b5f9081d14094572156",
    "option" : "Use Route 53 with the failover option, to failover to a static S3 website bucket or CloudFront distribution.",
    "isCorrect" : "true"
  }, {
    "id" : "79469b8fa8514661aa32a2b331c1ae24",
    "option" : "Add more servers in case the application fails.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon Route 53 health checks monitor the health and performance of your web applications, web servers, and other resources.\nIf you have multiple resources that perform the same function, you can configure DNS failover so that Amazon Route 53 will route your traffic from an unhealthy resource to a healthy resource.\nFor example, if you have two web servers and one web server becomes unhealthy, Amazon Route 53 can route traffic to the other web server.\nSo you can route traffic to a website hosted on S3 or to a CloudFront distribution.\nFor more information on DNS failover using Route 53, please refer to the link below:\nhttp://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html\n\nSure, I'd be happy to provide a detailed explanation for each answer choice.\nA. Duplicate the exact application architecture in another region and configure DNS Weight-based routing.\nThis answer choice is suggesting that you duplicate your application in another region and use DNS Weight-based routing to direct traffic to both regions. Weight-based routing allows you to split the traffic between regions in a specific ratio. For example, you could direct 80% of your traffic to your primary region and 20% to your secondary region. If the primary region becomes overloaded, the traffic will automatically failover to the secondary region.\nThis approach requires you to duplicate your entire application architecture in another region, which can be expensive and time-consuming. Additionally, this solution may not be the best fit for a static website as it may not require such a complex setup.\nB. Enable failover to an on-premises data center to the application hosted there.\nThis answer choice suggests that you can host your website in both AWS and an on-premises data center, and failover to the on-premises data center if AWS fails. While this approach can provide additional resilience, it adds complexity to the architecture and may not be necessary for a static website.\nC. Use Route 53 with the failover option, to failover to a static S3 website bucket or CloudFront distribution.\nThis answer choice is the most appropriate option for a static website hosted on AWS. Route 53, AWS's DNS service, offers a failover option that can automatically route traffic to a different resource when the primary resource is unavailable. In this case, you can configure Route 53 to failover to a static S3 website bucket or CloudFront distribution if the primary resource becomes unavailable. This is a simple and cost-effective solution that requires minimal setup.\nD. Add more servers in case the application fails.\nThis answer choice is not the best solution for a static website. Adding more servers may help to handle additional traffic, but it does not address the issue of a load failure or provide automatic failover. Additionally, adding more servers can be expensive and may not be necessary for a static website.\nIn summary, the best answer choice for setting up DNS failover to a static website in the event of a load failure is to use Route 53 with the failover option, to failover to a static S3 website bucket or CloudFront distribution.\n\n"
}, {
  "id" : 225,
  "question" : "A company is running web server reserved EC2 Instances with EBS-backed root volumes.\nThese instances have a consistent CPU load of 80%\nTraffic is being distributed to these instances by an Elastic Load Balancer.\nThey also have Multi-AZ RDS MySQL databases in both development and production environments.\nWhat recommendation would you make to reduce cost without affecting the availability of mission-critical systems? Choose the correct answer from the options given below.\n",
  "answers" : [ {
    "id" : "dfea8c15a05f435589522e3c210a9458",
    "option" : "Consider using On-demand instances instead of Reserved EC2 instances.",
    "isCorrect" : "false"
  }, {
    "id" : "6fcd03e6f7494d44b6f49789ed569932",
    "option" : "Consider not using the Multi-AZ RDS deployment for the database in the development environment.",
    "isCorrect" : "true"
  }, {
    "id" : "5f9b94f5a6d8486397c3cca19944d2df",
    "option" : "Consider using Spot instances instead of Reserved EC2 instances.",
    "isCorrect" : "false"
  }, {
    "id" : "3d672766b96c47e5917990ca97809f01",
    "option" : "Consider removing the Elastic Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nMulti-AZ databases are more suitable for production environments than for development environments.\nSo you can reduce costs by not using these for development environments.\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads.\nWhen you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ)\nEach AZ runs on its own physically distinct, independent infrastructure and is engineered to be highly reliable.\nIn case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora)\nThus you can resume database operations as soon as the failover is complete.\nSince the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.\nFor more information on Multi-AZ RDS, please refer to the link below:\nhttps://aws.amazon.com/rds/details/multi-az/\nNote:\nMission Critical system refers to production Instances and Databases.\nHowever, if you notice, they have Multi-AZ RDS in the Development environment, which is unnecessary.\nBecause management is always concerned about production, the environment should be perfect.\nTo reduce the cost, we can disable the Multi-AZ RDS for the Development environment and keep it only for the Production environment.\n\nGiven the scenario, where the company is running web server reserved EC2 instances with EBS-backed root volumes, consistently using 80% of the CPU with traffic being distributed by Elastic Load Balancer, and also has Multi-AZ RDS MySQL databases in both development and production environments, the question asks for the recommendation that can reduce costs without affecting the availability of mission-critical systems.\nOption A: Consider using On-demand instances instead of Reserved EC2 instances.\nThis option suggests using on-demand instances instead of reserved instances. On-demand instances are instances with no upfront payment or long-term commitment, which means the user pays for what they use with no upfront fees. Although this option is cost-effective in the short term, it is not suitable for long-term use as it is more expensive than reserved instances in the long run.\nOption B: Consider not using the Multi-AZ RDS deployment for the database in the development environment.\nThis option suggests not using Multi-AZ deployment for the database in the development environment. Multi-AZ deployment provides high availability for the database by replicating data to a standby instance in another Availability Zone (AZ). However, since the environment in question is a development environment, it might be okay not to use Multi-AZ deployment as the environment is not mission-critical.\nOption C: Consider using Spot instances instead of Reserved EC2 instances.\nThis option suggests using spot instances instead of reserved instances. Spot instances are spare EC2 capacity that Amazon can offer at a lower price than on-demand instances. They can be terminated at any time as the demand for EC2 capacity increases. However, they are suitable for workloads that can tolerate interruptions, and they might not be suitable for mission-critical systems.\nOption D: Consider removing the Elastic Load Balancer.\nThis option suggests removing the Elastic Load Balancer. However, this would not reduce costs and would instead increase downtime and decrease availability for the web servers. The Elastic Load Balancer helps distribute traffic to the web servers, and without it, the web servers would not be able to handle the traffic effectively.\nConsidering the above options, the recommended solution that would reduce costs without affecting the availability of mission-critical systems is Option B: Consider not using the Multi-AZ RDS deployment for the database in the development environment. This option would not affect the production environment's Multi-AZ deployment, which is essential for high availability, while still saving costs in the development environment.\n\n"
}, {
  "id" : 226,
  "question" : "To manage a large number of AWS accounts in a better way, you create a new AWS Organization and invite multiple accounts.\nYou only enable the â€œConsolidated billingâ€ feature set in the organization.\nWhich of the following signifies Consolidated billing in the AWS Organization?\n",
  "answers" : [ {
    "id" : "52e6607eb8194e8a92a0f85d88ad00af",
    "option" : "Apply SCPs to restrict the services that IAM users can access.",
    "isCorrect" : "false"
  }, {
    "id" : "21abc75840ba4eceae4fcf988feacafb",
    "option" : "Configure tag policies to maintain consistent tags for resources in the organization`s accounts.",
    "isCorrect" : "false"
  }, {
    "id" : "8d2c0ba1affc4c519425679d115bfd13",
    "option" : "Configure a policy to prevent IAM users in the organization from disabling AWS CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "f51828ccb19b49b091d5255e8abfc61e",
    "option" : "Combine the usage across all accounts to share the volume pricing discounts.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nFor the differences between â€œConsolidated billingâ€ and â€œAll featuresâ€, refer to the reference in https://docs.aws.amazon.com/organizations/latest/userguide/orgs_getting-started_concepts.html#feature-set-cb-only.\nOption A is incorrect: Because SCP is part of the advanced features which belong to â€œAll featuresâ€.\nOption B is incorrect: Because tag policies can be applied under the feature set of â€œAll featuresâ€.\nOption C is incorrect: This is implemented using SCP which is not supported in â€œConsolidated billingâ€.\nOption D is CORRECT: This is supported in â€œConsolidated billingâ€ according to.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html.\n\nThe correct answer is D. Combine the usage across all accounts to share the volume pricing discounts.\nWhen you create an AWS Organization and enable the Consolidated billing feature set, you can consolidate payment for multiple AWS accounts within your organization by designating a single account to pay for all of the other accounts.\nThis consolidated billing feature provides a way to consolidate billing information across multiple AWS accounts, making it easier to track and analyze costs. By combining the usage across all accounts, you can benefit from the volume pricing discounts. For example, if you have multiple AWS accounts, each with its own S3 bucket, by using consolidated billing, you can take advantage of the pricing tiers that are based on the combined usage of all of the S3 buckets.\nThe other options listed in the question are not related to consolidated billing.\nOption A, Apply SCPs to restrict the services that IAM users can access, refers to Service Control Policies (SCPs), which are used to control which AWS services and features can be accessed by IAM users and roles within an AWS account.\nOption B, Configure tag policies to maintain consistent tags for resources in the organization's accounts, refers to the Tagging feature, which allows you to assign metadata to AWS resources in the form of tags. A tag policy helps you maintain consistency in the way tags are applied across AWS accounts in your organization.\nOption C, Configure a policy to prevent IAM users in the organization from disabling AWS CloudTrail, refers to AWS CloudTrail, which is a service that provides a record of actions taken by a user, role, or an AWS service in your account. This option is not related to consolidated billing.\n\n"
}, {
  "id" : 227,
  "question" : "A Solutions Architect has been asked to design a solution that will deliver digital content to users through Amazon CloudFront.\nYou have set up an Amazon S3 bucket as the origin and by default, CloudFront never exposes Amazon S3 URLs.\nThe contents should only be accessed through the CloudFront distribution.\nHow can this be achieved?\n",
  "answers" : [ {
    "id" : "9c768bdf477249e1af281ab06564cedf",
    "option" : "Store the digital contents in the S3 bucket and create signed URLs to access S3 through CloudFront.",
    "isCorrect" : "false"
  }, {
    "id" : "86d0c3025f70423bbf9ba8edfc572043",
    "option" : "Create OAI in CloudFront. Use the S3 bucket policy to ensure that only the OAI can access the files in the Amazon S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "e38e9b21fd344ef99bc25265a7ffa260",
    "option" : "Store the digital contents as private objects in the S3 buckets and use the S3 ACL to ensure that only the CloudFront distribution ARN can access the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "2e33d2db00424fd0ab54b252e6dcc855",
    "option" : "Store the digital contents in the S3 bucket and configure signed cookies for users to access contents through CloudFront.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWhen you first set up an Amazon S3 bucket as the origin for a CloudFront distribution, you grant everyone permission to read the files in your bucket which allows anyone to access your files either through CloudFront or using the Amazon S3 URL.\nCloudFront doesn't expose Amazon S3 URLs, but your users might have those URLs if your application serves any files directly from Amazon S3 or if anyone gives out direct links to specific files in Amazon S3.\nIf you use CloudFront signed URLs or signed cookies to restrict access to files in your Amazon S3 bucket, you probably also want to prevent users from accessing your Amazon S3 files by using Amazon S3 URLs.\nIf users access your files directly in Amazon S3, they bypass the controls provided by CloudFront signed URLs or signed cookies.\nOption A is incorrect because signed URLs cannot ensure that the contents can only be accessed through CloudFront.\nOption B is CORRECT as it ensures that the contents are only delivered through the CloudFront distribution.\nOption C is incorrect because OAI should be created and configured in the bucket policy.\nThis option is invalid to control the access for the S3 bucket.\nOption D is incorrect because signed cookies cannot ensure that the contents are only accessed through CloudFront.\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html#private-content-restricting-access-to-s3-overview\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\n\nSure! Let me explain each of the options and their respective solutions.\nOption A: Store the digital contents in the S3 bucket and create signed URLs to access S3 through CloudFront.\nThis option involves creating signed URLs that allow access to your content in the S3 bucket through CloudFront. With signed URLs, you can control access to your content by setting expiration times and IP ranges that can access the content.\nTo implement this option, you would need to:\nConfigure CloudFront to use your S3 bucket as the origin. Create a CloudFront distribution and specify the S3 bucket as the origin. Configure CloudFront to create signed URLs for your content. Provide the signed URLs to your users, who can then use them to access the content through CloudFront.\nOption B: Create OAI in CloudFront. Use the S3 bucket policy to ensure that only the OAI can access the files in the Amazon S3 bucket.\nThis option involves creating an Origin Access Identity (OAI) in CloudFront, which is a special CloudFront user that can be used to control access to your S3 bucket. By creating an OAI, you can ensure that only CloudFront can access the content in your S3 bucket.\nTo implement this option, you would need to:\nCreate an OAI in CloudFront. Modify the S3 bucket policy to allow access only to the OAI. Configure CloudFront to use the OAI to access the content in the S3 bucket. Provide the CloudFront URL to your users, who can then use it to access the content through CloudFront.\nOption C: Store the digital contents as private objects in the S3 buckets and use the S3 ACL to ensure that only the CloudFront distribution ARN can access the bucket.\nThis option involves setting the access control list (ACL) on your S3 bucket to allow access only to your CloudFront distribution. By doing this, you can ensure that the content in your S3 bucket can be accessed only through CloudFront.\nTo implement this option, you would need to:\nConfigure CloudFront to use your S3 bucket as the origin. Modify the S3 bucket ACL to allow access only to your CloudFront distribution. Provide the CloudFront URL to your users, who can then use it to access the content through CloudFront.\nOption D: Store the digital contents in the S3 bucket and configure signed cookies for users to access contents through CloudFront.\nThis option involves creating signed cookies that allow access to your content in the S3 bucket through CloudFront. Signed cookies are similar to signed URLs, but they are stored as browser cookies, which can be useful in situations where users need to access multiple files over a period of time.\nTo implement this option, you would need to:\nConfigure CloudFront to use your S3 bucket as the origin. Create a CloudFront distribution and specify the S3 bucket as the origin. Configure CloudFront to create signed cookies for your content. Provide the signed cookies to your users, who can then use them to access the content through CloudFront.\nOut of the four options, option B is the best solution as it provides the strongest security by only allowing access to your content through your CloudFront distribution.\n\n"
}, {
  "id" : 228,
  "question" : "You are working as an AWS Architect for an IT Company.\nYour Company is using EC2 instances in multiple VPCs spanning Availability Zones in (US-EAST-1) Region.\nThe Development Team has deployed a new Intranet application that needs to be accessed via VPC.\nYou need to make sure that the connectivity to this particular application uses the internal AWS network between different VPCs, and the solution is highly scalable and secure.\nWhich of the following solution would you recommend?\n",
  "answers" : [ {
    "id" : "72412afab152468eb27d4d000039ae3c",
    "option" : "Attach an Internet Gateway to all the VPCs at the \"US-EAST-1\" region and allow all users to access this application over the internet.",
    "isCorrect" : "false"
  }, {
    "id" : "188f8d45c198447db331338221fb9bd5",
    "option" : "Deploy Network Load Balancers along with VPC endpoint service (AWS PrivateLink) to establish connectivity between the VPC`s in the \"US-EAST-1\" region.",
    "isCorrect" : "true"
  }, {
    "id" : "5026a3b43d09426296b69349a4c37275",
    "option" : "Use the VPC Gateway Endpoint service between all the VPCs at the \"US-EAST-1\" region to provide connectivity between users &amp; servers.",
    "isCorrect" : "false"
  }, {
    "id" : "5087803f18524bd2a71f38668e0ffd39",
    "option" : "Create a VPN between instances at the various VPCs in \"US-EAST-1\" region to establish connectivity.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS PrivateLink provides secure private connectivity for services between separate VPC's.\nFor this, Network Load Balancers can be used in service provider while Elastic Network Interface is created in service, consuming VPC.\nUsing DNS, service provider service is resolved to the local IP address assigned to Elastic Network Interface which will forward all traffic to the Network Load Balancer in the provider network.\nNetwork Load Balancer will perform a source NAT for all traffic &amp; forward it to the provider instance.\nOption A is incorrect.\nUsing the Internet to establish connectivity between users &amp; servers will not be a highly secure solution.\nOption C is incorrect.\nVPC Gateway Endpoint service is for S3 and DynamoDB, which is unsuitable for this scenario.\nOption D is incorrect as VPN connectivity between the instance of various VPCs will not be a scalable solution.\nFor more information on AWS PrivateLink, refer to the following URL:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html\nAccessing Services Through AWS Private Links:\nAWS PrivateLink is a highly available, scalable technology that enables you to connect your VPC to supported AWS services privately, services hosted by other AWS accounts (VPC endpoint services), and supported AWS Marketplace partner services.\nYou do not require an internet gateway, NAT device, public IP address, AWS Direct Connect connection, or AWS Site-to-Site VPN connection to communicate with the service.\nThe traffic between your VPC and the service does not leave the Amazon network.\nTo use AWS PrivateLink, create an interface VPC endpoint for a service in your VPC.\nThis creates an elastic network interface in your subnet with a private IP address that serves as an entry point for the traffic, destined to the service.\nFor more information, see VPC Endpoints.\nYou can create your own AWS PrivateLink-powered service (endpoint service) and enable other AWS customers to access your service.\nFor more information, see VPC Endpoint Services (AWS PrivateLink).\nFor more information, refer to the following URLs:\nhttps://aws.amazon.com/privatelink/\nhttps://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html#what-is-privatelink\n\n\nThe recommended solution for connecting the new Intranet application across multiple VPCs in the US-EAST-1 region while ensuring scalability and security is to deploy Network Load Balancers with VPC endpoint service (AWS PrivateLink).\nOption A, which suggests attaching an Internet Gateway to all the VPCs and allowing users to access the application over the internet, is not recommended. This solution introduces security risks as it exposes the application to the public internet, making it vulnerable to attacks.\nOption C, which suggests using the VPC Gateway Endpoint service to provide connectivity between users and servers, is not suitable for this scenario. VPC Gateway Endpoint is used to connect a VPC to AWS services like S3 and DynamoDB. It doesn't provide connectivity between different VPCs.\nOption D, which suggests creating a VPN between instances in different VPCs, is also not the best solution in this case. While VPNs can provide secure connectivity between VPCs, they can be complex to manage and scale, and the performance may not be optimal.\nOption B, which suggests deploying Network Load Balancers along with VPC endpoint service (AWS PrivateLink), is the recommended solution. The Network Load Balancers can distribute traffic across the instances running the Intranet application, and AWS PrivateLink provides private connectivity between the VPCs over the AWS network. With this solution, traffic between the VPCs never leaves the AWS network, ensuring better security and performance.\nIn summary, deploying Network Load Balancers along with VPC endpoint service (AWS PrivateLink) is the recommended solution for connecting the new Intranet application across multiple VPCs in the US-EAST-1 region while ensuring scalability and security.\n\n"
}, {
  "id" : 229,
  "question" : "You want to host a new website on AWS.\nAs a Solutions architect, you have been given a task to design a serverless architecture for the website.\nWhich of the following services are suitable to store the application data? (Select TWO.)\n",
  "answers" : [ {
    "id" : "0f2ea69f264a43f0bc776c4bd43621ab",
    "option" : "Use DynamoDB to store dynamic data in tables.",
    "isCorrect" : "true"
  }, {
    "id" : "493782dc564e41ab90ddc91188557781",
    "option" : "Use EBS volumes to store data.",
    "isCorrect" : "false"
  }, {
    "id" : "76b750c9c7e749cd8038500e4945fca3",
    "option" : "Use the Simple Storage Service (S3) to store data.",
    "isCorrect" : "true"
  }, {
    "id" : "c812e2c829f245e3b326934e14bf3745",
    "option" : "Use AWS RDS to store dynamic data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and C.\nBoth Simple Storage Service and DynamoDB are serverless in AWS for which you do not need to maintain servers.\nFor more information on S3 and DynamoDB, please refer to the links below:\nhttps://aws.amazon.com/s3/\nhttps://aws.amazon.com/dynamodb/\nhttps://aws.amazon.com/serverless/\nhttps://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/\n\nAs a Solutions architect, when designing a serverless architecture for a website, it is essential to choose the right storage services that meet the requirements of the application. In this case, we are looking for services that can store the application data.\nOut of the given options, two services that are suitable for storing application data in a serverless architecture are:\nA. DynamoDB: Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB is an excellent choice for storing application data that requires frequent read-write operations, high scalability, and high availability. It can store structured, semi-structured, and unstructured data in tables. DynamoDB provides automatic scaling, backup and restore, and security features, making it a suitable choice for hosting a serverless website.\nC. Simple Storage Service (S3): Amazon S3 is an object storage service that is used to store and retrieve any amount of data, at any time, from anywhere on the web. It is highly durable and reliable, making it a popular choice for storing static website files such as images, videos, and HTML pages. S3 provides features like versioning, lifecycle policies, and access control, which makes it suitable for storing application data that requires high durability and low latency.\nOptions B and D are not suitable for storing application data in a serverless architecture:\nB. EBS volumes are attached to EC2 instances, and in a serverless architecture, EC2 instances are not used, so EBS is not a suitable option for storing application data.\nD. AWS RDS is a managed relational database service that can be used to store structured data in a variety of database engines. However, in a serverless architecture, it is recommended to use DynamoDB instead of RDS, as DynamoDB provides better scalability, availability, and performance for a serverless application.\nIn summary, suitable options for storing application data in a serverless architecture are DynamoDB and S3, whereas EBS volumes and AWS RDS are not recommended for this purpose.\n\n"
}, {
  "id" : 230,
  "question" : "Currently, you're responsible for the design and architect of a highly available application.\nAfter building the initial environment, you discover that your application does not work correctly until port 443 is added to the security group.\nAfter adding port 443 to the appropriate security group, how much time will it take for the application to work correctly?\n",
  "answers" : [ {
    "id" : "b97a2f384a9349fb9b66dfe75855c99e",
    "option" : "Generally, it takes 2-5 minutes for the rules to propagate.",
    "isCorrect" : "false"
  }, {
    "id" : "838ec2db00944c3babc21df9a122a5d3",
    "option" : "Immediately after a reboot of the EC2 Instances, belonging to that security group.",
    "isCorrect" : "false"
  }, {
    "id" : "ad7812942d2545f4ad0a21786aad0cc1",
    "option" : "Changes apply instantly to the security group, and the application should be able to respond to 443 requests.",
    "isCorrect" : "true"
  }, {
    "id" : "5d0cf4b44b7b49a39d4036c5602a14d9",
    "option" : "It will take 60 seconds for the rules to apply to all Availability Zones within the region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThis is given in the AWS Documentation:\n\"Some systems for setting up firewalls let you filter on source ports.\nSecurity groups let you filter only on destination ports.\nWhen you add or remove rules, they are automatically applied to all instances associated with the security group\".\nFor more information on Security Groups, please refer to the below link:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html\n\nWhen you add or modify a security group rule in AWS, the changes should be applied immediately to the security group. However, it's important to note that the rules may not be applied immediately to all instances associated with the security group.\nAWS recommends waiting for a few minutes after adding or modifying a security group rule for the changes to propagate to all instances associated with the security group. This propagation time can vary depending on the size of the security group and the number of instances associated with it.\nTherefore, answer A, \"Generally, it takes 2-5 minutes for the rules to propagate,\" is partially correct. It's a general rule of thumb, but the propagation time can vary.\nAnswer B, \"Immediately after a reboot of the EC2 Instances, belonging to that security group,\" is incorrect. Rebooting the instances will not immediately apply the security group changes.\nAnswer C, \"Changes apply instantly to the security group, and the application should be able to respond to 443 requests,\" is partially correct. The changes do apply instantly to the security group, but it may take a few minutes for the changes to propagate to all instances associated with the security group.\nAnswer D, \"It will take 60 seconds for the rules to apply to all Availability Zones within the region,\" is incorrect. The propagation time for security group changes can vary and is not specific to the availability zones within a region.\nIn summary, the correct answer is A, \"Generally, it takes 2-5 minutes for the rules to propagate.\" However, it's important to keep in mind that this is a general rule of thumb and the propagation time can vary depending on the size of the security group and the number of instances associated with it.\n\n"
}, {
  "id" : 231,
  "question" : "You work in a large organization.\nYour team creates AWS resources such as Amazon EC2 dedicated hosts and reserved capacities that need to be shared by other AWS accounts.\nYou need an AWS service to centrally manage these resources so that you can easily specify which accounts or Organizations can access the resources.\nWhich AWS service would you choose to meet this requirement?\n",
  "answers" : [ {
    "id" : "ee762eb64f69444abff331bb16a682af",
    "option" : "IAM",
    "isCorrect" : "false"
  }, {
    "id" : "0f5f2a5f016549378a7cd07e048301bd",
    "option" : "Resource Access Manager",
    "isCorrect" : "true"
  }, {
    "id" : "a7ce0e7525604a658921aa39ca7e7eeb",
    "option" : "Service Catalog",
    "isCorrect" : "false"
  }, {
    "id" : "29ce966644e5447197b6b77af1cf41a9",
    "option" : "AWS Single Sign-On.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Resource Access Manager (AWS RAM) helps users to share resources with other AWS accounts or Organizations.\nRefer to the reference in https://docs.aws.amazon.com/ram/latest/userguide/what-is.html.\nOption A is incorrect: Because IAM cannot be used to manage and share these resources.\nOption B is CORRECT: EC2 dedicated hosts and reserved capacities are shareable resources that are supported by Resource Access Manager.\nCheck the reference in https://docs.aws.amazon.com/ram/latest/userguide/shareable.html.\nOption C is incorrect: Because Service Catalog is used to manage catalogs and cannot share resources with others.\nOption D is incorrect: Because AWS Single Sign-On is used for SSO access and does not share the mentioned resources.\n\nThe AWS service that can centrally manage and share resources such as Amazon EC2 dedicated hosts and reserved capacities with other AWS accounts or Organizations is Resource Access Manager (RAM). Therefore, the correct answer is B. Resource Access Manager.\nResource Access Manager is a service that enables you to share AWS resources such as Amazon EC2 instances, Amazon RDS databases, and Amazon S3 buckets with other AWS accounts or within your organization. With RAM, you can create resource shares that specify which accounts or Organizations can access the shared resources and what level of access they have. RAM simplifies resource sharing across accounts and reduces the need for manual processes or custom scripts to manage resource sharing.\nIAM (Identity and Access Management) is a service that helps you manage access to AWS resources within your own account. IAM is not designed for sharing resources with other accounts or Organizations.\nService Catalog is a service that enables organizations to create and manage catalogs of IT services that can be used by their employees or customers. Service Catalog is not designed for sharing resources with other accounts.\nAWS Single Sign-On (SSO) is a service that makes it easy to centrally manage single sign-on access to multiple AWS accounts and business applications. SSO is not designed for sharing resources with other accounts.\nTherefore, Resource Access Manager (RAM) is the correct answer for the requirement mentioned in the question.\n\n"
}, {
  "id" : 232,
  "question" : "Your company wants to use an S3 bucket for web hosting but have several different domains to perform operations on the S3 content.\nIn the CORS configuration, you have added CORSRule AllowedOrigin for the following Domains.\nhttp://www.domainnamea.com, https://www.secure.domainnamea.com, and http://www.domainnameb.com.\nFollowing Domains, https://domainnameb.com and http://www.domainnameb.com:80, are not allowed to access the S3 bucket. What could be the most likely cause behind the unexpected access behaviour of the domains?\n",
  "answers" : [ {
    "id" : "1d557a218e06467486aed9cc8c9e1eec",
    "option" : "Both request https://domainnameb.com and http://www.domainnameb.com:80 don`t match the allowed in configuration.",
    "isCorrect" : "true"
  }, {
    "id" : "fca57f07028f46738075bbb071dd5061",
    "option" : "HTTPS must contain a specific port in the request, e.g.https:// domainnameb.com:443",
    "isCorrect" : "false"
  }, {
    "id" : "68e80b52ba6540bca7460bb3487f591c",
    "option" : "Thereâ€™s a limit of two origin sites per S3 bucket allowed",
    "isCorrect" : "false"
  }, {
    "id" : "ecf5c8a6be8e4cbdaf2e18ce3fa6740c",
    "option" : "Adding CORS automatically removes the S3 ACL and bucket policies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is correct.\nThe origin was configured as http://www.domainnameb.com, and a request was sent for https://domainnameb.com and http://www.domainnameb.com:80 instead of http://www.domainnameb.com.\nThe exact syntax must be matched.\nIn some cases, wildcards can be used to help the origin URLs.\nOption B is incorrect.\nThis is not required to allow an origin domain to be included, although it can be.\nOption C is incorrect.\nThe limit is 100.\nOption D is incorrect.\nThe ACLs and policies continue to apply when you enable CORS on the bucket.\nVerify that the Origin header in your request matches at least one of the AllowedOrigin elements in the specified CORSRule.\nThe scheme, the host, and the port values in the Origin request header must match the AllowedOrigin elements in the CORSRule.\nFor example, if you set the CORSRule to allow the origin http://www.example.com, then both https://www.example.com and http://www.example.com:80 origins in your request don't match the allowed origin in your configuration.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\nhttps://aws.amazon.com/blogs/aws/amazon-S3-cross-origin-resource-sharing/\n\nThe most likely cause behind the unexpected access behavior of the domains is that both the domains, https://domainnameb.com and http://www.domainnameb.com:80, do not match the allowed origins in the CORS configuration.\nThe CORS (Cross-Origin Resource Sharing) mechanism is used to control access to resources that are hosted on different domains. When a web page from one domain requests a resource from another domain, the browser sends a CORS request to the server hosting the resource. The server responds with the Access-Control-Allow-Origin header that specifies which domains are allowed to access the resource.\nIn this scenario, the CORS configuration allows access from http://www.domainnamea.com, https://www.secure.domainnamea.com, and http://www.domainnameb.com, but it does not allow access from https://domainnameb.com and http://www.domainnameb.com:80. The reason for this could be that the request from these domains does not match the allowed origins in the CORS configuration.\nOption A is correct because it explains that the request from these domains does not match the allowed origins in the CORS configuration.\nOption B is incorrect because HTTPS requests do not need to contain a specific port in the request.\nOption C is incorrect because there is no limit to the number of origin sites that can access an S3 bucket.\nOption D is incorrect because adding CORS to an S3 bucket does not automatically remove the S3 ACL and bucket policies.\nIn summary, the most likely cause behind the unexpected access behavior of the domains is that the request from these domains does not match the allowed origins in the CORS configuration.\n\n"
}, {
  "id" : 233,
  "question" : "Your application provides data transformation services.\nFiles containing data to be transformed are first uploaded to Amazon S3 and then transformed by a fleet of Spot EC2 Instances.\nFiles submitted by your premium customers must be transformed at the highest priority.\nHow would you implement such a system?\n",
  "answers" : [ {
    "id" : "b8b99cd4658e4ddc8888d3d2f70c338b",
    "option" : "Use a DynamoDB table with an attribute defining the priority level. Transformation instances will scan the table for tasks, sorting the results by priority level.",
    "isCorrect" : "false"
  }, {
    "id" : "4343a3931b464a81912470ba7157d7cb",
    "option" : "Use Route 53 latency-based routing to send high priority tasks to the closest transformation instances.",
    "isCorrect" : "false"
  }, {
    "id" : "9f8341417cd34f15b2c5b083eaaeb986",
    "option" : "Use two SQS queues, one for high priority messages and the other for default priority. Transformation instances will first poll the high priority queue; if there is no message, they will poll the default priority queue.",
    "isCorrect" : "true"
  }, {
    "id" : "caaa343f178c480e87ed1ab0e503b98a",
    "option" : "Use a single SQS queue. Each message contains the priority level. Transformation instances poll high-priority messages first.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe best way is to use two SQS queues.\nEach queue can be polled separately.\nThe high priority queue can be polled first.\nFor more information on AWS SQS, please refer to the link below:\nhttps://aws.amazon.com/sqs/\n\nThe correct answer is C: Use two SQS queues, one for high priority messages and the other for default priority. Transformation instances will first poll the high priority queue; if there is no message, they will poll the default priority queue.\nHere is a detailed explanation of how this solution works:\nAmazon S3 is used to store the files containing the data to be transformed. Users upload their files to S3 and provide the necessary information for the transformation process. The application uses SQS (Simple Queue Service) to manage the transformation requests. A separate SQS queue is used for high-priority messages and default-priority messages. When a file is uploaded to S3, the application places a message in the appropriate SQS queue based on the priority level of the customer. The message includes information about the file and the priority level. A fleet of Spot EC2 instances is used for the data transformation. These instances are configured to poll the high-priority queue first. If there are no messages in the high-priority queue, they will then poll the default-priority queue. When an instance receives a message from the queue, it retrieves the file from S3 and performs the necessary transformations. Once the transformation is complete, the instance places the transformed file in another S3 bucket or sends it to the customer, depending on the application's requirements.\nUsing two separate SQS queues based on priority level ensures that premium customers receive the highest priority. Transformation instances are configured to prioritize high-priority messages and handle default-priority messages only when no high-priority messages are available. This solution is both scalable and cost-effective as it uses Spot EC2 instances, which offer significant cost savings compared to on-demand instances.\n\n"
}, {
  "id" : 234,
  "question" : "A VPC has been set up manually, with a subnet and an internet gateway.\nThe EC2 instance is set up with a public IP.\nBut you still can not connect to it via the Internet.\nThe security groups are also in place.\nWhat should you do to connect to the EC2 Instance from the Internet?\n",
  "answers" : [ {
    "id" : "ae4331cd3fe04b37ba13252a171ea0b7",
    "option" : "Set an Elastic IP Address to the EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "8aaf0cdec1d34f37b052a6915cec364e",
    "option" : "Set a Secondary Private IP Address to the EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "09d4fc9bd48c4f368bf063e944106efd",
    "option" : "Ensure that the right route entry is there in the Route table.",
    "isCorrect" : "true"
  }, {
    "id" : "14867ef8e48345c7b203fc43c6e4cf14",
    "option" : "There must be some issue in the EC2 Instance. Check the system logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nYou have to ensure that the Route table has an entry to the Internet Gateway because this is required for instances to communicate over the Internet.\nThe diagram shows the configuration of the public subnet in a VPC.Option A is incorrect.\nSince you already have a public IP assigned to the instance, this should have been enough to connect to the Internet.\nOption B is incorrect.\nPrivate IPs cannot be accessed from the Internet.\nOption D is incorrect.\nThe Route table is causing the issue and not the system.\nFor more information on AWS public subnet, please visit the link below:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html\n\n\nIf you have set up a VPC, subnet, and an internet gateway, and an EC2 instance with a public IP, but are unable to connect to the instance from the internet, there are a few things you can check to troubleshoot the issue.\nCheck the Security Groups: Ensure that the security group associated with the EC2 instance allows inbound traffic on the ports that you are trying to connect to from the internet. By default, security groups are set to deny all inbound traffic. Therefore, you need to allow traffic explicitly by adding inbound rules. Check the Network Access Control List (NACLs): Ensure that the NACLs associated with the subnet allow inbound traffic on the ports that you are trying to connect to from the internet. NACLs act as a firewall for subnets, and you need to explicitly allow traffic through them as well. Check the Routing Table: Ensure that the route table associated with the subnet that the EC2 instance is launched in has a route entry to the internet gateway. Without this entry, the traffic won't be able to reach the internet gateway and go out to the internet. Set an Elastic IP Address: If you have not set an Elastic IP address to the EC2 instance, you may want to consider doing so. An Elastic IP address is a static, public IP address that you can assign to your instance. With an Elastic IP address, you can ensure that the public IP address assigned to the instance does not change when you stop and start the instance. Check the System Logs: If you have verified the above steps and still cannot connect to the EC2 instance, you may want to check the system logs of the instance to see if there is any issue with the instance itself.\nIn summary, the correct answer is option A: Set an Elastic IP address to the EC2 instance. However, it is recommended to check all the steps mentioned above before assigning an Elastic IP address, as it might not be necessary in all cases.\n\n"
}, {
  "id" : 235,
  "question" : "A company has a PostgreSQL DB instance in Amazon RDS which is not encrypted.\nAs per security policy, data in the RDS instances should be encrypted at rest with AWS KMS. Which option is correct for RDS DB encryption?\n",
  "answers" : [ {
    "id" : "2eed1a040862436b89d2e88560656bae",
    "option" : "Amazon RDS for PostgreSQL DB instance can only be encrypted at creation time and not after its creation. There is no way to achieve this requirement.",
    "isCorrect" : "false"
  }, {
    "id" : "acbaeeaf7af14260870fe88394e93357",
    "option" : "Take a snapshot of the unencrypted DB instance. Copy the snapshot and encrypt the new snapshot with AWS KMS. Restore the DB instance with the new encrypted snapshot.",
    "isCorrect" : "true"
  }, {
    "id" : "788a14756bda4932872bb78b92486f79",
    "option" : "Take a snapshot of the unencrypted DB instance. Encryption can be enabled by restoring a DB instance from the unencrypted snapshot.",
    "isCorrect" : "false"
  }, {
    "id" : "4fcaabf454c04eed88a176770adb982e",
    "option" : "Stop the existing RDS instance and encrypt the DB with a KMS CMK.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect because there is a way to achieve encryption afterward which is given in option.\nB.Option B is CORRECT.\nYou can enable encryption for an RDS DB instance when you create it, but not after it's created.\nHowever, you can add encryption to an unencrypted DB instance by creating a snapshot of your DB instance and then creating an encrypted copy of that snapshot.\nYou can then restore a DB instance from the encrypted snapshot to get an encrypted copy of your original DB instance.\nOption C is incorrect because we have to copy and encrypt the snapshot first.\nOption D is incorrect because the currently running un-encrypted DB instance cannot be encrypted.\nReferences:\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/encrypt-an-existing-amazon-rds-for-postgresql-db-instance.html\nhttps://blog.theodo.com/2019/11/encrypt-existing-aws-rds-database/\nhttps://aws.amazon.com/blogs/security/architecting-for-database-encryption-on-aws/\n\nThe correct answer is B. Take a snapshot of the unencrypted DB instance. Copy the snapshot and encrypt the new snapshot with AWS KMS. Restore the DB instance with the new encrypted snapshot.\nExplanation: AWS RDS (Relational Database Service) provides easy management of relational databases, including PostgreSQL. RDS instances can be encrypted at rest using AWS KMS (Key Management Service) to secure data in the database.\nIn this scenario, the company's PostgreSQL RDS instance is not encrypted, and the security policy requires the encryption of data at rest with AWS KMS. Since the instance is already created, there is no option to enable encryption directly on it. Instead, the following steps can be taken to encrypt the database at rest:\nTake a snapshot of the unencrypted RDS instance. A snapshot is a backup of the database instance and can be used to create a new instance later. Copy the snapshot and encrypt the new snapshot with AWS KMS. During the snapshot copy process, you can choose to encrypt the copy using a KMS key. Restore the DB instance with the new encrypted snapshot. You can create a new RDS instance from the encrypted snapshot.\nThis option ensures that the data is encrypted at rest while maintaining the existing database instance.\nOption A is incorrect because it is possible to encrypt an existing RDS instance by creating an encrypted snapshot and restoring it.\nOption C is incorrect because restoring from an unencrypted snapshot will result in an unencrypted database instance.\nOption D is incorrect because stopping an existing RDS instance does not provide an option to encrypt it with KMS.\n\n"
}, {
  "id" : 236,
  "question" : "A company is planning to use the AWS ECS service to work with containers in the \"us-east-1\" region.\nThere is a need for the least amount of administrative overhead while provisioning and managing containers.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "62915a42cd264c98bb3416949c7729a1",
    "option" : "Use the Fargate launch type in AWS ECS.",
    "isCorrect" : "true"
  }, {
    "id" : "7e13684351034a11928c9adc1f777411",
    "option" : "Use the EC2 launch type in AWS ECS.",
    "isCorrect" : "false"
  }, {
    "id" : "d474a36d36f44f3fa3ef1a63870538b4",
    "option" : "Use the Auto Scaling launch type in AWS ECS.",
    "isCorrect" : "false"
  }, {
    "id" : "fa15c97fd99f465e9fccfdccb3b87253",
    "option" : "Use the ELB launch type in AWS ECS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following:\nThe Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure.\nJust register your task definition and Fargate launches the container for you.\nFor more information on the different launch types, please visit the links below:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html\n\nSure, I can explain in detail!\nAWS ECS (Elastic Container Service) is a fully managed container orchestration service that simplifies the deployment, management, and scaling of containerized applications on AWS. It allows users to run Docker containers on a managed cluster of EC2 instances or Fargate instances.\nWhen it comes to minimizing administrative overhead while working with containers, the Fargate launch type is the best choice among the given options. The Fargate launch type allows you to run containers without the need to manage any underlying EC2 instances. You only need to specify the CPU and memory requirements, define networking and security settings, and launch your containers.\nWith Fargate, you don't have to worry about patching or updating the operating system, configuring the underlying infrastructure, or scaling the underlying instances. AWS automatically takes care of these tasks for you, allowing you to focus on developing and deploying your containerized applications.\nOn the other hand, the EC2 launch type allows you to launch containers on a cluster of EC2 instances that you manage. This requires more administrative overhead, as you have to manage the underlying EC2 instances, configure them for optimal performance, and scale them up or down as needed.\nThe Auto Scaling launch type is a way to automatically scale the number of tasks (containers) based on demand, but it doesn't eliminate the need to manage the underlying EC2 instances.\nThe ELB launch type is not a valid launch type for ECS. ELB (Elastic Load Balancing) is a service that automatically distributes incoming traffic across multiple targets, such as EC2 instances, containers, or IP addresses.\nIn summary, if you want to minimize administrative overhead while provisioning and managing containers in AWS ECS, the best option is to use the Fargate launch type. It allows you to run containers without managing any underlying infrastructure, and AWS takes care of the operational tasks for you.\n\n"
}, {
  "id" : 237,
  "question" : "You currently manage a set of web servers hosted on EC2 Servers with public IP addresses.\nThese IP addresses are mapped to domain names.\nThere was an urgent maintenance activity that had to be carried out on the servers.\nThe servers had to be stopped and restarted.\nNow the web application hosted on these EC2 Instances is not accessible via the domain names configured earlier.\nWhich of the following could be a reason for this?\n",
  "answers" : [ {
    "id" : "c9e70a424dda43cb82561dff9a0b66a0",
    "option" : "The Route 53 hosted zone needs to be restarted.",
    "isCorrect" : "false"
  }, {
    "id" : "44867b6ce9f54315968b299544527a98",
    "option" : "The network interfaces need to initialized again.",
    "isCorrect" : "false"
  }, {
    "id" : "bc8673205dc243dfbb8ae3569450eba5",
    "option" : "The public IP addresses need to be associated with the ENI again.",
    "isCorrect" : "false"
  }, {
    "id" : "43542755a57c45da92f867392a57c21c",
    "option" : "The public IP addresses have changed after the instance was stopped and started again.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nBy default, the public IP address of an EC2 Instance is released after the instance is stopped and started.\nHence, the earlier IP address which was mapped to the domain names would have become invalid now.\nFor more information on public IP address, please visit the URL below:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.html#concepts-public-addresses\nNote:\nThis only applies to IPv4 public addresses.\nIPv6 public address isn't disassociated after an instance is stopped.\n\nWhen an EC2 instance is stopped and started again, it is possible that its public IP address may change. This is because EC2 instances are assigned public IP addresses dynamically from a pool of available addresses, and releasing an address when an instance is stopped can cause it to be assigned to another instance when it is started again. Therefore, option D could be the reason why the web application is not accessible via the domain names configured earlier.\nOption A is not a valid reason for this issue because Route 53 is a DNS service and is not directly involved in the configuration of public IP addresses or network interfaces on EC2 instances.\nOption B is also not a valid reason for this issue because the network interfaces are typically initialized automatically when an EC2 instance is started, and stopping and starting the instance should not affect this process.\nOption C could be a possible reason for this issue if the Elastic Network Interfaces (ENIs) associated with the EC2 instances were not re-associated with the correct public IP addresses after the instances were stopped and started again. However, this option is less likely to be the issue since public IP addresses are typically automatically re-associated with ENIs when an instance is started.\nTherefore, the most likely reason why the web application hosted on the EC2 Instances is not accessible via the domain names configured earlier is that the public IP addresses have changed after the instance was stopped and started again.\n\n"
}, {
  "id" : 238,
  "question" : "You are responsible for deploying a critical application to AWS.\nIt is required to ensure that the controls set for this application meet PCI compliance.\nAlso, there is a need to monitor web application logs to identify any malicious activity.\nWhich of the following services could be used to fulfill this requirement? (Select TWO)\n",
  "answers" : [ {
    "id" : "80936428881b40b681cc8a8662d3dffa",
    "option" : "Amazon CloudWatch Logs",
    "isCorrect" : "true"
  }, {
    "id" : "977330f1244d4a63ae3063837eeace6b",
    "option" : "AWS Personal Health Dashboard",
    "isCorrect" : "false"
  }, {
    "id" : "3de030b9aeda403daa429e6bcc788221",
    "option" : "Amazon Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "9c3ae21f9dee4b5893424cf1b5b7a8d2",
    "option" : "Amazon CloudTrail.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers - A and D.\nAWS Documentation mentions the following about these services:\nOption A is correct as Amazon CloudWatch Logs is used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, and other sources.\nYou can then retrieve the associated log data from CloudWatch Logs.\nhttps://aws.amazon.com/compliance/services-in-scope/\nOption B is incorrect because AWS Personal Health Dashboard provides alerts and guidance for AWS events that might affect your environment.\nIt will not help to monitor the web application.\nhttps://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/\nOption C is incorrect because AWS Trusted Advisor is an online tool that provides you real-time guidance to help you provision your resources following AWS best practices.\nIt is not required as per the requirement.\nThe question asks for monitoring services, not logging or some visualizing service.\nOption D is correct as AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nhttps://aws.amazon.com/cloudtrail/\nReferences:\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\nhttps://aws.amazon.com/blogs/aws/vpc-flow-logs-log-and-view-network-traffic-flows/\n\nTo fulfill the requirement of deploying a critical application to AWS and ensure PCI compliance and monitoring of web application logs for identifying malicious activity, the following services can be used:\nA. Amazon CloudWatch Logs: Amazon CloudWatch Logs is a service that helps to monitor, store, and access log files from Amazon EC2 instances, AWS CloudTrail, and other AWS services. It can be used to collect logs from the critical application and store them centrally in CloudWatch Logs. CloudWatch Logs can be configured to detect specific log events and trigger alerts for any abnormal activity. It also provides real-time analysis and visualization of log data to identify trends and troubleshoot issues. To meet the PCI compliance requirements, it is essential to ensure that CloudWatch Logs are encrypted during transit and at rest.\nD. Amazon CloudTrail: Amazon CloudTrail is a service that logs API calls made within the AWS infrastructure. It provides a detailed record of events related to account activity and resource usage. CloudTrail can be used to track user activity and API usage within the critical application environment. It can also help to identify any unauthorized access or changes to resources. CloudTrail provides an audit trail that can be used to demonstrate compliance with PCI requirements. To ensure PCI compliance, it is essential to ensure that CloudTrail logs are encrypted during transit and at rest.\nTherefore, A (Amazon CloudWatch Logs) and D (Amazon CloudTrail) are the correct answers. AWS Personal Health Dashboard and Amazon Trusted Advisor do not provide log management or monitoring capabilities and are not relevant to fulfilling the given requirement.\n\n"
}, {
  "id" : 239,
  "question" : "There is a requirement to host a database server.\nThe server needs to connect to the Internet while downloading the required database patches.\nBut the ingress traffic to the instances is not allowed.\nWhich of the following solutions would satisfy the above requirements at its best?\n",
  "answers" : [ {
    "id" : "b2c99f40b5f3432e9f9419571f9f1d9b",
    "option" : "Setup the database in a private subnet with a security group that only allows outbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "f23e82b4595b432190589e75786f8b03",
    "option" : "Setup the database in a public subnet with a security group that only allows inbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "36615a2224444032888a4299a990d9cc",
    "option" : "Setup the database in a local data center and use a private gateway to connect the application to the database.",
    "isCorrect" : "false"
  }, {
    "id" : "f20ace0f5ee94dac8ba33af0879d41d0",
    "option" : "Setup the database in a private subnet that connects to the Internet via NAT Gateway.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThis setup coincides with Scenario 2 of setting up a VPC as per AWS documentation:\nScenario 2: VPC with Public and Private Subnets (NAT)\nThe instances in the public subnet can send outbound traffic directly to the internet, whereas the instances in the private subnet can't.\nInstead, the instances in the private subnet can access the internet by using a network address translation (NAT) gateway that resides in the public subnet.\nThe database servers can connect to the internet for software updates using the NAT gateway, but the internet cannot establish connections to the database servers.\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\n\nThe requirement is to host a database server that can connect to the Internet for downloading patches, but ingress traffic to the instance is not allowed. To satisfy this requirement, we need to set up the database server in a secure manner.\nA. Setup the database in a private subnet with a security group that only allows outbound traffic. This option is a viable solution. A private subnet is not directly accessible from the Internet, so ingress traffic is blocked. The outbound traffic can be allowed through the security group, which enables the database server to connect to the Internet and download the required patches.\nB. Setup the database in a public subnet with a security group that only allows inbound traffic. This option is not secure as the database server will be directly accessible from the Internet, which poses a security risk. Even though inbound traffic is not allowed, the database server will still be publicly available, which is not the intended requirement.\nC. Setup the database in a local data center and use a private gateway to connect the application to the database. This option is not suitable as it does not meet the requirement of hosting the database server in the cloud. Additionally, setting up a private gateway can be a complex and costly solution, and it may not be necessary in this scenario.\nD. Setup the database in a private subnet that connects to the Internet via NAT Gateway. This option is a viable solution. A private subnet provides an additional layer of security, and the NAT Gateway allows the database server to connect to the Internet while blocking ingress traffic to the instance.\nIn summary, options A and D are viable solutions, but option D may be preferred as it offers an additional layer of security. Therefore, the correct answer is D.\n\n"
}, {
  "id" : 240,
  "question" : "You have both production and development based instances running on your VPC.\nIt is required to ensure that people responsible for the development instances do not have access to work on production instances for better security.\nWhich of the following would be the best way to accomplish this using policy with minimal efforts?\n",
  "answers" : [ {
    "id" : "189c16bffad74c4dba6580acfd4cee99",
    "option" : "Launch the development and production instances in separate VPCs and use VPC Peering.",
    "isCorrect" : "false"
  }, {
    "id" : "6e2c3ee019a3490f8ce49889d181e4f4",
    "option" : "Create an IAM group with a condition that allows access to only those instances which are used for production or development.",
    "isCorrect" : "false"
  }, {
    "id" : "2e7cd3fe27344729beb6fb3e72a255d5",
    "option" : "Launch the development and production instances in different Availability Zones and use Multi-Factor Authentication.",
    "isCorrect" : "false"
  }, {
    "id" : "9404f30f6fe94ecc8df9d1238825785e",
    "option" : "Define the tags on the Development and production servers and add a condition to the IAM Policy which allows access to specific tags.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nYou can easily add tags to define which instances are the production instances and which ones are development instances.\nThese tags can then be used while controlling access via an IAM Policy.\nAlso the question is asking for minimal efforts and overhead and therefore Option D is the CORRECT choice here.\nFor more information on tagging your resources, please refer to the link below:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\nNote:\nIt can be done with the help of option B as well.\nHowever, the question is looking for the \"best way to fulfill the requirement using policies\".\nBy using the option D, you can reduce the usage of different IAM Policies on each instance.\n\nThe best way to accomplish this would be to use option D: Define the tags on the development and production servers and add a condition to the IAM policy which allows access to specific tags.\nIAM (Identity and Access Management) is the service provided by AWS for managing access to AWS resources. IAM allows you to create policies which define the permissions for your users and groups. The policies are based on JSON and can be customized to allow or deny access to specific resources.\nIn this scenario, you have both production and development instances running on your VPC. You want to ensure that people responsible for the development instances do not have access to work on production instances. To achieve this, you can define tags on the development and production servers and add a condition to the IAM policy which allows access to specific tags.\nTags are key-value pairs that can be assigned to resources in AWS. They can be used to categorize and organize your resources. In this case, you can assign a \"Environment\" tag with the value \"Production\" or \"Development\" to the instances.\nTo implement this, you can create an IAM policy that allows access to instances with a specific tag. For example, you could create a policy that allows access to instances with the \"Environment\" tag set to \"Production\". You can then assign this policy to the users or groups that need access to the production instances.\nThis approach is easy to implement and maintain. It allows you to control access to specific instances based on tags without the need for complex configurations. Additionally, it ensures that users responsible for development instances are not able to access production instances, thus improving the security of your infrastructure.\nOption A: Launch the development and production instances in separate VPCs and use VPC Peering. This option would also work, but it would require more effort to set up and maintain. VPC peering is used to connect two VPCs together so that they can communicate with each other as if they were on the same network. This approach would require creating two separate VPCs, configuring VPC peering, and ensuring that the appropriate security groups and routing tables are set up correctly. While this approach would provide additional security by isolating the environments, it would require more effort to set up and maintain.\nOption B: Create an IAM group with a condition that allows access to only those instances which are used for production or development. This approach would also work, but it would be less granular than using tags. It would require creating two separate groups for production and development instances and assigning the appropriate policies to each group. This approach would not allow for finer control of access based on specific instances or tags.\nOption C: Launch the development and production instances in different Availability Zones and use Multi-Factor Authentication. This option would not address the issue of controlling access to specific instances based on their environment. Multi-factor authentication is a security feature that requires users to provide additional authentication beyond their password, such as a code sent to their phone or a security token. While this feature can add an extra layer of security, it does not address the issue of controlling access to specific instances based on their environment.\n\n"
}, {
  "id" : 241,
  "question" : "A company has applications running in multiple VPCs.\nThese applications require interaction between Amazon S3 buckets and DynamoDB.\nThe company's security policy requires that communication should be secure and should not go over the public internet.\nHow does a solutions architect design this solution to meet these requirements?\n",
  "answers" : [ {
    "id" : "54576d0bc7ae4d39810724a06093e3f5",
    "option" : "Create VPC Gateway Endpoints for S3 and DynamoDB and update route tables for all the availability zones.",
    "isCorrect" : "true"
  }, {
    "id" : "3888866881b1435d9750b8a8d9603ff4",
    "option" : "Use the NAT Gateway and Internet Gateway for all the egress communication to these AWS services.",
    "isCorrect" : "false"
  }, {
    "id" : "3ad6e3ae32784a94b9bac104412cc426",
    "option" : "Set up VPC peering and use VPC gateway endpoint for S3 and interface endpoint for DynamoDB to communicate over AWS network.",
    "isCorrect" : "false"
  }, {
    "id" : "ef3e745db31949eb87399d8813fc7ff2",
    "option" : "Set up a VPC interface endpoint for both S3 and DynamoDB to communicate over the AWS network.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nTypes of VPC endpoints for Amazon S3\nYou can use two types of VPC endpoints to access Amazon S3: gateway endpoints and interface endpoints.\nA gateway endpoint is a gateway that you specify in your route table to access Amazon S3 from your VPC over the AWS network.\nInterface endpoints extend the functionality of gateway endpoints by using private IP addresses to route requests to Amazon S3 from within your VPC, on-premises, or from a VPC in another AWS Region using VPC peering or AWS Transit Gateway.\nThere are two types of VPC endpoints:\nInterface endpoint is an elastic network interface (ENI) with a private IP address from the IP address range of the user's subnet that serves as an entry point for traffic destined to a supported service.\nIt enables you to privately access services by using private IP addresses.\nGateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to support AWS service.\nCurrently supports S3 and DynamoDB services.\nOption A is CORRECT because the VPC gateway endpoint supports S3 and DynamoDB.\nUsing the VPC endpoint, communication will not go over the internet, and it will use AWS private network.\nA VPC endpoint does not require an IGW, NAT device.\nInstances in the VPC do not require public IP addresses to communicate with resources in the service.\nTraffic between VPC and the other service stays in the Amazon network.\nOption B is incorrect as NAT gateway and IGW use public internet and do not provide secure channels.\nOption C is incorrect as DynamoDB can be accessed using Gateway endpoint and not interface endpoint.\nVPC peering is between VPCs.\nOption D is incorrect as S3 can be accessed using the Interface endpoint but the DynamoDB requires the gateway endpoint.\nThe option says to use the interface endpoint for both AWS S3 and the DynamoDB table.\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/endpoint-service.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html\n\n\nThe requirement is to enable communication between Amazon S3 buckets and DynamoDB for applications running in multiple VPCs in a secure manner without going over the public internet. To achieve this, there are several options available, and the best solution depends on the specific needs and constraints of the company.\nOption A: Create VPC Gateway Endpoints for S3 and DynamoDB and update route tables for all the availability zones.\nA VPC Gateway Endpoint is a private endpoint for accessing AWS services over a direct and secure connection from within a VPC, without using an internet gateway, NAT device, or VPN connection. It allows traffic to flow between the VPC and the AWS service without traversing the internet.\nTo use this option, a VPC Gateway Endpoint must be created for both Amazon S3 and DynamoDB services. Once created, the route tables for all availability zones in the VPCs must be updated to ensure that traffic destined for these services is directed to the appropriate gateway endpoint.\nThis option is a good fit for scenarios where there are multiple VPCs that need to access S3 and DynamoDB services in a secure manner without going over the public internet.\nOption B: Use the NAT Gateway and Internet Gateway for all the egress communication to these AWS services.\nA NAT Gateway is a managed service that enables instances in a private subnet to connect to the internet or other AWS services, but prevents the internet from initiating a connection with those instances. An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the internet.\nTo use this option, a NAT Gateway must be created in each VPC, and the instances in the VPC must be configured to use the NAT Gateway as the default route for all outbound traffic. An Internet Gateway must also be created to allow the NAT Gateway to access the AWS services over the internet.\nThis option is a good fit for scenarios where there are few VPCs and the traffic between the VPCs and the AWS services is relatively low.\nOption C: Set up VPC peering and use VPC gateway endpoint for S3 and interface endpoint for DynamoDB to communicate over AWS network.\nVPC peering is a networking connection between two VPCs that enables instances in one VPC to communicate with instances in the other VPC as if they were on the same network. To use this option, VPC peering must be established between the VPCs. Once the peering is established, VPC Gateway Endpoint for S3 and Interface Endpoint for DynamoDB can be created in the destination VPCs.\nThis option is a good fit for scenarios where there are two or more VPCs that need to communicate with each other and with AWS services in a secure manner.\nOption D: Set up a VPC interface endpoint for both S3 and DynamoDB to communicate over the AWS network.\nA VPC interface endpoint is an elastic network interface with a private IP address that serves as an entry point for traffic destined to an AWS service. When instances in a VPC access the AWS service, the traffic is directed to the VPC interface endpoint, which routes the traffic to the appropriate AWS service.\nTo use this option, a VPC interface endpoint must be created for both Amazon S3 and DynamoDB services. Once created, the instances in the VPC can communicate with the AWS services through the VPC interface endpoints.\nThis option is a good fit for scenarios where there are few VPCs and the traffic between the VPCs and the AWS services is relatively low.\nIn conclusion, the best option for the company depends on the specific needs and constraints. Option A is a good fit for scenarios where there are multiple VPCs that need to access S3 and DynamoDB services in a\n\n"
}, {
  "id" : 242,
  "question" : "A company consists of 50 plus AWS accounts.\nEach account has multiple VPCs with egress internet connectivity using NAT gateway per Availability Zone (AZ)\nA solution architect has been asked to redesign the network architecture that will reduce costs, and manage egress traffic, and the growing needs of new accounts. Which solution meets the requirements?\n",
  "answers" : [ {
    "id" : "7f2224d02fc84e59ae5c778f929f494f",
    "option" : "Create an egress VPC for outbound internet traffic. Use VPC peering between AWS accounts` VPCs and connect to a set of redundant NAT gateway in the egress VP.",
    "isCorrect" : "false"
  }, {
    "id" : "a0a27cdf08ad4c0da0b28ad338bc415d",
    "option" : "Create a Transit Gateway in one central AWS account that will work as a hub and spoke model to other accounts VPCs using VPC attachments. Setup an egress VPC for egress traffic with redundant Nat Gateways.",
    "isCorrect" : "true"
  }, {
    "id" : "7babf0994fc94bdebe48b2e305f22b91",
    "option" : "Create a central VPC for outbound internet traffic. Use AWS Private links between AWS accounts` VPCs to a redundant NAT gateway in the central VP.",
    "isCorrect" : "false"
  }, {
    "id" : "1453e51f14cd40eab6b3f3006529a163",
    "option" : "Create a Transit Gateway in one central AWS account that will work as a hub and spoke model to other accounts VPCs using VPN attachments. Set up a Central VPC for egress traffic with redundant Nat Gateways.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nThis architecture shows AWS Transit Gateway to centralize outbound internet traffic from multiple VPCs using hub-and-spoke design.\nOption A is incorrect because VPC peering doesn't support transitive routing between Account A VPC to Account B VPC to egress the internet.\nOption B is CORRECT because this design supports transitive routing and centralizing egress, which helps to reduce cost.\nOption C is incorrect because AWS Private Link doesn't provide NAT.\nOption D is incorrect because the VPN attachment is more on VPN connectivity.\nReferences:\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/creating-a-single-internet-exit-point-from-multiple-vpcs-using-aws-transit-gateway/\n\n\nSure, I'll be happy to provide you with a detailed explanation of the four possible solutions for redesigning the network architecture for a company with 50+ AWS accounts.\nOption A: Create an egress VPC for outbound internet traffic. Use VPC peering between AWS accounts' VPCs and connect to a set of redundant NAT gateways in the egress VPC.\nThis solution proposes creating an egress VPC that will serve as the hub for all egress traffic for all the accounts. The VPCs in each account will use VPC peering to connect to the egress VPC, and redundant NAT gateways will be set up in the egress VPC to handle the traffic. This solution would be effective in reducing costs and managing egress traffic as it centralizes the traffic into a single VPC, making it easier to manage, and reducing the number of NAT gateways needed overall. However, it may be more complex to set up and manage the VPC peering connections between all the accounts, and there could be potential issues with latency and throughput for traffic that has to travel through multiple VPCs.\nOption B: Create a Transit Gateway in one central AWS account that will work as a hub and spoke model to other accounts' VPCs using VPC attachments. Setup an egress VPC for egress traffic with redundant NAT gateways.\nThis solution proposes creating a Transit Gateway in a central AWS account that will act as the hub for all VPCs in the other accounts. The VPCs in each account will use VPC attachments to connect to the Transit Gateway, and an egress VPC will be set up in the central account with redundant NAT gateways to handle egress traffic. This solution offers the benefits of centralizing traffic into a single VPC while reducing the complexity of managing VPC peering connections between accounts. Additionally, this solution allows for future expansion and management of new accounts with less effort. The only concern would be the cost incurred for setting up a Transit Gateway.\nOption C: Create a central VPC for outbound internet traffic. Use AWS Private links between AWS accounts' VPCs to a redundant NAT gateway in the central VPC.\nThis solution proposes creating a central VPC that will act as the hub for all egress traffic for all accounts. AWS Private Links will be used to connect VPCs in each account to a redundant NAT gateway in the central VPC. This solution is similar to option A but differs in the use of AWS Private Links instead of VPC peering. Using Private Links would avoid the issue of latency and throughput but could also increase the cost of data transfer. Managing Private Links can also be complex as it would require creating multiple connections between accounts, which could be a challenge to set up and maintain.\nOption D: Create a Transit Gateway in one central AWS account that will work as a hub and spoke model to other accounts' VPCs using VPN attachments. Set up a Central VPC for egress traffic with redundant NAT gateways.\nThis solution proposes using a Transit Gateway to centralize traffic as in option B, but instead of using VPC attachments, VPN attachments will be used to connect VPCs in each account to the Transit Gateway. A central VPC will be set up in the central account to handle egress traffic with redundant NAT gateways. This solution offers the benefits of centralizing traffic into a single VPC while reducing the complexity of managing VPC peering connections between accounts. VPN attachments may be a more affordable option than VPC attachments, but they could also impact performance and increase complexity.\nIn summary, Option B appears to be the most effective solution that meets the requirements of reducing costs and managing egress traffic while providing flexibility for future growth of new accounts. However\n\n"
}, {
  "id" : 243,
  "question" : "A company is hosting a MySQL database in AWS using the AWS RDS service.\nA Read Replica has been created, and reports are run off the Read Replica database to offload the reads.\nBut at certain times, the reports show stale data.\nWhat could be the possible reason behind this?\n",
  "answers" : [ {
    "id" : "df769793b1e648e2b9ddb9e8390db910",
    "option" : "The Read Replica has not been created properly.",
    "isCorrect" : "false"
  }, {
    "id" : "da7351b8d9574cbba7920a69fb3a82d5",
    "option" : "The backup of the original database has not been set properly.",
    "isCorrect" : "false"
  }, {
    "id" : "2033982c0f254904a682dfc14d402dae",
    "option" : "This is due to the replication lag.",
    "isCorrect" : "true"
  }, {
    "id" : "0bc01f7ef66f4f8b8f66008f0f7a8dfd",
    "option" : "The Multi-AZ feature is not enabled.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAn AWS Whitepaper on the caveat for reading Replicas is given below which must be considered by architects.\nRead Replicas are separate database instances that are replicated asynchronously.\nAs a result, they are subject to replication lag and might be missing some of the latest transactions.\nApplication architects need to consider which queries have the tolerance to slightly stale data.\nThose queries can be executed on a Read Replica, while the rest should run on the primary node.\nRead Replicas may also not accept any write queries.\nFor more information on AWS Cloud best practices, please visit the following URL-\nhttps://d1.awsstatic.com/whitepapers/AWS_Cloud_Best_Practices.pdf\n\nWhen a MySQL database is hosted on AWS using the RDS service, a Read Replica can be created to offload the read traffic from the primary database. This can help improve performance and reduce the load on the primary database. However, at times, the reports run on the Read Replica may show stale data, which could be due to several reasons.\nOption A - The Read Replica has not been created properly: This option is unlikely to be the cause of the problem as creating a Read Replica is a simple process and can be easily verified.\nOption B - The backup of the original database has not been set properly: This option is also unlikely to be the cause of the problem as the backup process does not directly affect the Read Replica. However, if the backup is not set up properly, it could lead to data loss, which could cause the data on the Read Replica to be stale.\nOption C - This is due to replication lag: This is the most likely reason for the problem. Replication lag is the time it takes for changes made on the primary database to be replicated to the Read Replica. During this time, the data on the Read Replica may be stale. Replication lag can be caused by several factors, such as network latency, database workload, or insufficient resources on the Read Replica. It's important to monitor the replication lag and ensure that it stays within an acceptable range.\nOption D - The Multi-AZ feature is not enabled: This option is also unlikely to be the cause of the problem as the Multi-AZ feature is not directly related to the Read Replica. The Multi-AZ feature is used to provide high availability by automatically replicating the primary database to a standby database in a different Availability Zone.\nTo summarize, the most likely reason for the stale data on the Read Replica is replication lag. It's important to monitor the replication lag and ensure that it stays within an acceptable range. Additionally, other factors such as network latency, database workload, or insufficient resources on the Read Replica should be considered and addressed if necessary.\n\n"
}, {
  "id" : 244,
  "question" : "A company plans to use SQS queue and AWS Lambda to leverage the serverless aspects of the AWS Cloud.\nThere is a requirement to send a message to an SQS queue by each invocation of AWS Lambda.\nWhat should be required to achieve this?\n",
  "answers" : [ {
    "id" : "c89232bb9eb74f5aae832f15fb64b9ae",
    "option" : "The queue must be a FIFO queue.",
    "isCorrect" : "false"
  }, {
    "id" : "6a886142f0e249828068f7de1ee5e072",
    "option" : "An IAM Role must have the required permissions.",
    "isCorrect" : "true"
  }, {
    "id" : "eb3d7a63842f47d1a3f34a4056a799ad",
    "option" : "The code for Lambda must be written in C#.",
    "isCorrect" : "false"
  }, {
    "id" : "8cac34b78fed40beb34d6ffcf33ed48b",
    "option" : "An IAM Group must have the required permissions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nWhile working with AWS Lambda functions, if there is a need to access other resources, you ensure that an IAM role is in place.\nThe IAM role will have the required permissions to access the SQS queue.\nFor more information on AWS IAM Roles, please visit the following URL-\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\n\nThe correct answer is B. An IAM Role must have the required permissions.\nExplanation: AWS Lambda is a serverless computing service that allows users to run code without having to manage servers. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables users to decouple and scale microservices, distributed systems, and serverless applications.\nTo send a message to an SQS queue by each invocation of AWS Lambda, the Lambda function needs to have the necessary permissions to interact with the SQS queue. These permissions can be granted by creating an IAM role with the appropriate permissions and then assigning this role to the Lambda function.\nOption A, The queue must be a FIFO queue, is not correct as the type of SQS queue does not impact the ability to send messages from Lambda to SQS.\nOption C, The code for Lambda must be written in C#, is not correct as AWS Lambda supports various programming languages such as Node.js, Python, Java, Go, and more.\nOption D, An IAM Group must have the required permissions, is not correct as IAM groups are not used to grant permissions to Lambda functions.\nTherefore, the correct answer is B, An IAM Role must have the required permissions.\n\n"
}, {
  "id" : 245,
  "question" : "You have enabled CloudTrail logs for your company's AWS account and store the logs in an S3 bucket.\nIn addition, the IT Security department has mentioned that the logs need to be encrypted.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "a151def708f14676b39a145f226d675e",
    "option" : "Enable SSL certificates for the CloudTrail logs.",
    "isCorrect" : "false"
  }, {
    "id" : "31787b38dca04a009ea135eab071d93d",
    "option" : "There is no need to do anything since the logs are encrypted by default.",
    "isCorrect" : "true"
  }, {
    "id" : "33e19d47e9284d3cb2cf2526a18d593f",
    "option" : "Enable Server-Side Encryption for the trail.",
    "isCorrect" : "false"
  }, {
    "id" : "ff514bd4d67c4f859e86b9102f1ce78e",
    "option" : "Enable Server-Side Encryption for the destination S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nBy default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE)\nYou can also choose to encrypt your log files with an AWS Key Management Service (AWS KMS) key.\nYou can store your log files in your bucket for as long as you want.\nYou can also define Amazon S3 lifecycle rules to archive or delete log files automatically.\nIf you want notifications about log file delivery and validation, you can set up Amazon SNS notifications.\nFor more information on how CloudTrail works, please visit the following URL-\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\n\nThe correct answer is option D: Enable Server-Side Encryption for the destination S3 bucket.\nExplanation:\nAWS CloudTrail provides audit logs for monitoring AWS account activity. When you enable CloudTrail, AWS logs all events that occur in your account and delivers the logs to an S3 bucket that you specify. By default, the logs are stored in an S3 bucket without any encryption, but the IT Security department has mandated that the logs should be encrypted.\nTo encrypt the CloudTrail logs, you need to enable Server-Side Encryption for the S3 bucket where you store the logs. You can enable Server-Side Encryption in the following ways:\nUse Amazon S3-Managed Keys (SSE-S3): With this option, S3 encrypts your data at the object level using Advanced Encryption Standard (AES) 256-bit encryption. SSE-S3 encrypts data at rest and decrypts it when it is accessed. Use AWS Key Management Service-Managed Keys (SSE-KMS): With this option, AWS manages the encryption keys, and you can manage access to the keys using AWS Key Management Service (KMS). SSE-KMS offers additional features such as auditing, key rotation, and fine-grained access controls. Use Server-Side Encryption with Customer-Provided Keys (SSE-C): With this option, you provide your encryption keys to AWS to encrypt and decrypt the data. SSE-C gives you full control over your encryption keys and the encryption process.\nSo, to summarize, the correct way to encrypt CloudTrail logs in an S3 bucket is by enabling Server-Side Encryption for the destination S3 bucket using either SSE-S3, SSE-KMS, or SSE-C. Therefore, option D is the correct answer.\n\n"
}, {
  "id" : 246,
  "question" : "A company is developing a big data analytics application that requires access to an object immediately after a write.\nA big amount of objects will be stored for the application.\nYou need to design a proper service to store the data.\nIs AWS S3 service suitable?\n",
  "answers" : [ {
    "id" : "96fa8fa5f61d468694041f65b72a25cd",
    "option" : "No. AWS uses an eventual consistency model.",
    "isCorrect" : "false"
  }, {
    "id" : "a419eccd778c461bb024145caba79bf3",
    "option" : "Yes. AWS S3 delivers strong read-after-write and list consistency automatically.",
    "isCorrect" : "true"
  }, {
    "id" : "b6bb59ed3d6c48418558e2967c4a9fb9",
    "option" : "No. AWS S3 is not suitable for big data applications.",
    "isCorrect" : "false"
  }, {
    "id" : "496b8059d8ce42bc9178e1d3466d922c",
    "option" : "No. AWS S3 is not cost-effective to store a big amount of data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon S3 delivers strong read-after-write consistency and is suitable for this scenario.\nPlease check the reference in https://aws.amazon.com/cn/s3/consistency/.\n\nThe correct answer is B: Yes. AWS S3 delivers strong read-after-write and list consistency automatically.\nAmazon Simple Storage Service (S3) is a highly scalable, durable, and secure object storage service offered by Amazon Web Services (AWS). S3 can store and retrieve any amount of data, from anywhere on the web, and is designed to provide 99.999999999% (11 9's) durability of objects over a given year. It is ideal for storing and retrieving large amounts of data for big data applications.\nWhen data is written to an S3 bucket, it becomes available for retrieval immediately. S3 guarantees strong read-after-write consistency for new object uploads and overwrites. This means that as soon as a write operation has been completed on an object, subsequent read requests for that object will return the updated data. In addition, S3 provides list consistency, which means that after a new object has been created or a previous object has been deleted, subsequent list requests will reflect the change.\nS3 is designed to be highly scalable, allowing it to handle large amounts of data without any issues. It is also cost-effective and offers a variety of storage classes with different pricing tiers to suit different data access patterns and retention requirements.\nTherefore, the correct answer is B. AWS S3 delivers strong read-after-write and list consistency automatically, making it suitable for storing data for big data applications that require immediate access to data after a write operation.\n\n"
}, {
  "id" : 247,
  "question" : "Your company has enabled CORS on your S3 bucket to allow cross-origin resource sharing.\nIn the CORS configuration, you need to specify the values for the \"AllowedMethod\" element.\nIn terms of configuring the \"AllowedMethod\" element, which statement is correct?\n",
  "answers" : [ {
    "id" : "d56c6b31326a4960a4dcf7cdcfb865c1",
    "option" : "You can only enable CORS through AWS S3 CLI.",
    "isCorrect" : "false"
  }, {
    "id" : "7b2be53867e8403782960eaab826e838",
    "option" : "You can only configure the CORS configuration with a YAML configuration file.",
    "isCorrect" : "false"
  }, {
    "id" : "5002ae0c57554a529f12ebb817ed4c1a",
    "option" : "These methods are supported: GET, PUT, POST, DELETE, and HEA.",
    "isCorrect" : "true"
  }, {
    "id" : "5510685b99b54795ac57b6a3f7c99f68",
    "option" : "Only OPTIONS and CONNECT are supported for CORS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer: C.\nOnly these methods are supported: GET, PUT, POST, DELETE, and HEAD.Refer to page 163 on the below link under the topic \"AllowedMethod Element.\"\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/s3-dg.pdf\nReference for CORS:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\nhttps://aws.amazon.com/blogs/aws/amazon-S3-cross-origin-resource-sharing/\n\nThe correct statement regarding configuring the \"AllowedMethod\" element in the CORS configuration for an S3 bucket is:\nC. These methods are supported: GET, PUT, POST, DELETE, and HEAD.\nExplanation:\nCross-origin resource sharing (CORS) is a mechanism that allows web browsers to make cross-domain requests to a server, which is outside of the domain from which the web page originated. To enable CORS on an S3 bucket, you need to configure a CORS policy for the bucket.\nYou can configure the CORS policy either through the AWS Management Console, AWS CLI, or programmatically using an SDK. When configuring the CORS policy, you specify the values for the \"AllowedMethod\" element. The \"AllowedMethod\" element specifies the HTTP methods that are allowed to access the S3 bucket from another domain.\nThe \"AllowedMethod\" element can accept one or more HTTP methods that you want to allow for cross-origin requests. The supported HTTP methods are GET, PUT, POST, DELETE, and HEAD. These methods allow you to perform a range of actions on the resources in the S3 bucket.\nTo configure the \"AllowedMethod\" element in the CORS configuration for an S3 bucket, you need to specify the methods that you want to allow from other domains. For example, if you want to allow GET and PUT requests, you can configure the \"AllowedMethod\" element as follows:\nphp Copy code <AllowedMethod>GET</AllowedMethod> <AllowedMethod>PUT</AllowedMethod>\nThis configuration allows GET and PUT requests to be made from another domain. The other domains are specified in the \"AllowedOrigin\" element of the CORS configuration.\nIn summary, the correct statement regarding configuring the \"AllowedMethod\" element in the CORS configuration for an S3 bucket is that these methods are supported: GET, PUT, POST, DELETE, and HEAD.\n\n"
}, {
  "id" : 248,
  "question" : "Your company has a MySQL database deployed in an on-premise datacenter.\nYou start using AWS Database Migration Service (AWS DMS) to migrate the database to AWS RDS.\nYou have a replication instance in DMS to run the migration task.\nWhich of the following options assign permissions that determine who is allowed to manage AWS DMS resources?\n",
  "answers" : [ {
    "id" : "ceb6c105b1a94566b0a8b37275b01ebe",
    "option" : "Transport Layer Security (TLS) connections between AWS DMS and local datacenter.",
    "isCorrect" : "false"
  }, {
    "id" : "e8fa3d64513a4c4e8af788a764ba782c",
    "option" : "AWS Key Management Service (AWS KMS) encryption used by the replication instance.",
    "isCorrect" : "false"
  }, {
    "id" : "084b8c6e66144f49a58fc40c6ba9a81c",
    "option" : "AWS Identity and Access Management (IAM) policies.",
    "isCorrect" : "true"
  }, {
    "id" : "bd41e14afc8847a2859c7d37c00bd881",
    "option" : "Network Control Lists (NCLs) in VPC subnets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThere are multiple approaches to secure the AWS DMS resources.\nDetails can be found in.\nhttps://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html.\nOption A is incorrect: TLS protects the network layer.\nHowever, it does not assign permissions.\nOption B is incorrect: KMS encrypts data.\nBut it cannot manage the permissions.\nOption C is CORRECT: Because IAM policies can be assigned to IAM users who need to manage DMS resources.\nOption D is incorrect: NCL acts as a firewall to protect the VPC subnets.\nBut it does not determine who can access DMS resources.\n\nThe correct answer to this question is C. AWS Identity and Access Management (IAM) policies.\nExplanation: AWS DMS is a managed service provided by AWS to migrate data to and from various databases, data warehouses, and other data stores. AWS DMS allows the user to migrate data in real-time and also to perform one-time migrations. The user can create a replication instance in AWS DMS to run the migration task.\nAWS Identity and Access Management (IAM) is a web service that provides security controls for AWS services. IAM allows the user to manage access to AWS resources by creating IAM policies. These policies specify who is allowed to perform specific actions on specific resources.\nIn this scenario, the user needs to assign permissions that determine who is allowed to manage AWS DMS resources. To do this, the user needs to create IAM policies that define the actions that are allowed or denied on AWS DMS resources. These policies can be attached to IAM users, groups, or roles.\nOption A, Transport Layer Security (TLS) connections between AWS DMS and the local datacenter, is incorrect because TLS is a security protocol used for securing network communication between two endpoints. It is not used to manage AWS DMS resources.\nOption B, AWS Key Management Service (AWS KMS) encryption used by the replication instance, is incorrect because AWS KMS is a service that is used to create and manage encryption keys used to encrypt data stored in AWS services. It is not used to manage AWS DMS resources.\nOption D, Network Control Lists (NCLs) in VPC subnets, is incorrect because Network Control Lists (NCLs) are used to control traffic at the subnet level. They are not used to manage AWS DMS resources.\nTherefore, the correct answer is option C, AWS Identity and Access Management (IAM) policies, which are used to manage access to AWS DMS resources.\n\n"
}, {
  "id" : 249,
  "question" : "Your company stores a large amount of archive data in expensive on-premises storage systems.\nYou need to move the data to low-cost storage such as Amazon S3 Glacier.\nWhich of the following tools is the most suitable to simplify and automate the data transfer from on-premises to S3 Glacier?\n",
  "answers" : [ {
    "id" : "a86f54ef540e46fea2d7df8283efc8aa",
    "option" : "AWS DataSync",
    "isCorrect" : "true"
  }, {
    "id" : "53aa7a33055540a7a1f1e5380850fa7a",
    "option" : "Server Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "845bc33b849249afb0b2b8619b5fbee3",
    "option" : "Database Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "e2335fac96df44b0a714a22ab8d3864d",
    "option" : "Direct Connect.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS DataSync should be selected to simplify moving data between on-premises storage and AWS services such as S3 Glacier.\nCheck https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html.\nOption A is CORRECT: With AWS DataSync, users can create a task to specify the data source and destination and then configure the data transfer.\nOption B is incorrect: Because Server Migration Service is used to migrate on-premise servers such as VMware.\nOption C is incorrect: Database Migration Service is used to migrate a database instead of local storage.\nOption D is incorrect: Direct Connect sets up a dedicated network connection to AWS.\nHowever, it does not automate the data transfer to AWS.\n\nThe most suitable tool to simplify and automate the data transfer from on-premises to Amazon S3 Glacier is AWS DataSync.\nAWS DataSync is a fully-managed data transfer service that simplifies and accelerates moving large amounts of data online between on-premises storage and AWS storage services like Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server.\nAWS DataSync is designed to simplify the data transfer process and provides automated scheduling, bandwidth optimization, and network resilience. With AWS DataSync, you can easily move large amounts of data from on-premises storage to Amazon S3 Glacier without having to worry about managing the underlying infrastructure, network connectivity, or security.\nServer Migration Service is a tool designed to migrate on-premises servers to Amazon EC2 instances and does not support the transfer of data to Amazon S3 Glacier. Database Migration Service is designed to migrate databases to AWS and does not support the transfer of unstructured data like files to Amazon S3 Glacier. Direct Connect is a networking service that provides dedicated network connections between on-premises infrastructure and AWS, but it is not a data transfer service.\n\n"
}, {
  "id" : 250,
  "question" : "You use an Amazon S3 bucket as the origin for a CloudFront distribution.\nTo restrict access to S3 content, you create an Origin Access Identity (XXXX1234567890) in CloudFront and associate it with the distribution.\nYou need to modify the S3 bucket policy so that users cannot bypass CloudFront to access the S3 files.\nWhich of the following options contains the correct S3 bucket policy statement?\n",
  "answers" : [ {
    "id" : "577f199188a645cca23d2b0cbb8feeec",
    "option" : "{ \"Effect\": \"Deny\", \"Principal\": { \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity XXXX1234567890\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::aws-example-bucket/*\" }",
    "isCorrect" : "false"
  }, {
    "id" : "a868cbd6ac284137a822c9b043584854",
    "option" : "{ \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::cloudfront:user/XXXX1234567890\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::aws-example-bucket\" }",
    "isCorrect" : "false"
  }, {
    "id" : "df082142feab487bb0181a654ce8c994",
    "option" : "{ \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity XXXX1234567890\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::aws-example-bucket/*\" }",
    "isCorrect" : "true"
  }, {
    "id" : "a3525d9a7f8f4ef8ad4f11e049b9173d",
    "option" : "{ \"Effect\": \"Deny\", \"NotPrincipal\": { \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity XXXX1234567890\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::aws-example-bucket\" }",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - C.\nThe S3 bucket policy should allow the \"s3:GetObject\" action if the Principal comes from the CloudFront Origin Access Identity.\nDetails can be found in https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_notprincipal.html.\nOption A is incorrect: Because the \"Effect\" should be \"Allow\" instead of \"Deny.\"\nOption B is incorrect: Both \"Principal\" and \"Resource\" are incorrect according to the above reference.\nOption C is CORRECT: Because with this bucket policy, \"s3:GetObject\" action is only allowed for the CloudFront OAI.\nOption D is incorrect: The bucket policy denies the action if the Principal is NOT the Origin Access Identity.\nHowever, it still does not allow the action if the Principal comes from the CloudFront OAI.\nAn explicit Allow is required.\nBesides, the \"Resource\" field is incorrect.\n\nSure, I'd be happy to explain this in detail!\nThe scenario here is that you have an S3 bucket that is serving as the origin for a CloudFront distribution. You have also created an Origin Access Identity (OAI) in CloudFront and associated it with the distribution to restrict access to S3 content. Now you need to modify the S3 bucket policy to ensure that users cannot bypass CloudFront to access the S3 files.\nLet's go through each of the answer options to determine which one contains the correct S3 bucket policy statement:\nOption A:\njson Copy code {     \"Effect\": \"Deny\",     \"Principal\": {         \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity XXXX1234567890\"     },     \"Action\": \"s3:GetObject\",     \"Resource\": \"arn:aws:s3:::aws-example-bucket/*\" }\nThis policy statement denies access to any user with the specified Principal (in this case, the CloudFront OAI) to the S3 bucket resources specified by the Resource field. The Action field specifies the type of action that is being denied (in this case, s3:GetObject). This policy statement appears to be correctly configured to prevent users from bypassing CloudFront to access the S3 files.\nOption B:\njson Copy code {     \"Effect\": \"Allow\",     \"Principal\": {         \"AWS\": \"arn:aws:iam::cloudfront:user/XXXX1234567890\"     },     \"Action\": \"s3:GetObject\",     \"Resource\": \"arn:aws:s3:::aws-example-bucket\" }\nThis policy statement allows access to the S3 bucket resources specified by the Resource field to any user with the specified Principal (in this case, a CloudFront user with an unknown identity). This policy statement is not correctly configured to prevent users from bypassing CloudFront to access the S3 files.\nOption C:\njson Copy code {     \"Effect\": \"Allow\",     \"Principal\": {         \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity XXXX1234567890\"     },     \"Action\": \"s3:GetObject\",     \"Resource\": \"arn:aws:s3:::aws-example-bucket/*\" }\nThis policy statement allows access to the S3 bucket resources specified by the Resource field to any user with the specified Principal (in this case, the CloudFront OAI). The Action field specifies the type of action that is being allowed (in this case, s3:GetObject). This policy statement appears to be correctly configured to prevent users from bypassing CloudFront to access the S3 files.\nOption D:\njson Copy code {     \"Effect\": \"Deny\",     \"NotPrincipal\": {         \"AWS\": \"arn:aws:iam::cloudfront:user/CloudFront Origin Access Identity XXXX1234567890\"     },     \"Action\": \"s3:GetObject\",     \"Resource\": \"arn:aws:s3:::aws-example-bucket\" }\nThis policy statement denies access to the S3 bucket resources specified by the Resource field to any user that is not the specified NotPrincipal (in this case, the CloudFront OAI). The Action field specifies the type of action that is being denied (in this case, s3:GetObject). This policy statement is not correctly configured to prevent users from bypassing CloudFront to access the S3 files.\nTherefore, the correct answer is option A, as it is the only option that appears to be correctly configured to prevent users from bypassing CloudFront to access the S3 files.\n\n"
}, {
  "id" : 251,
  "question" : "You are working in a financial company, and you need to establish the network connections between on-premises data centers and AWS VPCs.\nThe connectivity needs to be secure with IPsec connections.\nA predictable and high-performance network is also required over private lines.\nWhich of the following methods would you select?\n",
  "answers" : [ {
    "id" : "f2444a0eef144b85b34fceceec65beaf",
    "option" : "AWS Direct Connect + VPN",
    "isCorrect" : "true"
  }, {
    "id" : "8cb1580d1b434fcfa458b58c6c55b9c8",
    "option" : "AWS Managed VPN",
    "isCorrect" : "false"
  }, {
    "id" : "1b0b1f8caa1a4fceae2088e28d49e3ac",
    "option" : "AWS Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "1328650541af4e6ab8de84f12b32c327",
    "option" : "Software VPN.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS has provided several connectivity options, according to https://d1.awsstatic.com/whitepapers/aws-amazon-vpc-connectivity-options.pdf.\nVPN is needed as it creates an IPsec connection.\nAWS Direct Connect is also required because it establishes a private connection with high bandwidth throughput.\nOption A is CORRECT: Because with AWS Direct Connect + VPN, you can create IPsec-encrypted private connections.\nOption B is incorrect: Because only with VPN, the network performance cannot be guaranteed.\nOption C is incorrect: Because AWS Direct Connect does not use IPsec protocol.\nOption D is incorrect: Because Software VPN is still based on the internet instead of a dedicated network.\n\nFor secure and predictable network connectivity between on-premises data centers and AWS VPCs with high-performance and private lines, the best option is to use AWS Direct Connect in combination with a VPN connection.\nAWS Direct Connect is a dedicated network connection service that establishes a private, high-bandwidth and low-latency connection between an on-premises data center and AWS VPCs. This connection provides a more reliable and consistent network performance than using the public internet.\nTo enhance the security of the connection, you can establish an IPsec VPN connection over the AWS Direct Connect link. This will encrypt the data being transmitted over the network, providing a secure and private connection between your on-premises infrastructure and your AWS resources.\nThe correct answer is A. AWS Direct Connect + VPN. AWS Managed VPN is also an option, but it's less ideal because it uses public internet connections which can be less reliable and less secure than a dedicated private line. Software VPN is also an option but it's not as performant as the Direct Connect + VPN solution because it depends on the public internet to transfer data, which can cause fluctuations in network performance.\n\n"
}, {
  "id" : 252,
  "question" : "Your IT Supervisor is worried about users accidentally deleting objects from an S3 bucket.\nWhich of the following can help prevent accidental deletion of objects in an S3 bucket? (Select Three)\n",
  "answers" : [ {
    "id" : "c462f57900dc4a0aba645fa56250e1ab",
    "option" : "Enable encryption for the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "4a9170bc1dce4f428fae4c759b359b57",
    "option" : "Enable MFA Delete on the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "9f661b48d5e34b509890a9729a3af126",
    "option" : "Enable Versioning on the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "2a883cc4f4844a21a498cabe052e6f1b",
    "option" : "Enable IAM Roles on the S3 bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers - B, C, and D.\nAWS Documentation mentions the following.\nWhen a user performs a DELETE operation on an object, subsequent simple (un-versioned) requests will no longer retrieve the object.\nHowever, all versions of that object will continue to be preserved in your Amazon S3 bucket and can be retrieved or restored.\nVersioning's MFA Delete capability, which uses multi-factor authentication, can be used to provide an additional layer of security.\nBy default, all requests to your Amazon S3 bucket require your AWS account credentials.\nIf you enable Versioning with MFA Delete on your Amazon S3 bucket, two forms of authentication are required to permanently delete a version of an object: your AWS account credentials and valid six-digit code and serial number from an authentication device in your physical possession.\nFor more information on the features of S3, please visit the following URL-\nhttps://aws.amazon.com/s3/faqs/\nTo know more about Option D, Please refer to the below AWS Document.\nhttps://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/\n\nSure, I'd be happy to explain the options available to prevent accidental deletion of objects in an S3 bucket.\nA. Enable encryption for the S3 bucket: Enabling encryption for an S3 bucket can help protect the confidentiality of data stored in the bucket, but it does not prevent accidental deletion of objects in the bucket.\nB. Enable MFA Delete on the S3 bucket: MFA Delete is a feature that requires a user to provide two forms of authentication to delete an object in an S3 bucket. This adds an extra layer of security and helps prevent accidental deletions. MFA Delete can be enabled at the bucket level, but it requires users to have an MFA device to delete objects, which may not be practical for all use cases.\nC. Enable Versioning on the S3 bucket: Enabling versioning on an S3 bucket allows you to keep multiple versions of an object in the bucket. When an object is deleted, the previous version of the object remains in the bucket, providing a way to recover accidentally deleted data. Versioning can be enabled at the bucket level, and once enabled, it cannot be disabled.\nD. Enable IAM Roles on the S3 bucket: IAM Roles are a way to grant permissions to AWS services and users. However, they do not prevent accidental deletion of objects in an S3 bucket.\nTherefore, the correct answers to the question are B, C, and D. Enabling MFA Delete, Versioning, and IAM Roles can all help prevent accidental deletion of objects in an S3 bucket.\n\n"
}, {
  "id" : 253,
  "question" : "A company has been using AWS cloud services for six months and has just finished a security review. Which of the following is considered a best practice in the security pillar of the well-architected framework?\n",
  "answers" : [ {
    "id" : "7b67095b959d4b0d89d24ee3dca7ee4a",
    "option" : "Using the root user to create all-new user accounts, at any time",
    "isCorrect" : "false"
  }, {
    "id" : "4221c7ec82bb416494836f4bf9cf5187",
    "option" : "Monitoring and using alerts using CloudTrail and CloudWatch",
    "isCorrect" : "true"
  }, {
    "id" : "ec7080cf34354e1c9d059ff804c44b76",
    "option" : "Assigning Private IP address ranges to VPCs that do not overlap",
    "isCorrect" : "false"
  }, {
    "id" : "02a1c1b82d184468b3e26bac42822f2f",
    "option" : "Designing the system using elasticity to meet changes in demand.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption B is correct.\nMonitoring and alerting for key metrics and events are the best practices of the Security pillar.\nOption A is incorrect.\nFor the root user, you should follow the best practice of using this login only to create another, an initial set of IAM users and groups for longer-term identity management operations.\nOption C is incorrect.\nNon-overlapping Private IP addresses are in the Reliability pillar.\nD.\nDesign using elasticity to meet demand is in the Performance Efficiency pillar (Design for Cloud Operations).\nReferences:\nhttps://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf\nhttps://d1.awsstatic.com/whitepapers/architecture/AWS-Security-Pillar.pdf\n\nSure, I'd be happy to explain the answer in detail!\nThe well-architected framework is a set of best practices developed by AWS to help organizations design and operate reliable, secure, efficient, and cost-effective systems in the cloud. The framework consists of five pillars: operational excellence, security, reliability, performance efficiency, and cost optimization.\nBased on the given scenario, the question is asking for the best practice in the security pillar. The security pillar of the well-architected framework includes several best practices, such as:\nImplementing a strong identity foundation: This involves using AWS Identity and Access Management (IAM) to manage user access and permissions, enforcing least privilege, and using multi-factor authentication (MFA) where appropriate. Protecting data: This includes using encryption to protect data at rest and in transit, implementing secure key management, and using secure protocols for communication. Securing network traffic: This involves using Amazon Virtual Private Cloud (VPC) to isolate and control network traffic, using security groups and network access control lists (ACLs) to restrict access, and using bastion hosts and jump boxes to access resources securely. Managing security events: This includes monitoring and logging all API calls and changes to AWS resources using AWS CloudTrail, using Amazon GuardDuty to detect and respond to threats, and using AWS Config to audit and assess compliance.\nOut of the given options, the best practice in the security pillar of the well-architected framework is option B: Monitoring and using alerts using CloudTrail and CloudWatch. This option aligns with the managing security events best practice and involves using AWS CloudTrail to log all API calls and changes to AWS resources and using Amazon CloudWatch to monitor and set up alerts on system metrics, logs, and events.\nOption A: Using the root user to create all-new user accounts, at any time, is not a best practice as it violates the principle of least privilege and increases the risk of unauthorized access.\nOption C: Assigning Private IP address ranges to VPCs that do not overlap is a best practice in the networking pillar of the well-architected framework, but it is not directly related to the security pillar.\nOption D: Designing the system using elasticity to meet changes in demand is a best practice in the performance efficiency pillar of the well-architected framework, but it is not directly related to the security pillar.\n\n"
}, {
  "id" : 254,
  "question" : "A company has a Redshift Cluster defined in AWS.\nThe IT Operations team has ensured that both automated and manual snapshots are in place.\nSince the cluster will be run for a couple of years, Reserved Instances have been purchased.\nThere has been a recent concern about the cost being incurred by the cluster.\nWhich step should be carried out to minimize the costs being incurred by the cluster?\n",
  "answers" : [ {
    "id" : "5857da9f7bb24f92a950b7a976d45554",
    "option" : "Delete the manual snapshots.",
    "isCorrect" : "true"
  }, {
    "id" : "25ad9cd6832447378b0858ec6abeacec",
    "option" : "Set the retention period of the automated snapshots to 35 days.",
    "isCorrect" : "false"
  }, {
    "id" : "f44a7eee72dd4e3697fbd9b0f1fdb19c",
    "option" : "Choose to use Spot Instances instead of Reserved Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "05b7be4237fa4c3984b4ec0132c13fd0",
    "option" : "Choose to use Instance store volumes to store the cluster data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following.\nRegardless of whether you enable automated snapshots, you can take a manual snapshot whenever you want at any time.\nBy default, manual snapshots are retained indefinitely, even after you delete your cluster.\nYou can specify the retention period when you create a manual snapshot or change the retention period by modifying the snapshot.\nTo reduce cost, we can delete the manual snapshots that are taken, if any.\nRefer link: https://aws.amazon.com/redshift/pricing/\nAutomated snapshots are automatically deleted within the period of 1(Least) to 35(Max) days (Based on the retention period settings)\nSo we have to take care of the Manual snapshots instead of Automated snapshots.\nAmazon Redshift never deletes Manual snapshots automatically, like how it does for Automatic Snapshots.\nOption B is incorrect because the Automated snapshots are automatically deleted within the period of 1(Least) to 35(Max) days.\nThis will NOT have any impact on the cost.\nOption C is incorrect because SPOT instances are not a good fit for snapshot and the question does not mention one of these.\nOption D is incorrect because instance store volumes are volatile and therefore is NOT a good fit for the snapshot.\nFor more information on working with Snapshots, please visit the following URL-\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\n\n\nThe most suitable step to minimize costs incurred by the Redshift cluster would be to set the retention period of the automated snapshots to 35 days.\nExplanation:\nReserved Instances are a cost-effective option for running Amazon Redshift Clusters for an extended period of time. However, there are additional measures that can be taken to minimize costs. Here are explanations for why the other options (A, C, and D) are not ideal in this scenario:\nOption A: Delete the manual snapshots. This option should not be chosen as manual snapshots serve as an additional layer of data protection and can help recover data in the case of accidental deletion or data corruption.\nOption B: Set the retention period of the automated snapshots to 35 days. Automated snapshots are a Redshift feature that enables the periodic backup of data stored in a Redshift cluster. The default retention period for automated snapshots is 1 day. However, this can be increased to up to 35 days to retain more snapshots. Reducing the retention period to a shorter time frame will reduce the number of snapshots stored, and therefore, reduce costs.\nOption C: Choose to use Spot Instances instead of Reserved Instances. Spot Instances can help to reduce AWS compute costs, but they are not recommended for a mission-critical workload like a Redshift cluster. Spot Instances can be interrupted at any time if AWS needs the capacity back, which may result in data loss or extended downtime.\nOption D: Choose to use Instance store volumes to store the cluster data. Instance store volumes are local disks that are physically attached to the Amazon EC2 instance. These disks are designed for temporary data storage and can be lost if the instance fails or is stopped. Using instance store volumes to store cluster data is not recommended because they do not provide the same level of durability and availability as Amazon EBS volumes.\nIn conclusion, setting the retention period of the automated snapshots to 35 days is the best option to minimize costs while maintaining data protection and availability.\n\n"
}, {
  "id" : 255,
  "question" : "A website is hosted on two EC2 instances that sit behind an Elastic Load Balancer.\nThe website's response time has been slowed down dramatically, and customers are placing fewer orders due to the wait time.\nTroubleshooting showed that one of the EC2 instances had been failed and only one instance is running now.\nWhat is the best course of action to prevent this from happening in the future?\n",
  "answers" : [ {
    "id" : "34e9328c55844ca299f697ba30658afc",
    "option" : "Change the instance size to the maximum available to compensate for the failure.",
    "isCorrect" : "false"
  }, {
    "id" : "58d4a1e9488c40e6a04b8ac49da1cd1c",
    "option" : "Use CloudWatch to monitor the VPC Flow Logs for the VPC, the instances are deployed in.",
    "isCorrect" : "false"
  }, {
    "id" : "7424c857362c49bd82419f17996f7882",
    "option" : "Configure the ELB to perform health checks on the EC2 instances and implement auto-scaling.",
    "isCorrect" : "true"
  }, {
    "id" : "1103a4c95a04446d9a86809ff1a88892",
    "option" : "Replicate the existing configuration in several regions for failover.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption C is correct.\nUsing the elastic load balancer to perform health checks will determine whether or not to remove a non-performing or underperforming instance, and have the auto-scaling group launch a new instance.\nOption A is incorrect.\nIncreasing the instance size doesn't prevent the failure of one or both the instances.\nTherefore the website can still become slow or unavailable.\nB.\nMonitoring the VPC flow logs for the VPC will capture the VPC traffic, not the traffic for the EC2 instance.\nYou would need to create a flow log for a network interface.\nD.\nReplicating the same two instance deployment may not prevent instances of failure and could still result in the website becoming slow or unavailable.\nReferences:\nhttps://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#working-with-flow-logs\n\nThe correct answer is C. Configure the ELB to perform health checks on the EC2 instances and implement auto-scaling.\nExplanation:\nThe scenario described suggests that the website has become slower due to one of the EC2 instances failing, and now only one instance is running. To prevent this from happening in the future, we need to ensure that the EC2 instances are highly available and can handle any increase in traffic.\nOption A is not the best course of action as increasing the instance size may provide a temporary solution, but it does not ensure high availability in case of an instance failure. Additionally, it can be more expensive to run a larger instance than to use multiple smaller instances.\nOption B suggests using CloudWatch to monitor VPC flow logs, but this does not address the issue of instance failure and does not provide a solution for handling traffic surges.\nOption D suggests replicating the website in multiple regions for failover, which can improve availability but can also be complex to set up, more expensive, and adds latency to the system.\nOption C, on the other hand, is the best course of action as it addresses the issue of instance failure and provides a solution for handling traffic surges. By configuring the Elastic Load Balancer to perform health checks on the EC2 instances, it can detect when an instance has failed and redirect traffic to healthy instances. Additionally, by implementing auto-scaling, we can ensure that additional instances are launched to handle traffic surges automatically. This solution provides both high availability and scalability, ensuring that the website can handle any increase in traffic and prevent slow response times in the future.\n\n"
}, {
  "id" : 256,
  "question" : "A company currently hosts a lot of data on its On-premises location.\nIt wants to start storing backups of this data with low latency access to data on AWS.\nHow could this be achieved most efficiently?\n",
  "answers" : [ {
    "id" : "bee19cda2bb0407a8d77a533c37d5187",
    "option" : "Create EBS Volumes and store the data.",
    "isCorrect" : "false"
  }, {
    "id" : "9ca87f3a7ed84958ac0cebc2c7585e97",
    "option" : "Create EBS Snapshots and store the data.",
    "isCorrect" : "false"
  }, {
    "id" : "4b9f888263664e42a24d44e1d09fbe4c",
    "option" : "Make use of Storage Gateway Stored volumes.",
    "isCorrect" : "false"
  }, {
    "id" : "129c0cb675ad43df83006e0bbf3b8081",
    "option" : "Make use of Amazon Glacier.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nFor storing backups of data from an on-premises location with low-latency access on AWS, the most efficient solution would be to make use of AWS Storage Gateway Stored Volumes, which is option C.\nAWS Storage Gateway is a hybrid storage service that enables companies to securely and seamlessly integrate their on-premises applications with cloud storage. It supports three types of storage volumes:\nCached Volumes Stored Volumes Tape Volumes\nCached volumes store data on-premises and cache frequently accessed data in AWS, while stored volumes store all data locally and asynchronously backup to AWS. Tape volumes provide a virtual tape library that can be backed up to AWS.\nIn this scenario, stored volumes would be the most appropriate option. Stored volumes store all data locally and asynchronously backup to AWS. The data is stored on-premises in a Storage Gateway appliance that connects to AWS over the internet, allowing for low-latency access to data on AWS. Stored volumes are ideal for frequently accessed data, providing low-latency access to the data stored on AWS.\nOption A, creating EBS volumes and storing the data, would not be the best solution as EBS volumes are block-level storage devices used to store data on EC2 instances, which are not ideal for backing up data.\nOption B, creating EBS snapshots and storing the data, is also not the best solution. EBS snapshots are point-in-time backups of EBS volumes and are primarily used to create backups of EC2 instances or EBS volumes. While it's possible to use EBS snapshots for backups of on-premises data, the process is not efficient, and the data would not be easily accessible.\nOption D, making use of Amazon Glacier, is not the most efficient solution for this scenario. Amazon Glacier is a low-cost storage service for data archiving and long-term backups. The data stored in Amazon Glacier is not readily accessible, and it can take several hours to retrieve the data, which is not ideal for backups requiring low-latency access.\nTherefore, the best solution for this scenario would be to use AWS Storage Gateway Stored Volumes, as it provides low-latency access to data stored in AWS, making it ideal for frequently accessed backups.\n\n"
}, {
  "id" : 257,
  "question" : "As a solutions architect, you need to design a multi-tier architecture for a project in AWS.\nThe application contains three tiers: the frontend layer, business logic layer, and storage layer.\nAuto Scaling groups implement the front-end and business logic layers.\nAWS RDS is selected as the storage layer.\nThe front-end layer is in public subnets.\nThe backend and data storage layers are in private subnets.\nWhich of the following options is NOT a feature of this architecture?\n",
  "answers" : [ {
    "id" : "f2acd15b17a847a48fc42ce59b5bc63b",
    "option" : "Each layer is modularized and managed independently.",
    "isCorrect" : "false"
  }, {
    "id" : "ddcd27eafa8549148e3c0368f73e897d",
    "option" : "The backend and data storage are not exposed to the internet and protected in private subnets.",
    "isCorrect" : "false"
  }, {
    "id" : "17ec592dd1744a0f96e0be0be812dd4c",
    "option" : "In this architecture, users do not have to configure the desired number of servers or capabilities.",
    "isCorrect" : "true"
  }, {
    "id" : "9665015bbddc4d1c87a0ad37005f4c45",
    "option" : "Frontend and backend servers can be configured in different availability zones for high availability.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe multi-tier architecture in AWS has several features such as high availability and scalability, fault-tolerant, and security.\nThe question asks for the option that is NOT a feature.\nOption C should be selected as the users need to configure the min/max/desired number of servers in ASGs and the read/write capabilities for the RDS database.\nOption A is incorrect because the application is divided into three components, and each component works independently.\nOption B is incorrect because the backend and data storage are located in private subnets.\nUsers can only reach the frontend layer.\nOption C is CORRECT Refer to the above explanations.\nThis architecture is not a serverless architecture.\nOption D is incorrect because the servers in ASG can be put into several availability zones, even if one AZ has an outage, the servers in other AZs can still work as normal.\nSample Architecture.\nReference:\nhttps://docs.aws.amazon.com/whitepapers/latest/serverless-multi-tier-architectures-api-gateway-lambda/single-page-application.html\n\n\nThe given architecture for the project in AWS consists of three tiers: frontend layer, business logic layer, and storage layer. The frontend and business logic layers are implemented using Auto Scaling groups and are placed in public subnets, while the backend and data storage layers are placed in private subnets and use AWS RDS as the storage layer.\nOption A: Each layer is modularized and managed independently.\nThis is a feature of the given architecture. Each layer is designed as a separate module and can be managed independently. This allows for easy maintenance and scalability of the application.\nOption B: The backend and data storage are not exposed to the internet and protected in private subnets.\nThis is a feature of the given architecture. The backend and data storage layers are placed in private subnets, which are not exposed to the internet. This ensures that the data stored in the backend layer is secure and protected from external threats.\nOption C: In this architecture, users do not have to configure the desired number of servers or capabilities.\nThis is not a feature of the given architecture. Users still need to configure the desired number of servers and capabilities in the Auto Scaling groups for the frontend and business logic layers.\nOption D: Frontend and backend servers can be configured in different availability zones for high availability.\nThis is a feature of the given architecture. The frontend and backend servers are placed in different availability zones to ensure high availability and redundancy. In case one availability zone fails, the other availability zone can take over, and the application can continue to run smoothly.\nTherefore, the correct answer is option C. In this architecture, users still need to configure the desired number of servers and capabilities in the Auto Scaling groups for the frontend and business logic layers.\n\n"
}, {
  "id" : 258,
  "question" : "Currently, you have a set of Lambda functions that have business logic embedded in them.\nYou want customers to have the ability to call these functions via HTTPS.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "959c5b15765c4ed08677fe4ed9b093c7",
    "option" : "Use the API Gateway and provide integration with the AWS Lambda functions.",
    "isCorrect" : "true"
  }, {
    "id" : "24ed6ba9b9c2440e8fdd9ef1fa4a560d",
    "option" : "Enable HTTP access on the AWS Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "c11d698e992b4bcc900d2aac8752dffe",
    "option" : "Add EC2 Instances with an API server installed. Integrate the server with AWS Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "6fe2259fa34946f0adbb5e4aef9965cb",
    "option" : "Use S3 websites to make calls to the Lambda functions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAn API Gateway provides the ideal access to your back end services via APIs.\nFor more information on the API Gateway service, please visit the following URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html\n\n\nThe correct answer is A. Use the API Gateway and provide integration with the AWS Lambda functions.\nExplanation: AWS Lambda is a serverless computing service that allows you to run code without provisioning or managing servers. However, you need a way to expose your Lambda functions to external clients, such as mobile applications, web applications, or other systems. One way to achieve this is by using Amazon API Gateway, which is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\nAPI Gateway allows you to create RESTful APIs or WebSocket APIs that act as a front door for your Lambda functions. You can configure your API Gateway to map incoming requests to specific Lambda functions, and then transform the output of the Lambda functions into the format that the client expects. You can also use API Gateway to manage API versions, throttling, caching, and access control.\nTo enable HTTPS access to your Lambda functions via API Gateway, you need to follow these steps:\nCreate a new REST API or WebSocket API in API Gateway. Define the resources, methods, and integration points for your API. Configure the integration type as AWS Lambda, and select the Lambda function you want to invoke. Map the request parameters and payloads to the input of the Lambda function, and map the output of the Lambda function to the response. Enable HTTPS access by creating a custom domain name or by using the default domain name provided by API Gateway. Deploy your API to a stage, which is a snapshot of your API configuration and resources at a specific point in time. Test your API using the API Gateway console, a client application, or a tool such as Postman.\nBy using API Gateway to expose your Lambda functions, you can separate the business logic from the presentation layer, and you can also scale your API independently of your Lambda functions. You can also apply security policies, such as API keys, IAM roles, and OAuth 2.0, to restrict access to your API and protect your resources.\n\n"
}, {
  "id" : 259,
  "question" : "For a new application, you need to build up the logic tier and data storage tier in AWS.\nThe whole architecture needs to be serverless so that designers can quickly deploy the application without the need to manage servers.\nWhich of the following AWS services would you choose?\n",
  "answers" : [ {
    "id" : "c583118199e24495891032c43fb524f0",
    "option" : "Logic tier: â€œAmazon Cognito + Lambdaâ€. Data Storage tier: â€œAmazon RDSâ€",
    "isCorrect" : "false"
  }, {
    "id" : "0622b9c49b064579a35b0ae77e2f5901",
    "option" : "Logic tier: â€œAPI Gateway + Lambdaâ€. Data Storage tier: â€œAmazon DynamoDBâ€",
    "isCorrect" : "true"
  }, {
    "id" : "593260455d724c5a8d34ca08490e9f32",
    "option" : "Logic tier: â€œAPI Gateway + Lambdaâ€. Data Storage tier: â€œAmazon Redshiftâ€",
    "isCorrect" : "false"
  }, {
    "id" : "1908eedc2090454dab1e2256371e46e3",
    "option" : "Logic tier: â€œElastic Beanstalkâ€. Data Storage tier: â€œAmazon Auroraâ€",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe key to the question is that the architecture should be serverless.\nFor AWS serverless multi-tier architectures, please refer to https://d1.awsstatic.com/whitepapers/AWS_Serverless_Multi-Tier_Architectures.pdf.\nOption A is incorrect: Because Amazon Cognito is not suitable for the logic tier, and Amazon RDS is not a serverless service.\nOption B is CORRECT: Because both API Gateway/Lambda and Amazon DynamoDB are serverless so that the process of deploying servers is simplified.\nOption C is incorrect: Amazon Redshift is a data warehouse service and is not serverless.\nOption D is incorrect: Because Elastic Beanstalk utilizes EC2 instances as its computing resources are not a serverless service.\n\nThe best option for building a serverless architecture for a new application would be option B. The logic tier can be built using Amazon API Gateway and AWS Lambda, while the data storage tier can be built using Amazon DynamoDB.\nHere's a more detailed explanation of each option:\nA. Logic tier: â€œAmazon Cognito + Lambdaâ€. Data Storage tier: â€œAmazon RDSâ€ Amazon Cognito is an authentication service that can be used to manage user sign-up, sign-in, and access control. It is not a suitable choice for building a logic tier. AWS Lambda can be used to run serverless code, but it is not designed to build the logic tier. Amazon RDS is a managed relational database service, which means it requires servers to be managed. This option is not a good fit for a serverless architecture.\nB. Logic tier: â€œAPI Gateway + Lambdaâ€. Data Storage tier: â€œAmazon DynamoDBâ€ Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. AWS Lambda is a serverless compute service that can run code in response to events and automatically manage the underlying compute resources. Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. This combination of services is a good fit for building a serverless architecture for a new application.\nC. Logic tier: â€œAPI Gateway + Lambdaâ€. Data Storage tier: â€œAmazon Redshiftâ€ Amazon Redshift is a fully managed data warehouse service that allows for fast querying and analysis of data using SQL. However, it is not suitable for a serverless architecture, as it requires servers to be managed. Amazon DynamoDB would be a better choice for a data storage tier.\nD. Logic tier: â€œElastic Beanstalkâ€. Data Storage tier: â€œAmazon Auroraâ€ AWS Elastic Beanstalk is a service that makes it easy to deploy, manage, and scale applications in the AWS Cloud. However, it requires servers to be managed, which is not a good fit for a serverless architecture. Amazon Aurora is a MySQL and PostgreSQL compatible relational database built for the cloud, but it also requires servers to be managed. This option is not a good fit for building a serverless architecture for a new application.\nIn conclusion, option B is the best choice for building a serverless architecture for a new application, as it uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB to build the logic and data storage tiers.\n\n"
}, {
  "id" : 260,
  "question" : "A Solutions Architect is designing a solution to store and archive corporate documents.\nThe Architect has determined that Amazon Glacier is the right solution.\nData has to be retrieved within 3-5 hrs as directed by the management. Which feature in Amazon Glacier could be helpful to meet this requirement and ensure cost-effectiveness?\n",
  "answers" : [ {
    "id" : "99209813e72e4282864a588346638ed2",
    "option" : "Vault Lock",
    "isCorrect" : "false"
  }, {
    "id" : "69ad422c7b1e4ce7ab8f0e00eb302b9c",
    "option" : "Expedited retrieval",
    "isCorrect" : "false"
  }, {
    "id" : "9ac0f290034b4ce0b96f62bbc35c95dc",
    "option" : "Bulk retrieval",
    "isCorrect" : "false"
  }, {
    "id" : "ca68c221cd1349548d5cec647e9a96bb",
    "option" : "Standard retrieval.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A - Vault Lock.\nThis feature of Amazon Glacier allows you to lock your vault with various compliance controls designed to support such long-term records retention.\nDue to this reason, this is not the correct answer.\nOption B - Expedited retrieval.\n- It allows you to quickly access your data when occasional urgent requests are required for a subset of archives.\nThe data is available within 1 - 5 minutes.\nSince our requirement is 3 - 5 hours, we do not need to use this option.\nOption C - Bulk retrieval.\n- They are the lowest-cost retrieval option, enabling you to retrieve large amounts of data within 5 - 12hours.\nDue to this reason, it is not the correct answer.\nAWS Documentation mentions the following on Standard retrievals.\nStandard retrievals are a low-cost way to access your data within just a few hours.\nFor example, you can use Standard retrievals to restore backup data, retrieve archived media content for same-day editing or distribution, or pull and analyze logs to drive business decisions within hours.\nFor more information on Amazon Glacier retrievals, please visit the following URL-\nhttps://aws.amazon.com/glacier/faqs/#dataretrievals\n\nAmazon Glacier is a cloud-based storage service provided by AWS that is primarily designed for data archiving and long-term storage of infrequently accessed data. Since the storage is designed for long-term retention of data, it is cost-effective but also has a longer retrieval time. However, AWS provides a few retrieval options that allow users to retrieve their data from Glacier at varying speeds based on their needs.\nIn this scenario, the Solutions Architect has determined that Amazon Glacier is the right solution for storing and archiving corporate documents. However, the data needs to be retrieved within 3-5 hours as directed by management. This means that the standard retrieval option would not be suitable as it may take several hours to days to retrieve the data. Therefore, we need to look for an option that can retrieve the data faster while also being cost-effective.\nOption A: Vault Lock - This feature is used to enforce compliance controls for data retention. It is not related to data retrieval speed, so it is not relevant to this scenario.\nOption B: Expedited retrieval - This option allows data to be retrieved in 1-5 minutes by paying a higher fee. This option is suitable for scenarios where the data needs to be retrieved quickly, but it is not cost-effective for large amounts of data.\nOption C: Bulk retrieval - This option allows data to be retrieved within 5-12 hours and is designed for larger datasets. The cost is lower than the expedited retrieval option, making it more cost-effective for larger datasets. However, it is still not suitable for data that needs to be retrieved within 3-5 hours.\nOption D: Standard retrieval - This option allows data to be retrieved within 3-5 hours, making it suitable for this scenario. The cost is lower than the expedited retrieval option, making it more cost-effective for smaller datasets. Therefore, the correct answer is D.\nIn conclusion, the Solutions Architect could use the standard retrieval option in Amazon Glacier to meet the data retrieval requirements while ensuring cost-effectiveness.\n\n"
}, {
  "id" : 261,
  "question" : "You have an application running in AWS.\nThe application has the frontend EC2 servers deployed in a public subnet.\nAnd the backend EC2 servers are hosted in a private subnet.\nThe frontend servers can communicate with the backend servers properly.\nOne day there is an issue in production, and you need to login to one backend EC2 instance to troubleshoot.\nThe connection to the backend servers should be made most securely.\nWhich of the following options is the most secure one to access the instance?\n",
  "answers" : [ {
    "id" : "6cc32d23f41e47b2a7db45ffedfab31a",
    "option" : "Generate a new SSH key and use the key to SSH to the backend instance.",
    "isCorrect" : "false"
  }, {
    "id" : "5c272fa39221436693c6cad79cfb05f9",
    "option" : "SSH to one of the frontend instances and then SSH to the backend.",
    "isCorrect" : "false"
  }, {
    "id" : "0b12f693d9564e0491b86426e4fa97ef",
    "option" : "Modify the security group of the instance to allow the SSH inbound traffic from your IP address. Revert the change after you do not need the access.",
    "isCorrect" : "false"
  }, {
    "id" : "7d9d26bbbd2e4b20a1f62d79a1d82d3b",
    "option" : "Configure a dedicated bastion host and SSH to the bastion host. Then SSH to the backend instance from the bastion.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nBastion host is the most suitable one in this scenario.\nIt is a dedicated server for users to securely SSH to the backend servers in the private subnet.\nOption A is incorrect: Since the backend instance is in the private subnet, users cannot directly SSH to it.\nOption B is incorrect: Because normally, front-end servers should not have SSH access to the backend.\nIn this scenario, we should use a dedicated jump host rather than the frontend instances.\nOption C is incorrect: Same as option A.\nThe backend instance does not have a public IP, and you cannot directly SSH to it from your server.\nOption D is CORRECT: Through the bastion host, the SSH connection can be safely established and monitored.\n\nIn this scenario, the backend EC2 instances are hosted in a private subnet and can only be accessed from within the VPC. To access the instance most securely, we need to consider security, accessibility, and ease of use. Here are the options provided in the question:\nA. Generate a new SSH key and use the key to SSH to the backend instance.\nThis option is relatively secure as it involves generating a new SSH key to access the backend instance. However, it may not be the most convenient option as we have to generate a new SSH key and manage it. Also, it assumes that SSH is already enabled on the instance.\nB. SSH to one of the frontend instances and then SSH to the backend.\nThis option is not recommended because it involves accessing the backend instance through the frontend instance, which could expose the backend instance to security risks. Also, it assumes that SSH is already enabled on the frontend and backend instances.\nC. Modify the security group of the instance to allow the SSH inbound traffic from your IP address. Revert the change after you do not need the access.\nThis option is not recommended because it involves modifying the security group of the instance to allow SSH inbound traffic from a specific IP address. This could pose a security risk as it exposes the backend instance to the internet. Additionally, it can be a cumbersome process to change the security group back to its original settings.\nD. Configure a dedicated bastion host and SSH to the bastion host. Then SSH to the backend instance from the bastion.\nThis is the most secure and recommended option. A bastion host is a specially configured EC2 instance that is used to securely access instances in a private subnet. By configuring a bastion host, we can establish a secure SSH connection to the bastion host and then access the backend instance from the bastion host. This approach ensures that the backend instance is not directly exposed to the internet, and it provides an additional layer of security. Additionally, it can be easier to manage and control access to the bastion host.\nTherefore, the most secure option to access the instance would be to configure a dedicated bastion host and SSH to the bastion host. Then SSH to the backend instance from the bastion.\n\n"
}, {
  "id" : 262,
  "question" : "Your application has two tiers in AWS: the frontend layer and the backend layer.\nThe frontend includes an Auto Scaling group deployed in a public subnet.\nThe backend Auto Scaling group is located in another private subnet.\nThe backend instances should only allow the incoming traffic from the frontend ASG through a custom port.\nFor the backend security group, how would you configure the source in its inbound rule?\n",
  "answers" : [ {
    "id" : "9b94a0f493194ad4a6258a4ae0b5a8e4",
    "option" : "Configure the frontend security group ID as the source.",
    "isCorrect" : "true"
  }, {
    "id" : "3f8cc4e63ea34bc5a5acce0e756b7df5",
    "option" : "Configure the public subnet IP range as the source.",
    "isCorrect" : "false"
  }, {
    "id" : "f28d3b37183c49108573fa67f8df46de",
    "option" : "Configure the frontend Auto Scaling group ARN as the source.",
    "isCorrect" : "false"
  }, {
    "id" : "a863ca645f2f4f98962f553932ad9d9c",
    "option" : "Configure the frontend Auto Scaling launch configuration as the source.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nRefer to https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html#security-group-rules for how to configure security group rules.\nOption A is CORRECT: By configuring the frontend security group as the source, any frontend instances that have the specified security group are allowed to access the backend.\nOption B is incorrect: Other instances in this subnet can also access the backend.\nThis option is not as good as option A.Option C is incorrect: Because Auto Scaling group ARN cannot be configured in the source of a security group inbound rule.\nOption D is incorrect: Because the launch configuration cannot be configured in the source.\n\nThe correct answer is A. Configure the frontend security group ID as the source.\nExplanation: To allow incoming traffic from the frontend Auto Scaling Group (ASG) to the backend ASG, we need to configure the inbound rules for the security group associated with the backend ASG. The security group is a virtual firewall that controls the incoming and outgoing traffic for the instances associated with it.\nSince the frontend ASG is in a public subnet, it is associated with a security group that allows incoming traffic from the Internet. To enable traffic from the frontend ASG to the backend ASG, we need to add an inbound rule to the security group associated with the backend ASG. This rule should allow incoming traffic from the security group associated with the frontend ASG, not from the public subnet IP range or any other source.\nTherefore, the correct answer is to configure the frontend security group ID as the source in the inbound rule for the backend ASG security group. This will ensure that only instances in the frontend ASG are allowed to communicate with instances in the backend ASG over the custom port.\nOption B is incorrect because it would allow any traffic coming from the public subnet IP range, not just traffic originating from the frontend ASG.\nOption C and D are incorrect because they refer to specific AWS resources (ARN and launch configuration), which are not relevant for configuring inbound rules for security groups. Instead, we need to specify the source security group ID in the inbound rule to allow traffic only from the instances associated with that security group.\n\n"
}, {
  "id" : 263,
  "question" : "A company has a business-critical standard three-tier highly available web portal in AWS.\nThe web tier runs on different types of Amazon EC2 family.\nThe application tier runs on one set ofEC2 family, and the database tier runs on Amazon RDS.\nThe company's leadership team asked the Solutions Architect to reduce the costs of this solution without any impact on the company's business.\nWhich of the following options is the most cost-effective while maintaining reliability and a high availability?\n",
  "answers" : [ {
    "id" : "00f31cfc6d2d43c085c4482d8d236745",
    "option" : "Use Compute Savings Plans for the web tier, EC2 Instance Savings Plans for the application tier, and Reserved Instances for the database tier.",
    "isCorrect" : "true"
  }, {
    "id" : "392ba175d9354d6c90251f11ad7ded0e",
    "option" : "Use Compute Savings Plans for the web tier, EC2 Instance Savings Plans for the application tier and the database tier.",
    "isCorrect" : "false"
  }, {
    "id" : "671fff1f78ef49769fb823f85bc5ab6b",
    "option" : "Use EC2 Instance Savings Plans for the web tier, Compute Savings Plans for the application tier and the database tier.",
    "isCorrect" : "false"
  }, {
    "id" : "ef14c7dbd6ad4dea93ca7c94069099c3",
    "option" : "Use EC2 Instance Savings Plans for the web tier, Compute Savings Plans for the application tier, and OnDemand Instances for the database tier.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nCompute Savings plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, region, and OS.\nThe web tier consists of multiple types of instance family.\nEC2 Instance Savings Plans automatically reduce cost on the selected instance family in that region regardless of AZ, size, OS, or tenancy.\nSince application tiers use the same set of instance families.\nThe savings plan doesn't support the RDS.\nSo, Reserved instances are the best option.\nOption B is incorrect because Savings plans don't support RDS.\nTo Reserved option is the best option.\nOption C is incorrect because EC2 Instance Savings Plans for the web tier don't support multiple instance families.\nOption D is incorrect because On-demand instances won't save cost for the long run.\nReferences:\nhttps://aws.amazon.com/savingsplans/\nhttps://go.forrester.com/blogs/aws-savings-plans-what-they-are-and-why-you-should-care/\n\nThe question is asking for the most cost-effective way to reduce the costs of a highly available three-tier web portal in AWS, without impacting business operations.\nFirstly, let's understand what the different types of pricing plans mean:\nOn-Demand Instances: This is the most flexible option, where you pay for compute capacity by the hour or second with no long-term commitments or upfront payments. It is suitable for applications with short-term, spiky, or unpredictable workloads that cannot be interrupted. Reserved Instances (RIs): This is a billing discount applied to EC2 instances that are purchased for a one-time upfront fee for a 1- or 3-year term commitment. The more you pay upfront and the longer the term commitment, the greater the discount. It is suitable for steady-state or predictable usage applications. Savings Plans: Savings Plans is a flexible pricing model that offers savings of up to 72% on Amazon EC2 and Fargate usage. Savings Plans offer significant savings over On-Demand prices by committing to a consistent amount of usage, measured in dollars per hour, for a 1- or 3-year term. You can use them to reduce costs on any Amazon EC2 instance, regardless of region, instance family, OS, or tenancy.\nBased on the above options, we can eliminate options D, as it suggests using On-Demand instances for the database tier, which is a business-critical application and needs to be highly available and reliable. So, it is not the best option.\nOption A suggests using Compute Savings Plans for the web tier, EC2 Instance Savings Plans for the application tier, and Reserved Instances for the database tier. This option is a good choice because Compute Savings Plans offer savings on any instance family, which is suitable for the web tier that runs on different types of EC2 families. EC2 Instance Savings Plans can be applied to a specific EC2 instance family, which is suitable for the application tier running on a single set of EC2 family. Reserved Instances provide a higher discount but need upfront payment, which is suitable for the database tier that requires steady-state and predictable usage.\nOption B suggests using Compute Savings Plans for the web tier, EC2 Instance Savings Plans for the application tier, and database tier. This option is similar to option A but does not suggest using Reserved Instances for the database tier, which could provide a higher discount.\nOption C suggests using EC2 Instance Savings Plans for the web tier, Compute Savings Plans for the application tier, and database tier. This option is not the best choice because EC2 Instance Savings Plans are only suitable for a specific EC2 instance family, which is not suitable for the web tier running on different types of EC2 families.\nTherefore, the most cost-effective option while maintaining reliability and a high availability is option A: Use Compute Savings Plans for the web tier, EC2 Instance Savings Plans for the application tier, and Reserved Instances for the database tier.\n\n"
}, {
  "id" : 264,
  "question" : "A small company started using EBS backed EC2 instances for the cost improvements over their own running servers.\nThe company's policy is to stop the development servers over the weekend and restart them next week.\nThe first time when the servers were brought back, none of the developers were able to SSH into them.\nWhat did the server most likely overlook?\n",
  "answers" : [ {
    "id" : "561f881302eb441cb47325bf6df02ff2",
    "option" : "The associated Elastic IP address has changed and the SSH configurations were not updated.",
    "isCorrect" : "false"
  }, {
    "id" : "a1a4aa71461b4e20bf4e2e15324ebd3b",
    "option" : "The security group for a stopped instance needs to be reassigned after the start.",
    "isCorrect" : "false"
  }, {
    "id" : "1e526c2378c9448aa32149d6c606a5dd",
    "option" : "The public IPv4 address has changed on the server start and the SSH configurations were not updated.",
    "isCorrect" : "true"
  }, {
    "id" : "9fc47741105f4036b8ac4e16be5df5e2",
    "option" : "EBS backed EC2 instances could not be stopped and were automatically terminated.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption C is correct.\nThe instance retains its private IPv4 addresses and any IPv6 addresses when stopped and started.\nAWS releases public IPv4 address and assigns a new one when it is stopped &amp; started.\nOption A is incorrect.\nAn EC2 instance retains its associated Elastic IP addresses.\nOption B is incorrect.\nSecurity groups do not need to be reassigned to instances that are restarted.\nOption D is incorrect.\nEBS backed instances are the only instance type that can be started and stopped.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html\n\nWhen an EC2 instance is launched, it is assigned a public IP address or an Elastic IP address (EIP) if one is allocated to it. If an EIP is associated with an instance, it remains associated with that instance until it is explicitly disassociated. However, public IP addresses are dynamic and can change every time an instance is stopped and started again.\nIn this scenario, the company is stopping their development servers over the weekend to save costs. When the servers were started again, the developers were not able to SSH into them, indicating that the servers had lost their connection to the internet or had their IP addresses changed.\nOption A states that the associated Elastic IP address has changed and the SSH configurations were not updated. This could be a possible reason for the issue. If the instance was assigned an Elastic IP address, it would remain associated with that instance even if it was stopped and started again. However, if the Elastic IP address was disassociated or allocated to another instance, the developers would not be able to SSH into the server.\nOption B states that the security group for a stopped instance needs to be reassigned after the start. This is not a valid reason for the issue as security groups are not affected when an instance is stopped and started again. The security group settings are preserved even after the instance is stopped and started.\nOption C states that the public IPv4 address has changed on the server start and the SSH configurations were not updated. This could be a possible reason for the issue. If the instance was assigned a public IP address, it would have a new IP address each time it was stopped and started again. If the developers were trying to SSH into the instance using its previous IP address, they would not be able to connect.\nOption D states that EBS backed EC2 instances could not be stopped and were automatically terminated. This is incorrect as EBS-backed instances can be stopped and started again without being terminated.\nBased on the information provided, Option A or Option C could be the most likely reason for the issue. The company needs to check if an Elastic IP address was associated with the instance and if it was disassociated or allocated to another instance. They also need to check if the instance was assigned a public IP address and if its IP address has changed after it was stopped and started again. Once the issue is identified, they need to update the SSH configurations accordingly to connect to the instance.\n\n"
}, {
  "id" : 265,
  "question" : "You created your own VPC and subnet in AWS and launched an instance in that subnet.\nOn attaching an Internet Gateway to the VPC, you see that the instance has a public IP.\nThe route table is shown below. Still, the instance cannot reach the Internet.\nWhat changes are required to be made to the route table to ensure that the issue is resolved?\n\n",
  "answers" : [ {
    "id" : "e202a6b883be4fc7a4bc7a5935a7b9a2",
    "option" : "Add the following entry to the route table â€“ Destination as 0.0.0.0/0 and Target as Internet Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "58051556dae247c0a5dc039871cf622b",
    "option" : "Modify the above route table â€“ Destination as 10.0.0.0/16 and Target as Internet Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "041badaa2c2c4b53ba0103d4676e2c58",
    "option" : "Add the following entry to the route table â€“Destination as 10.0.0.0/16 and Target as Internet Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "676bbc85f1c04ade9fbc028c99d2a3d6",
    "option" : "Add the following entry to the route table - Destination as 0.0.0.0/16 and Target as Internet Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThe route table needs to be modified as shown below to ensure that routes from the Instance reach the Internet.\nHence by default, all other options become invalid.\nFor more information on Route Tables, please visit the URL below.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html\n\n"
}, {
  "id" : 266,
  "question" : "A company plans to deploy a business-critical application on Amazon EC2 instances.\nThis application should be scalable in a clustered environment.\nIt supports high-performance computing (HPC) with consistency latency and highest throughput. How can this be achieved by meeting all the requirements above?\n",
  "answers" : [ {
    "id" : "9437edb06e4e4623a4214d7d0a1560fe",
    "option" : "Configure the application on EC2 instances with enhanced networking enabled using the Elastic Network Adapter (ENA).",
    "isCorrect" : "false"
  }, {
    "id" : "74eaddb5f59543b280ae5f8a5cb9b012",
    "option" : "Configure the application on EC2 instances with enhanced networking enabled using the Elastic Fabric Adapter (EFA).",
    "isCorrect" : "true"
  }, {
    "id" : "ee8f58a37113401eb5a0af802a628db7",
    "option" : "Configure the application on EC2 instances with enhanced networking enabled using the Elastic Network Interface (ENI).",
    "isCorrect" : "false"
  }, {
    "id" : "9acaa40c5c1243a4a26dc9ff69970ed6",
    "option" : "Configure the application on EC2 instances with enhanced networking enabled using the Elastic Network (EN).",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect.\nAlthough ENA supports low latency and high throughput, it doesn't meet high-performance requirements.\nOption B is CORRECT.\nEFA supports low latency and high throughput with high-performance computing with the scalability, flexibility, and elasticity provided by the AWS Cloud.\nOption C is incorrect as ENI is used for creating management networks and deploying low budget solutions.\nThis will not meet HPC and low latency requirements.\nOption D is incorrect.\nHowever, ENA or EN supports low latency and high throughput.\nBut it doesn't meet high-performance requirements.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html\nhttps://medium.com/@varunmanik1/aws-eni-vs-en-vs-efa-22250513590f\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-ena.html\n\nTo achieve scalability in a clustered environment and support high-performance computing with consistent latency and highest throughput, the best option would be to configure the application on EC2 instances with enhanced networking enabled using the Elastic Fabric Adapter (EFA).\nEFA is a network interface designed for HPC workloads and provides low-latency, high-bandwidth connectivity between EC2 instances. It enables a direct, high-speed communication between instances without going through the network infrastructure. EFA provides the lowest network latency in EC2 and is the most performant option for HPC applications.\nThe other options provided, ENA, ENI, and EN, are also network interfaces that offer enhanced networking features for EC2 instances. However, they may not provide the same level of performance and low latency that is required for a high-performance computing workload.\nTherefore, option B, Configure the application on EC2 instances with enhanced networking enabled using the Elastic Fabric Adapter (EFA), would be the best option to meet all the requirements for a scalable, high-performance computing workload on Amazon EC2 instances.\n\n"
}, {
  "id" : 267,
  "question" : "You create an Auto Scaling Group which is used to spin up instances on demand.\nAs an architect, you need to ensure that the instances are pre-installed with software when they are launched.\nWhat are the different ways to achieve this? (Select Two)\n",
  "answers" : [ {
    "id" : "343d0118cd394b4a94fe0f6da74020c1",
    "option" : "Add the software installation to the configuration for the Auto Scaling Group.",
    "isCorrect" : "false"
  }, {
    "id" : "ce4f18c0d4e5493292c15abd3fe1e516",
    "option" : "Add the scripts for the installation in the User data section.",
    "isCorrect" : "true"
  }, {
    "id" : "d27f4ff6d17a40fd9a776dabcb25eadb",
    "option" : "Create an AMI and then create a launch configuration.",
    "isCorrect" : "true"
  }, {
    "id" : "afbb9c4c7afb4596b5eb083ae7776eb7",
    "option" : "Ask the IT operations team to install the software as soon as the instance is launched.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - B and C.\nThe instance launch's User data section can be used to pre-configure software after the instance is initially booted.\nFor more information on User data, please visit the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html\nAlso, you can create an AMI or a golden image with the already installed software.\nThen create a launch configuration that can be used by that Auto Scaling Group.\nFor more information on AMIs, please visit the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nAs an AWS Solutions Architect, you can ensure that the instances launched by the Auto Scaling Group (ASG) have the required software pre-installed in two ways:\nAdd the software installation to the configuration for the Auto Scaling Group: In this method, you can create a custom Amazon Machine Image (AMI) with the required software installed and then configure the Auto Scaling Group to use that AMI. You can use any method to create the AMI such as EC2 Image Builder, AWS CLI, or AWS Management Console. After creating the AMI, update the launch configuration of the ASG to use this custom AMI instead of the default AMI. By doing so, each instance launched by the ASG will have the required software pre-installed. Add the scripts for the installation in the User data section: In this method, you can use the User data section of the EC2 instance to specify the commands to install the software. User data is a script that runs when the instance launches. You can add the required commands to install the software in the User data section while launching the instance. When the instance is launched, the script in the User data section runs automatically and installs the software. You can use any scripting language like Bash, PowerShell, or Python to write the installation script.\nOptions C and D are not correct: C. Create an AMI and then create a launch configuration: This option is similar to option A but is not entirely correct. The correct approach is to create a custom AMI with the required software installed and then use it in the ASG launch configuration. D. Ask the IT operations team to install the software as soon as the instance is launched: This option is not recommended because it is manual and error-prone. It may take time for the IT operations team to install the software, which can lead to delays and may not scale well.\n\n"
}, {
  "id" : 268,
  "question" : "You maintain a DynamoDB table that stores customers' subscription data.\nHigh availability is very important for the table.\nEven if there is an outage in an AWS region, the application should still access the data from other regions.\nWhich method would you take to achieve this requirement?\n",
  "answers" : [ {
    "id" : "353cb4357f8d46e1a4fb23f5102ede65",
    "option" : "Create a read replica in another region as a backup.",
    "isCorrect" : "false"
  }, {
    "id" : "682fbdae5ba1493ab2b8934ba95f39d2",
    "option" : "Configure a Multi-AZ backup for the DynamoDB table.",
    "isCorrect" : "false"
  }, {
    "id" : "64add4ba0a594d93ae75887abfddea12",
    "option" : "Configure a global table to use DynamoDB as a multi-region database.",
    "isCorrect" : "true"
  }, {
    "id" : "f54fb1119c344838affceb7227639185",
    "option" : "No actions required as DynamoDB is a global service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon DynamoDB global tables provide a solution for deploying a multi-region, multi-master database.\nThe reference can be found in this link- https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html.\nOption A is incorrect: Because DynamoDB does not have such a feature.\nOption B is incorrect: Because there is no need to configure Multi-AZ for a DynamoDB table, and it is highly available by default.\nOption C is CORRECT: With the DynamoDB global table, when one AWS region becomes unavailable, users can still access the same data in other regions.\nOption D is incorrect: Refer to option.\nC.\n\nThe best option to achieve high availability and fault tolerance for a DynamoDB table across regions is to configure a global table. Therefore, the correct answer is C.\nExplanation:\nDynamoDB is a managed NoSQL database service provided by Amazon Web Services (AWS), which is designed to provide high availability and fault tolerance. DynamoDB is a regional service, which means that it stores data within a single region. However, to ensure high availability and fault tolerance, DynamoDB offers different options, such as Multi-AZ and Global Tables.\nOption A is incorrect because creating a read replica in another region only provides a backup for read operations. It doesn't provide high availability for write operations. Additionally, read replicas are not synchronized in real-time, which means that there may be some delay between the original table and the read replica.\nOption B is also incorrect because Multi-AZ backup only replicates data within the same region. Multi-AZ backup provides high availability within a region but does not provide fault tolerance across regions.\nOption D is incorrect because although DynamoDB is a global service, it only stores data within a single region. Therefore, in the event of an outage in a region, the application cannot access the data from other regions.\nOption C is the correct answer because a global table is a fully managed solution that enables automatic replication of tables across multiple regions. When a global table is created, DynamoDB automatically creates replicas of the table in all selected regions. This provides high availability and fault tolerance for both read and write operations, as well as the ability to scale globally. As a result, if an outage occurs in one region, the application can still access the data from another region.\nIn conclusion, the best option to achieve high availability and fault tolerance for a DynamoDB table across regions is to configure a global table.\n\n"
}, {
  "id" : 269,
  "question" : "You have a set of IIS Servers running on EC2 Instances.\nYou want to collect and process the log files generated from these IIS Servers.\nWhich service would be ideal for utilizing the Big Data analytics in this scenario?\n",
  "answers" : [ {
    "id" : "c84bed0ec76e46629e2944837a5c1b69",
    "option" : "Amazon S3 for storing the log files and Amazon EMR for processing the log files.",
    "isCorrect" : "true"
  }, {
    "id" : "91752e7f24bc46c7b2d8d3ae319fc070",
    "option" : "Amazon S3 for storing the log files and EC2 Instances for processing the log files.",
    "isCorrect" : "false"
  }, {
    "id" : "aab368e2bc3c4bb2a2ba392b1de8dfbd",
    "option" : "Amazon EC2 for storing and processing the log files.",
    "isCorrect" : "false"
  }, {
    "id" : "037d9df1933e458a8780d37a76710b14",
    "option" : "Amazon DynamoDB to store the logs and EC2 for running custom log analysis scripts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon EMR is a managed cluster platform that simplifies running big data frameworks, such as Apache Hadoop and Apache Spark, on AWS to process and analyze vast amounts of data.\nUsing these frameworks and related open-source projects, such as Apache Hive and Apache Pig, you can process data for analytics purposes and business intelligence workloads.\nAdditionally, you can use Amazon EMR to transform and move a large amount of data into and out of other AWS data stores and databases, such as Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB.Options B and C are incorrect as it would be an overhead for EC2 Instances to process log files when you already have a ready-made service to help in this regard.\nOption D is incorrect because DynamoDB is not an ideal option to store log files.\nFor more information on EMR, please visit the URL below:\nhttp://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html\n\nThe ideal service for collecting and processing log files generated from IIS servers on EC2 instances is Amazon S3 for storing the log files and Amazon EMR for processing the log files.\nAmazon S3 is a highly scalable, secure, and durable object storage service that allows you to store and retrieve any amount of data from anywhere on the web. With Amazon S3, you can store log files generated from IIS servers on EC2 instances easily, and it provides an efficient way to access and manage the data.\nAmazon EMR (Elastic MapReduce) is a fully-managed big data processing service that makes it easy to process large amounts of data using open-source frameworks such as Apache Hadoop, Spark, and Presto. EMR is designed to handle large datasets, and it can quickly process and analyze log files generated from IIS servers on EC2 instances.\nUsing Amazon EMR, you can run big data analytics jobs on your log data using Hadoop, Spark, or other distributed processing frameworks. This allows you to extract valuable insights and patterns from your log files, which can help you optimize your IIS server performance, troubleshoot issues, and improve overall system efficiency.\nOption B, Amazon S3 for storing the log files and EC2 instances for processing the log files, is not as ideal as it requires more management overhead and configuration, as well as a significant amount of time and effort to set up and maintain the EC2 instances.\nOption C, using Amazon EC2 for storing and processing the log files, is not recommended because it is not an efficient way to store and process large amounts of data, and it requires more management overhead and configuration compared to using Amazon S3 and EMR.\nOption D, using Amazon DynamoDB to store the logs and EC2 for running custom log analysis scripts, is not recommended as DynamoDB is not ideal for storing large amounts of unstructured data such as log files, and it would require significant effort to develop custom log analysis scripts to process the data.\n\n"
}, {
  "id" : 270,
  "question" : "You need to ensure that new objects being uploaded to an S3 bucket are available in another region due to the criticality of the data hosted in the S3 bucket.\nHow could you achieve this in the easiest way possible?\n",
  "answers" : [ {
    "id" : "1de4a9af1aa14311a94d91bba340ac15",
    "option" : "Enable Cross-Region Replication for the bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "cae3f8d0ce444be19820bd9756b9faca",
    "option" : "Write a script to copy the objects to another bucket in the destination region.",
    "isCorrect" : "false"
  }, {
    "id" : "ab0748feb0dd41edb77ab6c2cee7a731",
    "option" : "Create an S3 snapshot in the destination region.",
    "isCorrect" : "false"
  }, {
    "id" : "f6722b8c3dc2453599409360723cbe47",
    "option" : "Enable versioning that will copy the objects to the destination region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following.\nCross-Region Replication is a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions.\nFor more information on Cross-Region Replication in the Simple Storage Service, please visit the URL below.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html\n\nThe easiest and recommended way to ensure that new objects being uploaded to an S3 bucket are available in another region is to enable Cross-Region Replication (CRR) for the bucket.\nCross-Region Replication is a feature provided by Amazon S3 that allows automatic and asynchronous replication of objects across different regions. Once enabled, every new object uploaded to the source bucket is automatically replicated to the destination bucket in another region. This ensures that the data in the source bucket is available in the destination bucket for redundancy, data durability, and data recovery purposes.\nTo enable Cross-Region Replication, you need to follow these steps:\nCreate a new S3 bucket in the destination region. Enable versioning on both the source and destination buckets. Create a replication rule that specifies the source bucket, the destination bucket, and the replication configuration. Enable the replication rule.\nOnce the replication rule is enabled, any new objects uploaded to the source bucket are automatically replicated to the destination bucket, where they are available for use. Note that Cross-Region Replication is an asynchronous process, so there may be some delay between when an object is uploaded to the source bucket and when it becomes available in the destination bucket.\nOption B (writing a script to copy objects to another bucket in the destination region) is not the recommended way to achieve this since it requires writing custom code, and it may not be reliable or scalable. Option C (creating an S3 snapshot in the destination region) is not a valid solution because S3 does not offer snapshot capability. Option D (enabling versioning that will copy objects to the destination region) is incorrect since enabling versioning does not automatically replicate objects to another region.\n\n"
}, {
  "id" : 271,
  "question" : "You want to use AWS to route a static website with a unique domain name that uses the format 'www.exaxyz.com'\nHow would you achieve this efficiently?\n",
  "answers" : [ {
    "id" : "cc67bdf33875429daccee1b8c929f487",
    "option" : "Register a domain with Route53 and use it to route requests to your website.",
    "isCorrect" : "true"
  }, {
    "id" : "7be163fe192f4e3e9fa2ce8157a02723",
    "option" : "Create an auto-scaling group of EC2 instances and manage the web hosting on these instances.",
    "isCorrect" : "false"
  }, {
    "id" : "bc245b631c8c48f5917dea5f95b1355b",
    "option" : "Create one large EC2 instance to host the website and replicate it in every region.",
    "isCorrect" : "false"
  }, {
    "id" : "6b48480ec1984cc884adc1b47397b730",
    "option" : "Create a Content Delivery Network (CDN) to deliver your images and files.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is correct.\nRoute53 is used to register a domain and route requests to a website.\nOption B is incorrect.\nHosting on EC2 is unnecessary here as server-side scripting is not needed, and S3 will scale automatically.\nOption C is incorrect.\nHosting on EC2 is unnecessary, and this particular implementation can lead to different configurations on each server.\nOption D is incorrect.\nA CDN will improve the delivery time of your files and pages to the customer but is not a hosting solution itself.\nAmazon Route 53 is a highly available and scalable Domain Name System (DNS) web service.\nYou can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking.\nIf you choose to use Route 53 for all three functions, be sure to follow the order below:\n1\nRegister domain names.\nYour website needs a name, such as exaxyz.com.\nRoute 53 lets you register a name for your website or web application, known as a domain name.\n2\nRoute internet traffic to the resources for your domain.\nWhen a user opens a web browser and enters your domain name (exaxyz.com) or subdomain name (acme.exaxyz.com) in the address bar, Route 53 helps connect the browser with your website or web application.\n3\nCheck the health of your resources.\nRoute 53 sends automated requests over the internet to a resource, such as a web server, to verify that it's reachable, available, and functional.\nYou also can choose to receive notifications when a resource becomes unavailable and choose to route internet traffic away from unhealthy resources.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-cloudfront-walkthrough.html\n\nThe most efficient way to route a static website with a unique domain name that uses the format www.exaxyz.com on AWS is by registering a domain with Route53 and using it to route requests to your website.\nOption A is the correct answer. Here is a detailed explanation of each option and why Option A is the most efficient one:\nA. Register a domain with Route53 and use it to route requests to your website: Route53 is a highly available and scalable DNS service offered by AWS. It allows you to register domain names, such as www.exaxyz.com, and route traffic to various AWS services, including Amazon S3, which can host static websites. By using Route53, you can ensure high availability, low latency, and automatic failover for your website.\nB. Create an auto-scaling group of EC2 instances and manage the web hosting on these instances: An auto-scaling group of EC2 instances can be used to host a website, but it is not the most efficient way to host a static website. EC2 instances are virtual servers that require manual scaling, maintenance, and monitoring. This option is also more expensive than using Amazon S3 for hosting a static website.\nC. Create one large EC2 instance to host the website and replicate it in every region: This option is also not the most efficient way to host a static website. Replicating a large EC2 instance in every region is time-consuming, expensive, and requires additional maintenance and monitoring.\nD. Create a Content Delivery Network (CDN) to deliver your images and files: A CDN is a network of servers distributed around the world that can cache and deliver your website's content, including images and files. While a CDN can improve website performance and availability, it is not the most efficient way to host a static website. A CDN can be used in conjunction with Amazon S3 for improved website performance and scalability.\nIn summary, registering a domain with Route53 and using it to route requests to your website hosted on Amazon S3 is the most efficient way to host a static website on AWS.\n\n"
}, {
  "id" : 272,
  "question" : "A storage solution is required in AWS to store videos uploaded by the user.\nAfter accessing these videos frequently for a period of a month, these videos can be deleted.\nHow could this be implemented in the most cost-effective manner?\n",
  "answers" : [ {
    "id" : "6ba06e1f32594ea084b3415e7d86e34a",
    "option" : "Use EBS Volumes to store the videos. Create a script to delete the videos after a month.",
    "isCorrect" : "false"
  }, {
    "id" : "3e16b40df40e4c84b2dada26ff8f1036",
    "option" : "Configure object expiration lifecycle policy rule on the S3 bucket and the policy will take care of deleting the videos on the completion of 30 days.",
    "isCorrect" : "true"
  }, {
    "id" : "c2b0ef9593fb41db9a362b0e12e7026a",
    "option" : "Store the videos in Amazon Glacier and then use Lifecycle Policies.",
    "isCorrect" : "false"
  }, {
    "id" : "ae01142b57c140dbb023f3fab71094cd",
    "option" : "Store the videos using Stored Volumes. Create a script to delete the videos after a month.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following on Lifecycle Policies.\nLifecycle configuration enables you to specify the lifecycle management of objects in a bucket.\nThe configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects.\nThese actions can be classified as follows.\nTransition actions - In which you define when objects transition occurs to another storage class.\nFor example, you may choose transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation or archive objects to the GLACIER storage class one year after creation.\nExpiration actions - In which you specify when the objects expire.\nThen Amazon S3 deletes the expired objects on your behalf.\nFor more information on AWS S3 Lifecycle policies, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\n\nThe most cost-effective solution for storing videos uploaded by users on AWS would be to use Amazon S3 (Simple Storage Service) with a lifecycle policy. Option B is the correct answer.\nAmazon S3 provides a scalable and durable object storage service that is designed for storing and retrieving any type of data, including videos. It allows users to store and retrieve objects from anywhere on the web, making it a popular choice for storing large media files like videos.\nA lifecycle policy in S3 is used to automatically transition objects between different storage classes or delete them when they are no longer needed. By configuring a lifecycle policy, we can specify rules for transitioning objects to lower-cost storage classes or deleting them after a certain period of time.\nOption A suggests using EBS (Elastic Block Store) volumes to store the videos. However, this is not the recommended solution for storing large media files like videos, as EBS volumes are typically used for block-level storage, such as operating system files, application data, and database storage.\nOption C suggests using Amazon Glacier to store the videos. While Amazon Glacier is a low-cost storage solution designed for long-term archiving of data, it is not suitable for storing frequently accessed data like videos that need to be accessed frequently.\nOption D suggests using stored volumes to store the videos, but this is not a correct answer because Stored Volumes is not an AWS storage service.\nIn conclusion, the most cost-effective solution for storing videos uploaded by users in AWS would be to use Amazon S3 with a lifecycle policy. We can create a lifecycle policy that specifies a rule to delete objects after 30 days. This ensures that we only pay for the storage that we need and don't incur unnecessary costs by storing data that is no longer required.\n\n"
}, {
  "id" : 273,
  "question" : "You are working as an AWS Architect for a global media firm.\nThey have web servers deployed on EC2 instances across multiple regions.\nFor audit purposes, you have created a CloudTrail trail that delivers the CloudTrail event log files to the S3 bucket This trail applies to all regions &amp; delivers the CloudTrail event log files to the S3 buckets in the EU-Central region.\nDuring last year's audit, auditors have raised a query on the integrity of log files that are delivered to the S3 buckets and raised a Non-Compliance flag against them.\nWhich feature could help you to gain compliance from Auditors for given issue?\n",
  "answers" : [ {
    "id" : "1d8e4a29c6eb4684aabe8e581eb13e69",
    "option" : "Use Amazon SSE-S3 encryption for the CloudTrail log file while storing it to S3 buckets.",
    "isCorrect" : "false"
  }, {
    "id" : "092cb2e8342c48a9b535729edb0e932c",
    "option" : "Use Amazon SSE-KMS encryption for CloudTrail log file while storing it to S3 buckets.",
    "isCorrect" : "false"
  }, {
    "id" : "6e43803218904475a1c071d63ea3d36e",
    "option" : "Use an S3 bucket policy to grant access to only Security head for S3 buckets having CloudTrail log files.",
    "isCorrect" : "false"
  }, {
    "id" : "91a8ae6c22fa491989f3daa7b2fcc15f",
    "option" : "Enable the CloudTrail log file integrity validation feature.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nTo determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation.\nThis feature is built using industry-standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing.\nThis makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection.\nOption A is incorrect as, by default, all CloudTrail log files are delivered to S3 buckets using SSE-S3 encryption.\nThis will not ensure the integrity of log files.\nOption B is incorrect as with Amazon SSE-KMS encryption for CloudTrail log files.\nThere would be an additional layer of security for log files.\nBut it won't ensure the integrity of log files.\nOption C is incorrect as this will restrict access to the bucket.\nBut it won't ensure that no modification has been done to log files post delivering in S3 buckets.\nFor more information on CloudTrail Log files Integrity, please refer to the following URLs-\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html\nhttps://aws.amazon.com/blogs/aws/aws-cloudtrail-update-sse-kms-encryption-log-file-integrity-verification/\n\nThe auditors have raised a Non-Compliance flag against the integrity of log files that are delivered to the S3 buckets. To gain compliance from the auditors, we need to implement a solution that ensures the integrity of log files in transit and at rest.\nOption A - Using Amazon SSE-S3 encryption for CloudTrail log files while storing it to S3 buckets: Amazon S3 Server-Side Encryption with Amazon S3 Managed Keys (SSE-S3) is a feature that encrypts the S3 object using server-side encryption. With this option, S3 encrypts the data at rest within the S3 bucket. However, this feature does not address the issue of log file integrity, and data in transit between the EC2 instances and S3 is still not encrypted.\nOption B - Using Amazon SSE-KMS encryption for CloudTrail log files while storing it to S3 buckets: Amazon S3 Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS) is a feature that encrypts the S3 object using server-side encryption and provides an additional layer of security by integrating with AWS Key Management Service (KMS). With this option, S3 encrypts the data at rest within the S3 bucket, and we can also control the encryption keys used for encryption. However, this feature does not address the issue of log file integrity, and data in transit between the EC2 instances and S3 is still not encrypted.\nOption C - Using an S3 bucket policy to grant access to only Security head for S3 buckets having CloudTrail log files: S3 bucket policies are used to manage access to S3 buckets and objects. With this option, we can restrict access to the S3 bucket containing the CloudTrail log files to only the security head. However, this option does not address the issue of log file integrity.\nOption D - Enabling the CloudTrail log file integrity validation feature: CloudTrail log file integrity validation is a feature that verifies the integrity of CloudTrail log files in transit and at rest. With this option, CloudTrail verifies the integrity of log files using SHA-256 hash, which ensures that log files have not been tampered with. The hash is included in the S3 object metadata, so we can check the hash value to ensure the log file has not been altered. This option provides end-to-end data protection, including data in transit between EC2 instances and S3, and data at rest within S3.\nIn conclusion, Option D is the best option for gaining compliance from auditors as it ensures the integrity of log files in transit and at rest.\n\n"
}, {
  "id" : 274,
  "question" : "You have an EC2 Instance in a particular region.\nThis EC2 Instance has a preconfigured software running on it.\nYou have been requested to create a disaster recovery solution in case the instance in the region fails.\nWhich of the following is the best solution?\n",
  "answers" : [ {
    "id" : "6ea9ec570d8148139d0eba2f71c47012",
    "option" : "Create a duplicate EC2 Instance in another AZ. Keep it in the shutdown state. When required, bring it back up.",
    "isCorrect" : "false"
  }, {
    "id" : "88401e59c14e44539f785f68bdfda0f8",
    "option" : "Backup the EBS data volume. If the instance fails, bring up a new EC2 instance and attach the volume.",
    "isCorrect" : "false"
  }, {
    "id" : "5c1399a16de34e059dcb0b6f1ffd0893",
    "option" : "Store the EC2 data on S3. If the instance fails, bring up a new EC2 instance and restore the data from S3.",
    "isCorrect" : "false"
  }, {
    "id" : "2abd4681305b4bde82f53ef8934bd57c",
    "option" : "Create an AMI of the EC2 Instance and copy it to another region.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nYou can copy an Amazon Machine Image (AMI) within or across an AWS region using the AWS Management Console, the AWS command-line tools or SDKs, or the Amazon EC2 API, all of which support the CopyImage action.\nYou can copy both Amazon EBS-backed AMIs and instance store-backed AMIs.\nYou can copy AMIs with encrypted snapshots and encrypted AMIs.\nCopying a source AMI results in an identical but distinct target AMI with its own unique identifier.\nIn the case of an Amazon EBS-backed AMI, each of its backing snapshots is, by default, copied to an identical but distinct target snapshot.\nOption A is invalid because it is a maintenance overhead to maintain another non-running instance.\nOption B is invalid because the preconfigured software could have settings on the root volume.\nOption C is invalid because this is a long and inefficient way to restore a failed instance.\nFor more information on Copying AMIs, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/CopyingAMIs.html\n\nSure! I'll provide a detailed explanation of each of the options.\nA. Create a duplicate EC2 Instance in another AZ. Keep it in the shutdown state. When required, bring it back up.\nThis option involves creating a duplicate EC2 instance in another availability zone (AZ) within the same region and keeping it in a shutdown state. In case of a disaster, the instance can be started up quickly to minimize downtime. However, this solution only provides redundancy within the same region, and if the entire region goes down, this solution won't be helpful. Additionally, this option may result in higher costs since a second EC2 instance is required to be kept running even when it's not being used.\nB. Backup the EBS data volume. If the instance fails, bring up a new EC2 instance and attach the volume.\nThis option involves taking a backup of the EBS data volume of the EC2 instance. In case of a disaster, a new EC2 instance can be launched in the same or a different AZ, and the backup volume can be attached to the new instance. This solution is more cost-effective than Option A since a new EC2 instance doesn't need to be kept running all the time. However, it requires more time to restore the backup volume to the new instance, which may result in more downtime.\nC. Store the EC2 data on S3. If the instance fails, bring up a new EC2 instance and restore the data from S3.\nThis option involves storing the data of the EC2 instance on S3. In case of a disaster, a new EC2 instance can be launched, and the data can be restored from S3. This solution is more cost-effective than Option B since the storage costs of S3 are generally lower than EBS volumes. However, restoring data from S3 may take more time than attaching an EBS volume to a new instance, which may result in more downtime.\nD. Create an AMI of the EC2 Instance and copy it to another region.\nThis option involves creating an Amazon Machine Image (AMI) of the EC2 instance and copying it to another region. In case of a disaster, a new EC2 instance can be launched in the secondary region using the AMI. This solution provides the highest level of redundancy since it ensures that the application can be quickly recovered even if the entire primary region goes down. However, this solution may result in higher costs since running EC2 instances in two different regions can be expensive.\nIn conclusion, each of the options has its own advantages and disadvantages, and the best solution will depend on the specific requirements and constraints of the scenario. Option D provides the highest level of redundancy but may be more expensive, while Option A is simpler but may result in higher costs. Option B and C provide a good balance between cost and recovery time but may require more effort to set up and maintain.\n\n"
}, {
  "id" : 275,
  "question" : "You work in the media industry and have deployed a web application on a large EC2 instance where users can upload photos to your website.\nThis web application must be able to call the S3 API to function properly.\nWhere would you store your API credentials while maintaining the maximum level of security?\n",
  "answers" : [ {
    "id" : "d3533f80092f4ea5b078bf8de81f9aec",
    "option" : "Save the API credentials to your PHP files.",
    "isCorrect" : "false"
  }, {
    "id" : "73930146ef5549d0b9290d368dce6f5c",
    "option" : "Donâ€™t save your API credentials. Instead, create an IAM role and assign that role to an EC2 instance.",
    "isCorrect" : "true"
  }, {
    "id" : "a19e8fce90654100b0e8443aae6d43fd",
    "option" : "Save your API credentials in a public Github repository.",
    "isCorrect" : "false"
  }, {
    "id" : "a0b81f027b63407f8c6c067b16a98d8e",
    "option" : "Pass API credentials to the instance using instance user data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nWe designed IAM roles so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the applications use.\nInstead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles as follows:\nCreate an IAM role.\nDefine which accounts or AWS services can assume the role.\nDefine which API actions and resources the application can use after assuming the role.\nSpecify the role when you launch your instance or attach the role to an existing instance.\nHave the application retrieve a set of temporary credentials and use them.\nFor more information on IAM Roles, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\n\nThe correct answer is B. Don't save your API credentials. Instead, create an IAM role and assign that role to an EC2 instance.\nExplanation: Storing API credentials directly in code or configuration files is a security risk because anyone with access to these files could potentially access the credentials and use them for malicious purposes. Therefore, it's recommended to store API credentials securely.\nIn this scenario, the best solution is to use an IAM role to grant access to the S3 API. An IAM role is an AWS identity with specific permissions that determine what the identity can and cannot do in AWS. By creating an IAM role and assigning it to an EC2 instance, you can grant the instance access to specific AWS resources without having to share or store any credentials.\nHere's how you can set this up:\nCreate an IAM role: Go to the AWS IAM console and create a new role. Select \"AWS service\" as the trusted entity and choose EC2 as the service that will use this role. Then, attach the required S3 permissions to the role. Assign the role to an EC2 instance: Launch a new EC2 instance or select an existing one, and then specify the IAM role you created in step 1. This will automatically provide the EC2 instance with the necessary credentials to access the S3 API.\nBy using an IAM role, you can avoid storing API credentials on your EC2 instance and still provide your web application with the necessary permissions to call the S3 API securely.\n\n"
}, {
  "id" : 276,
  "question" : "Your company has a new web application that needs to be deployed in AWS as soon as possible.\nThe application is backed in an AMI, and you plan to use an Application Load Balancer to distribute the traffic to an Auto Scaling group.\nWhich of the following methods helps you to achieve a highly available system?\n",
  "answers" : [ {
    "id" : "a5cb2dc1cec6420bb2846177ea9f91bb",
    "option" : "Assign an Elastic IP to each instance under the Auto Scaling group.",
    "isCorrect" : "false"
  }, {
    "id" : "a0870c402e9c4527b794e4d7a35b8fbd",
    "option" : "Use the Application Load Balancer to send traffic across instances in multiple availability zones.",
    "isCorrect" : "true"
  }, {
    "id" : "4e97d49091eb463db0d6afeb80d37712",
    "option" : "Configure the Application Load Balancer to distribute traffic to instances in multiple AWS regions.",
    "isCorrect" : "false"
  }, {
    "id" : "60355af3df2c459e97b9edc48ea04865",
    "option" : "Assign a dedicated IP address to the Application Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nRefer to https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf for how to build highly available and fault-tolerant software systems in AWS.\nOption A is incorrect: It is not necessary to assign Elastic IPs to ASG instances.\nThe instances may be terminated when the ASG scales in and new instances will be created.\nOption B is CORRECT: Because this ensures that when one availability zone has an outage, instances in other availability zones can still serve traffic from the Application Load Balancer.\nOption C is incorrect: Because Application Load Balancer is a regional service and cannot distribute traffic to instances in multiple regions.\nOption D is incorrect: Because users cannot assign IP addresses to an Application Load Balancer.\n\nThe correct answer is B. Use the Application Load Balancer to send traffic across instances in multiple availability zones.\nExplanation:\nWhen deploying a web application in AWS, it is important to ensure that the application is highly available, meaning it can continue to operate even if some parts of the infrastructure fail. To achieve high availability, the application should be deployed across multiple availability zones (AZs) and regions.\nAn Amazon Machine Image (AMI) is a pre-configured virtual machine image used to create EC2 instances. To deploy an application in AWS, you need to create an EC2 instance from the AMI. However, deploying a single EC2 instance is not enough for high availability as the instance may fail or become unavailable. Therefore, to ensure high availability, you should use Auto Scaling groups. Auto Scaling groups enable you to automatically adjust the number of instances in response to changes in demand, and they can span multiple AZs.\nAn Application Load Balancer (ALB) is a load balancer that distributes incoming traffic to multiple targets, such as EC2 instances, in a target group. ALB supports distributing traffic across multiple AZs to ensure high availability. When you create an ALB, you can choose to create it in one or more AZs. If you create it in multiple AZs, the ALB will distribute traffic across instances in those AZs. This helps to ensure that if one AZ fails, traffic will be routed to instances in other AZs.\nOption A, Assign an Elastic IP to each instance under the Auto Scaling group, is incorrect because Elastic IPs are not a high availability solution. Elastic IPs are static IP addresses that can be assigned to an instance. While Elastic IPs can help to provide a static IP address to an instance, they do not provide high availability as an Elastic IP can only be assigned to one instance at a time. If that instance fails, the Elastic IP must be re-assigned to another instance manually.\nOption C, Configure the Application Load Balancer to distribute traffic to instances in multiple AWS regions, is incorrect because ALB cannot distribute traffic across instances in multiple AWS regions. ALB can only distribute traffic across instances in one or more AZs within a region.\nOption D, Assign a dedicated IP address to the Application Load Balancer, is incorrect because ALB already has a static IP address that is used to route traffic to instances. Assigning a dedicated IP address to the ALB does not provide any additional high availability.\n\n"
}, {
  "id" : 277,
  "question" : "An organization is managing a Redshift Cluster in AWS.\nThey need to monitor this Redshift cluster's performance to ensure that it is performing as efficiently as possible.\nWhich of the following services should be used for achieving this requirement? (Select Two)\n",
  "answers" : [ {
    "id" : "c1635bcfd0544ca5954620bb5756e423",
    "option" : "CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "fe5e09e231e446278f607005de4e7fce",
    "option" : "SQS",
    "isCorrect" : "false"
  }, {
    "id" : "ee2c6edc1fb8492e9ba01d271b75cfb4",
    "option" : "CloudWatch",
    "isCorrect" : "true"
  }, {
    "id" : "62c51cd5dd064cfdbeb2f0aff6b59243",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers - C and D.\nAWS Documentation mentions the following on monitoring Redshift Clusters.\nAmazon CloudWatch metrics help you monitor the physical aspects of your cluster, such as CPU utilization, latency, and throughput.\nMetric data is displayed directly in the Amazon Redshift console.\nYou can also view it in the Amazon CloudWatch console, or you can consume it in any other way you work with metrics such as with the Amazon CloudWatch Command Line Interface (CLI) or one of the AWS Software Development Kits (SDKs).\nFor more information on monitoring Redshift, please visit the URL below.\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/metrics.html\nFor Option D, please go through the link below.\nhttps://aws.amazon.com/about-aws/whats-new/2016/03/aws-trusted-advisor-adds-checks-for-amazon-s3-amazon-redshift-reserved-instances-security-and-service-limits/\n\nThe two services that should be used for monitoring the performance of a Redshift cluster in AWS are CloudWatch and AWS Trusted Advisor.\nCloudWatch: CloudWatch is a monitoring service provided by AWS that allows users to monitor various resources and applications on the AWS platform. For a Redshift cluster, CloudWatch can be used to monitor and collect metrics such as CPU utilization, disk usage, network throughput, and query execution time. By monitoring these metrics, users can identify performance bottlenecks and take corrective actions to optimize the cluster's performance. AWS Trusted Advisor: AWS Trusted Advisor is a service that provides recommendations for optimizing the performance, security, and cost-efficiency of AWS resources. For a Redshift cluster, AWS Trusted Advisor can provide recommendations for optimizing query performance, reducing storage costs, and improving security. By following these recommendations, users can ensure that the Redshift cluster is performing as efficiently as possible and minimize any potential issues or downtime.\nTherefore, the correct options are C. CloudWatch and D. AWS Trusted Advisor.\n\n"
}, {
  "id" : 278,
  "question" : "You are working as an AWS Architect for a global insurance firm.\nFor the web application, you are using S3 buckets and have configured CloudFront to cache image files.\nFor audit purposes, you have created a CloudTrail trail in each region.\nThe events logs files are logged in the S3 bucket in the us-west-1 region. There have been changes in CloudFront, which have caused all traffic to be routed to the origin, resulting in increased latency for users in other continents.\nAfter scrutinizing CloudTrail logs, you found that duplicate CloudFront events are being logged.\nWhat configuration changes would you perform to eliminate duplicate CloudFront logs?\n",
  "answers" : [ {
    "id" : "9e6fa6a510374a64b4bcb4a9de52b068",
    "option" : "Using AWS CLI, update CloudTrail trail to disable global service events that are delivered in all regions except US-West-1.",
    "isCorrect" : "true"
  }, {
    "id" : "ad42665eb7024c4eb0a70f81db42c321",
    "option" : "Using AWS CLI, change the configuration of a trail to logging a single region instead of logging all regions.",
    "isCorrect" : "false"
  }, {
    "id" : "c153a876c3d64907b0909957b94a1378",
    "option" : "Using AWS console, update CloudTrail trail to disable global service events to be delivered in all regions except US-West-1.",
    "isCorrect" : "false"
  }, {
    "id" : "b58717f52d264a9e82b72a50102b7ac1",
    "option" : "Using the AWS console, change the configuration of a trail to logging a single region instead of logging all regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon CloudFront is a global service for which events are delivered to CloudTrail trails, including global services.\nTo avoid duplicate Amazon CloudFront events, you can disable these events from delivering to CloudTrail trails in all regions &amp; enable them in only one region.\nOptions B &amp; D is incorrect as if CloudTrail trail is changed to logging a single region.\nGlobal service event logging is off automatically.\nThis will disable CloudFront events being logged instead of avoiding duplicate logs.\nOption C is incorrect as Changes to Global service event logs can be done only via AWS CLI &amp; not via AWS console.\nFor more information on Global Service Events with CloudTrail, refer to the following URL-\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html#cloudtrail-concepts-regional-and-global-services\n\nAs an AWS Architect for a global insurance firm, you have configured CloudFront to cache image files and created a CloudTrail trail in each region to monitor the events logs. However, you have found that duplicate CloudFront events are being logged, and this is causing increased latency for users in other continents.\nTo eliminate the duplicate CloudFront logs, you need to configure the CloudTrail trail appropriately. There are two options available to you:\nOption A: Using AWS CLI, update CloudTrail trail to disable global service events that are delivered in all regions except US-West-1. Option B: Using AWS CLI, change the configuration of a trail to logging a single region instead of logging all regions.\nOption A involves disabling global service events that are delivered in all regions except US-West-1. This option can be accomplished using the AWS CLI. By disabling global service events in all regions except US-West-1, you can prevent duplicate CloudFront events from being logged in other regions. This will help reduce the traffic routed to the origin, resulting in decreased latency for users in other continents. This option is recommended if you want to continue logging CloudTrail events in all regions but only want to disable global service events in other regions.\nOption B involves changing the configuration of the CloudTrail trail to log only a single region instead of logging all regions. This option can also be accomplished using the AWS CLI. By logging only a single region, you can prevent duplicate CloudFront events from being logged in other regions. This will help reduce the traffic routed to the origin, resulting in decreased latency for users in other continents. This option is recommended if you only want to log CloudTrail events in a specific region and do not need to log events in other regions.\nBoth options can be accomplished using either the AWS CLI or the AWS console. However, option C and D are incorrect as they do not address the issue of eliminating duplicate CloudFront logs. Therefore, the correct answers are either option A or B depending on the requirements of the situation.\n\n"
}, {
  "id" : 279,
  "question" : "You work for a start-up firm developing a new multilingual website to share images and video files.\nYou are using EC2 instances to host this web application.\nTo deliver these web content with the lowest latency to end-users, you have configured Amazon CloudFront, which forwards query strings to origin servers based on selected parameter values and caches web content based on these parameter values. During the trial, it was observed that caching was not happening based upon query strings and these requests hit origin servers.\nWhich of the following need to be checked to see if CloudFront is caching properly based upon query strings? (Select Three)\n",
  "answers" : [ {
    "id" : "b6c84e14b90a434cabef6329a5da4ba9",
    "option" : "Make sure that the distribution is an RTMP distribution.",
    "isCorrect" : "false"
  }, {
    "id" : "4bedb2eda507472e81a8e0e2d1472783",
    "option" : "Make sure that the delimiter character between query string parameters is a `&amp;` character.",
    "isCorrect" : "true"
  }, {
    "id" : "975dc64372204e75a5f7cc98558fb0a2",
    "option" : "Check if parameters` names and values use the same case.",
    "isCorrect" : "true"
  }, {
    "id" : "4b6a623cdd5246e5a499454458e15064",
    "option" : "Make sure that the delimiter character between query string parameters is a â€œ/â€ character.",
    "isCorrect" : "false"
  }, {
    "id" : "622e8d40dac4404e82d9359b668bbf07",
    "option" : "Make sure that the distribution is a web distribution.",
    "isCorrect" : "true"
  }, {
    "id" : "8a4067ac920a4e86b1c514e14560a39a",
    "option" : "Check only that the query parameter names use the lower case.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - B, C, E.\nCloudFront Query String Forwarding only supports Web distribution.\nFor query string forwarding, the delimiter character must always be a '&amp;' character.\nParameters' names and values used in the query string are case sensitive.\nParameter Names and Values should use the same case.\nOption A is incorrect as CloudFront Query String Forwarding does not support RTMP distribution.\nOption D is incorrect as Delimiter Character should always be '&amp;', not '\\' character.\nOption F is incorrect as in the case of Parameters in the query string, both the parameters' names and values should use the same case.\nFor more information on Query String forwarding, refer to the following URL-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html.\n\nTo ensure that Amazon CloudFront is caching properly based upon query strings, we need to check the following:\nA. Make sure that the distribution is a web distribution, not an RTMP distribution. RTMP (Real-Time Messaging Protocol) is used for streaming media content, whereas a web distribution is used for delivering static and dynamic web content. If we are delivering a website, we need to use a web distribution for CloudFront.\nB. Make sure that the delimiter character between query string parameters is a &amp; character. The delimiter character between query string parameters should be an & character, not a &amp; character. The &amp; character is an HTML entity that represents the & character. If we use the wrong delimiter character, CloudFront won't be able to parse the query string parameters correctly.\nC. Check if parameters' names and values use the same case. Query string parameter names and values are case-sensitive. If we use different cases for the parameter names and values in the URL, CloudFront won't be able to match the requests with the cached objects.\nD. Make sure that the delimiter character between query string parameters is a â€œ/â€ character. The delimiter character between query string parameters should be an & character, not a forward slash (/). Using a forward slash as a delimiter can cause CloudFront to treat the query string as part of the URL path, which can result in a cache miss.\nE. Make sure that the distribution is a web distribution, not an RTMP distribution. As mentioned earlier, we need to use a web distribution for delivering static and dynamic web content.\nF. Check only that the query parameter names use the lower case. Query parameter names are case-sensitive. However, CloudFront treats them as case-insensitive by default. If we want CloudFront to treat query parameter names as case-sensitive, we need to configure it to do so explicitly.\nTherefore, the correct answers are A, B, and C.\n\n"
}, {
  "id" : 280,
  "question" : "IoT sensors monitor the number of bags that are handled at an airport.\nThe data is sent back to a Kinesis stream with default settings.\nEvery alternate day, the data from the stream is sent to S3 for processing.\nBut it is noticed that S3 is not receiving all of the data being sent to the Kinesis stream.\nWhat could be the reason for this?\n",
  "answers" : [ {
    "id" : "5d241d6292b443c5aaaaf0cbffdd8196",
    "option" : "The sensors probably stopped working on somedays, hence data is not sent to the stream.",
    "isCorrect" : "false"
  }, {
    "id" : "ad5049ffa564464f9a88164c0017228a",
    "option" : "S3 can only store data for a day.",
    "isCorrect" : "false"
  }, {
    "id" : "4a01910678224835b5fc740d7801491b",
    "option" : "The default retention period of the data stream is set to 24 hours only, and hence the failure.",
    "isCorrect" : "true"
  }, {
    "id" : "eb2cda9108614b3baccb245054f65703",
    "option" : "Kinesis streams are not meant to handle IoT related data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nKinesis Streams support changes to the data record retention period of your stream.\nA Kinesis stream is an ordered sequence of data records, meant to be written to and read in real-time.\nData records are therefore stored in shards in your stream temporarily.\nWhen a record is added to when it is no longer accessible, the time period is called the retention period.\nA Kinesis stream stores the records from 24 hours (by default) up to 168 hours.\nOption A is incorrect, even though a possibility cannot be considered as the right option.\nOption B is incorrect since S3 can store data indefinitely unless you have a lifecycle policy defined.\nOption D is incorrect because the Kinesis service is perfect for this sort of data ingestion.\nFor more information on Kinesis data retention, please refer to the URL below.\nhttp://docs.aws.amazon.com/streams/latest/dev/kinesis-extended-retention.html\n\nThe most likely reason for S3 not receiving all of the data being sent to the Kinesis stream is that the default retention period of the data stream is set to 24 hours only, and hence the failure (option C).\nKinesis streams are a way to collect and process large amounts of data records in real-time. They can handle data from hundreds of thousands of sources, and support different types of data, including IoT data. When data is sent to a Kinesis stream, it is stored in shards, which are the building blocks of a Kinesis stream. Each shard is essentially a sequence of data records, and data is stored in the shard until it expires or is deleted.\nBy default, the retention period of a Kinesis stream is 24 hours. This means that data records that are older than 24 hours are automatically deleted from the stream. In this scenario, the data from the sensors is being sent to the Kinesis stream with default settings, and the data is being sent to S3 every alternate day. If the data from the stream is not being sent to S3, it is possible that some of the data records have already expired and been deleted from the stream.\nOption A is not a likely reason for S3 not receiving all of the data, as it assumes that the sensors stopped working on some days. It is possible that there are other issues with the sensors or the data transmission, but this is not the most likely reason for S3 not receiving all of the data.\nOption B is incorrect, as S3 can store data for much longer than a day. S3 is designed to store large amounts of data for long periods of time, and data can be stored in S3 for months or even years.\nOption D is incorrect, as Kinesis streams are designed to handle a wide range of data, including IoT data. Kinesis streams are well-suited for processing large amounts of IoT data in real-time, and are often used for IoT use cases such as monitoring sensor data, analyzing log data, and processing video and audio streams.\nIn summary, the most likely reason for S3 not receiving all of the data being sent to the Kinesis stream is that the default retention period of the data stream is set to 24 hours only, and some of the data records have already expired and been deleted from the stream.\n\n"
}, {
  "id" : 281,
  "question" : "Your company uses an S3 bucket to store data for an application.\nSometimes the team also downloads the S3 files for further analysis.\nAs the data is very important, you need to protect against accidental deletions initiated by someone or an application and restore the files when needed.\nWhich of the following options is appropriate?\n",
  "answers" : [ {
    "id" : "6f96052789d64aad99559da97a0e03bb",
    "option" : "Enable the versioning feature in the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "373484b066fe4f5a87e098758bf54f10",
    "option" : "Modify the S3 bucket to be read-only.",
    "isCorrect" : "false"
  }, {
    "id" : "295eb7e4c2094d99930e252695cb22cf",
    "option" : "Use an S3 Lifecycle policy to transfer objects to a lower cost storage.",
    "isCorrect" : "false"
  }, {
    "id" : "ed6b23229f494c8796a10c905bc64bf7",
    "option" : "Enable the Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS).",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT: The versioning feature in Amazon S3 helps retain prior versions of objects stored in S3\nEven if the current version is deleted accidentally, the data can still be recovered from the previous version.\nCheck the reference here https://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf.\nOption B is incorrect: The application may still need to write data in the S3 bucket.\nSo the option is not appropriate.\nOption C is incorrect: This method helps to save costs but not protect the data.\nOption D is incorrect: Because the Server-Side Encryption cannot protect against accidental deletions.\n\nThe appropriate option for protecting against accidental deletions and restoring files when needed is to enable the versioning feature in the S3 bucket.\nOption A: Enable the versioning feature in the S3 bucket. When versioning is enabled, S3 automatically creates a new version of an object every time it is updated or deleted. This means that even if an object is deleted or overwritten, the previous version(s) will still be available, and they can be restored at any time. Versioning can also help protect against data corruption or accidental overwriting of data.\nOption B: Modify the S3 bucket to be read-only. Making the bucket read-only means that no one will be able to write or delete objects from the bucket, which would protect against accidental deletions. However, it would also prevent the team from downloading the S3 files for further analysis, which is not desirable.\nOption C: Use an S3 Lifecycle policy to transfer objects to a lower cost storage. This option is not relevant for protecting against accidental deletions or restoring files. S3 Lifecycle policies are used to automatically transition objects to a lower cost storage class or delete them after a certain period of time.\nOption D: Enable the Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS). Enabling SSE-KMS would encrypt the data at rest and protect it from unauthorized access, but it does not provide protection against accidental deletions or overwrite.\nTherefore, option A is the appropriate option for protecting against accidental deletions and restoring files when needed.\n\n"
}, {
  "id" : 282,
  "question" : "You create an Amazon SQS queue to decouple software components.\nThe messages are processed by a Lambda function.\nSometimes, the Lambda function fails to process messages in the queue.\nYou need a mechanism to isolate the message failures to determine why the processing was unsuccessful.\nWhich of the following options would you choose?\n",
  "answers" : [ {
    "id" : "b5a6ffa3f280444f9402bdb6e3c5b73d",
    "option" : "FIFO (First-In-First-Out) queue",
    "isCorrect" : "false"
  }, {
    "id" : "426d8d3e1d7f426c85e5bbdd47110a11",
    "option" : "Visibility timeout",
    "isCorrect" : "false"
  }, {
    "id" : "ad0ccda967cb4f2b89d556c93caa8f8e",
    "option" : "SQS dead-letter queue",
    "isCorrect" : "true"
  }, {
    "id" : "bf866f6e0a784839bfd933ab07cfb4fa",
    "option" : "SQS long polling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nSQS dead-letter queue should be selected as it is designed to isolate problematic messages in the queue.\nFor details, please check the link-https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html.\nOption A is incorrect: Because the FIFO queue itself does not isolate the messages that have failures.\nOption B is incorrect: Visibility timeout prevents other consumers from processing the message again.\nIt does not resolve the mentioned problem.\nOption C is CORRECT: Because users can configure the dead letter queue to isolate the message failures as follows:\nOption D is incorrect: SQS long polling reduces the number of empty responses.\nIt cannot handle the problematic messages.\n\n\nThe correct answer for this scenario is C. SQS dead-letter queue.\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It allows you to send, store, and receive messages between software components at any volume without losing messages or requiring other services to be available. Amazon SQS provides a reliable, highly available, and scalable platform for message-oriented middleware and application integration.\nIn this scenario, messages are being processed by a Lambda function from an SQS queue. However, sometimes the Lambda function fails to process the messages in the queue, which is a common scenario when using a message-queueing system. Therefore, there is a need to isolate the message failures and determine why the processing was unsuccessful.\nSQS provides a feature called dead-letter queue, which allows you to capture and store messages that can't be delivered to their intended destination. When a message can't be processed by the Lambda function, it's moved to the dead-letter queue. This enables you to debug the processing errors and take corrective actions.\nTherefore, the correct option for this scenario is C. SQS dead-letter queue.\nOption A, FIFO (First-In-First-Out) queue, is not the right option because it ensures that messages are processed in the order they are received. This means that a failed message would hold up the processing of other messages in the queue, which is not desirable.\nOption B, visibility timeout, is a setting that determines the amount of time a message is invisible to other consumers after it has been retrieved by a consumer. This option does not help to isolate message failures.\nOption D, SQS long polling, is a mechanism for reducing the number of API calls made to SQS while increasing the responsiveness of your application when there are no messages in the queue. It doesn't help in isolating message failures.\n\n"
}, {
  "id" : 283,
  "question" : "A company requires to deploy an existing Java-based web application to AWS.\nWhich of the following should be used to fulfill this requirement in the quickest way possible?\n",
  "answers" : [ {
    "id" : "360b13c676f04d7e8f6d0d455f322ba8",
    "option" : "Deploy to an S3 bucket and enable website hosting.",
    "isCorrect" : "false"
  }, {
    "id" : "5a8eaa2703ee49c39fc6e4d344b3b631",
    "option" : "Use the Elastic Beanstalk service to provision the environment.",
    "isCorrect" : "true"
  }, {
    "id" : "90d92f79b8f44e549a1ba5b60f1ef5d9",
    "option" : "Use EC2 with Auto Scaling for the environment.",
    "isCorrect" : "false"
  }, {
    "id" : "d76ea621284049fcae2ff1c3faae0b37",
    "option" : "Use AMIs to build EC2 instances for deployment.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nYou can upload your code.\nAnd Elastic Beanstalk will automatically handle the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring.\nAt the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time.\nFor more information on the Elastic Beanstalk service, please visit the following URL-\nhttps://aws.amazon.com/elasticbeanstalk/\n\nOut of the given options, the quickest way to deploy an existing Java-based web application to AWS is by using the Elastic Beanstalk service to provision the environment.\nOption A: Deploy to an S3 bucket and enable website hosting is not an appropriate method for deploying Java-based web applications as S3 is a storage service for static content such as HTML, CSS, JavaScript, images, and videos.\nOption C: Use EC2 with Auto Scaling for the environment requires configuring and managing the underlying infrastructure which can be time-consuming and not the quickest way to deploy the application.\nOption D: Use AMIs to build EC2 instances for deployment requires creating the AMIs, configuring the instances, and managing the underlying infrastructure, which can be time-consuming and not the quickest way to deploy the application.\nTherefore, option B: Use the Elastic Beanstalk service to provision the environment is the quickest way to deploy an existing Java-based web application to AWS. Elastic Beanstalk is a fully managed service that allows for easy deployment, scaling, and management of web applications. It automatically provisions the infrastructure, deploys the application, and monitors the environment, thus reducing the time and effort required for deployment. Additionally, Elastic Beanstalk supports Java applications, making it a suitable option for deploying the given application.\n\n"
}, {
  "id" : 284,
  "question" : "You are working as an AWS Administrator for a software firm with a popular Web application hosted on EC2 instance in various regions.\nYou are using AWS CloudHSM for offloading SSL/TLS processing from Web servers.\nSince this is a critical application for the firm, you need to ensure that proper backups are performed for data in AWS CloudHSM daily.\nWhat does the AWS CloudHSM use to perform a secure &amp; durable backup?\n",
  "answers" : [ {
    "id" : "365238f325bd49b1b816a58a6fa80ff0",
    "option" : "Ephemeral backup key (EBK) is used to encrypt data &amp; Persistent backup key (PBK) is used to encrypt EBK before saving data to the Amazon S3 bucket in the same region as that of AWS CloudHSM cluster.",
    "isCorrect" : "true"
  }, {
    "id" : "e2a30c039b42410db75c8d40e15ebd7d",
    "option" : "Data Key is used to encrypt data &amp; Customer Managed Key (CMK) is used to encrypt Data Key before saving data to the Amazon S3 bucket in the same region as that of AWS CloudHSM cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "905142c3be8b4882b9e1e7303f643b4a",
    "option" : "Ephemeral Backup Key (EBK) is used to encrypt data &amp; Persistent backup Key (PBK) is used to encrypt EBK before saving data to the Amazon S3 bucket in a different region than the AWS CloudHSM cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "194152d6b5384de58afd25cf414457fa",
    "option" : "Data Key is used to encrypt data &amp; Customer Managed Key (CMK) is used to encrypt Data Key before saving data to Amazon S3 bucket in a different region than the AWS CloudHSM cluster.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nTo back up the AWS CloudHSM data to Amazon S3 buckets in the same region, AWS CloudHSM generates a unique Ephemeral Backup Key (EBK) to encrypt all data using AES 256-bit encryption key.\nThis Ephemeral Backup Key (EBK) is further encrypted using Persistent Backup Key (PBK), which is also an AES 256-bit encryption key.\nOption B is incorrect as AWS CloudHSM does not use data Key &amp; Customer Managed Key to encrypt data, instead of that EBK &amp; PBK are used.\nOption C is incorrect.\nWhile taking the backup of data from different AWS CloudHSM clusters to the Amazon S3 bucket, the Amazon S3 bucket should be in the same region as that of the AWS CloudHSM cluster.\nOption D is incorrect as AWS CloudHSM does not use data Key &amp; Customer Managed Key to encrypt data.\nInstead of that, EBK &amp; PBK are used to encrypt and save data to the Amazon S3 bucket in the same region.\nFor more information on backing data from AWS CloudHSM, refer to the following URL-\nhttps://docs.aws.amazon.com/cloudhsm/latest/userguide/backups.html\n\nAWS CloudHSM is a Hardware Security Module (HSM) that provides secure key storage and cryptographic operations for use with AWS services and applications. It enables customers to generate and use their encryption keys to protect sensitive data such as credit card numbers, personally identifiable information, and intellectual property.\nIn this scenario, the AWS CloudHSM is being used for offloading SSL/TLS processing from web servers for a critical web application hosted on EC2 instances across multiple regions. As the data in AWS CloudHSM is critical, it is essential to ensure proper backups are performed daily to ensure the security and durability of the data.\nTo perform a secure and durable backup in AWS CloudHSM, two types of keys are used - Ephemeral Backup Key (EBK) and Persistent Backup Key (PBK).\nEBK is a temporary key that is used to encrypt the data for backup purposes. PBK is a long-term key that is used to encrypt the EBK before saving the data to an Amazon S3 bucket for backup.\nNow, let's examine each answer choice to see which one is the correct answer.\nOption A - Ephemeral Backup Key (EBK) is used to encrypt data & Persistent backup key (PBK) is used to encrypt EBK before saving data to the Amazon S3 bucket in the same region as that of AWS CloudHSM cluster.\nThis option is incorrect as it suggests that the backup is being performed in the same region as the AWS CloudHSM cluster, which may not be the best practice for disaster recovery. It is always recommended to perform backups in a different region to ensure that data can be restored in case of a region-wide outage.\nOption B - Data Key is used to encrypt data & Customer Managed Key (CMK) is used to encrypt Data Key before saving data to the Amazon S3 bucket in the same region as that of AWS CloudHSM cluster.\nThis option is incorrect as it suggests that Data Key and Customer Managed Key (CMK) are used for backup purposes. However, in AWS CloudHSM, Data Key and CMK are used for different purposes such as encrypting and decrypting data, and managing keys.\nOption C - Ephemeral Backup Key (EBK) is used to encrypt data & Persistent backup Key (PBK) is used to encrypt EBK before saving data to the Amazon S3 bucket in a different region than the AWS CloudHSM cluster.\nThis option is correct as it suggests that the backup is being performed in a different region than the AWS CloudHSM cluster. This ensures that the backup data is safe and can be restored in case of a region-wide outage. The EBK is used to encrypt the data for backup, and the PBK is used to encrypt the EBK before saving the data to the Amazon S3 bucket.\nOption D - Data Key is used to encrypt data & Customer Managed Key (CMK) is used to encrypt Data Key before saving data to Amazon S3 bucket in a different region than the AWS CloudHSM cluster.\nThis option is incorrect as it suggests that Data Key and CMK are used for backup purposes. As mentioned earlier, in AWS CloudHSM, Data Key and CMK are used for different purposes.\nTherefore, the correct answer is Option C. Ephemeral Backup Key (EBK) is used to encrypt data & Persistent backup Key (PBK) is used to encrypt EBK before saving data to the Amazon S3 bucket in a different region than the AWS CloudHSM cluster.\n\n"
}, {
  "id" : 285,
  "question" : "You want to build a decoupled, highly available and fault-tolerant architecture, including buffered request for your application in AWS.\nYou decide to use EC2, the Classic Load Balancer, Auto Scaling, and Route 53\nWhich one of the following additional services should you involve in this architecture?\n",
  "answers" : [ {
    "id" : "1ba3f66d4c374614b9cff0f32027bfb6",
    "option" : "AWS SNS",
    "isCorrect" : "false"
  }, {
    "id" : "a089e59c032b4708b7093d7fd0df84e2",
    "option" : "AWS SQS",
    "isCorrect" : "true"
  }, {
    "id" : "339dfeff29654eee929ea026bea39599",
    "option" : "AWS API Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "2fd39b2c089c40669317d33d39a35c8f",
    "option" : "AWS Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe Simple Queue Service can be used to build a decoupled architecture.\nAWS Documentation further mentions the following.\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that makes it easy to decouple and scale microservices, distributed systems, and serverless applications.\nBuilding applications from individual components that perform a discrete function improves scalability and reliability.\nIt is the best practice design for modern applications.\nâ€¢ Scalability - Amazon SQS can process each buffered request (p.\n202) independently, scaling transparently to handle any load increases or spikes without any provisioning instructions.\nFor more information on the Simple Queue Service, please visit the following URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dg.pdf\n\nThe correct answer is B. AWS SQS.\nExplanation: To build a decoupled, highly available and fault-tolerant architecture, including buffered request for your application in AWS, using EC2, Classic Load Balancer, Auto Scaling, and Route 53, you should involve AWS SQS.\nAWS SQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It eliminates the complexity and overhead associated with managing and operating message-oriented middleware, and provides a highly available and scalable queue for buffering messages between distributed application components.\nBy using SQS in this architecture, you can ensure that the application is decoupled, and that all requests are buffered before they are processed, ensuring that the application can handle large bursts of traffic without being overwhelmed.\nHere's how each of the services listed in the question can be used in this architecture:\nEC2: This is the compute service that provides resizable compute capacity in the cloud. EC2 instances can be used to run your application code. Classic Load Balancer: This is the load balancing service that distributes incoming traffic across multiple EC2 instances. It helps to improve the availability and fault tolerance of your application. Auto Scaling: This is the scaling service that automatically adjusts the number of EC2 instances in response to changes in demand. It helps to ensure that you have enough compute capacity to handle varying levels of traffic. Route 53: This is the DNS service that provides global DNS resolution for your domain name. It helps to route traffic to the appropriate endpoint.\nNone of these services alone can provide the buffering of requests that is required to ensure that the application can handle large bursts of traffic without being overwhelmed. This is where AWS SQS comes in. It can be used to buffer incoming requests, and then process them in a controlled and scalable manner, ensuring that the application remains highly available and fault tolerant.\nHere's how AWS SQS can be used in this architecture:\nWhen a request is received, it is added to an SQS queue. This ensures that the request is buffered before it is processed. An application component retrieves messages from the SQS queue and processes them. This ensures that the application can handle requests at a controlled and scalable rate. As the number of requests increases, the number of EC2 instances can be automatically scaled up using Auto Scaling.\nOverall, using AWS SQS in this architecture ensures that the application is decoupled, highly available, and fault tolerant, and that all requests are buffered before they are processed. This helps to ensure that the application can handle large bursts of traffic without being overwhelmed.\n\n"
}, {
  "id" : 286,
  "question" : "You have been assigned the task of architecting an application in AWS.\nThe architecture would consist of EC2, the Classic Load Balancer, Auto Scaling, and Route 53\nYou need to ensure that Blue-Green deployments are possible in this architecture.\nWhich routing policy should you ideally use in Route 53 to achieve Blue-Green deployments?\n",
  "answers" : [ {
    "id" : "22322a205ba5492bb07bec14e2a9e509",
    "option" : "Simple",
    "isCorrect" : "false"
  }, {
    "id" : "5b333840e4bd40bebc87428e33b2938a",
    "option" : "Multivalue Answer",
    "isCorrect" : "false"
  }, {
    "id" : "41198b501e3c415dbfee82452ef52374",
    "option" : "Latency",
    "isCorrect" : "false"
  }, {
    "id" : "0ddddbb582464a949b6bddbf9811147c",
    "option" : "Weighted.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions that the Weighted routing policy is good for testing new versions of the software.\nAlso, It is the ideal approach for Blue-Green deployments.\nWeighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource.\nThis can be useful for various purposes, including load balancing and testing new versions of the software.\nFor more information on Route 53 routing policies, please visit the following URL-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\nNote: Multivalue-answer is recommended to use only when you want to route traffic randomly to multiple resources, such as web servers.\nYou can create one multivalue answer record for each resource and, optionally, associate an Amazon Route 53 health check with each record.\nHowever, in this case, we need to choose how much traffic is routed to each resource (blue and green)\nFor example, Blue is currently live, and we need to send less portion of traffic to Green to check everything works fine.\nIf yes, then we can decide to go with Green resources.\nIf no, we can change the weight for that record to 0\nBlue will be completely live again.\nNOTE:\nWhen you implement the Blue-Green Deployment, it's not always fixed that the Blue environment is in an Alive state and the Green environment in an Idle state vice versa.\nDuring the testing phase, you can route your traffic to both the Blue and Green environments with a specified traffic load.\nFor more information, please visit the link below.\nhttps://d1.awsstatic.com/whitepapers/AWS_Blue_Green_Deployments.pdf\nAWS explained with the proper diagram.\n\nTo achieve Blue-Green deployments in AWS, the ideal routing policy to use in Route 53 is Weighted Routing Policy.\nHere is an explanation of why:\nBlue-Green deployments are a technique that allows for the deployment of two identical environments, one is the current production environment (blue), and the other is a new environment (green) that has the updated code. Once the new environment is fully tested and validated, traffic is redirected to it, and the old environment is decommissioned.\nTo achieve this architecture in AWS, the following components will be required:\nEC2 instances - These are virtual machines that will host the application. Classic Load Balancer - This distributes traffic across the EC2 instances. Auto Scaling - This automatically scales the EC2 instances in and out based on traffic. Route 53 - This is a Domain Name System (DNS) service that routes traffic to the application.\nIn this architecture, to achieve Blue-Green deployments, we need to have two identical environments. We can achieve this by using two Auto Scaling groups, each with the same EC2 instances and the same Classic Load Balancer. One Auto Scaling group will be the blue environment, and the other Auto Scaling group will be the green environment.\nTo implement Blue-Green deployments, we will use the Weighted Routing Policy in Route 53. The Weighted Routing Policy enables us to route a portion of the traffic to a specific environment based on a weight assigned to each environment.\nFor example, we can assign a weight of 90 to the blue environment and a weight of 10 to the green environment. This means that 90% of the traffic will be routed to the blue environment, and 10% of the traffic will be routed to the green environment.\nOnce the green environment has been fully tested and validated, we can change the weight of the blue environment to 0, and the weight of the green environment to 100. This will redirect all traffic to the green environment, making it the new production environment.\nIn summary, to achieve Blue-Green deployments in AWS, we need to use two Auto Scaling groups, each with the same EC2 instances and Classic Load Balancer. We also need to use the Weighted Routing Policy in Route 53 to route traffic to each environment based on a weight assigned to each environment.\n\n"
}, {
  "id" : 287,
  "question" : "A company is planning to deploy an application in AWS.\nThis application requires an EC2 Instance to continuously perform log processing activities requiring Max 500MiB/s of data throughput.\nWhich of the following is the most cost-effective storage option for this requirement?\n",
  "answers" : [ {
    "id" : "4d9c14dae03a44769d6c949d63a42d04",
    "option" : "EBS IOPS",
    "isCorrect" : "false"
  }, {
    "id" : "7a9289634dbd4afeb1efe90e6bfd9961",
    "option" : "EBS SSD",
    "isCorrect" : "false"
  }, {
    "id" : "2103254da7c649ce84bd5828a33bda6b",
    "option" : "EBS Throughput Optimized",
    "isCorrect" : "true"
  }, {
    "id" : "5f4eea7eb9a442c792ac048929022b14",
    "option" : "EBS Cold Storage.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nWhile considering storage volume types for batch processing activities with large throughput, consider using the EBS Throughput Optimized volume type.\nAWS Documentation mentions this, as shown below.\nFor more information on EBS Volume Types, please visit the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\n\n\nBased on the requirements mentioned in the question, the application requires an EC2 instance to perform log processing activities continuously, with a maximum data throughput of 500MiB/s. To determine the most cost-effective storage option for this requirement, we need to consider the various EBS volume types that are available in AWS.\nHere is a brief overview of the EBS volume types:\nGeneral Purpose SSD (gp2): This type of EBS volume is designed for general-purpose workloads and offers a balance of price and performance. It is suitable for most workloads, including boot volumes, low-latency interactive apps, and dev/test environments. Provisioned IOPS SSD (io1): This type of EBS volume is designed for I/O-intensive workloads, such as large databases and applications that require high IOPS, high throughput, and low latency. It provides predictable and consistent performance and can be scaled up to 64,000 IOPS per volume. Throughput Optimized HDD (st1): This type of EBS volume is designed for frequently accessed, throughput-intensive workloads, such as big data, data warehouses, and log processing. It provides low-cost storage with high throughput performance, suitable for large sequential reads and writes. Cold HDD (sc1): This type of EBS volume is designed for infrequently accessed workloads, such as backups and long-term archives. It provides low-cost storage with low throughput performance, suitable for large sequential reads and writes.\nBased on the requirement mentioned in the question, the application requires an EC2 instance to continuously perform log processing activities, requiring a maximum data throughput of 500MiB/s. Therefore, the most cost-effective storage option for this requirement is EBS Throughput Optimized (st1).\nEBS Throughput Optimized (st1) volumes provide low-cost storage optimized for throughput-intensive workloads. These volumes are designed to deliver high throughput performance that scales up to 500MB/s per volume, making them ideal for log processing activities.\nEBS IOPS (io1) volumes are designed for I/O-intensive workloads that require high IOPS, high throughput, and low latency, and are therefore not the best option for this requirement. EBS SSD (gp2) volumes are designed for general-purpose workloads and may not provide the required throughput performance. EBS Cold Storage (sc1) volumes provide low-cost storage with low throughput performance, making them unsuitable for log processing activities.\nTherefore, based on the requirements mentioned in the question, EBS Throughput Optimized (st1) is the most cost-effective storage option for this application.\n\n"
}, {
  "id" : 288,
  "question" : "A company is planning on moving its applications to the AWS Cloud.\nThey have some large SQL data sets that need to be hosted in a data store on the cloud.\nThe data store needs to have features that support client connections with many types of applications, including business intelligence (BI), reporting, data, and analytics tools.\nWhich of the following service should be considered for this requirement?\n",
  "answers" : [ {
    "id" : "4c452ecfc46d4983a1f7b4d5986febf5",
    "option" : "Amazon DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "9c3b8c07f8f24778a79956570d2bc693",
    "option" : "Amazon Redshift",
    "isCorrect" : "true"
  }, {
    "id" : "782c526e0e7147d7a6a176f396d8d5db",
    "option" : "Amazon Kinesis",
    "isCorrect" : "false"
  }, {
    "id" : "3b1587c29bb649e6b4e2993333168749",
    "option" : "Amazon Simple Queue Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nThe AWS Documentation mentions the following.\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.\nYou can start with just a few hundred gigabytes of data and scale to a petabyte or more.\nThis enables you to use your data to acquire new insights for your business and customers.\nAlthough Kinesis has capabilities for analyzing &amp; transforming streaming data, the question refers to having a data store for storing data from different applications BI, Reporting â€¦which is where a Data Lake or Data Warehouse solutions come into the picture.\nKinesis would be more appropriate in situations where one needs to process streaming data.\nAmazon Redshift supports client connections with many types of applications, including business intelligence (BI), reporting, data, and analytics tools.\nOption A is incorrect since DynamoDB is used for NoSQL datastore.\nOption C is incorrect since Kinesis is used for analyzing &amp; transforming streaming data.\nOption D is incorrect since SQS is a message queue service used by distributed applications to exchange messages through a polling mode.\nFor more information on AWS Redshift, please visit the below URL-\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html\nhttps://aws.amazon.com/kinesis/data-streams/\n\nThe service that should be considered for hosting large SQL data sets with features that support client connections with many types of applications, including business intelligence (BI), reporting, data, and analytics tools is Amazon Redshift.\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to efficiently analyze data using existing business intelligence tools. Redshift is optimized for complex queries and large-scale data analytics, and can handle large SQL data sets with ease.\nRedshift also supports client connections with many types of applications, including BI, reporting, data, and analytics tools, through industry-standard ODBC and JDBC connections. It is compatible with most popular ETL tools, including AWS Glue, making it easy to move data into and out of Redshift.\nAmazon DynamoDB is a NoSQL database service that is optimized for fast and flexible document and key-value storage, but it may not be the best fit for hosting large SQL data sets. Amazon Kinesis is a real-time streaming data service that is designed for ingesting and processing large volumes of streaming data, but it may not be the best fit for hosting large SQL data sets either. Amazon Simple Queue Service is a messaging service that enables decoupling and scaling of microservices, but it is not designed for hosting large SQL data sets.\nTherefore, Amazon Redshift is the most appropriate service for hosting large SQL data sets with features that support client connections with many types of applications, including business intelligence (BI), reporting, data, and analytics tools.\n\n"
}, {
  "id" : 289,
  "question" : "You are working as an AWS Architect for a start-up company.\nYou have developed an application that will read out AWS Blogs to AWS professionals using \"Amazon Polly.\" You need to perform a trial with the \"Amazon S3\" blog, in which \"S3\" should be read as \"Amazon Simple Storage Service\". This test needs to be done in 2 different regions, us-west-1 &amp; us-east-1\nWhat could be done to perform the test successfully?\n",
  "answers" : [ {
    "id" : "b6362912759a4f2d9a4ac8903ad96499",
    "option" : "Use a Lexicon to create an alias for the word â€œS3â€. Configure the Lexicon globally through Amazon Polly Console.",
    "isCorrect" : "false"
  }, {
    "id" : "5c180550c38848f0b71f35720cf06a34",
    "option" : "Use a Lexicon to create an alias for the word â€œS3â€. Configure the Lexicon in both the us-west-1 region and the us-east-1 region.",
    "isCorrect" : "true"
  }, {
    "id" : "d548605a53b8433299294109187a97fd",
    "option" : "In the Neural TTS (NTTS) system of Amazon Polly, add an entry for the word \"S3\" and translate it to \"Amazon Simple Storage Service\".",
    "isCorrect" : "false"
  }, {
    "id" : "b0a25a5edb2f451c9fe65677e56ae239",
    "option" : "No actions are required as Amazon Polly can automatically translate \"S3\" to \"Amazon Simple Storage Service\".",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nLexicons are specific to a region.\nYou will need to upload Lexicon in each region where you need to use it.\nOption A is incorrect as Lexicons need to upload in all regions where content uses Amazon Polly.\nOption C is incorrect because there is no such configuration.\nOption D is incorrect because Amazon Polly does not automactially translate it.\nYou need to achieve this function through Lexicons.\nFor more information on managing Lexicons, refer to the following URL-\nhttps://docs.aws.amazon.com/polly/latest/dg/managing-lexicons.html\n\nThe correct answer is B. Use a Lexicon to create an alias for the word â€œS3â€. Configure the Lexicon in both the us-west-1 region and the us-east-1 region.\nExplanation: Amazon Polly is a text-to-speech service that converts text into spoken audio. Polly uses machine learning to generate lifelike speech, and it supports several languages and voices. In this scenario, the application needs to read out the word \"S3\" as \"Amazon Simple Storage Service\" in two different regions, us-west-1 and us-east-1.\nTo achieve this, we can create a Lexicon in Amazon Polly, which is a dictionary of words and their pronunciations. In this case, we can create an entry for \"S3\" in the Lexicon and map it to \"Amazon Simple Storage Service\". When the application requests Polly to read out \"S3\", Polly will look up the Lexicon and read out \"Amazon Simple Storage Service\" instead.\nSince the trial needs to be performed in two different regions, we need to configure the Lexicon in both regions. This ensures that the application in both regions can access the same Lexicon and get the correct pronunciation for \"S3\".\nOption A is incorrect because configuring the Lexicon globally through Amazon Polly Console would only affect one region, and we need to configure it in both regions.\nOption C is incorrect because the Neural TTS (NTTS) system of Amazon Polly is used to generate more natural-sounding speech, but it's not necessary in this case. The standard text-to-speech system is sufficient.\nOption D is incorrect because Amazon Polly does not automatically translate words. We need to create a Lexicon to map \"S3\" to \"Amazon Simple Storage Service\".\n\n"
}, {
  "id" : 290,
  "question" : "You need to ensure that instances in a private subnet can access the Internet.\nThe solution should be highly available and ensure less maintenance overhead.\nWhich of the following would ideally fit this requirement?\n",
  "answers" : [ {
    "id" : "a151bced349943c98afba088b1750efe",
    "option" : "Host the NAT Instance in the private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "c6b099731f4f4841940a6b1806cab366",
    "option" : "Host the NAT Instance in the public subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "4ab5db5bcfa748989754f3dc8fe758e2",
    "option" : "Host the NAT Gateway in the private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "04409b48a4ef48248a109a0c45733175",
    "option" : "Host the NAT Gateway in the public subnet.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nNAT gateway is used to enable instances in a private subnet to connect to the internet.\nHowever, the NAT gateway is implemented or placed in the public subnet of a VPC.For more information, please check out the following URLs-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nhttps://docs.aws.amazon.com/appstream2/latest/developerguide/managing-network-internet-manual.html\nShown below is a comparison of the NAT Gateway and NAT Instances as per the AWS Documentation.\nThe documentation states that the NAT Gateway is highly available and requires less management.\nFor more information on the above comparison, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html\n\n\nTo allow instances in a private subnet to access the internet, we need to use a NAT (Network Address Translation) instance or NAT Gateway. Both options allow instances to initiate outbound traffic to the internet, but there are some differences between them.\nOption A: Host the NAT Instance in the private subnet.\nThis option requires setting up a NAT instance in the private subnet, which will then allow instances in the private subnet to access the internet. This solution is highly available if we launch multiple NAT instances in different availability zones, but it requires more maintenance overhead as we have to manage the NAT instance ourselves. This option is also not the recommended approach since NAT instances are generally considered a legacy solution.\nOption B: Host the NAT Instance in the public subnet.\nThis option requires setting up a NAT instance in the public subnet, which will then allow instances in the private subnet to access the internet. However, hosting a NAT instance in the public subnet increases the attack surface area of the system, as it is accessible from the internet. Therefore, it is not a recommended approach.\nOption C: Host the NAT Gateway in the private subnet.\nThis option requires setting up a NAT Gateway in the private subnet, which will then allow instances in the private subnet to access the internet. This solution is highly available and requires less maintenance overhead as Amazon manages the NAT Gateway. Therefore, this is the recommended approach for this scenario.\nOption D: Host the NAT Gateway in the public subnet.\nThis option is similar to Option B and is not recommended as it increases the attack surface area of the system.\nIn summary, option C, hosting the NAT Gateway in the private subnet, is the recommended solution as it is highly available and requires less maintenance overhead.\n\n"
}, {
  "id" : 291,
  "question" : "You need to have a Data storage layer in AWS.\nFollowing are the key requirements: a) Storage of documents in JSON file format. b) Availability of Indexes c) Automatic scaling What would be an ideal storage layer for the above requirements?\n",
  "answers" : [ {
    "id" : "a190133261ab4d66a920ee455714ce7f",
    "option" : "AWS DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "8f6cb1446f534a51b4839df8cb2f7354",
    "option" : "AWS EBS Volumes",
    "isCorrect" : "false"
  }, {
    "id" : "4aeda7be597d4873ae520af825372b0f",
    "option" : "AWS S3",
    "isCorrect" : "false"
  }, {
    "id" : "38d66196908d45a68103e55a50aab696",
    "option" : "AWS Glacier.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following.\nAmazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.\nDynamoDB enables customers to offload the administrative burdens of operating and scaling distributed databases to AWS.\nFor that, they don't have to worry about hardware provisioning, setup, and configuration, throughput capacity planning, replication, software patching, or cluster scaling.\n- Indexes give you access to alternate query patterns and can speed up queries.\nThis section compares and contrasts index creation and usage in SQL and Amazon DynamoDB.- Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns.\nThis enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic, without throttling.\nFor more information on DynamoDB, please visit the following URL-\nhttps://aws.amazon.com/dynamodb/faqs/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\n\nBased on the key requirements mentioned, the ideal storage layer for this scenario would be AWS DynamoDB.\nHere is why:\nStorage of documents in JSON file format: DynamoDB is a NoSQL database service provided by AWS that allows storing unstructured data in JSON format. It is optimized for fast and predictable performance, making it an ideal choice for storing large volumes of structured and semi-structured data. Availability of Indexes: DynamoDB allows creating indexes on any attribute in a table, enabling fast and efficient querying of data. These indexes can be created with either the primary key or non-primary key attributes, providing flexibility in the query patterns. Automatic Scaling: DynamoDB is designed to automatically scale up or down based on the traffic to the database. This means that the database can handle sudden spikes in traffic without impacting performance, ensuring that the application remains highly available.\nAWS EBS volumes are a block storage service that provides persistent storage for EC2 instances. It is not designed for storing unstructured data in JSON format, and does not provide automatic scaling. Therefore, it is not a suitable option for this scenario.\nAWS S3 is an object storage service that provides scalable and durable storage for data. While S3 allows storing unstructured data in JSON format, it does not provide the ability to create indexes for efficient querying of data. Additionally, S3 is not designed for high performance and low latency access to data, making it a less ideal choice for this scenario.\nAWS Glacier is a low-cost archival storage service designed for infrequently accessed data. It is not optimized for fast and efficient querying of data, making it a less ideal choice for this scenario.\nIn conclusion, AWS DynamoDB is the most suitable option for storing JSON documents with indexes and automatic scaling requirements.\n\n"
}, {
  "id" : 292,
  "question" : "You have a set of Docker images that you use for building containers.\nYou want to start using the Elastic Container Service and utilize the Docker images.\nYou need a place to store these Docker images.\nWhat would you use for this purpose?\n",
  "answers" : [ {
    "id" : "6c8342c44f214defb4bbd54c5c6cca5e",
    "option" : "Use AWS DynamoDB to store the Docker images.",
    "isCorrect" : "false"
  }, {
    "id" : "3c3260d3066a4f099a653a40fed7175a",
    "option" : "Use AWS RDS to store the Docker images.",
    "isCorrect" : "false"
  }, {
    "id" : "1b356e367f0c43b08b753c625cf950df",
    "option" : "Use EC2 Instances with EBS Volumes to store the Docker images.",
    "isCorrect" : "false"
  }, {
    "id" : "3bc1489da8bb48dfa13350d196614c27",
    "option" : "Use the ECR Service to store the Docker images.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Documentation mentions the following.\nAmazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\nAmazon ECR is integrated with Amazon Elastic Container Service (ECS), simplifying your development to production workflow.\nFor more information on the Elastic Container Service, please visit the following URL-\nhttps://aws.amazon.com/ecr/?nc2=h_m1\n\nThe correct answer is D. Use the ECR Service to store the Docker images.\nExplanation:\nAmazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy to store, manage, and deploy Docker container images. It is a secure, scalable, and reliable container image management solution that eliminates the need for managing your own container repositories.\nECR integrates with Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes Service (EKS), and the Docker CLI. This allows you to easily store, manage, and deploy Docker container images, with no need to manage infrastructure, scale, or availability.\nUsing ECR, you can create and manage Docker container images, scan images for vulnerabilities, and deploy images to your ECS clusters or Kubernetes clusters in a simple and streamlined manner. ECR is highly available, secure, and compliant with industry standards.\nOption A is incorrect because DynamoDB is a NoSQL database service and not suitable for storing Docker images.\nOption B is incorrect because RDS is a relational database service and not suitable for storing Docker images.\nOption C is incorrect because EC2 instances with EBS volumes are not optimized for storing Docker images and are more suitable for running containers.\nTherefore, the best option for storing Docker images is to use the ECR service, which is a fully-managed Docker container registry that integrates with ECS, EKS, and the Docker CLI.\n\n"
}, {
  "id" : 293,
  "question" : "A company has confidential documents stored on Amazon S3\nThe Company wants to share these documents securely to 3rd parties without exposing the S3 files in public.\nThe files are shared only within a limited timeframe. What is the best, most reliable and secure way to deliver these documents?\n",
  "answers" : [ {
    "id" : "b388124eabb449ffb4b918c765639108",
    "option" : "Assign IAM users for the 3rd party and create a bucket policy to allow access of bucket though IAM users.",
    "isCorrect" : "false"
  }, {
    "id" : "2d8bf54cfe5d4860b7d0728c600f98e1",
    "option" : "Create presigned URLs for the objects and share the URLs with the 3rd parties.",
    "isCorrect" : "true"
  }, {
    "id" : "f04ffdc8e6d44c53a866879616b25998",
    "option" : "Create a bucket policy that allows access to CloudFront distribution Ids.",
    "isCorrect" : "false"
  }, {
    "id" : "6f860ee244314db59016947df8bcac70",
    "option" : "Assign IAM Roles for the 3rd party and create a bucket policy to allow access of bucket through IAM Roles.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect because assigning IAM users is not a reliable way to securable sharing of content.\nOption B is CORRECT.\nWith presigned URLs, the objects can be shared with others securely.\nThe presigned URLs are valid only for the specified duration.\nOption C is incorrect as CloudFront distribution IDs won't give secure access.\nOption D is incorrect as Assigning AWS Roles will not be a reliable way to secure content sharing.\nReferences:\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\nThe best, most reliable and secure way to share confidential documents stored on Amazon S3 with 3rd parties without exposing the S3 files in public is to use presigned URLs for the objects and share the URLs with the 3rd parties. Therefore, option B is the correct answer.\nHere's a detailed explanation of why presigned URLs are the most appropriate solution:\nPresigned URLs provide a secure way to share confidential documents stored on Amazon S3 with 3rd parties. With presigned URLs, a user can generate a URL that provides time-limited access to a specific S3 object. The user can then share the URL with a 3rd party, who can use the URL to access the object for a limited time period.\nPresigned URLs are more secure than assigning IAM users or IAM roles to the 3rd party, as they do not require the 3rd party to have an IAM account or access key. This means that the 3rd party cannot access other S3 objects or resources in the AWS account, and the access is limited only to the specific S3 object.\nPresigned URLs can also be created with specific conditions, such as an expiration time, an IP address range, or a specific HTTP method. This provides an additional layer of security and control over who can access the S3 object and how they can access it.\nCreating a bucket policy that allows access to CloudFront distribution Ids (Option C) can also be used to securely share S3 objects, but this method is more complex and involves additional setup and configuration. In addition, it requires the use of a CloudFront distribution, which may not be necessary for all use cases.\nAssigning IAM users (Option A) or IAM roles (Option D) to the 3rd party may also work, but it can be more complicated and time-consuming to set up. It also requires managing and maintaining IAM accounts and access keys, which can be challenging to handle if there are multiple 3rd parties who need access to different S3 objects.\nIn summary, the most appropriate and secure way to share confidential documents stored on Amazon S3 with 3rd parties is to use presigned URLs for the objects and share the URLs with the 3rd parties.\n\n"
}, {
  "id" : 294,
  "question" : "You are developing a mobile application for your company with DynamoDB as the back end and JavaScript as the front end.\nDuring the daytime, you notice that sometimes there are spikes in the application, especially in the DynamoDB write throughput.\nWhat would be the most cost-effective and scalable architecture for this application?\n",
  "answers" : [ {
    "id" : "c3ade23fce7048cb8f4f7b9ac977664f",
    "option" : "Enable DynamoDB Auto Scaling to meet the requirements.",
    "isCorrect" : "true"
  }, {
    "id" : "8e461d962837466a8484399bc46861ae",
    "option" : "Increase the write capacity of DynamoDB tables by 20% to meet the peak loads.",
    "isCorrect" : "false"
  }, {
    "id" : "c2ca8ea569104adf8537bc24e3d2446e",
    "option" : "Create a service that pulls SQS messages and writes them to DynamoDB to handle sudden spikes in DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "8647a6dcaf064acba18e1a285bd14364",
    "option" : "Launch DynamoDB in Multi-AZ configuration with a global index to balance writes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT because with DynamoDB Auto Scaling, DynamoDB can automatically increase its write capacity for the spike and decrease the throughput after the spike.\nOption B is incorrect because the extra write capacity is a waste after the spike.\nIt is not a cost-efficient solution.\nOption C is incorrect because the new SQS is not cost-efficient, and it is not needed after the spike.\nOption D is incorrect because this causes extra costs and is not a cost-efficient solution.\nFor more information on DynamoDB auto-scaling, please refer to the below URL-\nhttps://aws.amazon.com/blogs/aws/new-auto-scaling-for-amazon-dynamodb/\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\n\nThe most cost-effective and scalable architecture for a mobile application with DynamoDB as the backend and JavaScript as the frontend, experiencing spikes in DynamoDB write throughput during the daytime would be option A: Enable DynamoDB Auto Scaling to meet the requirements.\nExplanation:\nOption A: Enable DynamoDB Auto Scaling to meet the requirements Enabling DynamoDB Auto Scaling is the most cost-effective and scalable option for handling sudden spikes in DynamoDB write throughput. DynamoDB Auto Scaling automatically adjusts the provisioned throughput capacity of a DynamoDB table in response to traffic changes, ensuring that the application has enough capacity to handle the traffic spikes. Auto Scaling adjusts the capacity up and down based on predefined scaling policies, which can be based on metrics such as CPU usage, disk usage, or the number of requests to the table. With DynamoDB Auto Scaling, there is no need to manually adjust the write capacity of the DynamoDB tables, which can result in overprovisioning and unnecessary costs. Additionally, Auto Scaling can provide quick scaling for sudden traffic spikes, which can improve the application's performance during peak loads.\nOption B: Increase the write capacity of DynamoDB tables by 20% to meet the peak loads Increasing the write capacity of DynamoDB tables by 20% can help meet the peak loads, but it is not the most cost-effective or scalable option. Overprovisioning the write capacity can result in unnecessary costs, especially if the peak loads are infrequent or unpredictable. Additionally, manually adjusting the write capacity of the DynamoDB tables can be time-consuming and may not be able to provide quick scaling for sudden traffic spikes.\nOption C: Create a service that pulls SQS messages and writes them to DynamoDB to handle sudden spikes in DynamoDB Creating a service that pulls SQS messages and writes them to DynamoDB can handle sudden spikes in DynamoDB write throughput, but it is not the most cost-effective or scalable option. This option requires additional development effort, and the service needs to be monitored and maintained. Additionally, this option may not provide quick scaling for sudden traffic spikes, as the service needs to pull messages from SQS before writing them to DynamoDB.\nOption D: Launch DynamoDB in Multi-AZ configuration with a global index to balance writes Launching DynamoDB in Multi-AZ configuration with a global index to balance writes can improve the application's availability and performance, but it is not the most cost-effective or scalable option for handling sudden spikes in DynamoDB write throughput. Multi-AZ configuration provides high availability and automatic failover capabilities, while global indexes allow for low-latency access to data in any region. However, this option does not provide quick scaling for sudden traffic spikes, as it requires manually adjusting the write capacity of the DynamoDB tables. Additionally, launching DynamoDB in Multi-AZ configuration with a global index can result in additional costs.\n\n"
}, {
  "id" : 295,
  "question" : "You are building a large-scale confidential documentation web server on AWS such that all of its documentation will be stored on S3\nOne of the requirements is that it should not be publicly accessible from S3 directly.\nCloudFront would be needed to accomplish this.\nWhich method would satisfy the outlined requirements?\n",
  "answers" : [ {
    "id" : "1aa3fdc0d5ad46e695ff658e6b52dc66",
    "option" : "Create an Identity and Access Management (IAM) User for CloudFront and grant access to the objects in your S3 bucket to that IAM User.",
    "isCorrect" : "false"
  }, {
    "id" : "b924261716f74170a1e6225096baeb21",
    "option" : "Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI.",
    "isCorrect" : "true"
  }, {
    "id" : "e2e07041c0d54dab920738c0c1164637",
    "option" : "Create individual policies for each bucket the documents are stored in, and grant access only to CloudFront in these policies.",
    "isCorrect" : "false"
  }, {
    "id" : "37d9e2436d9f4e6c857badc8c2e5df8b",
    "option" : "Create an S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN).",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nIf you want to use CloudFront signed URLs or signed cookies to provide access to objects in your Amazon S3 bucket, you probably want to prevent users from accessing your Amazon S3 objects using Amazon S3 URLs.\nIf users access your objects directly in Amazon S3, they bypass the controls provided by CloudFront signed URLs or signed cookies.\nFor example, control over the date and time that a user can no longer access your content and control over which IP addresses can be used to access the content.\nBesides, if users access objects both through CloudFront and directly using Amazon S3 URLs, CloudFront access logs are less useful because they're incomplete.\nTo ensure that your users access your files using only CloudFront URLs, regardless of whether the URLs are signed, do the following:\nCreate an origin access identity, which is a special CloudFront user, and associate the origin access identity with your distribution.\nYou associate the origin access identity with origins so that you can secure all or just some of your Amazon S3 content.\nYou can also create an origin access identity and add it to your distribution when you create the distribution.\nFor more information, see Creating a CloudFront OAI and Adding it to Your Distribution.\nChange the permissions either on your Amazon S3 bucket or on the files in your bucket so that only the origin access identity has read permission (or read and download permission)\nWhen your users access your Amazon S3 files through CloudFront, the CloudFront origin access identity gets the files on behalf of your users.\nIf your users request files directly by using Amazon S3 URLs, they're denied access.\nThe origin access identity has permission to access files in your Amazon S3 bucket, but users don't.\nFor more information, see Granting the OAI Permission to Read Files in Your Amazon S3 Bucket.\nFor more information on Origin Access Identity, please visit the link below.\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html\n\nTo make a confidential documentation web server on AWS, you can store all the documents on Amazon S3. However, to ensure that the documents are not publicly accessible from S3 directly, you need to use Amazon CloudFront.\nAmazon CloudFront is a content delivery network that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, and no minimum usage commitments.\nTo satisfy the outlined requirements, you need to create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI. Option B is the correct answer.\nOption A, creating an IAM User for CloudFront and granting access to the objects in your S3 bucket, is incorrect because IAM users are used to manage AWS services and resources, not to grant access to an S3 bucket.\nOption C, creating individual policies for each bucket the documents are stored in, and granting access only to CloudFront in these policies, is incorrect because it would be cumbersome to maintain policies for each bucket, and it is unnecessary because you can use the OAI to grant access to all the objects in the bucket.\nOption D, creating an S3 bucket policy that lists the CloudFront distribution ID as the Principal and the target bucket as the Amazon Resource Name (ARN), is incorrect because it would allow anyone with the CloudFront distribution ID to access the bucket, which is not a good practice for securing confidential data.\nTherefore, the correct method to secure confidential documentation stored on S3 is to create an OAI for CloudFront and grant access to the objects in your S3 bucket to that OAI.\n\n"
}, {
  "id" : 296,
  "question" : "Your company plans to host its development, test, and production applications on EC2 Instances in AWS.\nThe team is worried about how access control would be given to relevant IT Admins for each of the above environments.\nAs an architect, what would you suggest to manage the relevant accesses?\n",
  "answers" : [ {
    "id" : "c786abc24f7747b8a1224a36c1f8e2cc",
    "option" : "Add tags to the instances marking each environment and then segregate access using IAM Policies.",
    "isCorrect" : "true"
  }, {
    "id" : "db848b847a7746868d3699069118feb5",
    "option" : "Add Userdata to the underlying instances to mark each environment.",
    "isCorrect" : "false"
  }, {
    "id" : "11659a990eac43989cc37b84be0a7fc5",
    "option" : "Add Metadata to the underlying instances to mark each environment.",
    "isCorrect" : "false"
  }, {
    "id" : "5127c697441440cb952a71f363104e4e",
    "option" : "Add each environment to a separate Auto Scaling Group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following to support this requirement.\nTags enable you to categorize your AWS resources differently, for example, by purpose, owner, or environment.\nThis is useful when you have many resources of the same type - you can quickly identify a specific resource based on the tags you've assigned to it.\nEach tag consists of a key and an optional value, both of which you define.\nFor example, you could define a set of tags for your account's Amazon EC2 instances that help you track each instance's owner and stack level.\nWe recommend you to devise a set of tag keys that will meet your needs for each resource type.\nUsing a consistent set of tag keys makes it easier for you to manage your resources.\nYou can search and filter the resources based on the tags you add.\nFor more information on using tags, please visit the link below.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\n\nAs an AWS Solutions Architect, I would recommend option A, i.e., Add tags to the instances marking each environment and then segregate access using IAM Policies.\nHere's why:\nTags are an essential part of AWS resource management. They enable you to categorize resources in logical groupings and apply various policies to these groups based on business or organizational requirements. In this scenario, by adding tags to the instances, you can segregate the instances into development, test, and production environments. This categorization will enable you to apply different access policies to each of these environments. With IAM policies, you can easily control who has access to which resources in your AWS account. IAM policies define permissions for users, groups, and roles, allowing you to grant or deny access to specific AWS resources. By creating separate IAM policies for each environment, you can grant or deny access to the relevant IT admins. For example, you can create a policy that grants full access to instances tagged as \"development\" to your development team and deny access to your production team. Additionally, you can use AWS Resource Access Manager (RAM) to share resources across multiple AWS accounts, making it easier to manage permissions across multiple accounts and environments.\nOption B, adding userdata, is not a recommended approach as userdata is generally used to configure an instance when it is launched, and it is not a reliable way to mark the environment of an instance.\nOption C, adding metadata, is not recommended either as metadata is a set of data that provides information about an instance and is not designed to categorize resources based on their environment.\nOption D, adding each environment to a separate Auto Scaling Group, is not a recommended approach either. While Auto Scaling Groups can help you scale your application based on demand, they are not designed to manage access control. Moreover, creating separate Auto Scaling Groups for each environment can lead to additional management overhead and increased costs.\n\n"
}, {
  "id" : 297,
  "question" : "You want to set up a public website on AWS.\nYour requirements are as follows. You want the database and the application server running on AWS VPC.\nYou want the database to be able to connect to the Internet, specifically for patch upgrades. You do not want to receive any incoming requests from the Internet to the database. Which of the following solutions would best satisfy all these requirements?\n",
  "answers" : [ {
    "id" : "77bc50ac986842c991901492c786f27d",
    "option" : "Set up the database in a private subnet with a security group that only allows outbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "33fe3b0a553a4ab4abf206d764ce82e6",
    "option" : "Set up the database in a public subnet with a security group that only allows inbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "c82ccad2041c4c1991fa72203fc77f08",
    "option" : "Set up the database in a local data center and use a private gateway to connect the application to the database.",
    "isCorrect" : "false"
  }, {
    "id" : "11af872fe686412ebac266b336f18127",
    "option" : "Set up the public website on a public subnet and setup the database in a private subnet that connects to the Internet via a NAT Gateway.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThe below diagram from AWS Documentation showcases this architecture.\nFor more information on the VPC Scenario for public and private subnets, please visit the link below.\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\n\n\nThe best solution that satisfies all the given requirements is to set up the public website on a public subnet and set up the database in a private subnet that connects to the Internet via a NAT Gateway. Therefore, the correct answer is option D.\nHere's a detailed explanation of why this solution is the best fit:\nSet up the database in a private subnet with a security group that only allows outbound traffic: This option fulfills the requirement of keeping the database server within the VPC. However, it does not allow the database server to connect to the Internet for patch upgrades. Set up the database in a public subnet with a security group that only allows inbound traffic: This option violates the requirement of not allowing incoming requests from the Internet to the database. Hence, this is not a suitable solution. Set up the database in a local data center and use a private gateway to connect the application to the database: This option does not fulfill the requirement of running the database on AWS VPC. Set up the public website on a public subnet and set up the database in a private subnet that connects to the Internet via a NAT Gateway: This solution meets all the given requirements. The public website can be hosted on a public subnet, and the database can be hosted on a private subnet. The database can connect to the Internet via a NAT Gateway to receive patch upgrades while not allowing incoming requests from the Internet to the database. Therefore, this is the best solution among the given options.\nIn conclusion, setting up the public website on a public subnet and the database in a private subnet that connects to the Internet via a NAT Gateway is the best solution that satisfies all the requirements stated in the question.\n\n"
}, {
  "id" : 298,
  "question" : "Your company has a legacy application that uses monolithic architecture.\nYou need to design a new microservices architecture for the application and host it in AWS.\nThe application should be dockerized so that it can be easily deployed. Which of the following AWS services would best suit this requirement?\n",
  "answers" : [ {
    "id" : "9872b8f5682547c2b09ed808233cc0b4",
    "option" : "Amazon Elastic Kubernetes Engine",
    "isCorrect" : "false"
  }, {
    "id" : "ff9049ed35b046928edc0915c86f59f2",
    "option" : "AWS CodeCommit",
    "isCorrect" : "false"
  }, {
    "id" : "7b590d7ed14b420681f77f4cb5c8d8ae",
    "option" : "Amazon Elastic Container Registry",
    "isCorrect" : "false"
  }, {
    "id" : "d9cb9983e8b54086bd58b968eab396d5",
    "option" : "Amazon Elastic Container Service.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect: Because the service name should be Elastic Kubernetes Service (EKS).\nOption B is incorrect: Because AWS CodeCommit cannot be used to run the dockerized applications.\nOption C is incorrect: Because Elastic Container Registry is the service to store Docker images and cannot run Docker containers.\nOption D is CORRECT: Because Elastic Container Service (ECS) allows users to run Docker container applications easily.\nECS is a suitable AWS compute service for microservices.\nReference for how to build highly available microservices in AWS:\nhttps://aws.amazon.com/microservices/\n\nThe best AWS service to host a microservices architecture application that is dockerized is Amazon Elastic Kubernetes Service (EKS) because it provides a managed Kubernetes environment that can be used to deploy, scale, and manage containerized applications. EKS is a fully-managed service that simplifies the deployment, management, and operation of Kubernetes clusters. By using EKS, you can focus on your applications instead of managing the underlying infrastructure.\nOption A: Amazon Elastic Kubernetes Engine (EKE) is not a valid AWS service.\nOption B: AWS CodeCommit is a source code management service that allows you to securely store and manage your application's source code in a private Git repository. It is not directly related to hosting and deploying containerized applications.\nOption C: Amazon Elastic Container Registry (ECR) is a fully-managed Docker container registry that makes it easy to store, manage, and deploy Docker container images. It is used to store and manage container images that can be used with other container management services such as Amazon Elastic Container Service (ECS) or EKS.\nOption D: Amazon Elastic Container Service (ECS) is a fully-managed container orchestration service that allows you to easily run and scale containerized applications on AWS. It supports Docker containers and can be used to deploy and manage containerized applications. However, EKS is a better choice for a microservices architecture application that is dockerized because it provides a managed Kubernetes environment, which is more flexible and extensible than ECS.\nIn summary, Amazon Elastic Kubernetes Service (EKS) is the best choice for hosting a microservices architecture application that is dockerized because it provides a fully-managed Kubernetes environment that simplifies the deployment, management, and operation of containerized applications.\n\n"
}, {
  "id" : 299,
  "question" : "You have a web application that processes customer orders.\nThe frontend application forwards the order messages to an SQS queue.\nThe backend contains an Elastic Load Balancer and an Auto Scaling group.\nYou want the ASG to auto-scale depending on the queue size.\nWhich of the following CloudWatch metrics would you choose to discover the SQS queue length?\n",
  "answers" : [ {
    "id" : "308b7961dbc54d77b0fb2c7cd83745e8",
    "option" : "ApproximateNumberOfMessagesVisible",
    "isCorrect" : "true"
  }, {
    "id" : "912c34c639c14fc8880e538b43a16289",
    "option" : "NumberOfMessagesReceived",
    "isCorrect" : "false"
  }, {
    "id" : "82fee42ceb9a422391cfc49eeb18005f",
    "option" : "NumberOfMessagesDeleted",
    "isCorrect" : "false"
  }, {
    "id" : "543be8aba0394cb293e2dade96625f19",
    "option" : "ApproximateNumberOfMessagesNotVisible.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThe backend nodes can scale based on the queue length.\nCheck the reference in https://aws.amazon.com/blogs/compute/building-loosely-coupled-scalable-c-applications-with-amazon-sqs-and-amazon-sns/.\nOption A is CORRECT: ApproximateNumberOfMessagesVisible describes the number of messages available for retrieval.\nIt can be used to decide the queue length.\nOption B is incorrect: Because NumberOfMessagesReceived is the number of messages returned by calls to the ReceiveMessage action.\nIt does not measure the queue length.\nOption C is incorrect: Because NumberOfMessagesDeleted is the number of messages deleted from the queue.\nIt is not suitable to be used in this scenario.\nOption D is incorrect: Because ApproximateNumberOfMessagesNotVisible measures the number of messages in flight.\nIt should not be used here.\n\nThe correct answer is A. ApproximateNumberOfMessagesVisible.\nThe ApproximateNumberOfMessagesVisible metric reports the approximate number of messages available for retrieval from the queue. This is the most suitable metric for determining the queue size because it includes all messages that are visible and available for processing by the consumers. This metric can be used to trigger an alarm in CloudWatch that can then trigger a scaling action on the Auto Scaling group.\nThe NumberOfMessagesReceived metric reports the number of messages that have been received by the queue, but not yet processed. This metric is not a good indicator of queue size because it does not take into account messages that have already been retrieved by consumers but have not been deleted.\nThe NumberOfMessagesDeleted metric reports the number of messages that have been successfully deleted from the queue. This metric is not useful for determining the queue size because it only reports on messages that have been deleted and does not take into account messages that are still in the queue.\nThe ApproximateNumberOfMessagesNotVisible metric reports the approximate number of messages that have been received by the queue, but are not available for retrieval by consumers. This includes messages that have been retrieved by consumers but not yet deleted, and messages that are being processed by consumers. This metric is not a good indicator of queue size because it does not include messages that are available for retrieval by consumers.\nTherefore, A. ApproximateNumberOfMessagesVisible is the correct metric to use for determining the queue size in this scenario.\n\n"
}, {
  "id" : 300,
  "question" : "You are performing a Load Testing exercise on your application that is hosted on AWS.\nWhile testing your Amazon RDS MySQL DB Instance, you notice that your application becomes non-responsive when you reach 100% CPU utilization.\nYour application is read-heavy.\nWhich methods would help scale your data-tier to meet the application's needs? (Select Three)\n",
  "answers" : [ {
    "id" : "e83e13254a3b49dea9ed1537951525b8",
    "option" : "Add Amazon RDS DB Read Replicas, and have your application direct read queries to them.",
    "isCorrect" : "true"
  }, {
    "id" : "f5953265688c4266bef967004d156aeb",
    "option" : "Add your Amazon RDS DB instance to Storage Auto Scaling, and set your desired maximum storage limit.",
    "isCorrect" : "false"
  }, {
    "id" : "5251407715404e12ade6973cac996f37",
    "option" : "Use an Amazon SQS queue to throttle data going to the Amazon RDS DB Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "2c8b2e0594fc48c7ae18797ef7167d0e",
    "option" : "Use ElastiCache to cache common queries of your Amazon RDS D.",
    "isCorrect" : "true"
  }, {
    "id" : "3be9831abb7a4b07b278b038e2188964",
    "option" : "Shard your data set among multiple Amazon RDS DB Instances.",
    "isCorrect" : "true"
  }, {
    "id" : "560f61e0c04c4e25be8acba78e7c11bc",
    "option" : "Enable Multi-AZ for your Amazon RDS DB Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - A, D, and E.\nAmazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances.\nThis replication feature makes it easy to elastically scale out beyond the capacity constraints of a single DB Instance for read-heavy database workloads.\nYou can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.\nFor more information on Read Replicas, please refer to the link below.\nhttps://aws.amazon.com/rds/details/read-replicas/\nSharding is a common concept to split data across multiple tables in a database.\nLet's consider the following example.\nApplication Shards.\nIn this example, we assume that our application currently doesn't have enough load to need an application shard for each category, but we want to plan ahead with growth in mind.\nTo make future growth easier, we make use of application shards.\nSo our application code will act as if it has seven shards, but Hibernate will map those seven shards onto a smaller number of application shards.\nEach application shard will map to a MySQL database instance.\nBy using this mapping, we can distribute the load to best suit our needs.\nFor our application, assume that sports and entertainment generate as much load as the other five categories combined.\nThese two categories will map to one application shard, and the other five categories will map to the other application shard.\nThe two application shards will be mapped as follows.\nFor more information on sharding, please refer to the link below.\nhttps://forums.aws.amazon.com/thread.jspa?messageID=203052\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud.\nThe service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores instead of relying entirely on slower disk-based databases.\nFor more information on ElastiCache, please refer to the link below.\nhttps://aws.amazon.com/elasticache/\nOption B is incorrect because it is not an ideal way to scale a database.\nAmazon RDS Auto Scaling is to scale the storage capacity.\nIf the storage capacity threshold is reached, then capacity will be scaled through Auto Scaling.\nRDS Auto Scaling does not look for the CPU utilization threshold.\nSo it cannot be a solution for bottlenecks to read heavy databases.\nOption C is not an ideal choice.\nBecause our application is read-heavy and this is the cause of the problem we face with the RDS.\nSo for this issue, Creating Read replicas, Elastic cache implementation, and Sharding the dataset are the ways through which we can tackle this issue.\nBut if we have too many PUT requests for the DB that is causing the issue, we can create an SQS queue and store these PUT requests in the message queue and then process it accordingly.\nOption F is invalid because the Multi-AZ feature is only a failover option.\n\n\nSure, I can provide a detailed explanation of each method that can help scale your data-tier to meet the application's needs when you reach 100% CPU utilization during Load Testing on your Amazon RDS MySQL DB Instance.\nAdd Amazon RDS DB Read Replicas, and have your application direct read queries to them: Amazon RDS Read Replicas allow you to create one or more read-only copies of your RDS database instance, which can be used to offload read traffic from your primary database instance. Read replicas can be used to scale read-heavy workloads horizontally. By directing read queries to read replicas, you can reduce the load on the primary database instance and improve read performance for your application. Use ElastiCache to cache common queries of your Amazon RDS: Amazon ElastiCache is a fully managed in-memory data store and caching service. It supports popular in-memory caching engines such as Redis and Memcached. By caching frequently used data in ElastiCache, you can reduce the number of queries made to your Amazon RDS database instance, thereby reducing the load on your database and improving the overall performance of your application. Shard your data set among multiple Amazon RDS DB Instances: Sharding is the process of partitioning a large database into smaller, more manageable pieces called shards. Each shard can be hosted on a separate database instance. By distributing the workload across multiple database instances, you can scale your data-tier horizontally. Sharding is particularly useful for read-heavy workloads, as it allows you to distribute the read traffic across multiple database instances. Add your Amazon RDS DB instance to Storage Auto Scaling, and set your desired maximum storage limit: Amazon RDS Storage Auto Scaling automatically scales the storage capacity of your RDS database instance in response to changes in the demand for storage. By setting your desired maximum storage limit, you can ensure that your database has enough storage capacity to handle the workload. Use an Amazon SQS queue to throttle data going to the Amazon RDS DB Instance: Amazon SQS is a message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. By using an SQS queue to throttle the data going to your Amazon RDS database instance, you can smooth out spikes in demand and prevent the database from becoming overwhelmed. Enable Multi-AZ for your Amazon RDS DB Instance: Amazon RDS Multi-AZ deployments provide enhanced availability and durability for RDS database instances. With Multi-AZ, Amazon RDS automatically replicates data to a standby replica in a different Availability Zone (AZ). In the event of a planned or unplanned outage, Amazon RDS automatically fails over to the standby replica, minimizing downtime. Although Multi-AZ does not directly help scale the data tier, it can help ensure that the database remains available and resilient to failures.\nIn summary, you can choose from several methods to scale your data-tier to meet the application's needs when you reach 100% CPU utilization during Load Testing on your Amazon RDS MySQL DB Instance. These methods include adding Amazon RDS DB Read Replicas, using ElastiCache to cache common queries, sharding your data set among multiple Amazon RDS DB instances, adding your Amazon RDS DB instance to Storage Auto Scaling, using an Amazon SQS queue to throttle data going to the Amazon RDS DB instance, and enabling Multi-AZ for your Amazon RDS DB instance.\n\n"
}, {
  "id" : 301,
  "question" : "You work for a big company having multiple web applications that are very different from each other.\nThese applications are built using different programming languages.\nHow could you deploy these applications as quickly as possible?\n",
  "answers" : [ {
    "id" : "46d3fedc5dea4b4ba3003c3d465a0ac5",
    "option" : "Develop all the apps in a single Docker container and deploy using Elastic Beanstalk.",
    "isCorrect" : "false"
  }, {
    "id" : "79c6d2e361344be28a8577662b5b125d",
    "option" : "Create a Lambda function deployment package consisting of code and any dependencies.",
    "isCorrect" : "false"
  }, {
    "id" : "67983c94f23f4b688b2c08068d153d95",
    "option" : "Develop each app in a separate Docker container and deploy using Elastic Beanstalk.",
    "isCorrect" : "true"
  }, {
    "id" : "d6287ae095eb428a81107ef01e3224cb",
    "option" : "Develop each app in separate Docker containers and deploy using CloudFormation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nElastic Beanstalk supports the deployment of web applications from Docker containers.\nWith Docker containers, you can define your own runtime environment.\nYou can choose your own platform, programming language, and application dependencies (such as package managers or tools) that aren't supported by other platforms.\nDocker containers are self-contained and include all the configuration information and software your web application requires to run.\nOption A is incorrect because the requirement is to deploy multiple apps that are very different from each other and developed with different programming languages.\nOption B is ideally used for running code and not packaging the applications and dependencies.\nOption D is incorrect as Deploying Docker containers using CloudFormation is also not an ideal choice.\nFor more information on Docker and Elastic Beanstalk, please visit the URL below.\nhttp://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.html\n\nThe best option for deploying multiple web applications that are very different from each other, and built using different programming languages, would be to develop each app in separate Docker containers and deploy using Elastic Beanstalk (option C). Here's why:\nDocker containers: Docker is a platform that enables developers to package an application and its dependencies into a container. Containers are lightweight and portable, making it easy to move them between different environments. Each application can be packaged in its own container, with all the necessary dependencies included, ensuring consistency across different environments. Elastic Beanstalk: Elastic Beanstalk is a fully managed service provided by AWS that simplifies the deployment of web applications. It provides an easy-to-use console to manage and deploy applications, including the ability to scale up or down as needed. Elastic Beanstalk supports Docker containers, making it an ideal choice for deploying applications packaged in Docker containers. Separate containers: Developing each application in a separate Docker container provides a number of advantages. Firstly, it enables each application to be deployed independently, without affecting other applications. Secondly, it allows for greater flexibility in terms of the underlying technology stack, enabling different programming languages, frameworks, and libraries to be used for each application. Finally, it ensures that each application is isolated from other applications, improving security and reducing the risk of conflicts.\nOption A (Develop all the apps in a single Docker container and deploy using Elastic Beanstalk) is not recommended as it would result in a monolithic container that would be difficult to manage and scale. Additionally, different applications may require different dependencies, making it difficult to manage in a single container.\nOption B (Create a Lambda function deployment package consisting of code and any dependencies) is not recommended as Lambda functions are designed for short-lived, event-driven applications. They may not be suitable for web applications that require persistent connections.\nOption D (Develop each app in separate Docker containers and deploy using CloudFormation) is also a valid option. However, Elastic Beanstalk provides a simpler, more streamlined approach to deploying and managing web applications. CloudFormation is better suited for complex infrastructures, such as those with multiple services and dependencies.\n\n"
}, {
  "id" : 302,
  "question" : "You are designing a system that needs at minimum, 8 m4.large instances operating to service traffic.\nWhile designing a system for high availability in the us-east-1 region having 6 Availability Zones, your company needs to handle the death of a full availability zone.\nHow should you distribute the EC2 servers in a cost-effective way?\n",
  "answers" : [ {
    "id" : "3599df2da426448eb1f8aa7d89cae62f",
    "option" : "3 servers in each of AZs a through d, inclusive.",
    "isCorrect" : "false"
  }, {
    "id" : "b62ebbdef76c40aa936b186edf011e95",
    "option" : "16 servers in AZa.",
    "isCorrect" : "false"
  }, {
    "id" : "f7a16a535d3a4107a4f5d3036dd8cae6",
    "option" : "2 servers in each of AZs a through e, inclusive.",
    "isCorrect" : "true"
  }, {
    "id" : "228ad9a941244aec8999cf733bf298a2",
    "option" : "4 servers in each of AZs a through c, inclusive.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe best way is to distribute the instances across multiple AZs to get the best performance and avoid a disaster scenario.\nWith this solution, you will always have a minimum of more than 8 servers, even if one AZ went down.\nEven though options A and D are also valid, they need more EC2 instances than Option.\nC.For more information on High Availability and Fault tolerance, please refer to the link below.\nhttps://media.amazonwebservices.com/architecturecenter/AWS_ac_ra_ftha_04.pdf\nNote:\nIn option A, we need to distribute 3 servers in each AZ.\nSo,A=3, B=3, C=3, D=3\nTotal servers used=12.\nIn option B, we will not get high availability.\nThere will be no servers available if AZa is down.\nIn option C, we need to distribute 2 servers in each AZ.\nso, A=2, B=2, C=2, D=2, E=2\nTotal servers used=10.\nIn option D, we need to distribute 4 servers in each AZ.\nSo, A=4, B=4, C=4\nTotal servers used=12.\nIn the question, a cost-effective method is required.\nIn option C, we are using fewer servers, i.e., 10 servers distributed in more AZ's.\n\nWhen designing a highly available system, it's important to consider the potential loss of an Availability Zone (AZ) and ensure that the system can continue to operate with minimal disruption.\nIn this scenario, we need a minimum of 8 m4.large instances to handle traffic. We also have 6 AZs in the us-east-1 region, so we can distribute the instances across multiple AZs to ensure high availability.\nOption A suggests distributing 3 servers in each of AZs a through d. This would result in a total of 12 instances, which exceeds our minimum requirement of 8 instances. However, if AZ e were to fail, we would not have enough instances to maintain the required level of service. This option is not cost-effective as it results in unnecessary additional instances.\nOption B suggests placing all 16 servers in AZa. This option is not recommended because if AZa were to fail, all 16 instances would be lost, and we would not have enough instances to maintain the required level of service. This option is not cost-effective as it results in a single point of failure.\nOption C suggests distributing 2 servers in each of AZs a through e. This results in a total of 10 instances, which meets our minimum requirement of 8 instances. If any one AZ were to fail, we would still have enough instances to maintain the required level of service. This option is cost-effective and provides high availability.\nOption D suggests distributing 4 servers in each of AZs a through c. This results in a total of 12 instances, which exceeds our minimum requirement of 8 instances. However, if AZ d, e or f were to fail, we would not have enough instances to maintain the required level of service. This option is not cost-effective as it results in unnecessary additional instances.\nIn conclusion, the best option to distribute the EC2 servers in a cost-effective way, while ensuring high availability and meeting the minimum requirement of 8 instances, is option C, which suggests distributing 2 servers in each of AZs a through e, inclusive.\n\n"
}, {
  "id" : 303,
  "question" : "You create several SQS queues to store different types of customer requests.\nEach SQS queue has a backend node that pulls messages for processing.\nNow you need a service to collect messages from the frontend and push them to the related queues using the publish/subscribe model.\nWhich service would you choose?\n",
  "answers" : [ {
    "id" : "108e237e621e4ca69718dc14012060b2",
    "option" : "Amazon MQ",
    "isCorrect" : "false"
  }, {
    "id" : "af7d1b77c03c48bf8cc849b90445ab99",
    "option" : "Amazon Simple Notification Service (SNS)",
    "isCorrect" : "true"
  }, {
    "id" : "4aef7ab2677c434585f41a3ca99431ea",
    "option" : "Amazon Simple Queue Service (SQS)",
    "isCorrect" : "false"
  }, {
    "id" : "4a575556ffb0417c89d7022865a6a535",
    "option" : "AWS Step Functions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS SNS can push notifications to the related SQS endpoints.\nSNS uses a publish/subscribe model that provides instant event notifications for applications.\nOption A is incorrect: Amazon MQ is a managed message broker service, which is not suitable for this scenario.\nOption B is CORRECT: Because SNS uses Pub/Sub messaging to provide asynchronous event notifications.\nPlease check the link-https://aws.amazon.com/pub-sub-messaging/.\nOption C is incorrect: Because SQS does not use the publish/subscribe model.\nOption D is incorrect: AWS Step Functions coordinate application components using visual workflows.\nThe service should not be used in this scenario.\n\nThe most appropriate service to collect messages from the frontend and push them to the related queues using the publish/subscribe model would be Amazon Simple Notification Service (SNS) (Option B).\nExplanation: Amazon Simple Notification Service (SNS) is a highly available, durable, and fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. It can be used to send messages to a variety of endpoints, including AWS Lambda functions, Amazon SQS queues, HTTP/S webhooks, email, SMS text messages, and mobile push notifications.\nIn this scenario, SNS can act as a broker between the frontend and the backend nodes by publishing messages to the relevant SQS queues. The frontend can send messages to a specific SNS topic, and the backend nodes can subscribe to that topic to receive messages. When a message is published to the topic, SNS will distribute a copy of the message to each subscribed endpoint.\nOption A, Amazon MQ is a message broker service that supports multiple protocols, including MQTT, AMQP, and STOMP. It is a good choice if you need to migrate from an existing message broker to a managed service. However, it is not the best fit for this scenario because it does not support SQS as an endpoint for messages.\nOption C, Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. While SQS can be used to store and retrieve messages, it does not support the publish/subscribe model.\nOption D, AWS Step Functions is a serverless workflow service that enables you to coordinate and orchestrate microservices and distributed systems. While it can be used to invoke AWS Lambda functions and other AWS services, it is not the best fit for this scenario because it does not support the publish/subscribe model.\n\n"
}, {
  "id" : 304,
  "question" : "You need a new S3 bucket to store objects using the write-once-read-many (WORM) model.\nAfter objects are saved in the bucket, they cannot be deleted or overwritten for a fixed amount of time.\nWhich option would you select to achieve this requirement?\n",
  "answers" : [ {
    "id" : "21a388bc0ec9457e88a8875f134ec3a7",
    "option" : "Enable the Amazon S3 object lock when creating the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "8bc045c8ff954bf89c0da1ec9a5f611b",
    "option" : "Enable versioning for the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "244f1f9a92e34fa095c3c56cf61256bd",
    "option" : "Modify the S3 bucket policy to only allow the read operation.",
    "isCorrect" : "false"
  }, {
    "id" : "2c1c390c328b491d95b5847926c7e18c",
    "option" : "Enable the WORM model in the S3 Access Control List (ACL) configuration.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon S3 object lock should be enabled to store objects using the write once and read many (WORM) models.\nThe reference can be found in https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lock.html.\nOption A is CORRECT: After the S3 object lock is enabled, you can prevent the S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely.\nOption B is incorrect: Because versioning does not prevent objects from being deleted or modified.\nOption C is incorrect: Because the S3 bucket should still allow the write operation.\nOtherwise, new objects cannot be saved in the bucket.\nOption D is incorrect: Because there is no such configuration in Access Control List (ACL).\n\nTo store objects using the write-once-read-many (WORM) model in an Amazon S3 bucket, you need to ensure that the objects cannot be deleted or overwritten for a fixed amount of time after they are saved. To achieve this requirement, you should enable the Amazon S3 object lock when creating the S3 bucket. Therefore, the correct answer is A.\nHere are detailed explanations of each answer choice:\nA. Enable the Amazon S3 object lock when creating the S3 bucket. The Amazon S3 object lock provides a way to enforce write-once-read-many (WORM) retention periods for objects in Amazon S3. Once you enable the object lock for a bucket or an object, it cannot be removed or altered. When you enable the object lock, you can choose either the Governance mode or the Compliance mode. In Governance mode, users with sufficient permissions can overwrite or delete objects during the retention period, but they cannot do so after the retention period expires. In Compliance mode, users cannot overwrite or delete objects during the retention period, even if they have sufficient permissions. Therefore, enabling the Amazon S3 object lock is the best option to achieve the requirement.\nB. Enable versioning for the S3 bucket. Enabling versioning allows you to keep multiple versions of an object in the same bucket. When you overwrite an object, Amazon S3 automatically saves the previous version, so you can retrieve it later if needed. However, versioning does not prevent users from deleting or overwriting an object, so it does not meet the requirement of the write-once-read-many (WORM) model.\nC. Modify the S3 bucket policy to only allow the read operation. Modifying the bucket policy to only allow the read operation does not meet the requirement of the write-once-read-many (WORM) model. While it would prevent users from deleting or overwriting objects, it would also prevent them from writing new objects to the bucket.\nD. Enable the WORM model in the S3 Access Control List (ACL) configuration. There is no such thing as a WORM model in the S3 Access Control List (ACL) configuration. The S3 ACL controls access to individual objects, not to the bucket as a whole. While you could configure the ACL to prevent users from deleting or overwriting individual objects, it would not prevent them from doing so to other objects in the bucket. Therefore, this option does not meet the requirement of the write-once-read-many (WORM) model.\n\n"
}, {
  "id" : 305,
  "question" : "There are two folders, A and B, in an S3 bucket.\nFolder A stores objects that are frequently accessed.\nFolder B saves objects that are long-lived, infrequently accessed, and non-critical.\nThe retrieval time for files in folder B should be within milliseconds.\nYou want to use different storage classes for objects in these two folders to save cost.\nWhich storage classes are proper?\n",
  "answers" : [ {
    "id" : "0a2309480ffd456588a879d8e8848e90",
    "option" : "Standard for folder A and S3 Glacier for folder",
    "isCorrect" : "false"
  }, {
    "id" : "54be63ed22eb4245ae4504faf629c49e",
    "option" : "Intelligent-Tiering for folder A and Reduced Redundancy for folder",
    "isCorrect" : "false"
  }, {
    "id" : "b4d9ec96816d4eb2b6d8d2beb537ad92",
    "option" : "Standard for folder A and One Zone-IA for folder",
    "isCorrect" : "true"
  }, {
    "id" : "8cd1cf74ecd64121a1dd89be83f04143",
    "option" : "Two S3 buckets are required as an S3 bucket cannot have two storage classes simultaneously.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAbout different storage classes of S3 objects, please check https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html.\nFolder A should use Standard as objects are frequently accessed.\nFolder B should use infrequently accessed storage classes such as Standard-IA or One Zone-IA.Option A is incorrect: Because S3 Glacier is improper as files cannot be retrieved within milliseconds.\nOption B is incorrect: Because Reduced Redundancy is not cost-efficient for infrequently accessed objects.\nOption C is CORRECT: Check the above explanations.\nOption D is incorrect: Each object in an S3 bucket can have a user-defined storage class.\nThere is no need to maintain two S3 buckets in this scenario.\n\nThe proper storage classes for objects in folders A and B of an S3 bucket, considering cost and access frequency, are:\nC. Standard for folder A and One Zone-IA for folder B.\nExplanation: S3 Standard is suitable for frequently accessed data as it offers low latency, high throughput, and high durability. This storage class is designed to provide high performance and is suitable for applications that require real-time access to data.\nOne Zone-IA (Infrequent Access) is a cost-effective storage class for long-lived, infrequently accessed data that can be recreated if lost, damaged, or unavailable for a short period. This storage class stores data in a single availability zone and provides lower cost compared to S3 Standard. Retrieval time for objects in One Zone-IA is within milliseconds, which makes it suitable for objects that are not critical but need to be available quickly.\nIntelligent-Tiering, as mentioned in option B, is suitable for objects with unknown or changing access patterns. This storage class automatically moves objects between two access tiers (frequent and infrequent) based on access patterns, optimizing costs while ensuring low latency and high throughput. However, as the question states that objects in folder A are frequently accessed, Intelligent-Tiering is not the optimal choice.\nReduced Redundancy, also mentioned in option B, is a deprecated storage class that offers lower durability and lower cost than S3 Standard. This storage class is suitable for data that can be recreated if lost and has been replaced by the S3 Standard-Infrequent Access storage class. Therefore, Reduced Redundancy is not a suitable option for long-lived, infrequently accessed data that needs to be available quickly.\nOption A suggests using S3 Glacier for objects in folder B, which is an archival storage class that is designed for long-term retention of data that is infrequently accessed and can tolerate a retrieval time of several hours. Therefore, S3 Glacier is not suitable for objects that need to be retrieved within milliseconds.\nOption D is incorrect as an S3 bucket can have different storage classes for different objects, and there is no need to create two separate buckets.\nIn summary, option C is the optimal choice as it balances cost and access frequency, with S3 Standard for frequently accessed data and One Zone-IA for long-lived, infrequently accessed data that needs to be available quickly.\n\n"
}, {
  "id" : 306,
  "question" : "You are working in a Global Pharma firm, having its Head Office in Washington &amp; Branch offices in Chicago &amp; Paris.\nThe Firm has a two-tier Intranet website deployed in US-East-1 Region &amp; database servers deployed on-premise at the Head office.\nThe Head Office has a Direct Connect link to VPC, and it is connected to Chicago &amp; Paris offices via WAN links, while each of these offices has separate internet links from the local ISP.\nRecently they faced link outage issues with WAN links that resulted in the isolation of the branch offices from the head office.\nThey are looking for a cost-effective backup solution that could be set-up quickly without any additional devices and links.\nWhat would be the most suitable connectivity option in this scenario?\n",
  "answers" : [ {
    "id" : "aacb5bd78db84ef6a983f37cc2d44bad",
    "option" : "Using existing Internet connection in Washington, Chicago, and Paris, set up a Direct Connection with us-east-1 VGW advertising prefixes via BGP. BGP ASN should be unique at these locations. VGW at us-east-1 will re-advertise these prefixes to the Washington office.",
    "isCorrect" : "false"
  }, {
    "id" : "073c88bca3fc41a38f85ffb060d1c853",
    "option" : "Using existing Internet connection in Chicago and Paris, set up a VPN connection with us-east-1 VGW advertising prefixes via BGP. BGP ASN should be unique at these locations. VGW at us-east-1 will re-advertise these prefixes to the Washington office.",
    "isCorrect" : "true"
  }, {
    "id" : "f71bf3b290044f4b8080204d6e0facb7",
    "option" : "Using existing Internet connection in Chicago and Paris, set up a VPN connection between us-west-1 and eu-west-3 regions.",
    "isCorrect" : "false"
  }, {
    "id" : "eaafaec7494b47fd8ffdd7ba967f7fa5",
    "option" : "Using existing Internet connection in Chicago and Paris, set up VPC peering connections from the branch offices to the VPC in the head office.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect as there is no need to setup Direct Connection in us-east-1.\nOption B is CORRECT because VPN connection is suitable to establish the communications between the branch offices and the head office VPC.Option C is incorrect as the question needs to setup VPN connections between branch offices and the head office.\nOption D is incorrect as VPC peering connection is used for the connections between VPCs.\nIn this question, the branch offices in Chicago &amp; Paris do not use VPCs.\nPlease check the following diagram:\nFor more information on using VPN connections, please refer to the following reference:\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html\n\n\nIn this scenario, the company is looking for a cost-effective backup solution that can be set up quickly without any additional devices and links. The company has its head office in Washington, with branch offices in Chicago and Paris. The two-tier Intranet website is deployed in the US-East-1 region, while the database servers are deployed on-premise at the head office. The head office has a Direct Connect link to the VPC, and the branch offices are connected via WAN links. Each office has a separate internet connection from the local ISP.\nOption A: Using existing Internet connection in Washington, Chicago, and Paris, set up a Direct Connection with us-east-1 VGW advertising prefixes via BGP. BGP ASN should be unique at these locations. VGW at us-east-1 will re-advertise these prefixes to the Washington office.\nThis option suggests setting up a Direct Connect connection with the VGW (Virtual Private Gateway) in the US-East-1 region. This would require setting up BGP (Border Gateway Protocol) advertising prefixes that are unique at each location. The VGW in the US-East-1 region would then re-advertise these prefixes to the Washington office. This option can be a suitable solution as it utilizes the existing internet connection in each office and leverages Direct Connect, which provides a more reliable and consistent connection compared to VPN connections. However, it requires the setup of BGP advertising prefixes, which can be complex and time-consuming.\nOption B: Using existing Internet connection in Chicago and Paris, set up a VPN connection with us-east-1 VGW advertising prefixes via BGP. BGP ASN should be unique at these locations. VGW at us-east-1 will re-advertise these prefixes to the Washington office.\nThis option suggests setting up a VPN connection with the VGW in the US-East-1 region. This would require setting up BGP advertising prefixes that are unique at each location. The VGW in the US-East-1 region would then re-advertise these prefixes to the Washington office. This option can be a suitable solution as it utilizes the existing internet connection in each office and leverages VPN connections, which are simpler to set up than Direct Connect. However, VPN connections can be less reliable than Direct Connect connections, especially during high traffic periods.\nOption C: Using existing Internet connection in Chicago and Paris, set up a VPN connection between us-west-1 and eu-west-3 regions.\nThis option suggests setting up a VPN connection between the US-West-1 and EU-West-3 regions. This option does not utilize the existing Direct Connect connection to the US-East-1 region and may require additional setup time and resources to set up a new VPN connection. Additionally, this option does not provide a direct connection to the two-tier Intranet website deployed in the US-East-1 region, which may be a critical resource for the company.\nOption D: Using existing Internet connection in Chicago and Paris, set up VPC peering connections from the branch offices to the VPC in the head office.\nThis option suggests setting up VPC peering connections between the branch offices and the VPC in the head office. This option does not utilize the existing Direct Connect connection to the US-East-1 region and may require additional setup time and resources to set up VPC peering connections. Additionally, this option does not provide a direct connection to the two-tier Intranet website deployed in the US-East-1 region.\nOverall, option A seems to be the most suitable option in this scenario as it leverages the existing Direct Connect connection and utilizes BGP advertising prefixes to provide a reliable and consistent connection between the offices. However, each option has its advantages and disadvantages, and the final decision should consider the specific needs and resources of the company.\n\n"
}, {
  "id" : 307,
  "question" : "You have a lifecycle rule for an S3 bucket that archives objects to the S3 Glacier storage class 60 days after creation.\nThe archived objects are no longer needed one year after being created.\nHow would you configure the S3 bucket to save more cost?\n",
  "answers" : [ {
    "id" : "f81377062147408e811587ac532ff396",
    "option" : "Configure a rule in S3 Glacier to place delete markers for objects that are one year old.",
    "isCorrect" : "false"
  }, {
    "id" : "e07ac4cd832a41fda19533eef7e4a07b",
    "option" : "Configure the S3 lifecycle rule to expire the objects after 365 days from object creation.",
    "isCorrect" : "true"
  }, {
    "id" : "b7523113d01a4f00934c8dc3cc454ab4",
    "option" : "Modify the S3 lifecycle rule to clean up expired object delete markers for one year old objects.",
    "isCorrect" : "false"
  }, {
    "id" : "3f2bfafa4df54518b464485b345be07c",
    "option" : "Modify the S3 lifecycle rule to use S3 Glacier Deep Archive which automatically deletes objects one year after creation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nUsers can configure the expiration actions in S3 lifecycle management.\nDetails can be found in https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html.\nOption A is incorrect: Because users cannot configure a rule to place delete markers in S3 Glacier.\nOption B is CORRECT: Because users can configure the object expiration in the S3 life cycle, Amazon S3 will remove the expired objects.\nOption C is incorrect: Because cleaning up expired object delete markers does not expire the objects.\nThis action does not save costs.\nOption D is incorrect: Because S3 Glacier Deep Archive does not automatically delete objects.\n\nThe most cost-effective option for managing the lifecycle of objects in an S3 bucket is to use S3 lifecycle policies. These policies can be used to automatically move objects to a different storage class or delete them after a specified period of time.\nIn this scenario, the lifecycle rule for the S3 bucket is currently configured to archive objects to the S3 Glacier storage class 60 days after creation. However, it is stated that these archived objects are no longer needed one year after being created. Therefore, we need to modify the S3 lifecycle rule to save more cost.\nOption A, to configure a rule in S3 Glacier to place delete markers for objects that are one year old, is incorrect because it does not address the need to reduce costs. The delete markers do not actually delete the objects themselves, they just mark them for deletion in the future. Therefore, this option would not reduce the storage costs associated with the S3 Glacier storage class.\nOption B, to configure the S3 lifecycle rule to expire the objects after 365 days from object creation, is a possible solution to reduce storage costs. By expiring the objects after one year, they will be automatically deleted and will no longer incur storage costs. This option should be considered if it is confirmed that the archived objects are no longer needed after one year.\nOption C, to modify the S3 lifecycle rule to clean up expired object delete markers for one year old objects, is incorrect because it only addresses the delete markers and not the objects themselves. This option would not reduce the storage costs associated with the S3 Glacier storage class.\nOption D, to modify the S3 lifecycle rule to use S3 Glacier Deep Archive which automatically deletes objects one year after creation, is also a possible solution to reduce storage costs. The S3 Glacier Deep Archive storage class is the lowest-cost storage option offered by S3, and it is designed for long-term data archiving. It is configured to automatically delete objects after 180 days of no access or after 1 year from creation, whichever comes first. Therefore, this option would be an effective way to reduce storage costs.\nIn summary, the correct answer would be either B or D, depending on the specific needs and use case. Option B would be the best option if it is confirmed that the archived objects are no longer needed after one year. Option D would be the best option if there is no specific requirement to retain the archived objects beyond one year, as it offers the lowest-cost storage option.\n\n"
}, {
  "id" : 308,
  "question" : "An application currently allows users to upload files to an S3 bucket.\nYou want to ensure that the file name for each uploaded file is stored in a DynamoDB table.\nHow could this be achieved? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "78f881aca12d44f4b96634829537724e",
    "option" : "Create an AWS Lambda function to insert the required entry for each uploaded file.",
    "isCorrect" : "true"
  }, {
    "id" : "60d993758ab1477b9362ce2e3c7af4bc",
    "option" : "Use AWS CloudWatch to probe for any S3 event.",
    "isCorrect" : "false"
  }, {
    "id" : "d9c8b0d2e5724372ab95963b318d18a6",
    "option" : "Add an event in S3 with notification send to Lambda.",
    "isCorrect" : "true"
  }, {
    "id" : "3b9068657ce34f1fbd0f53ed13897706",
    "option" : "Add the CloudWatch event to the DynamoDB table streams section.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - A and C.\nYou can create a Lambda function containing the code to process the file and add the file's name to the DynamoDB table.\nYou can then use an Event Notification from the S3 bucket to invoke the Lambda function whenever the file is uploaded.\nFor more information on Amazon S3 Event Notifications, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\n\n\nTo achieve this requirement, we need to capture the events that occur when a file is uploaded to the S3 bucket and store the filename in a DynamoDB table. There are different ways to achieve this, but two possible solutions are:\nCreate an AWS Lambda function to insert the required entry for each uploaded file, and configure S3 to trigger the Lambda function whenever a new object is created in the S3 bucket. To do this, follow these steps:\nCreate a new DynamoDB table with a primary key. Create a new Lambda function that will insert a new item in the DynamoDB table with the filename of the uploaded object. Add an S3 trigger to the Lambda function. To do this, go to the S3 bucket, select the Properties tab, then select the Events pane. Add a new event for the ObjectCreated event type and select the Lambda function as the target. Test the solution by uploading a file to the S3 bucket and verifying that a new item with the filename was inserted in the DynamoDB table.\nUse AWS CloudWatch to probe for any S3 event and trigger a Lambda function that will insert the filename in a DynamoDB table. To do this, follow these steps:\nCreate a new DynamoDB table with a primary key. Create a new Lambda function that will insert a new item in the DynamoDB table with the filename of the uploaded object. Create a new CloudWatch event rule that will match any S3 event and trigger the Lambda function. To do this, go to the CloudWatch console, select Rules, then Create Rule. Select the S3 service as the event source and configure the rule to match any S3 event. Then select the Lambda function as the target. Test the solution by uploading a file to the S3 bucket and verifying that a new item with the filename was inserted in the DynamoDB table.\nBoth solutions will achieve the requirement of storing the filename for each uploaded file in a DynamoDB table. The choice of solution will depend on the specific requirements and constraints of the application.\n\n"
}, {
  "id" : 309,
  "question" : "A famous mobile brand is launching its much-awaited mobile phone on Christmas weekend.\nThe company's web applications are deployed in multiple regions and expecting a huge increase in traffic.\nThey want to prioritize their Platinum customers in us-east-1 over new global customers to select various models of new mobile.\nThe IT Team wants the infrastructure to handle huge amounts of traffic without any impact on latency to global users.\nWhich of the following cost-effective design solutions will meet this requirement?\n",
  "answers" : [ {
    "id" : "cffcbddf9dd241bea2f566a52c4f329b",
    "option" : "Create a Lambda@Edge function in all regions to segregate Platinum users along with Amazon CloudFront to cache content nearer to users in all regions.",
    "isCorrect" : "false"
  }, {
    "id" : "7e0af6f241674cd0ab009a5bf1dba87a",
    "option" : "Create a Lambda@Edge function in the US-East-1 region to segregate Platinum users &amp; execute at all regions along with Amazon CloudFront to cache content nearer to users in all regions.",
    "isCorrect" : "true"
  }, {
    "id" : "b66a4d1b796b4f6087bc4058b743bda4",
    "option" : "Use Auto-scaling for origin servers to scale dynamically along with creating separate distribution for Platinum users with Amazon CloudFront to cache content nearer to users in all regions.",
    "isCorrect" : "false"
  }, {
    "id" : "8d533b10c8544e5f8b612463475904e9",
    "option" : "Use Auto-scaling for origin servers to scale on a predefined schedule along with creating separate distribution for Platinum Users with Amazon CloudFront to cache content nearer to users in all regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nLambda@Edge can be a scalable solution to segregate different types of users accessing web applications.\nAmazon CloudFront can be used to cache web content from the origin server to provide users with low latency access.\nOption A is incorrect as Lambda@Edge needs not be created in all regions.\nIt needs to be created in the US-East-1 region &amp; replicated to all regions.\nOption C is incorrect as using On-Demand Auto-scaling with separate distributions will incur additional cost &amp; is not a scalable option.\nOption D is incorrect as using Schedule Auto-scaling with separate distributions will incur additional cost &amp; is not a scalable option.\nFor more information using Lambda@Edge with Amazon CloudFront, refer to the following URL-\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/visitor-prioritization-on-e-commerce-websites-with-cloudfront-and-lambdaedge/\n\nThe requirement of the scenario is to prioritize the Platinum customers in the US-East-1 region over new global customers while ensuring low latency for all users. Additionally, the solution should be cost-effective.\nOption A suggests creating a Lambda@Edge function in all regions to segregate Platinum users along with Amazon CloudFront to cache content nearer to users in all regions. This solution may not be cost-effective as it requires setting up Lambda@Edge function in all regions, which may result in additional costs. Also, it may not effectively segregate Platinum users as there is no guarantee that they are only located in the US-East-1 region.\nOption B suggests creating a Lambda@Edge function in the US-East-1 region to segregate Platinum users and execute it in all regions along with Amazon CloudFront to cache content nearer to users in all regions. This solution is more cost-effective than Option A as it requires setting up a Lambda@Edge function in only one region. It also segregates Platinum users effectively as the Lambda@Edge function executes in all regions, and Platinum users are guaranteed to be located in the US-East-1 region.\nOption C suggests using Auto-scaling for origin servers to scale dynamically along with creating a separate distribution for Platinum users with Amazon CloudFront to cache content nearer to users in all regions. This solution may not effectively segregate Platinum users as there is no guarantee that they are only located in the US-East-1 region. Additionally, dynamically scaling origin servers can result in additional costs.\nOption D suggests using Auto-scaling for origin servers to scale on a predefined schedule along with creating a separate distribution for Platinum Users with Amazon CloudFront to cache content nearer to users in all regions. This solution may not effectively segregate Platinum users as there is no guarantee that they are only located in the US-East-1 region. Also, scaling on a predefined schedule may not be appropriate as traffic may fluctuate unpredictably.\nTherefore, the best solution would be Option B, which involves creating a Lambda@Edge function in the US-East-1 region to segregate Platinum users and executing it in all regions along with Amazon CloudFront to cache content nearer to users in all regions. This solution effectively prioritizes Platinum users and ensures low latency for all users while being cost-effective.\n\n"
}, {
  "id" : 310,
  "question" : "You are working for a global software firm having offices in various continents.\nThe pre-sales team needs to provide a new application demo to a prospective customer.\nFor this, they are looking urgently for a separate temporary connection between 3 on-premises regional offices at Sydney, London, and Tokyo &amp; Demo VPC at the us-west-1 region. You are planning to set up a VPN CloudHub in VGW (Virtual Private Gateway) at us-west-1 for the other three on-premise sites to connect.\nWhat are the factors required to meet this connectivity solution? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "69cbdcb3767f47abb2e5ba9df7c3ac68",
    "option" : "VGW at us-west-1 should be enabled to advertise IP prefixes of each regional office to other regional offices.",
    "isCorrect" : "false"
  }, {
    "id" : "a375780d8bd541e18c5d4c709e444018",
    "option" : "Non-overlapping IP address pool should be configured at each of the regional offices.",
    "isCorrect" : "true"
  }, {
    "id" : "5766b01647354d5782c79d32f643aac8",
    "option" : "Each router should have a BGP (Border Gateway Protocol) peering with other routers at each regional office over VPN connection.",
    "isCorrect" : "false"
  }, {
    "id" : "90e74abb3a92437e862fc48f43e703ba",
    "option" : "BGP (Border Gateway Protocol) ASN (Autonomous System Number) should be unique at these regional offices.",
    "isCorrect" : "true"
  }, {
    "id" : "6317f0a1463b4d77ab866b43105cb79a",
    "option" : "Each of these offices should set up VPN connection to VGW only in that specific region instead of to VGW at us-west-1.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - B, D.\nAWS VPN CloudHub provides connectivity between spoke location over VPN connection.\nIn this case, VGW acts as a Hub &amp; re-advertise prefixes received from one regional office to another regional office.\nFor this connectivity to establish, each regional site should have non-overlapping IP prefixes &amp; BGP ASN unique at each site.\nIf BGP ASN is not unique, additional ALLOWS-IN will be required.\nOption A is incorrect as VGW by default acts as a Hub and spoke &amp; no additional configuration needs to be done at the VGW end.\nOption C is incorrect as the router needs to have BGP peering only with VGW &amp; not with routers in other locations.\nOption E is incorrect as a regional office can set up a VPN connection to VGW of the different regions.\nFor more information on using AWS VPN CloudHub, refer to the following URL-\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html\n\nThe scenario presented in the question requires setting up a temporary connection between three on-premises regional offices located in Sydney, London, and Tokyo, and a Demo VPC located in the us-west-1 region. The proposed solution is to set up a VPN CloudHub in VGW (Virtual Private Gateway) at us-west-1 for the other three on-premise sites to connect. To meet this connectivity solution, the following factors are required:\nA. VGW at us-west-1 should be enabled to advertise IP prefixes of each regional office to other regional offices: When a VPN CloudHub is set up in VGW, it can act as a hub for all the VPN connections from the on-premises regional offices. To enable communication between the offices, the VGW must be configured to advertise the IP prefixes of each regional office to the other regional offices. This way, the on-premises routers can learn about the IP addresses of other on-premises sites, and traffic can be routed accordingly.\nC. Each router should have a BGP (Border Gateway Protocol) peering with other routers at each regional office over VPN connection: To enable the on-premises routers to exchange routing information and learn about the IP prefixes advertised by other on-premises routers, each router should have a BGP peering with the other routers at each regional office over VPN connection. BGP is a protocol used to exchange routing information between different autonomous systems (AS), and it is used in this scenario to exchange routing information between the on-premises regional offices.\nTherefore, options A and C are the correct answers to this question.\nB. Non-overlapping IP address pool should be configured at each of the regional offices: This statement is partially correct. Non-overlapping IP address pools should be configured at each of the regional offices to avoid IP address conflicts. However, this factor is not required to meet the connectivity solution described in the question.\nD. BGP (Border Gateway Protocol) ASN (Autonomous System Number) should be unique at these regional offices: This statement is also partially correct. BGP ASN should be unique at each regional office to ensure proper routing. However, this factor is not required to meet the connectivity solution described in the question.\nE. Each of these offices should set up VPN connection to VGW only in that specific region instead of to VGW at us-west-1: This statement is incorrect. The proposed solution in the question is to set up a VPN CloudHub in VGW at us-west-1 for the other three on-premise sites to connect. Therefore, all the on-premises regional offices should set up VPN connections to VGW at us-west-1.\n\n"
}, {
  "id" : 311,
  "question" : "A hybrid architecture is used for a popular blogging website.\nApplication servers are spread between On-premise Data Centre &amp; EC2 Instance deployed in a custom VPC.\nAn Application Load Balancer is used to offload traffic to the cloud due to capacity constraints at Data Centre.\nFrom Traffic trends, it is observed that the first week of every month, when new blogs are uploaded, a spike in traffic is observed.\nThey are looking for an automated faster option to mitigate additional load on EC2 servers launched behind ALB for this period.\nWhich of the following options can be implemented to meet this requirement?\n",
  "answers" : [ {
    "id" : "66be83cbb2a5461493f4e3febb6165a5",
    "option" : "Use Auto-Scaling OnDemand Scaling to add additional EC2 instances on a VPC different from the VPC in which the ALB is located.",
    "isCorrect" : "false"
  }, {
    "id" : "d31b6325bcf14e22a49b5f493b1165ea",
    "option" : "Use Auto-Scaling Scheduled Scaling to add additional EC2 instances on a VPC different from the VPC in which the ALB is located.",
    "isCorrect" : "false"
  }, {
    "id" : "149426a0984040548643f1f8b03613c9",
    "option" : "Use Auto-Scaling Scheduled Scaling to add additional EC2 instances within the same VPC as the AL.",
    "isCorrect" : "true"
  }, {
    "id" : "86c88643966a42b58f5211aa0189f43b",
    "option" : "Use Auto-Scaling OnDemand Scaling to add additional EC2 instances within the same VPC as the AL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAutoscaling provides an automated way to scale EC2 instances as per capacity requirements.\nFor using ELB with Autoscaling group, both should be in the same region &amp; launched in the same VPC.\nEC2 Auto-Scaling Scheduled Scaling allows you to set your own scaling schedule with a Cron expression that specifies when to act.\nOption A &amp; B are incorrect as ELB &amp; EC2 instances launched should be in the same VPC.Option D is incorrect as for the above case, Scheduled Scaling is a better option than OnDemand scaling.\nFor more information on using Auto-Scaling with ELB, refer to the following URLs-\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/scaling_plan.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\n\nThe best option to meet the requirement of mitigating additional load on EC2 servers launched behind the ALB during the first week of every month is to use Auto-Scaling Scheduled Scaling to add additional EC2 instances within the same VPC as the ALB. This is because Scheduled Scaling allows for pre-defined scaling actions to be taken based on a set schedule.\nOption A is not the best choice because adding additional EC2 instances to a VPC different from the VPC in which the ALB is located can introduce additional network latency and configuration complexity.\nOption B is also not the best choice because Scheduled Scaling is better suited for predictable, planned scaling events rather than for handling sudden traffic spikes.\nOption C is a better choice than option D because Scheduled Scaling allows for pre-defined scaling actions to be taken based on a set schedule, which is suitable for the predictable traffic spikes observed during the first week of every month. Additionally, launching EC2 instances within the same VPC as the ALB reduces network latency and configuration complexity.\nOption D is not the best choice because On-Demand Scaling is better suited for handling sudden traffic spikes rather than for predictable, planned scaling events.\nIn summary, option C, Use Auto-Scaling Scheduled Scaling to add additional EC2 instances within the same VPC as the ALB, is the best choice for meeting the requirement of mitigating additional load on EC2 servers launched behind the ALB during the first week of every month.\n\n"
}, {
  "id" : 312,
  "question" : "You are working for an electrical appliance company that has a web-application hosted in AWS.\nThis is a two-tier web application with web-servers hosted in VPC's &amp; on-premise data-center.You are using a Network Load balancer in the front end to distribute traffic between these servers.\nYou are using instance Id for configuring targets for Network Load Balancer.\nSome clients are complaining about the delay in accessing this website. To troubleshoot this issue, you are looking for a list of Client IP address having longer TLS handshake time.\nYou have enabled access logging on Network Load balancing with logs saved in Amazon S3 buckets.\nWhich tool could be used to quickly analyze many log files without any visualization in a cost-effective way?\n",
  "answers" : [ {
    "id" : "be2753c5948d4cba87b576c36e79cdc6",
    "option" : "Use Amazon Athena to query logs saved in Amazon S3 buckets.",
    "isCorrect" : "true"
  }, {
    "id" : "2c57ac216e024c0c9b1a82305f694fad",
    "option" : "Use Amazon S3 console to process logs.",
    "isCorrect" : "false"
  }, {
    "id" : "c0be737741f94ad5be7134347da71ea4",
    "option" : "Export Network Load Balancer access logs to third-party application.",
    "isCorrect" : "false"
  }, {
    "id" : "35b699ac32a94b689517f7c6da66affa",
    "option" : "Use Amazon Athena along with Amazon QuickSight to query logs saved in Amazon S3 buckets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon Athena is a suitable tool for querying Network Load Balancers logs.\nIn the above case, since a large amount of logs are saved in S3 buckets from the Network load balancer, Amazon Athena can be used to query logs and generate required details of client IP address and TLS handshake time.\nOption B is incorrect as processing many logs directly from the S3 console will be a time-consuming process.\nOption C is incorrect as using a third-party tool will not be a cost-effective solution.\nOption D is incorrect as in the above case, we require only details of Client IP details along with TLS handshake time for troubleshooting purposes.\nAmazon QuickSight will be useful in case you need data visualization.\nFor more information on using Amazon Athena to query Network Load Balancer logs, refer to the following URL-\nhttps://docs.aws.amazon.com/athena/latest/ug/networkloadbalancer-classic-logs.html.\n\nThe best tool to analyze the Network Load Balancer access logs saved in the Amazon S3 bucket in a cost-effective way is Amazon Athena.\nAmazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. It is serverless, meaning there is no infrastructure to manage, and it automatically scales to handle any query load. Amazon Athena allows querying of large amounts of unstructured data stored in S3 buckets quickly and cost-effectively.\nBy using Amazon Athena, it is possible to query the Network Load Balancer access logs saved in the Amazon S3 bucket quickly and easily. It can also help identify the client IP address having longer TLS handshake time.\nAmazon S3 console can be used to process logs, but it is not an efficient way to analyze large amounts of data. Moreover, it does not support querying the data in S3.\nExporting Network Load Balancer access logs to third-party applications can be an option, but it may incur additional costs and require additional setup.\nUsing Amazon Athena along with Amazon QuickSight can provide an interactive dashboard with visualizations to analyze the logs in a more user-friendly manner. However, this may incur additional costs for using Amazon QuickSight.\nTherefore, the correct answer is A. Use Amazon Athena to query logs saved in Amazon S3 buckets.\n\n"
}, {
  "id" : 313,
  "question" : "You are requested to guide a large Pharma company.\nThey are looking for a solution to save all their R&amp;D test analysis data securely.\nDaily large numbers of reports are generated; this data would be accessed parallel from multiple R&amp;D centers spread across the globe.\nThe company requires this data to be instantaneously available to all users.\nWhich of the following is the most suitable way for AWS storage to provide low latency access to users across the globe with the least cost?\n",
  "answers" : [ {
    "id" : "9430e1ccb5cd486683a07ad17bc6f629",
    "option" : "Use Amazon EC2 instance with instance store to store data.",
    "isCorrect" : "false"
  }, {
    "id" : "355afa18251b49a09118df151107212b",
    "option" : "Use Amazon EFS volumes to store data.",
    "isCorrect" : "false"
  }, {
    "id" : "6f7588a9d8d242928ce3fa1692f32276",
    "option" : "Use Amazon EBS volumes connected to the EC2 instance to store data",
    "isCorrect" : "false"
  }, {
    "id" : "ebc008613e9d4fb7b5e805ee47ef55c5",
    "option" : "Use Amazon S3 Buckets to store data.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAmazon S3 can be used to store any amount of data that can be accessed from multiple locations concurrently without any impact on latency.\nOption A is incorrect as the Instance store will provide low latency access from EC2 instances, but it's a temporary storage option.\nOption B is incorrect.\nSince this data would be accessed globally, Amazon EFS will not be ideal for this requirement.\nOption C is incorrect as Amazon EBS volumes would be useful storage for single Amazon EC2 instances.\nFor more information on AWS Storage options, refer to the following URLs-\nhttps://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf\n\nThe most suitable option for storing and accessing data in a low-latency and cost-effective manner for a large Pharma company would be to use Amazon S3 buckets. Here's why:\nA. Using Amazon EC2 instances with instance store to store data would be a bad choice as instance store volumes are temporary, local storage, and not durable. They are also not accessible from other EC2 instances or from outside the instance, which means that the data would be lost if the instance is stopped or terminated. Also, the data would not be accessible to all users globally due to the limited availability of the instance.\nB. Using Amazon EFS volumes to store data would be an option, but it may not be the most cost-effective solution for this use case. EFS is a shared, elastic file storage system that can be accessed by multiple EC2 instances and is designed for scalable, highly available, and durable storage. However, it can be more expensive than S3 and can incur additional charges for data transfer, metadata operations, and accessing files from different regions.\nC. Using Amazon EBS volumes connected to the EC2 instance to store data is similar to using EC2 instances with instance store, as it only provides local storage for the EC2 instance. This solution may not be suitable for storing large volumes of data or providing low-latency access to users across the globe.\nD. Using Amazon S3 buckets to store data is the best solution for this use case. S3 is a highly scalable, durable, and secure object storage service designed for storing and retrieving any amount of data from anywhere in the world. S3 provides low-latency access to data and can be accessed through a simple API or web interface. S3 also provides versioning, lifecycle policies, cross-region replication, and security features to ensure data protection and compliance.\nIn conclusion, the most suitable option for this Pharma company would be to use Amazon S3 buckets to store their R&D test analysis data securely and provide low-latency access to users across the globe at a low cost.\n\n"
}, {
  "id" : 314,
  "question" : "A financial firm, which has a web server in an EC2 Instance, is developing a new web application with static informational content and dynamic functional content with server-side scripting.\nThey expect heavy traffic on the launch of the application.\nThe dynamic content should be stored as files in a file system.\nThe storage of static content should be highly available and cost-effective.\nWhich of the following solutions is the most suitable?\n",
  "answers" : [ {
    "id" : "7cc0772ebed2448b85464106961b64cd",
    "option" : "Use Amazon EFS for dynamic content &amp; Amazon S3 for static content.",
    "isCorrect" : "true"
  }, {
    "id" : "599d123d86ed42188d5331395499eb63",
    "option" : "Use Amazon EBS for dynamic content &amp; Amazon EFS for static content.",
    "isCorrect" : "false"
  }, {
    "id" : "2dcf441882fc4433bbd1fa68e80cb0d5",
    "option" : "Use Amazon S3 for dynamic content &amp; Amazon EBS for static content.",
    "isCorrect" : "false"
  }, {
    "id" : "7c7de2df94e94818b411636aa4ce1dab",
    "option" : "Use Amazon Instance Store for dynamic content &amp; Amazon S3 for static content.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nFor websites with dynamic user interactions, using Amazon EFS is an ideal option to use along with using Amazon S3 for static non-changing data.\nEFS provides a scalable, distributed file system solution where dynamic content can be stored and scaled elastically allowing parallel access from EC2 instances while S3 is best suited for storing &amp; serving static content using a CloudFront distribution.\nOption B is incorrect as EBS is for block storage, not suitable for storing dynamic content.\nEFS can be used to store static content but it is not cost-efficient as S3.\nOption C is incorrect as Amazon S3 cannot be used for dynamic content.\nAmazon EC2 or EFS can be used.\nOption D is incorrect as Amazon Instance Store is temporary storage and is not suited for dynamic web content.\nFor more information on AWS Storage Options, refer to the following URLs-\nhttps://d0.awsstatic.com/whitepapers/AWS%20Storage%20Services%20Whitepaper-v9.pdf\n\nOption A is the most suitable solution for this scenario: using Amazon EFS for dynamic content and Amazon S3 for static content.\nAmazon EFS is a fully-managed, highly scalable, and elastic file system for Linux-based workloads. It is ideal for applications that require shared access to a file system from multiple instances. In this scenario, the dynamic content should be stored as files in a file system, which makes Amazon EFS a suitable choice.\nAmazon S3 is a highly available and cost-effective object storage service that can be used to store and retrieve any amount of data. It provides durability, scalability, and security for data storage needs. The static content of the web application can be stored in Amazon S3, which makes it a suitable choice for this use case.\nOption B is not suitable because Amazon EBS is not suitable for shared access. It is a block-level storage service that can be attached to a single EC2 instance. Therefore, it is not suitable for storing dynamic content that needs to be accessed by multiple instances.\nOption C is not suitable because Amazon S3 is not suitable for storing dynamic content. Amazon S3 is an object storage service and is not suitable for file storage needs, which makes it unsuitable for storing dynamic content.\nOption D is not suitable because Amazon Instance Store is ephemeral storage that is attached to an EC2 instance. The data stored in the instance store is lost when the instance is terminated, which makes it unsuitable for storing dynamic content that needs to be persistent.\nIn summary, using Amazon EFS for dynamic content and Amazon S3 for static content is the most suitable solution for this scenario, as it provides the required features and functionality for the web application's storage needs.\n\n"
}, {
  "id" : 315,
  "question" : "You are working for a global financial company.\nCompany locations spread across various countries upload transaction details data to the S3 bucket in the US-West region.\nA large amount of data is uploaded daily from each of these locations simultaneously.\nYou are using Amazon Athena to query this data &amp; create reports using Amazon QuickSight to create a daily dashboard for the management team.\nIn some cases, while running queries, you are observing Amazon S3 exception errors. Also, in the monthly bills, a high percentage of cost is associated with Amazon Athena.\nWhich of the following could help eliminate S3 errors while querying data and reducing the cost associated with queries? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "218eb32cf20e4121b172e5abd3847c57",
    "option" : "Partition data based upon user credentials",
    "isCorrect" : "false"
  }, {
    "id" : "fece9c8f57f24fc48a1ae9df82e58a55",
    "option" : "Partition data based upon date &amp; location.",
    "isCorrect" : "true"
  }, {
    "id" : "2d2e735511e74d8e9e17005e3a1391a9",
    "option" : "Create a separate Workgroups based upon user groups.",
    "isCorrect" : "true"
  }, {
    "id" : "30c10fde50d149bc899b78bfccd10910",
    "option" : "Create a single Workgroup for all users.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - B and C.\nAWS Athena pricing is based upon per query and the amount of data scanned in each query.\nIn the above case, each regional office is uploading a large amount of data simultaneously.\nThis data needs to be partitioned based upon location &amp; date.\nA separate Workgroup can be created based upon users, teams, applications or workloads.\nThis will minimize the amount of data scanned for each query, improve performance &amp; reducing cost.\nOption A is incorrect as partitioning the data on user credentials is irrelevant here.\nOption D is incorrect as a single Workgroup will not decrease the amount of data scanned per query.\nFor more information on Partitioning data &amp; using Workgroups, refer to the following URLs-\nhttps://docs.aws.amazon.com/athena/latest/ug/partitions.html\nhttps://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html\n\nBased on the scenario described, there are two main challenges that need to be addressed: reducing the occurrence of Amazon S3 exceptions while querying data and reducing the cost associated with Amazon Athena queries. The following options can help to address these challenges:\nA. Partition data based upon user credentials: This option involves partitioning the data based on user credentials. In other words, each user or group of users has their own partition, and they can only access the data within their partition. This approach can help to reduce the amount of data that each user or group needs to query, which can reduce the likelihood of Amazon S3 exceptions. Additionally, since each user or group is only querying their own partition, the overall cost of queries may be reduced. However, this approach may require more upfront planning and management to ensure that partitions are set up correctly and users are only accessing the data they need.\nB. Partition data based upon date and location: This option involves partitioning the data based on date and location. This approach can help to reduce the amount of data that needs to be queried, which can reduce the likelihood of Amazon S3 exceptions. Additionally, since data is partitioned based on location, users may be able to query data that is geographically closer to them, which can help to reduce query times and costs. However, this approach may require more upfront planning and management to ensure that partitions are set up correctly and that queries are optimized to take advantage of partitioning.\nC. Create a separate Workgroups based upon user groups: This option involves creating separate Workgroups for different user groups. Each Workgroup can have its own query queue and query execution settings, which can help to prevent queries from one group from interfering with queries from another group. Additionally, each Workgroup can have its own cost allocation tags, which can help to track costs associated with each group. This approach can help to reduce the likelihood of Amazon S3 exceptions by ensuring that queries are executed in an orderly manner. However, this approach may require more management overhead to set up and maintain the separate Workgroups.\nD. Create a single Workgroup for all users: This option involves creating a single Workgroup for all users. This approach may be simpler to set up and manage compared to option C, but it may not be as effective in reducing the likelihood of Amazon S3 exceptions. Additionally, it may be more difficult to track costs associated with each user or group.\nIn summary, options A and B can help to address the challenges of reducing Amazon S3 exceptions and reducing query costs. Option C may be effective in preventing queries from interfering with each other, while option D may be simpler to set up but may not be as effective in reducing Amazon S3 exceptions or tracking costs.\n\n"
}, {
  "id" : 316,
  "question" : "You are planning to use Auto Scaling groups to maintain the performance of your web application.\nHow would you ensure that the scaling activity has sufficient time to stabilize without executing another scaling action?\n",
  "answers" : [ {
    "id" : "e68a9988846941e383e659cdf761c8de",
    "option" : "Modify the Instance User Data property with a timeout interval.",
    "isCorrect" : "false"
  }, {
    "id" : "393bdf735bf544ebb1b6e4e6c45a648f",
    "option" : "Increase the Auto Scaling Cooldown timer value.",
    "isCorrect" : "true"
  }, {
    "id" : "da8659d90ca64a7783cbdbe4faaa7940",
    "option" : "Enable the Auto Scaling cross zone balancing feature.",
    "isCorrect" : "false"
  }, {
    "id" : "78379d38f64441fe9f9d7e1c9eec5b81",
    "option" : "Disable CloudWatch alarms till the application stabilizes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nThe Cooldown period is a configurable setting for your Auto Scaling group, ensuring that it doesn't launch or terminate additional instances before the previous scaling activity takes effect.\nAfter the Auto Scaling group dynamically scales using a simple Scaling Policy, it waits for the Cooldown period to complete before resuming scaling activities.\nFor more information on Auto Scaling Cooldown, please visit the following URL-\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/Cooldown.html\n\nAuto Scaling groups are a way to ensure that the resources available to an application match the demand of users. With Auto Scaling, you can define rules that determine when to add or remove instances based on certain metrics or conditions.\nWhen scaling up, it's important to ensure that the newly launched instances have enough time to stabilize before executing another scaling action. This is because launching new instances takes time and resources, and if scaling happens too frequently, it can lead to unnecessary cost and disruption to the application.\nTo ensure that scaling activity has sufficient time to stabilize without executing another scaling action, you can increase the Auto Scaling Cooldown timer value. The cooldown period is the amount of time after a scaling activity completes before another scaling activity can begin. By increasing the cooldown timer, you can ensure that there is enough time for the newly launched instances to stabilize before another scaling activity begins.\nOption A, modifying the Instance User Data property with a timeout interval, is not a valid solution as the Instance User Data property is used to specify data that is passed to an instance when it launches, such as bootstrap scripts or user data. It has no direct impact on Auto Scaling cooldown timers.\nOption C, enabling the Auto Scaling cross-zone balancing feature, is also not a valid solution to this problem. Cross-zone balancing distributes instances evenly across multiple Availability Zones. While this can help ensure that the load is balanced across instances, it does not directly impact the cooldown timer.\nOption D, disabling CloudWatch alarms till the application stabilizes, is not a recommended approach as CloudWatch alarms are used to trigger Auto Scaling actions based on predefined metrics. Disabling alarms can lead to delays in scaling activities or missed opportunities to scale up or down, which can negatively impact application performance.\nTherefore, the correct answer to this question is B, increase the Auto Scaling Cooldown timer value.\n\n"
}, {
  "id" : 317,
  "question" : "A company hosts a popular web application that connects to an Amazon RDS MySQL DB instance running in a private VPC subnet created with default ACL settings.\nThe IT Security department has identified a DoS attack from a suspecting IP.\nHow would you protect the subnets from this attack?\n",
  "answers" : [ {
    "id" : "dda9c3fb6eab473eb57952d8f95bed12",
    "option" : "Change the Inbound Security Groups to deny access from the suspecting IP.",
    "isCorrect" : "false"
  }, {
    "id" : "9674d1dc07444047bbafa854244b29bc",
    "option" : "Change the Outbound Security Groups to deny access from the suspecting IP.",
    "isCorrect" : "false"
  }, {
    "id" : "e768bff5dfbe42cab4272879ee258806",
    "option" : "Change the Inbound NACL to deny access from the suspecting IP.",
    "isCorrect" : "true"
  }, {
    "id" : "fc66d3614c27444b8e1db9f388a6ccc2",
    "option" : "Change the Outbound NACL to deny access from the suspecting IP.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOptions A and B are invalid because the Security Groups block traffic by default.\nYou can use NACLs as an additional security layer for the subnet to deny traffic.\nOption D is invalid since just changing the Inbound Rules is sufficient.\nAWS Documentation mentions the following.\nA Network Access Control List (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\nYou might set up network ACLs with rules similar to your security groups to add an additional layer of security to your VPC.\nFor more information on Network Access Control Lists, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\n\nIn this scenario, the company hosts a popular web application that connects to an Amazon RDS MySQL DB instance running in a private VPC subnet created with default ACL settings. The IT Security department has identified a DoS (Denial-of-Service) attack from a suspecting IP. To protect the subnets from this attack, the following options are provided:\nA. Change the Inbound Security Groups to deny access from the suspecting IP. B. Change the Outbound Security Groups to deny access from the suspecting IP. C. Change the Inbound NACL to deny access from the suspecting IP. D. Change the Outbound NACL to deny access from the suspecting IP.\nOut of these four options, the best approach to protect the subnets from a DoS attack from the suspecting IP is to change the Inbound NACL (Network Access Control List) to deny access from the suspecting IP (Option C).\nA Network ACL is a firewall-like feature in AWS that controls inbound and outbound traffic at the subnet level. Network ACLs are stateless, meaning that responses to allowed inbound traffic are not automatically allowed to flow out, and vice versa. They are also ordered, meaning that the rules are evaluated in order, starting with the lowest numbered rule.\nBy default, all inbound and outbound traffic is allowed in a subnet that has the default ACL settings. In this scenario, changing the Inbound NACL to deny access from the suspecting IP is the best option because the DoS attack is coming from that IP address. Denying access from that IP address will prevent the attacker from sending any further traffic to the subnet.\nOption A, changing the Inbound Security Groups to deny access from the suspecting IP, is incorrect because Security Groups are used to control traffic at the instance level, whereas the question is asking about protecting the subnet.\nOption B, changing the Outbound Security Groups to deny access from the suspecting IP, is incorrect because the attacker is sending traffic to the subnet, not receiving traffic from it.\nOption D, changing the Outbound NACL to deny access from the suspecting IP, is incorrect because the attacker is not sending traffic from the subnet, but to it. Changing the Outbound NACL would prevent instances in the subnet from sending traffic to the attacker's IP address, but it would not prevent the attacker from sending traffic to the subnet.\nTherefore, the best approach in this scenario would be to change the Inbound NACL to deny access from the suspecting IP.\n\n"
}, {
  "id" : 318,
  "question" : "A popular educational website is facing a surge in demand for online video training.\nThey have their large number of video content distributed between on-premise data centers &amp; on Amazon S3 bucket in the us-west region.\nStudents worldwide face glitches in videos &amp; complaining about the time required to get these videos running even though each video size is less than 1 Gb.\nThe Marketing Team expects a further increase in demand &amp; you need to provide a scalable solution for this concern that can be deployed in the shortest time frame.\nWhich of the following is a recommended cost-optimized scalable solution?\n",
  "answers" : [ {
    "id" : "7c8190bc5fda4ed58be7dbec8a576a6a",
    "option" : "Use Amazon S3 Cross-Region Replication to replicate content from the us-west region to other regions.",
    "isCorrect" : "false"
  }, {
    "id" : "fb200d7985184b9c8c4a5a10876f75ce",
    "option" : "Use Throughput optimized EBS volumes to save video content.",
    "isCorrect" : "false"
  }, {
    "id" : "ce9f451f94fc43bf84509dd8f37c7d56",
    "option" : "Use Amazon CloudFront for videos saved in on-premise &amp; Amazon S3 origin.",
    "isCorrect" : "true"
  }, {
    "id" : "e166393e7fff47e696f5d2a16315ab43",
    "option" : "Move all content from on-premise data centers to Amazon S3 &amp; enable Transfer Acceleration on this bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon CloudFront Can be used to offload origin server loads.\nIn the above case, for videos saved in on-premise servers &amp; Amazon S3 bucket, Amazon CloudFront can be used to deliver this content to users with less latency &amp; offload load on servers.\nOption A is incorrect as Using storing video content in the Amazon S3 bucket in different regions would incur additional charges.\nOption B is incorrect as EBS would be preferred for rapidly changing web content.\nIn the above case, video content would not be rapidly changing.\nSo storing content in Amazon S3 bucket with Amazon CloudFront distribution is a better option.\nOption D is incorrect as using Amazon S3 transfer Acceleration would be costly compared to Amazon CloudFront.\nAlso, for files less than 1 Gb, using Amazon CloudFront would provide better performance than Amazon S3 Transfer Acceleration.\nFor more information on using Amazon CloudFront for scalable web content, refer to the following URLs-\nhttps://aws.amazon.com/blogs/architecture/scale-your-web-application-one-step-at-a-time/\nhttps://aws.amazon.com/s3/faqs/\n\nThe recommended cost-optimized scalable solution for the educational website facing a surge in demand for online video training is to use Amazon CloudFront for videos saved in on-premise and Amazon S3 origin.\nAmazon CloudFront is a content delivery network (CDN) that delivers content from edge locations closest to the viewer's location, reducing latency and improving performance. By using Amazon CloudFront, the website can distribute the video content to edge locations worldwide, reducing the load on the origin server and improving the user experience.\nThe on-premise video content can be uploaded to Amazon S3 and then distributed through Amazon CloudFront. Amazon CloudFront can also be configured to automatically replicate the content to multiple edge locations, ensuring the content is available worldwide with high availability and low latency.\nUsing Amazon CloudFront is a cost-effective solution as it charges only for the data transfer and the number of requests made to the CDN. Additionally, Amazon CloudFront can be easily integrated with other AWS services such as Amazon S3 and Amazon EC2, making it a scalable solution that can be deployed quickly.\nOption A, which suggests using Amazon S3 Cross-Region Replication, can be a viable solution for disaster recovery purposes. Still, it may not be the most efficient solution for delivering content to end-users due to the increased latency and data transfer costs associated with replicating data across regions.\nOption B, which suggests using Throughput-optimized EBS volumes, is not an optimal solution for video content delivery as EBS volumes are designed for block-level storage, not for content delivery.\nOption D, which suggests moving all content from on-premise data centers to Amazon S3 and enabling Transfer Acceleration on this bucket, may not be the most cost-effective solution as it incurs additional costs for data transfer acceleration. Additionally, it may take time to transfer all the content from the on-premise data center to Amazon S3, which may not be feasible given the time constraints mentioned in the question.\n\n"
}, {
  "id" : 319,
  "question" : "A global conglomerate is looking for a Multi-site DR plan for an application deployed on a server fleet at the on-premises Data Centre.\nThere is also a large database that needs to back up daily.\nIncomplete backups can impact RPO in case of failure.\nThey are looking for high bandwidth links with fast data transfer speed from on-premises to AWS VPC.\nThe connections should be reliable with redundancy.\nWhich of the following is the most appropriate?\n",
  "answers" : [ {
    "id" : "d13d466331744bf98824dc16c28da818",
    "option" : "Create a Direct Connection between on-premise and VP.",
    "isCorrect" : "false"
  }, {
    "id" : "66f26ffce14a49898b1082f1414a775f",
    "option" : "Create multiple Direct Connections with LAG enabled in active mode to provide redundancy.",
    "isCorrect" : "true"
  }, {
    "id" : "1bb239d1d2ba425c8f6f13d7f9e2b8e4",
    "option" : "Create multiple VPN connections with LAG enabled in active mode to provide secure connections.",
    "isCorrect" : "false"
  }, {
    "id" : "6d4f4f50679046ffaddd6ed5df1fbbee",
    "option" : "Create a VPN CloudHub.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nFor higher throughput, LAG can aggregate multiple DX connections to give a maximum of 40 Gig bandwidth.\nOption A is incorrect because there is no redundancy provided in this option.\nOption C is incorrect because the VPN connections cannot provide high performance.\nOption D is incorrect because this option does not provide high speed connections.\nFor more information on using LAG over AWS Direct Connect, refer to the following URLs-\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/lags.html\n\nThe best solution for the scenario described in the question is option B, which suggests creating multiple Direct Connections with Link Aggregation Group (LAG) enabled in active mode to provide redundancy.\nA Direct Connection is a dedicated network connection between on-premises infrastructure and AWS cloud. It provides higher bandwidth and lower latency compared to VPN connections. Direct Connections can be established over a physical fiber-optic cable, and they are more reliable and secure than VPN connections.\nLink Aggregation Group (LAG) is a mechanism to combine multiple physical connections into a logical connection to increase bandwidth and provide redundancy. With LAG enabled in active mode, all physical connections are active and carry traffic simultaneously, and if any connection fails, the traffic is automatically redirected to the remaining active connections.\nBy creating multiple Direct Connections with LAG enabled in active mode, the global conglomerate can have a highly available and reliable connection between on-premises infrastructure and AWS VPC. This will ensure that the Multi-site DR plan and daily backups of the database are carried out smoothly without any impact on RPO.\nOption A, which suggests creating a single Direct Connection, does not provide redundancy in case of failure, which makes it less appropriate than option B.\nOption C, which suggests creating multiple VPN connections with LAG enabled in active mode, is less suitable than option B because VPN connections have lower bandwidth and higher latency than Direct Connections. Also, VPN connections are less reliable and secure than Direct Connections.\nOption D, which suggests creating a VPN CloudHub, is not appropriate for the scenario described in the question because it does not provide the required high bandwidth and fast data transfer speed between on-premises infrastructure and AWS VPC. A VPN CloudHub is used to connect multiple remote sites to a central hub using VPN connections.\n\n"
}, {
  "id" : 320,
  "question" : "Videos are uploaded to an S3 bucket, and you need to provide access to users to view the same.\nWhat is the best way to do so while maintaining a good user experience for all users regardless of the region in which they are located?\n",
  "answers" : [ {
    "id" : "e061103215e64352bbc0a276d5858c28",
    "option" : "Enable Cross-Region Replication for the S3 bucket to all regions.",
    "isCorrect" : "false"
  }, {
    "id" : "1149af8a2830464785ba86d3694ee34b",
    "option" : "Use CloudFront with the S3 bucket as the source.",
    "isCorrect" : "true"
  }, {
    "id" : "f76ed9ca44ac4dd68cac5365b3a4f40a",
    "option" : "Use API Gateway with S3 bucket as the source.",
    "isCorrect" : "false"
  }, {
    "id" : "7270966e69d8488fafe20819e47774a5",
    "option" : "Use AWS Lambda functions to deliver the content to users.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following to back up this requirement.\nAmazon CloudFront is a web service that speeds up the distribution of static and dynamic web content, such as .html, .css, .js, and image files to your users.\nCloudFront delivers your content through a worldwide network of data centers called edge locations.\nWhen a user requests content that you're serving with CloudFront, the user is routed to the edge location that provides the lowest latency (time delay) so that content is delivered with the best possible performance.\nIf the content is already in the edge location with the lowest latency, CloudFront delivers it immediately.\nIf the content is not in that edge location, CloudFront retrieves it from an Amazon S3 bucket or an HTTP server (for example, a web server) that you have identified as the source for the definitive version of your content.\nOption A is incorrect.\nS3 Cross-region replication is a feature that enables an automatic and asynchronous copy of user data from one destination bucket to another destination bucket located in one of the other AWS regions.\nIt is region-based rather than a global scale, which is what the question asks regarding all users having a good experience, regardless of region locale.\nFor more information on Amazon CloudFront, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nThe best way to provide access to users to view videos uploaded to an S3 bucket while maintaining a good user experience for all users regardless of the region in which they are located is to use CloudFront with the S3 bucket as the source (Option B).\nAmazon CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, and no minimum usage commitments. It caches content at edge locations, which are data centers located in multiple geographic locations around the world, providing faster access to content for users.\nUsing CloudFront with S3 as the origin allows you to take advantage of the benefits of a CDN, including faster content delivery, improved user experience, and lower costs, while also leveraging the reliability and scalability of S3 for storing your video files.\nIn addition, CloudFront has built-in features that allow you to optimize the delivery of video content, such as:\nOn-demand streaming: CloudFront supports on-demand streaming of video content, which means that users can start watching a video before it has finished downloading. Adaptive bitrate streaming: CloudFront supports adaptive bitrate streaming, which means that users can receive video content at the highest quality that their device and internet connection can support, ensuring a smooth and uninterrupted viewing experience. Content protection: CloudFront supports various content protection mechanisms, including signed URLs and signed cookies, to ensure that your video content is delivered securely and only to authorized users.\nTherefore, the correct answer is B: Use CloudFront with the S3 bucket as the source.\n\n"
}, {
  "id" : 321,
  "question" : "Your company has a set of 100 servers hosted on the AWS Cloud.\nThere is a need to stream the Logs from the Instances for analysis purposes.\nFrom a security compliance perspective, additional logic will be executed to analyze the data for any sort of abnormal behaviour.\nWhich of the following would be used to stream the log data?\n",
  "answers" : [ {
    "id" : "a054d2af12e14759a278be094b84c2dc",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "dfbf3e2059f9448a9377a22574d0778d",
    "option" : "Amazon SQS",
    "isCorrect" : "false"
  }, {
    "id" : "614cc1bbdb8f48a1819c92263fc47c07",
    "option" : "Amazon Kinesis Data Streams (KDS)",
    "isCorrect" : "true"
  }, {
    "id" : "bbb8fcc9baa54f3eb5c43f3800037b1d",
    "option" : "Amazon SES (Simple Email Service)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nThe AWS Documentation mentions the following.\nAmazon Kinesis Data Streams enables you to build custom applications that process or analyze streaming data for specialized needs.\nYou can continuously add various types of data such as clickstreams, application logs, and social media to an Amazon Kinesis data stream from hundreds of thousands of sources.\nWithin seconds, the data will be available for your Amazon Kinesis Applications to read and process from the stream.\nUse cases of Kinesis Data Streams:\nLog and event data collection.\nReal-time analytics.\nMobile data capture.\nGaming data feed.\nOption A is incorrect since CloudFront is a content distribution service.\nOption B is incorrect since SQS is used as a messaging service.\nOption D is incorrect since SES is an email service.\nFor more information on AWS Kinesis, please visit the below URL.\nhttps://aws.amazon.com/kinesis/data-streams/faqs/\nhttps://aws.amazon.com/kinesis/data-streams/\n\n\nThe appropriate service to use for streaming logs from AWS instances for analysis and additional processing is Amazon Kinesis Data Streams (KDS), which is a scalable and durable real-time data streaming service.\nAmazon CloudFront is a content delivery network (CDN) that accelerates the delivery of static and dynamic web content, such as websites, APIs, and video. It does not support the streaming of log data for analysis.\nAmazon SQS (Simple Queue Service) is a managed message queuing service used to decouple and scale microservices, distributed systems, and serverless applications. While it is possible to use SQS to transport log data from instances to a processing system, it is not the best service for real-time streaming and processing of log data.\nAmazon SES (Simple Email Service) is a cloud-based email sending service designed for marketing and transactional email messages. It is not intended for log data streaming and analysis.\nTherefore, Amazon Kinesis Data Streams (KDS) is the most appropriate service for real-time streaming and processing of log data for analysis purposes. With KDS, you can collect and process large amounts of data in real-time, and analyze it using AWS services such as AWS Lambda, Amazon Elasticsearch Service, or Amazon Kinesis Data Analytics. Additionally, KDS provides scalability, durability, and fault tolerance, which makes it a reliable choice for log data streaming.\n\n"
}, {
  "id" : 322,
  "question" : "A startup company wants to launch an online learning portal on AWS using CloudFront and S3\nThey have different subscription models.\nOne model where all the members will have access to basic content but another model where the company provides premium content that includes access to multiple private contents without changing their current links. How should a Solution Architect design this solution to meet the requirements?\n",
  "answers" : [ {
    "id" : "82e69302e9e540efad3ac93c6a9f9a70",
    "option" : "Design the learning portal using CloudFront web distribution to deliver the premium private content using Signed Cookies.",
    "isCorrect" : "true"
  }, {
    "id" : "981b06a6cbf249ce9e9617ce6c2c6c52",
    "option" : "Design the learning portal using CloudFront web distribution to deliver the premium private content using Signed URLs.",
    "isCorrect" : "false"
  }, {
    "id" : "61639c16fd2f414581cda0d008e05d7a",
    "option" : "Design the learning portal using CloudFront web distribution to deliver the premium private content using S3 pre-signed URLs.",
    "isCorrect" : "false"
  }, {
    "id" : "b355f94967cb40e29e4a319f4f780615",
    "option" : "Design the learning portal using CloudFront web distribution to deliver the premium private content using CloudFront signed URLs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nUse signed cookies in the following cases to provide access to multiple restricted files.\nFor example, all the files for a video in HLS format or all of the files in the subscribers' area of the website.\nYou don't want to change your current URLs.\nOption B is incorrect.CloudFront signed URLs and signed cookies provide the same basic functionality, and they allow users to control who can access the content.\nUse signed URLs in the following cases.\nÂ· You want to restrict access to individual files, for example, an installation download for your application-\nÂ· Your users are using a client (for example, a custom HTTP client) that doesn't support cookies.\nOption C is incorrect because S3 pre-signed URLs won't provide access without changing current links.\nOption D is incorrect because CloudFront UTLS won't provide access without changing current links.\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\n\nThe best answer for this scenario is B. Design the learning portal using CloudFront web distribution to deliver the premium private content using Signed URLs.\nExplanation: In this scenario, the startup company wants to launch an online learning portal on AWS using CloudFront and S3. The company has two different subscription models, where all the members will have access to basic content, but the premium content includes access to multiple private contents without changing their current links.\nCloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally. Amazon S3 is a highly scalable and reliable object storage service that can store and retrieve any amount of data from anywhere on the web.\nTo meet the startup company's requirements, the solution architect needs to design a solution that allows the delivery of premium private content to the subscribers who have paid for it.\nOption A: Design the learning portal using CloudFront web distribution to deliver the premium private content using Signed Cookies.\nSigned cookies are used to control access to the content for authorized users. However, this option does not meet the requirement of providing premium private content to subscribers who have paid for it. Instead, signed cookies would grant access to specific content to all authorized users.\nOption B: Design the learning portal using CloudFront web distribution to deliver the premium private content using Signed URLs.\nThis option meets the requirement of providing premium private content to subscribers who have paid for it. Signed URLs are a secure way of granting access to specific content to authorized users who have paid for it. When a user accesses a signed URL, they are granted temporary access to the content that is specified in the URL.\nOption C: Design the learning portal using CloudFront web distribution to deliver the premium private content using S3 pre-signed URLs.\nPre-signed URLs grant time-limited permission to access a specific object in Amazon S3. However, this option does not meet the requirement of providing premium private content to subscribers who have paid for it. Instead, pre-signed URLs would grant access to specific content to all authorized users.\nOption D: Design the learning portal using CloudFront web distribution to deliver the premium private content using CloudFront signed URLs.\nThis option is similar to Option B but does not meet the requirement of providing premium private content to subscribers who have paid for it. Instead, CloudFront signed URLs would grant access to specific content to all authorized users.\nTherefore, the best answer for this scenario is B. Design the learning portal using CloudFront web distribution to deliver the premium private content using Signed URLs.\n\n"
}, {
  "id" : 323,
  "question" : "An organization has an on-premises messaging application.\nThey want to migrate this application to the AWS cloud without making much code changes while running an on-prem system parallel with the AWS cloud in the hybrid model. How would Solution Architect migrate the highly available solution and support JMS APIs, AMQP, and MQTT protocols?\n",
  "answers" : [ {
    "id" : "39446482190e4e1683d36bb8caa4c33c",
    "option" : "Design the solutions using SNS that also supports integration with other AWS services.",
    "isCorrect" : "false"
  }, {
    "id" : "df5af10b93954d0294c28eed03e0443e",
    "option" : "Design the solutions using SQS that also supports integration with other AWS services.",
    "isCorrect" : "false"
  }, {
    "id" : "f6df7afefce146db870ee591fc15c802",
    "option" : "Design the solutions using SQS, SNS and Lambda.",
    "isCorrect" : "false"
  }, {
    "id" : "25f6a6abb13c43b09bb159ffb8d27fc2",
    "option" : "Design the solutions using Amazon MQ in 2 private subnets across multiple Availability Zones.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer : D.\nKey points to note in this question are hybrid running two systems.\nSystems should be highly available, cloud migration, supports both queues and topics (pub and sub), supports JMS API and MQTT protocols, without making code change.\nOption A is incorrect: SNS is a highly available AWS service that provides topics but doesn't make code change requirements and hybrid running of two systems.\nSNS supports the push model of asynchronous communications in the following picture.\nOption B is incorrect: SQS is a highly available AWS service that provides queues but doesn't make code change requirements and hybrid running of two systems.\nOption C is incorrect: SQS and SNS are highly available AWS services that provide queues and topics but don't make code change requirements.\nThis will be a good solution for the long run if I need to rewrite with SNS and SQS that supports other AWS services.\nOption D is CORRECT: Amazon MQ is a managed AWS service for Apache Active MQ that meets both SQS (queues) and SNS (Topics) functionality.It also supports JMS API and other key protocols like MQTT etc.\nThis is a good fit to migrate on-prem applications using traditional message brokers.\nReference:\nhttps://aws.amazon.com/amazon-mq/?amazon-mq.sort-by=item.additionalFields.postDateTime&amp;amazon-mq.sort-order=des\n\n\nThe organization wants to migrate an on-premises messaging application to AWS cloud while running the on-premises system in parallel. The migration should require minimal code changes and should support JMS APIs, AMQP, and MQTT protocols.\nTo meet these requirements, the recommended approach is to use a messaging service that can support these protocols and can integrate with other AWS services. There are multiple messaging services available on AWS, including Amazon SNS, Amazon SQS, and Amazon MQ.\nOption A: Design the solutions using SNS that also supports integration with other AWS services. Amazon SNS (Simple Notification Service) is a messaging service that can support JMS APIs, AMQP, and MQTT protocols. It can integrate with other AWS services such as AWS Lambda, AWS CloudFormation, and Amazon EC2 instances. However, SNS is not a queue-based messaging system, and it may not be suitable for applications that require message buffering or queuing. Therefore, this option may not be the best fit for this scenario.\nOption B: Design the solutions using SQS that also supports integration with other AWS services. Amazon SQS (Simple Queue Service) is a fully managed message queuing service that supports JMS APIs, AMQP, and MQTT protocols. It can integrate with other AWS services such as AWS Lambda, AWS CloudFormation, and Amazon EC2 instances. With SQS, the organization can migrate the application with minimal code changes and keep both on-premises and AWS systems running in parallel. Therefore, this option is a good fit for this scenario.\nOption C: Design the solutions using SQS, SNS and Lambda. This option includes using Amazon SQS for message queuing and Amazon SNS for notification and AWS Lambda for serverless computing. It can be used to build a highly scalable and fault-tolerant messaging system that can handle a large volume of messages. However, this option may be more complex than required for this scenario, and it may require significant code changes to implement.\nOption D: Design the solutions using Amazon MQ in 2 private subnets across multiple Availability Zones. Amazon MQ is a managed message broker service that supports JMS APIs, AMQP, and MQTT protocols. It can be deployed in multiple Availability Zones to provide high availability and fault tolerance. This option may require minimal code changes and can support the desired protocols, but it may require more setup and configuration than other options.\nBased on the requirements and considerations, Option B: Design the solutions using SQS that also supports integration with other AWS services, is the most suitable solution.\n\n"
}, {
  "id" : 324,
  "question" : "A company needs to monitor the read and write IOPS metrics for its AWS MySQL RDS instance and send real-time alerts to its Operations team.\nWhich AWS services could help to accomplish this? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "ecb5671e7a804779a6b7d838d3d649de",
    "option" : "Amazon Simple Email Service",
    "isCorrect" : "false"
  }, {
    "id" : "ca626afab9ca4e1fa0b8096e5b72700b",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "true"
  }, {
    "id" : "cf193ea04656455f881e6593e99c72ec",
    "option" : "Amazon Simple Queue Service",
    "isCorrect" : "false"
  }, {
    "id" : "ecf8f2f463024a3db8cba7213834cc1b",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "c2441bb771cc4f688ceb5619971ad9e0",
    "option" : "Amazon Simple Notification Service.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers - B and E.\nAmazon CloudWatch may be used to monitor IOPS metrics from the RDS instance and Amazon Simple Notification Service to send the notification if an alarm is triggered.\nFor more information on CloudWatch metrics, please refer to the link below.\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CW_Support_For_AWS.html\n\nThe two AWS services that could help accomplish this are Amazon CloudWatch and Amazon Simple Notification Service (SNS).\nAmazon CloudWatch is a monitoring and management service that provides data and insights into AWS resources and applications. It can be used to monitor metrics such as read and write IOPS for an RDS instance. CloudWatch can also be used to set alarms on these metrics, which can trigger automated actions such as sending a notification via SNS.\nAmazon Simple Notification Service (SNS) is a fully managed messaging service that enables the sending of notifications to a variety of endpoints, such as email, SMS, mobile push notifications, and more. SNS can be used in combination with CloudWatch to send real-time alerts when specific conditions are met, such as when read and write IOPS exceed a certain threshold.\nAmazon Simple Email Service (SES) is a cloud-based email sending service, which is not directly relevant to this scenario. Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables decoupling and scaling of microservices, but it does not directly relate to real-time monitoring and alerting. Amazon Route 53 is a scalable domain name system (DNS) service that routes users to AWS resources, but it does not provide monitoring or alerting capabilities.\n\n"
}, {
  "id" : 325,
  "question" : "You run an ad-supported photo-sharing website using S3 to serve photos to visitors of your site.\nAt some point, you find out that other sites have been linking to photos on your site, causing loss to your business.\nWhat would be an effective method to mitigate this?\n",
  "answers" : [ {
    "id" : "89236167ae5a49d482978dad526cf764",
    "option" : "Use CloudFront distributions for static content.",
    "isCorrect" : "false"
  }, {
    "id" : "37bdb515308d40a7951d749e5b048e3a",
    "option" : "Store photos on an EBS volume of the web server.",
    "isCorrect" : "false"
  }, {
    "id" : "b82373efa1164777b59ef121e1809c03",
    "option" : "Remove public read access and use presigned URL with expiration.",
    "isCorrect" : "true"
  }, {
    "id" : "1eec8dd765534981a828c318169b34d7",
    "option" : "Block the IPs of the offending websites in Security Groups.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nWhen you create a presigned URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time.\nThe presigned URLs are valid only for the specified duration.\nThe credentials that you can use to create a presigned URL include:\nIAM instance profile: Valid up to 6 hours.\nAWS Security Token Service: Valid up to 36 hours when signed with permanent credentials, such as the credentials of the AWS account root user or an IAM user.\nIAM user: Valid up to 7 days when using AWS Signature Version 4\nTo create a presigned URL that's valid for up to 7 days, first designate IAM user credentials (the access key and secret access key) to the SDK that you're using.\nThen, generate a presigned URL using AWS Signature Version 4.\nFor more information on presigned URLs, please visit the link below.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\n\nThe correct answer for this scenario would be C - Remove public read access and use presigned URL with expiration.\nExplanation: The scenario describes a problem where other websites are linking to photos on the photo-sharing website, causing a loss to the business. This indicates that the photos are publicly accessible, which means that anyone can access them if they know the URL. To mitigate this, the website owner should remove the public read access from the S3 bucket and instead use presigned URLs with expiration.\nPresigned URLs are a way to grant temporary access to objects in an S3 bucket. When a user requests a presigned URL, they receive a URL that contains a signature. This signature allows them to access the object for a limited time. After the expiration time has passed, the URL will no longer work. This means that even if someone knows the URL for a photo on the website, they will not be able to access it indefinitely.\nBy using presigned URLs with expiration, the website owner can control who has access to the photos on their website. They can generate new URLs as needed, and they can set the expiration time to a short period, such as a few minutes or hours, to ensure that the URLs are not shared or used for malicious purposes.\nOption A - Using CloudFront distributions for static content - is a good solution for improving the performance of the website by caching the content and distributing it through a global network. However, it does not address the issue of unauthorized access to the photos.\nOption B - Storing photos on an EBS volume of the web server - is not a good solution because it can result in slower performance and higher costs. It also does not address the issue of unauthorized access to the photos.\nOption D - Blocking the IPs of the offending websites in Security Groups - is not a good solution because it is a reactive measure and does not prevent other websites from linking to the photos in the future. Additionally, it can result in false positives and block legitimate users from accessing the website.\n\n"
}, {
  "id" : 326,
  "question" : "An Organization has an application using Amazon S3 Glacier to store large CSV objects.\nWhile retrieving these large objects end users are observing some performance issue.\nIn most cases, users only need a small part of the data instead of the entire objects.\nA solutions Architect has been asked to re-design this solution to improve the performance.\nWhich solution is the most cost-effective?\n",
  "answers" : [ {
    "id" : "379d030978b14dfcb594628093c9194b",
    "option" : "Use AWS Athena to retrieve only the data that users need.",
    "isCorrect" : "false"
  }, {
    "id" : "e7bd2ba49c914ab3833adc3c182f340c",
    "option" : "Use S3 Select to retrieve only the data that users need.",
    "isCorrect" : "false"
  }, {
    "id" : "e578cb8230744719a5d97940703b3781",
    "option" : "Use Glacier Select to retrieve only the data that users need.",
    "isCorrect" : "true"
  }, {
    "id" : "d0a29f1a4b3741cc89d6ab86f85e7afd",
    "option" : "Use custom SQL statements and S3 APIs to retrieve only the data that users need.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer : C.\nOption A is incorrect because Athena supports standard SQL to retrieve data, but it's more used for analytical data.\nOption B is incorrect because the objects are stored in Glacier.\nThe correct method should be Glacier Select.\nOption C is CORRECT because the application needs to retrieve data from Glacier.\nWith Glacier Select, you can perform filtering directly against a Glacier object using standard SQL statements.\nOption D is incorrect because writing custom SQL statements with S3 APIS wouldn't improve performance.\nReference:\nhttps://aws.amazon.com/blogs/aws/s3-glacier-select/\n\nThe most cost-effective solution for improving the performance of retrieving large objects from Amazon S3 Glacier, when users only need a small part of the data, is to use S3 Select.\nS3 Select is an Amazon S3 feature that enables you to retrieve a subset of data from an object using simple SQL expressions. It allows you to filter and transform data within an object without having to retrieve the entire object, which can greatly improve performance.\nUsing AWS Athena, a serverless query service that makes it easy to analyze data in Amazon S3 using standard SQL, is another option. However, it may not be as cost-effective as S3 Select since it requires a dedicated Athena instance and incurs charges based on the amount of data scanned.\nGlacier Select is also an option for retrieving only the necessary data from Glacier archives, but it incurs additional charges for data retrieval and is more suited for when you need to retrieve specific data from large archives.\nUsing custom SQL statements and S3 APIs to retrieve only the data that users need is another option, but it requires additional development effort and may not be as efficient as using S3 Select.\nTherefore, the most cost-effective and efficient solution for improving the performance of retrieving large objects from Amazon S3 Glacier, when users only need a small part of the data, is to use S3 Select.\n\n"
}, {
  "id" : 327,
  "question" : "A cash-starved start-up firm is using AWS Storage Gateway to back up all on-premise data to Amazon S3\nFor this, they have set up VPN connectivity to VGW from client end devices using existing internet links.\nThey are recently observing data backups taking a long time to complete due to large data size.\nThey are also looking for an immediate resolution for quick data backup.\nWhich of the following is a cost-effective way to faster data backups on the VPN tunnel?\n",
  "answers" : [ {
    "id" : "3089c538090d426fbd289d6b72048977",
    "option" : "Create a new VPN tunnel with ECMP enabled on a separate VGW.",
    "isCorrect" : "false"
  }, {
    "id" : "91fbc544c6544a8eb2665f582edf04d4",
    "option" : "Create a new VPN tunnel with ECMP enabled on the same VGW.",
    "isCorrect" : "false"
  }, {
    "id" : "c55c5c34ffee4514aaa1829193b62305",
    "option" : "Create an additional VPN tunnel using a different VGW-Client end device",
    "isCorrect" : "false"
  }, {
    "id" : "d89efc7cb790419ab2aca4b4c66133d8",
    "option" : "Enable ECMP with multiple VPN tunnels associated with a transit gateway.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nFor each VPN Tunnel, AWS provides two different VPN endpoints.\nECMP (Equal Cost Multi-Path) can be used to carry traffic on both VPN endpoints, increasing performance &amp; faster data transfer.\nOptions A &amp; B are incorrect because ECMP needs to be enabled through a transit gateway.\nOption C is incorrect as creating a separate VPN tunnel will incur additional cost &amp; will not enhance any performance without enabling ECMP.\nFor more information on AWS VPN performance, refer to the following URL-\nhttps://aws.amazon.com/vpn/features/\nhttps://aws.amazon.com/premiumsupport/knowledge-center/transit-gateway-ecmp-multiple-tunnels/\n\nThe situation here is that a startup firm is using AWS Storage Gateway to back up on-premise data to Amazon S3, and they have set up VPN connectivity to VGW from client end devices using existing internet links. However, the firm has noticed that data backups are taking a long time to complete due to large data sizes. They are looking for a cost-effective way to quickly backup data over the VPN tunnel.\nOption A: Create a new VPN tunnel with ECMP enabled on a separate VGW. ECMP stands for Equal Cost Multipath, which enables the use of multiple paths for traffic between two endpoints. In this option, a new VPN tunnel would be created with ECMP enabled on a separate VGW. This would allow traffic to be split between two different paths, increasing bandwidth and potentially reducing backup time. However, this option requires the startup firm to have additional VGW, which may not be cost-effective.\nOption B: Create a new VPN tunnel with ECMP enabled on the same VGW. This option is similar to option A, but instead of using a separate VGW, ECMP is enabled on the same VGW. This would still allow traffic to be split between two different paths, potentially increasing bandwidth and reducing backup time. This option may be more cost-effective than option A, as it does not require additional VGW.\nOption C: Create an additional VPN tunnel using a different VGW-Client end device. In this option, an additional VPN tunnel would be created using a different VGW-Client end device. This would allow traffic to be split between two different VPN tunnels, potentially increasing bandwidth and reducing backup time. However, this option requires the startup firm to have additional VGW, which may not be cost-effective.\nOption D: Enable ECMP with multiple VPN tunnels associated with a transit gateway. A transit gateway is a network transit hub that enables interconnection between multiple VPCs and VPN connections. In this option, ECMP would be enabled with multiple VPN tunnels associated with a transit gateway. This would allow traffic to be split between multiple VPN tunnels, potentially increasing bandwidth and reducing backup time. This option may be more cost-effective than option C and A, as it uses a transit gateway, which can be more cost-effective than additional VGW.\nOverall, the best option would be D. Enabling ECMP with multiple VPN tunnels associated with a transit gateway. This option offers the potential to split traffic between multiple VPN tunnels, which can increase bandwidth and reduce backup time, while also being cost-effective.\n\n"
}, {
  "id" : 328,
  "question" : "Developer Team is working on a new mobile game that will use Amazon DynamoDB to store player details.\nThe team is unsure of the success of this game, but needs to make sure it will meet demand for any number of concurrent players.\nDuring the table's initial creation, they are planning to create a local secondary index to create a top ten players scores dashboard.\nAlso, a global secondary index is created to prepare a separate top ten players per country.\nIT Head is concerned about the game's performance, which will be used as a reference for all future games.\nWhich of the following can be used to meet this requirement?\n",
  "answers" : [ {
    "id" : "19ce86f087df473bb79d2bebad363a41",
    "option" : "Enable Auto-Scaling for DynamoDB Table with same setting applied to Global Secondary Index.",
    "isCorrect" : "true"
  }, {
    "id" : "30bba7851a904d0d96e5b712e2f5a21d",
    "option" : "Enable Auto-Scaling for DynamoDB Table with same setting applied to Local Secondary Index.",
    "isCorrect" : "false"
  }, {
    "id" : "c887badf0fc849318020d637327fd63f",
    "option" : "Enable Auto-Scaling only for Global Secondary Index.",
    "isCorrect" : "false"
  }, {
    "id" : "6721c712c4304b57af3210d143c7f744",
    "option" : "Enable Auto-Scaling only for Local Secondary Index.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nFor applications where database utilization cannot be predicted, Amazon DynamoDB can be used with Auto Scaling, which can help to scale dynamically to any load.\nAuto-Scaling needs to be applied to the DynamoDB table and Global Secondary Index that use separate read /write capacity.\nOption B is incorrect as the Local Secondary Index uses the same read &amp; write capacity as the primary DynamoDB table.\nHence, it needs to enable a separate Auto-Scaling policy.\nOption C is incorrect as Auto-Scaling needs to enable for both DynamoDB tables and Global Secondary Index.\nOption D is incorrect as Auto-Scaling needs to enable on the DynamoDB table &amp; Global Secondary Index &amp; not on the Local Secondary index.\nFor more information on using Auto-Scaling with Amazon DynamoDB, refer to the following URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\n\nTo meet the requirement of ensuring that the DynamoDB table can handle any number of concurrent players, while also ensuring optimal performance, auto-scaling should be enabled. Auto-scaling will automatically adjust the capacity of the DynamoDB table and indexes based on the actual usage and demand.\nOption A: Enable Auto-Scaling for DynamoDB Table with same setting applied to Global Secondary Index.\nThis option is the most appropriate solution to meet the requirements. Enabling auto-scaling for the DynamoDB table with the same settings applied to the Global Secondary Index will ensure that both the table and the index can handle any number of concurrent players. When auto-scaling is enabled, the table will automatically increase or decrease its capacity based on the demand. If the capacity of the table is increased, the Global Secondary Index will also automatically increase its capacity accordingly.\nOption B: Enable Auto-Scaling for DynamoDB Table with same setting applied to Local Secondary Index.\nThis option is not the best solution because the Local Secondary Index is dependent on the capacity of the DynamoDB table. If the capacity of the table is not enough to support the index, then enabling auto-scaling for the Local Secondary Index alone will not be sufficient to meet the requirements.\nOption C: Enable Auto-Scaling only for Global Secondary Index.\nThis option is not the best solution because enabling auto-scaling only for the Global Secondary Index will not be enough to ensure that the DynamoDB table can handle any number of concurrent players. The table capacity should also be increased accordingly if the demand increases.\nOption D: Enable Auto-Scaling only for Local Secondary Index.\nThis option is not the best solution because enabling auto-scaling only for the Local Secondary Index will not be enough to ensure that the DynamoDB table can handle any number of concurrent players. The table capacity should also be increased accordingly if the demand increases.\nIn conclusion, option A is the most appropriate solution to meet the requirement of ensuring that the DynamoDB table can handle any number of concurrent players while ensuring optimal performance.\n\n"
}, {
  "id" : 329,
  "question" : "A global pharma company has a tie-up with hospitals across the globe.\nThe hospitals share patient reports with the pharma company, which are further analyzed &amp; used to create new drug formulations.\nDaily large numbers of reports are shared by these hospitals, which are uploaded from various sources.\nPharma firm is planning to tie up with more hospitals, which will increase the data load.\nThese uploadings should be scalable to save a large amount of data for further analysis.\nWhich of the following can be used for a scalable application solution in AWS?\n",
  "answers" : [ {
    "id" : "9cc90c711c174ca5816aee407ca42fb2",
    "option" : "Use AWS Kinesis Streams to upload data to Amazon Redshift.",
    "isCorrect" : "false"
  }, {
    "id" : "c17be20783b3483ba9bcd16b5b9abc52",
    "option" : "Use AWS Kinesis Firehose to upload data to Amazon Redshift.",
    "isCorrect" : "true"
  }, {
    "id" : "87eeb3cc5dc84d2298e54e989cfd6e54",
    "option" : "Use AWS Kinesis Streams to upload data to Amazon RDS.",
    "isCorrect" : "false"
  }, {
    "id" : "83cdd31769154564a7aa40182e5c30bf",
    "option" : "Use AWS Kinesis Firehose to upload data to Amazon RDS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon Kinesis Firehose is a scalable option to load data to analytical tools like Amazon Redshift from multiple sources.\nAmazon Redshift can be used for complex analytical queries for large amounts of data.\nOption A is incorrect as AWS Kinesis Streams cannot directly upload data to Amazon Redshift.\nAdditional applications need to be installed to get these data from Kinesis Streams &amp; upload them to Amazon Redshift.\nOptions C &amp; D are incorrect as Amazon Redshift will be a better option for analyzing large streams of data instead of Amazon RDS.\nFor more information on using Amazon Redshift with Kinesis Firehose, refer to the following URLs-\nhttps://aws.amazon.com/kinesis/data-firehose/\nhttps://aws.amazon.com/redshift/faqs/\n\nFor a scalable solution in AWS, it is important to consider the requirements of the pharma company's tie-up with hospitals. The company needs to store and analyze a large amount of patient reports that are uploaded from various sources on a daily basis.\nAWS provides various services for data ingestion and storage, including Kinesis Streams, Kinesis Firehose, Amazon Redshift, and Amazon RDS.\nKinesis Streams is a real-time data streaming service that allows you to build custom applications that process or analyze streaming data for specialized needs. Amazon RDS is a managed relational database service that allows you to set up, operate, and scale a relational database in the cloud.\nKinesis Firehose is a fully managed service that delivers real-time streaming data to destinations such as Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk. Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse that makes it simple and cost-effective to analyze all your data using standard SQL and existing business intelligence tools.\nOption A: Use AWS Kinesis Streams to upload data to Amazon Redshift. This option would allow the pharma company to stream the data from the hospitals directly to Amazon Redshift. Kinesis Streams can handle real-time streaming data, but it requires custom coding to process and analyze the data. Amazon Redshift can handle petabyte-scale data and is designed for data warehousing and business intelligence. This option is feasible but would require custom coding to process the data, which may add complexity to the solution.\nOption B: Use AWS Kinesis Firehose to upload data to Amazon Redshift. This option would allow the pharma company to stream the data from the hospitals directly to Amazon Redshift via Kinesis Firehose. Kinesis Firehose handles the data delivery and automatically loads the data into Amazon Redshift. This option is more straightforward than Option A and is recommended for a scalable solution.\nOption C: Use AWS Kinesis Streams to upload data to Amazon RDS. This option would allow the pharma company to stream the data from the hospitals directly to Amazon RDS. However, RDS is a managed relational database service, and it may not be the best fit for handling large amounts of data. Additionally, custom coding would be required to process and analyze the data, which may add complexity to the solution.\nOption D: Use AWS Kinesis Firehose to upload data to Amazon RDS. This option would allow the pharma company to stream the data from the hospitals directly to Amazon RDS via Kinesis Firehose. However, RDS may not be the best fit for handling large amounts of data, and custom coding would be required to process and analyze the data, which may add complexity to the solution.\nIn summary, the best option for a scalable solution would be to use AWS Kinesis Firehose to upload data to Amazon Redshift. This option allows the pharma company to easily stream large amounts of data from the hospitals and load it directly into Amazon Redshift, which is designed for data warehousing and business intelligence.\n\n"
}, {
  "id" : 330,
  "question" : "An IT firm is using AWS cloud infrastructure for its three-tier web application.\nThey are using memory-optimized EC2 instances for application hosting &amp; SQL based database servers deployed in Multi-AZ with auto-failover.\nRecently, there are observing heavy loads on database servers.\nThis is impacting user data lookup from application servers resulting in slow access.\nAs AWS Consultants, they are looking for guidance to resolve this issue.\nWhich of the following will provide a faster scalable option to deliver data to users without impacting backend servers?\n",
  "answers" : [ {
    "id" : "05c9aa14f4f546a986a18f509bdc5940",
    "option" : "Use Amazon ElastiCache to cache data.",
    "isCorrect" : "true"
  }, {
    "id" : "30aafb1ef3124cf18047c61f18427389",
    "option" : "Configure the Multi-AZ replicas to serve the read traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "f9baa282517e453e862e62b21ae585ac",
    "option" : "Use Amazon CloudFront to save recently accessed data in cache.",
    "isCorrect" : "false"
  }, {
    "id" : "67ec14b01761473895b89cc9c6fbaefe",
    "option" : "Use on-host caching on memory optimised EC2 instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon ElastiCache provides a scalable faster approach to cache data which can be used with both SQL/NoSQL databases.\nThese can be used to save application data, significantly improving latency &amp; throughout the application and offloading load on back-end database servers.\nOption B is incorrect as a Multi-AZ replica is used for data redundancy.\nIt is not used to share the read traffic.\nOption C is incorrect as Amazon CloudFront is more suitable for website caching &amp; not for application caching.\nOption D is incorrect as this would provide a faster lookup of data, but it's not scalable.\nFor more information on using Amazon ElastiCache, refer to the following URLs-\nhttps://d0.awsstatic.com/whitepapers/performance-at-scale-with-amazon-elasticache.pdf\n\nIn this scenario, the IT firm is facing slow access to user data due to heavy loads on the database servers. The goal is to provide a faster and scalable option to deliver data to users without impacting backend servers.\nOption A: Use Amazon ElastiCache to cache data\nAmazon ElastiCache is a managed, in-memory caching service that allows you to improve the performance of your applications by retrieving data from fast, managed, in-memory caches instead of relying entirely on slower disk-based databases. This option can provide a faster response time to the end-users by storing frequently accessed data in the cache. It also reduces the load on the backend servers, as requests are served from the cache instead of hitting the database directly. This option is recommended when your application is read-heavy, and the data is not changing frequently.\nOption B: Configure the Multi-AZ replicas to serve the read traffic\nAmazon RDS Multi-AZ deployment automatically provisions and maintains a synchronous standby replica in a different availability zone (AZ). In the event of a primary database instance failure, Amazon RDS automatically fails over to the standby replica to minimize the impact of a potential outage. By configuring Multi-AZ replicas to serve the read traffic, you can offload the read workload from the primary database instance and distribute it across multiple replicas, providing faster read performance. However, this option will not scale well for write-intensive workloads, and the cost of running multiple replicas can add up.\nOption C: Use Amazon CloudFront to save recently accessed data in cache\nAmazon CloudFront is a global content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers worldwide. By caching content closer to the end-users, it reduces latency and improves the overall performance of the application. With Amazon CloudFront, you can configure the TTL (time-to-live) of cached objects to determine how long they should be stored in the cache. This option can be useful for serving static content such as images, CSS files, and JavaScript files, but it may not be the best option for dynamic content that changes frequently.\nOption D: Use on-host caching on memory optimized EC2 instance\nOn-host caching is a technique that involves storing frequently accessed data in memory on the application server to reduce the number of database queries. By caching data on the application server, you can improve application performance and reduce the load on the database server. However, this option is not scalable, as it is limited by the memory capacity of the host, and it may not be effective for read-heavy workloads.\nIn conclusion, based on the scenario provided, the most appropriate option to provide a faster and scalable option to deliver data to users without impacting backend servers is A: Use Amazon ElastiCache to cache data. It provides a high-performance caching layer, reducing the load on the backend servers and providing faster response times to end-users. However, each option has its advantages and limitations, and the best solution will depend on the specific requirements of the application.\n\n"
}, {
  "id" : 331,
  "question" : "Your company has setup EC2 Instances in a VPC for their application.\nThey now have a concern that not all of the EC2 instances are being utilized.\nWhich of the below mentioned services can help you find underutilized resources in AWS? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "30f54253f6e24d90b205a8e2e8b00564",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "true"
  }, {
    "id" : "4700d8a280634093a18e424b59951a92",
    "option" : "SNS",
    "isCorrect" : "false"
  }, {
    "id" : "012a75e9e3f0436cb3c3f5244712096a",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "true"
  }, {
    "id" : "3d3ae3edbeb64f2fab0d562293033360",
    "option" : "Cloudtrail.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: A and C.\nThe AWS Documentation mentions the following.\n\"An online resource to help you reduce cost, increase performance,and improve security by optimizing your AWS environment,Trust Advisor provides real time guidance to help you provison your resources following AWS best practices\"\nAmazon CloudWatch is a monitoring and management service built for developers, system operators, site reliability engineers (SRE), and IT managers.\nCloudWatch provides you with data and actionable insights to monitor your applications, understand and respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health.\nOption B is incorrect since this is a notification service.\nOption D is incorrect since this is an API monitoring service.\nFor more information on AWS Trusted Advisor and Cloudwatch, please visit the below URL.\nhttps://aws.amazon.com/premiumsupport/trustedadvisor/\nhttps://aws.amazon.com/cloudwatch/\n\n\nThe two services that can help you find underutilized resources in AWS are AWS CloudWatch and AWS Trusted Advisor.\nAWS CloudWatch: AWS CloudWatch is a monitoring service that provides metrics, logs, and alarms for your AWS resources. It collects and tracks metrics such as CPU utilization, network traffic, and disk usage for EC2 instances. By analyzing these metrics, you can identify underutilized instances and adjust their resources or terminate them. You can also use CloudWatch to create custom metrics and alarms to monitor specific performance metrics for your instances. AWS Trusted Advisor: AWS Trusted Advisor is a service that provides real-time guidance to help you optimize your AWS infrastructure, improve security and performance, and reduce costs. It analyzes your AWS usage and provides recommendations on best practices, cost optimization, and security based on your account activity. Trusted Advisor also includes a feature that can help you identify underutilized resources. The service provides a detailed report that shows which resources are being underutilized, such as idle EC2 instances, and suggests actions to take to improve utilization. SNS: Amazon SNS (Simple Notification Service) is a messaging service that enables you to send messages or notifications from one service to another or to a set of subscribers. It is not directly related to identifying underutilized resources in AWS. CloudTrail: AWS CloudTrail is a service that enables you to log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. While it can help you understand what activities are being performed in your environment, it does not directly provide information about underutilized resources.\nTherefore, the correct answers to this question are A. AWS CloudWatch and C. AWS Trusted Advisor.\n\n"
}, {
  "id" : 332,
  "question" : "Your architecture for an application currently consists of EC2 Instances sitting behind a classic ELB.\nThe EC2 Instances are used to serve an application and are accessible through the internet.\nWhat could be done to improve this architecture if the number of users accessing the application increases regularly?\n",
  "answers" : [ {
    "id" : "8c380895367649859542c1aed75e6adb",
    "option" : "Add another ELB to the architecture.",
    "isCorrect" : "false"
  }, {
    "id" : "1881bbd2bd30421aacc1fa07e41129f8",
    "option" : "Use Auto Scaling Groups.",
    "isCorrect" : "true"
  }, {
    "id" : "e39c00ff5e7b4a208db62e882e6fe354",
    "option" : "Use an Application Load Balancer instead.",
    "isCorrect" : "false"
  }, {
    "id" : "a331f213e1a14c8e91243a93179b717b",
    "option" : "Use the Elastic Container Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nAWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest possible cost.\nUsing AWS Auto Scaling, it is easy to setup application scaling for multiple resources across multiple services in minutes.\nFor more information on AWS Auto Scaling, please visit the following URL-\nhttps://aws.amazon.com/autoscaling/\n\nWhen the number of users accessing an application increases regularly, the architecture needs to be adjusted to ensure that the application can handle the increased traffic load. There are several possible solutions to this problem, but the best one depends on the specific requirements and constraints of the application.\nOption A: Add another ELB to the architecture. This option may provide some additional capacity, but it does not address the issue of automatically scaling the number of EC2 instances based on the traffic load. Therefore, it is not an ideal solution for applications that require automatic scaling.\nOption B: Use Auto Scaling Groups. Auto Scaling Groups allow for automatically scaling the number of EC2 instances based on the traffic load. This is a good option when the traffic load is unpredictable or varies greatly over time. Auto Scaling Groups can be configured to automatically launch additional EC2 instances when the traffic load increases, and terminate them when the load decreases.\nOption C: Use an Application Load Balancer instead. An Application Load Balancer (ALB) provides more advanced routing and load balancing capabilities than a classic ELB. It can route traffic based on the content of the request, and distribute traffic to target instances based on various criteria such as the least busy instance or the instance with the highest priority. ALB also provides support for WebSocket and HTTP/2 protocols. Using an ALB instead of a classic ELB can provide more flexibility and scalability for the application.\nOption D: Use the Elastic Container Service. Using the Elastic Container Service (ECS) can improve the scalability and availability of the application by providing a managed container orchestration platform. ECS allows for running and scaling containerized applications on AWS. It provides support for Docker containers and can integrate with other AWS services such as Auto Scaling Groups, ALB, and Elastic Block Store (EBS). Using ECS can simplify the deployment and management of containerized applications and improve the overall scalability of the application.\nIn summary, while all the options presented can improve the scalability of an application, the best solution depends on the specific requirements and constraints of the application. Auto Scaling Groups, an Application Load Balancer, or Elastic Container Service can all be effective solutions for increasing application scalability.\n\n"
}, {
  "id" : 333,
  "question" : "A solutions Architect has been asked to design a serverless media upload web application that will have the functionality to upload thumbnail images, transcode videos, index files, validate contents, and aggregate data in real-time.\nHe needs to visualize the distributed components of his architecture through a graphical console. How can this be designed wisely?\n",
  "answers" : [ {
    "id" : "de6c89b02835467fa5aea8f76eabc5b2",
    "option" : "Host a static web application using Amazon S3, upload images to Amazon S3, trigger a Lambda function when media files are uploaded, coordinate other media files processing Lambdas using several SQS queues, and store the aggregated data in DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "af5f6d56167a48b29f02c91d6d1d1ac8",
    "option" : "Host a static web application using Amazon S3, upload images to Amazon S3, trigger a Lambda function when media files are uploaded, coordinate other media files processing Lambdas using Simple Workflow Service, and store the aggregated data in DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "1773b18ebb1f4b6fae1f15f6505985c9",
    "option" : "Host a static web application using Amazon S3, upload images to Amazon S3, trigger a Lambda function when media files are uploaded, process media files using various Lambdas, and store the aggregated data in DynamoD.",
    "isCorrect" : "false"
  }, {
    "id" : "6fcdcc9a497a4f9c9547c55890aa7b83",
    "option" : "Host a static web application using Amazon S3, upload images to Amazon S3, use S3 event notification to trigger a Lambda function when media files are uploaded, coordinate other media files processing Lambda using Step functions, and store the aggregated data in DynamoD.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer : D.\nAs an above diagram, Steps functions are used to coordinate processing Lambda functions and store the data on DynamoDB.Differences between Steps function and SWF are as follows:\nStep Functions.\nSWF.\nGood for any new serverless application where coordination is required between various components using a visual workflow.\nNeed external signals to intervene in processes, OR Good if there are child processes and those require passing signals to parents.\nEasy to use while developing application.\nMore complex while developing application but complete control of orchestration logic.\nUses declarative JAON to write state machine.\nNeed to write decider program (programing of your choice) to separate activities between steps or use AWS flow framework.\nServerless, lower admin overhead.\nUses servers.\nShort running workflows.\nLong-running workflows,\nMostly used for synchronous tasks.\nMostly used for asynchronous tasks.\nNew AWS Service, less complex applications.\nLegacy application, Complex decisions (custom decide application)\nIntegrate with AWS Mechanical Turk.\nOption A is incorrect: Using SQS queues to coordinate several Lambda functions is not suitable.\nOption B is incorrect: SWF can manage workflows, but managing Lambda functions with step functions are better.\nSWF is used to manage the infrastructure that runs workflow logic and tasks.\nSWF is the least used service.\nOption C is incorrect: Managing Lambda functions will be challenging without a managed workflow service such as Step functions.\nOption D is CORRECT: This is the best answer as Step functions are used to coordinate the processing of Lambda functions.\nIt also provides a graphical way to visualize the processing steps.\nReference:\nhttps://aws.amazon.com/step-functions/faqs/\nhttps://aws.amazon.com/step-functions/use-cases/\n\n\nThe best solution for the given scenario is D: Host a static web application using Amazon S3, upload images to Amazon S3, use S3 event notification to trigger a Lambda function when media files are uploaded, coordinate other media files processing Lambda using Step functions, and store the aggregated data in DynamoDB.\nHere's a detailed explanation of the components and their roles in the architecture:\nStatic Web Application - Host the static web application using Amazon S3. S3 is an object storage service that offers high availability, durability, and scalability. The static web application can be served directly from S3, and it eliminates the need for a web server. S3 can also be used to store the media files uploaded by users. S3 Event Notification - When a user uploads media files to S3, an event notification is triggered. This notification can be used to invoke a Lambda function. Lambda Function - A Lambda function can be used to process the uploaded media files. For example, it can generate thumbnail images, transcode videos, validate contents, and index files. The Lambda function can also be used to store data in DynamoDB. Step Functions - Step Functions can be used to coordinate multiple Lambda functions into a workflow. In this architecture, Step Functions can be used to coordinate the processing of media files. For example, a Step Function can be used to transcode a video file, and then pass the transcoded file to another Lambda function for validation. DynamoDB - DynamoDB can be used to store aggregated data in real-time. DynamoDB is a NoSQL database that offers high performance, scalability, and availability. It can be used to store data generated by Lambda functions, as well as data aggregated by Step Functions.\nBy using this architecture, the Solutions Architect can create a serverless media upload web application that can process media files in real-time. The use of S3, Lambda, Step Functions, and DynamoDB offers a scalable and cost-effective solution for media processing and storage. Additionally, the graphical console can be used to visualize the components of the architecture and monitor the workflow.\n\n"
}, {
  "id" : 334,
  "question" : "You are the architect for a business intelligence application that reads data from a MySQL database hosted on an EC2 Instance.\nThe application experiences a high number of read and write requests. Which Amazon EBS Volume type can meet the performance requirements of this database?\n",
  "answers" : [ {
    "id" : "24b6efa15cf0470b98137dd547f2b8ad",
    "option" : "EBS Provisioned IOPS SSD",
    "isCorrect" : "true"
  }, {
    "id" : "c205f692cc6b408981c6a8267d96166a",
    "option" : "EBS Throughput Optimized HDD",
    "isCorrect" : "false"
  }, {
    "id" : "3db84340bf43465ca9734d812219d7d8",
    "option" : "EBS General Purpose SSD",
    "isCorrect" : "false"
  }, {
    "id" : "7420d31651ff407fb35a5000b520ffb2",
    "option" : "EBS Cold HDD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nSince there is a Business intelligence requirement with a large number of read and write requests, one needs to opt for EBS Provisioned IOPS SSD.Reference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\n\n\nFor a business intelligence application that generates a high number of read and write requests from a MySQL database hosted on an EC2 instance, the Amazon EBS Provisioned IOPS SSD volume type can meet the performance requirements.\nHere is the detailed explanation for each of the options:\nA. EBS Provisioned IOPS SSD: This volume type is designed to deliver predictable and consistent high performance for mission-critical and I/O intensive workloads. It is ideal for applications that require low-latency access to data, such as databases. It allows the user to provision a specific level of IOPS (input/output operations per second) based on their performance needs.\nB. EBS Throughput Optimized HDD: This volume type is designed for frequently accessed, throughput-intensive workloads. It is ideal for big data applications, data warehouses, and log processing applications that require low-cost storage and high throughput.\nC. EBS General Purpose SSD: This volume type is designed for a broad range of workloads, including small to medium-sized databases, development and test environments, and boot volumes. It provides a balance of price and performance and is a good choice for most workloads that require low to moderate IOPS.\nD. EBS Cold HDD: This volume type is designed for infrequently accessed workloads and provides low-cost storage for data that is accessed less than once a month. It is ideal for backups, disaster recovery, and long-term archival storage.\nConsidering the performance requirements of a business intelligence application with a high number of read and write requests, the EBS Provisioned IOPS SSD volume type is the most appropriate option. It provides the ability to provision the required level of IOPS, ensuring consistent and predictable performance for the application.\n\n"
}, {
  "id" : 335,
  "question" : "An organization is planning to use AWS for its production roll-out.\nThe organization wants to implement automation for deployment such that it will automatically create a LAMP stack, download the latest PHP installable from S3, set up the ELB and Auto Scaling.\nWhich AWS service would meet these requirements for making an orderly deployment of the software?\n",
  "answers" : [ {
    "id" : "df1f12e85b6f468f88537c798315612c",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "true"
  }, {
    "id" : "0a9ef8c2075c4cc98a03c1f3be77b7a3",
    "option" : "AWS CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "b1f7efae157a4ef9a4425ba218e97381",
    "option" : "AWS CodePipeline",
    "isCorrect" : "false"
  }, {
    "id" : "17929c11260e478fafc8d4ce64e93681",
    "option" : "AWS DevOps.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nThe Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services.\nFor a typical web application, configuring for HA requires running multiple web servers behind a load balancer, configuring Auto Scaling to replace lost instances and launch more instances in response to surges in traffic, and having a standby database instance configured for automatic failover.\nFor AWS Elastic Beanstalk, production HA configuration also includes running your database instances outside of your web server environment which allows you to perform blue/green deployments and advanced database management operations.\nAnd Elastic Beanstalk uses EC2 Autoscaling Group to handle elasticity but Lightsail doesn't support autoscaling.\nHence, option A is the correct answer.\nFor more information on launching a LAMP stack with Elastic Beanstalk, please refer to the link below.\nhttps://aws.amazon.com/getting-started/projects/launch-lamp-web-app/faq/\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/php-ha-tutorial.html?icmpid=docs_tutorial_projects\n\nThe service that would meet the requirements for automating the deployment of a LAMP stack, downloading the latest PHP installable from S3, and setting up the ELB and Auto Scaling is AWS Elastic Beanstalk (option A).\nAWS Elastic Beanstalk is a fully managed service that makes it easy to deploy and run applications in various programming languages, including PHP, on AWS. It allows you to quickly deploy and manage applications without worrying about infrastructure provisioning, load balancing, or scaling.\nWhen using Elastic Beanstalk, you can simply upload your application code and Elastic Beanstalk will automatically handle the deployment, scaling, and monitoring of your application. Elastic Beanstalk also integrates with various AWS services, including S3, ELB, and Auto Scaling, making it easy to configure and deploy your application in a scalable and reliable manner.\nTo deploy a LAMP stack with Elastic Beanstalk, you would first create an Elastic Beanstalk environment for your application. You can then upload your application code and specify the PHP version you want to use. Elastic Beanstalk will automatically provision the necessary infrastructure, including EC2 instances and an ELB, and configure them based on your specifications.\nYou can also configure Elastic Beanstalk to download the latest PHP installable from S3 during the deployment process. This ensures that your application is always using the latest version of PHP.\nFinally, you can configure Elastic Beanstalk to automatically scale your application using Auto Scaling. This ensures that your application can handle varying levels of traffic without manual intervention.\nIn summary, AWS Elastic Beanstalk is the most appropriate service to meet the requirements for automating the deployment of a LAMP stack, downloading the latest PHP installable from S3, and setting up the ELB and Auto Scaling.\n\n"
}, {
  "id" : 336,
  "question" : "Your company is planning to use the API Gateway service to manage APIs for developers and users.\nThere is a requirement to segregate access rights for both developers and users.\nHow could this be accomplished?\n",
  "answers" : [ {
    "id" : "32f0fa40c4ce494bb15a4f43834dbca2",
    "option" : "Use IAM permissions to control the access.",
    "isCorrect" : "true"
  }, {
    "id" : "b306014873f540aba3b92389d5025302",
    "option" : "Use AWS Access keys to manage the access.",
    "isCorrect" : "false"
  }, {
    "id" : "d9c0b2e5e4784aafbfb4ce155e470b31",
    "option" : "Use AWS KMS service to manage the access.",
    "isCorrect" : "false"
  }, {
    "id" : "8f639369457047758854988eca418289",
    "option" : "Use AWS Config Service to control the access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Documentation mentions the following.\nYou can control access to Amazon API Gateway with IAM permissions by controlling access to the following two API Gateway component processes.\nTo create, deploy, and manage an API in API Gateway, you must grant the API developer permissions to perform the required actions supported by the API management component of API Gateway.\nTo call a deployed API or to refresh the API caching, you must grant the API caller permissions to perform required IAM actions supported by the API execution component of API Gateway.\nFor more information on permissions for the API gateway, please visit the following URL-\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/permissions.html\n\nThe correct answer is A. Use IAM permissions to control the access.\nAPI Gateway is a fully managed service that enables developers to create, publish, monitor, and secure APIs at any scale. It provides features such as authentication, authorization, throttling, and caching to ensure that only authorized users and applications can access your API.\nTo segregate access rights for developers and users, you can use AWS Identity and Access Management (IAM) permissions to control access to your API. IAM is a web service that helps you securely control access to AWS resources. With IAM, you can create and manage AWS users and groups, and apply granular permissions to allow or deny access to specific resources.\nTo get started, you can create two IAM groups: one for developers and another for users. You can then create IAM policies that grant the appropriate permissions to each group. For example, you can create a policy that allows developers to create, update, and delete APIs, while users can only access and execute APIs.\nOnce you have created the IAM groups and policies, you can attach them to your API Gateway resources. This will ensure that only users with the appropriate permissions can access and interact with your APIs.\nOption B, using AWS Access keys, is not a recommended approach for managing access to APIs because access keys are long-term credentials that can be easily compromised if not properly secured.\nOption C, using AWS KMS service, is a service for managing encryption keys and is not directly related to managing access to APIs.\nOption D, using AWS Config Service, is a service for tracking resource inventory and changes over time and is not directly related to managing access to APIs.\n\n"
}, {
  "id" : 337,
  "question" : "You have 2 development environments hosted in 2 different VPCs in an AWS account in the same region.\nThere is now a requirement to access the resources of one VPC from another.\nHow could this be accomplished?\n",
  "answers" : [ {
    "id" : "9aa7279c137e45bcb717b8d525fbd2f7",
    "option" : "Establish a Direct Connect connection.",
    "isCorrect" : "false"
  }, {
    "id" : "c8a50c837f8e4cc88699148668784c15",
    "option" : "Establish a VPN connection.",
    "isCorrect" : "false"
  }, {
    "id" : "a685578f791348799224097654b37e55",
    "option" : "Establish VPC Peering.",
    "isCorrect" : "true"
  }, {
    "id" : "850e4d1b676e4e708eedf981225f8f16",
    "option" : "Establish Subnet Peering.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the following.\nA VPC peering connection is a networking connection between two VPCs that enable you to route traffic between them privately.\nInstances in either VPC can communicate with each other as if they are within the same network.\nYou can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.\nFor more information on VPC peering, please visit the URL below.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-peering.html\n\nThe requirement to access resources of one VPC from another VPC can be achieved by either establishing a Direct Connect connection, a VPN connection, VPC Peering, or Subnet Peering. Here are the explanations of each option:\nA. Direct Connect:\nDirect Connect is a dedicated network connection between an organization's data center and AWS. This option involves setting up a physical connection between the on-premises data center and the AWS cloud. By establishing a Direct Connect connection, the two VPCs can communicate with each other directly over the dedicated network connection.\nB. VPN Connection:\nA VPN (Virtual Private Network) connection allows users to securely connect to a private network over the internet. To establish a VPN connection between the two VPCs, you would need to set up a virtual private gateway in each VPC and create a VPN connection between them. Once the VPN connection is established, the VPCs can communicate with each other securely over the internet.\nC. VPC Peering:\nVPC Peering allows you to connect two VPCs via a private network connection. By creating a peering connection between the two VPCs, the instances in each VPC can communicate with each other as if they are on the same network.\nNote that VPC Peering works only if the two VPCs have non-overlapping IP ranges.\nD. Subnet Peering:\nSubnet Peering is a similar concept to VPC Peering, but instead of connecting two VPCs, it connects two subnets within the same VPC. Subnet Peering enables instances in one subnet to communicate with instances in another subnet using private IP addresses.\nIn conclusion, the correct option to access resources of one VPC from another VPC will depend on the specific use case, network topology, and IP addressing requirements. All four options - Direct Connect, VPN connection, VPC Peering, and Subnet Peering - offer different advantages and disadvantages.\n\n"
}, {
  "id" : 338,
  "question" : "Your company is planning to use the EMR service available in AWS to run its big data framework and minimize the cost of running the EMR service.\nHow would you achieve this?\n",
  "answers" : [ {
    "id" : "63b9341d4d484def8781dfe5472b3478",
    "option" : "Running the EMR cluster in a dedicated VPC",
    "isCorrect" : "false"
  }, {
    "id" : "8e3276a24d5d4d42960078270a443964",
    "option" : "Choosing Spot Instances for the underlying nodes",
    "isCorrect" : "true"
  }, {
    "id" : "ca90aee3968f40d2b400587f42ff97b3",
    "option" : "Choosing On-Demand Instances for the underlying nodes",
    "isCorrect" : "false"
  }, {
    "id" : "44f30d4896c14b6f87525251cd888f1c",
    "option" : "Disable automated backups.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nSpot Instances in Amazon EMR provide an option to purchase Amazon EC2 instance capacity at a reduced cost compared to On-Demand purchasing.\nFor more information on Instance types for EMR, please visit the following URLs-\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html\n\nEMR (Elastic MapReduce) is a managed big data platform offered by AWS that simplifies the processing and analysis of large datasets using popular distributed processing frameworks like Hadoop, Spark, and Presto. To minimize the cost of running the EMR service, you can take the following steps:\nChoose Spot Instances for the underlying nodes: Spot Instances are unused EC2 instances that are available for a fraction of the On-Demand price. When you choose Spot Instances for your EMR cluster, you can significantly reduce the cost of running the cluster. However, Spot Instances are not guaranteed to be available all the time, and AWS can terminate them at any time if the demand for EC2 instances increases. Therefore, you need to make sure that your EMR cluster can handle Spot Instance interruptions and gracefully recover from them. Use Auto Scaling to scale your cluster based on demand: Auto Scaling allows you to automatically adjust the size of your EMR cluster based on the current demand for resources. By using Auto Scaling, you can ensure that you have enough capacity to handle the workload while minimizing the cost of running the cluster. Use Reserved Instances for the underlying nodes: Reserved Instances are EC2 instances that you reserve for a one- or three-year term, which can significantly reduce the cost of running your EMR cluster. You can choose to pay for the entire term upfront, which provides the most significant discount, or you can pay a lower upfront fee and a reduced hourly rate. Choose the appropriate instance types: AWS offers a wide range of instance types with different sizes, CPU, memory, and storage configurations. Choosing the appropriate instance types for your workload can significantly reduce the cost of running your EMR cluster. For example, if your workload is CPU-bound, you can choose instances with a high CPU to memory ratio, while if your workload is memory-bound, you can choose instances with a high memory to CPU ratio. Optimize your data storage: AWS offers various storage options like Amazon S3, EBS, and EFS, which have different pricing models. By choosing the appropriate storage option and optimizing the data storage, you can reduce the cost of running your EMR cluster.\nIn conclusion, to minimize the cost of running your EMR cluster, you can choose Spot Instances, use Auto Scaling, use Reserved Instances, choose the appropriate instance types, and optimize your data storage.\n\n"
}, {
  "id" : 339,
  "question" : "You have an S3 bucket hosted in AWS that is used to store the promotional videos you upload.\nYou need to provide users access to the S3 bucket's object for a limited duration of time.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "1fcd65befe6442bb8d2a430d7989b228",
    "option" : "Use versioning and enable a timestamp for each version.",
    "isCorrect" : "false"
  }, {
    "id" : "044b56bc3bbf436d89604e4a8947470d",
    "option" : "Use Pre-signed URLs with session duration.",
    "isCorrect" : "true"
  }, {
    "id" : "8edb3c3fe42646eab1808ce4ed7fae61",
    "option" : "Use IAM Roles with a timestamp to limit the access.",
    "isCorrect" : "false"
  }, {
    "id" : "65ca69fd1b4b41e28b0b7e856e50a51d",
    "option" : "Use IAM policies with a timestamp to limit the access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nAll objects by default are private.\nOnly the object owner has permission to access these objects.\nHowever, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.\nFor more information on pre-signed URLs, please visit the URL below.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\n\nThe correct answer to this question is B. Use Pre-signed URLs with session duration.\nPre-signed URLs are URLs that grant temporary access to an S3 object. These URLs are generated using your AWS credentials and include a time limit for access, making them ideal for providing temporary access to S3 objects.\nTo generate a pre-signed URL, you need to have an AWS SDK or AWS CLI installed on your system. You can use the SDK or CLI to generate the URL and specify the time limit for access. Once the URL is generated, you can provide it to the users who require access to the S3 object.\nThe session duration parameter in the pre-signed URL allows you to set the amount of time the URL will be valid. When the time limit is reached, the URL will no longer be valid, and access to the S3 object will be denied.\nVersioning, IAM Roles, and IAM policies are all useful AWS services, but they are not the most suitable solution for providing temporary access to S3 objects.\nEnabling versioning for the S3 bucket will allow you to store multiple versions of the same object. However, it does not provide a way to limit access to a specific version for a limited time.\nUsing IAM Roles and policies is useful for managing access to AWS resources, but it is not the best solution for providing temporary access to S3 objects. IAM Roles are more suitable for granting long-term access to AWS resources, while IAM policies are more suited to defining permissions for AWS users and groups.\nIn summary, the most suitable solution for providing temporary access to S3 objects is to use pre-signed URLs with session duration.\n\n"
}, {
  "id" : 340,
  "question" : "An application currently writes a large number of records to a DynamoDB table in one region.\nThere is a requirement for a secondary application to retrieve new records written to the DynamoDB table every 2 hours and process the updates accordingly.\nWhat would be an ideal method to ensure that the secondary application gets the relevant changes from the DynamoDB table?\n",
  "answers" : [ {
    "id" : "b3056b42f4f14cbb8c2085485670e0bd",
    "option" : "Insert a timestamp for each record and then, scan the entire table for the timestamp as per the last 2 hours.",
    "isCorrect" : "false"
  }, {
    "id" : "364618b8b8b641659092057ae599253d",
    "option" : "Create another DynamoDB table with the records modified in the last 2 hours.",
    "isCorrect" : "false"
  }, {
    "id" : "7378718329c74143816af32c6cb218da",
    "option" : "Use DynamoDB Streams to monitor the changes in the DynamoDB table.",
    "isCorrect" : "true"
  }, {
    "id" : "8c75f0eeda3c4077a4d8a39640740149",
    "option" : "Transfer records to S3 which were modified in the last 2 hours.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the following.\nA DynamoDB Stream is an ordered flow of information about changes to items in an Amazon DynamoDB table.\nWhen you enable a stream on a table, DynamoDB captures information about every modification to data items in the table.\nWhenever an application creates, updates or deletes items in the table, DynamoDB Streams write a stream record with the primary key attribute(s) of the modified items.\nA stream record contains information about a data modification to a single item in a DynamoDB table.\nYou can configure the stream to capture additional information, such as the \"before\" and \"after\" images of modified items.\nFor more information on DynamoDB Streams, please visit the URL below.\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\n\nThe ideal method to ensure that the secondary application gets the relevant changes from the DynamoDB table is to use DynamoDB Streams to monitor the changes in the DynamoDB table.\nDynamoDB Streams is a feature of DynamoDB that allows you to capture changes made to a DynamoDB table in real-time. When an application writes to a DynamoDB table, DynamoDB Streams can provide a time-ordered sequence of item-level changes that can be consumed by multiple applications. These changes can be used to trigger actions, such as updating a search index or sending a notification to another application.\nIn this scenario, you can use DynamoDB Streams to capture the changes made to the DynamoDB table and send them to the secondary application every 2 hours. The secondary application can then process the changes accordingly.\nOption A, inserting a timestamp for each record and scanning the entire table for the timestamp as per the last 2 hours, is not an efficient solution as it requires scanning the entire table every 2 hours, which could be very time-consuming and expensive.\nOption B, creating another DynamoDB table with the records modified in the last 2 hours, is not a scalable solution as it would require duplicating data from the original table to the new table every 2 hours, which could lead to increased storage costs and maintenance overhead.\nOption D, transferring records to S3 which were modified in the last 2 hours, is not an optimal solution for this scenario as it requires additional processing to move the data from DynamoDB to S3 and then back to the secondary application.\nTherefore, the correct answer is option C, using DynamoDB Streams to monitor the changes in the DynamoDB table.\n\n"
}, {
  "id" : 341,
  "question" : "An organization has a distributed application running.\nThis application is implemented with microservices architecture using AWS services including Lambda, API Gateway, SNS and SQS. What is the cost-effective best way to analyze, debug and notify if any issues arise in production?\n",
  "answers" : [ {
    "id" : "57601806d31546279805ac5991117eea",
    "option" : "Use Cloud watch dashboard to monitor the application, create a cloud watch alarm to notify for any errors.",
    "isCorrect" : "false"
  }, {
    "id" : "5bf3702bccb14cd5a339694eed622477",
    "option" : "Use Cloud watch events to trigger a lambda and notify.",
    "isCorrect" : "false"
  }, {
    "id" : "e6d3a33dab704ea098c1bbd42057f632",
    "option" : "Use X-Ray to analyse and debug the application and use CloudWatch alarm to notify.",
    "isCorrect" : "true"
  }, {
    "id" : "9b38e32aec2a43609c3a4d5c8f751016",
    "option" : "Use 3rd party tools to debug and notify.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer : C.\nOption A is incorrect.\nAmazon cloud watch dashboards can help to monitor all the resources.\nYou can have a single view and multiple view across all the AWS regions to see how things are going across the services.\nBut it cannot give detailed debugging and monitoring of each service.\nOption B is incorrect because a cloud watch event is created if any event happens in a service and can not be used for detailed analysis and debugging.\nOption C is CORRECT as AWS X-ray collects data, analysis and debug of microservice application.\nAmazon X-Ray helps to analyze and debug modern applications.\nIt will also collect the traces about the request from each of the applications.\nIt also records the traces.\nAfter recording, it can create a view service map that can be seen to trace data latency and analyze the issues.\nThis can help to find any unusual behavior to identify any root cause.\nOption D is incorrect.\nAny custom monitoring tool will not help for detailed debugging of any microservices applications running on AWS.\n\n\nThe best cost-effective way to analyze, debug, and notify if any issues arise in production for an organization with a distributed application implemented with microservices architecture using AWS services including Lambda, API Gateway, SNS, and SQS is to use X-Ray to analyze and debug the application and use CloudWatch alarms to notify.\nAWS X-Ray is a debugging and tracing service that can be used to analyze and debug the application's behavior, including requests, calls, and services involved. X-Ray can help to identify the root cause of issues that might arise in production. Additionally, it can help in optimizing the application's performance, finding latency issues, and identifying the bottlenecks in the application. X-Ray can also provide a visual representation of the distributed application's architecture, which can help in identifying issues.\nCloudWatch is a monitoring service that can be used to collect and track metrics, collect and monitor log files, and set alarms. CloudWatch can be used to monitor AWS resources such as Lambda, API Gateway, SNS, and SQS. CloudWatch alarms can be used to trigger an alert when a metric crosses a specified threshold. CloudWatch alarms can be set to notify via email, SMS, or other methods.\nTherefore, option C is the correct answer as it provides a cost-effective way to analyze and debug the application using X-Ray and notify using CloudWatch alarms. Option A is not the best solution as it only provides monitoring and notification for errors but does not provide debugging and analysis. Option B is not the best solution as it only provides notification through a Lambda function triggered by CloudWatch events. Option D is not the best solution as it may not be cost-effective and may require additional effort to set up and integrate with existing AWS services.\n\n"
}, {
  "id" : 342,
  "question" : "Your IT Security department has mandated that all the traffic flowing in and out of EC2 instances needs to be monitored.\nThe EC2 instances in question are launched in a VPC.\nWhich services would you use to achieve this?\n",
  "answers" : [ {
    "id" : "771479951f264dc28f2f3dd4b27c7f5a",
    "option" : "Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "0172511388e44d7584b95c820fde8145",
    "option" : "VPC Flow Logs",
    "isCorrect" : "true"
  }, {
    "id" : "ec8e3d5977f74380b464a28e5918de53",
    "option" : "Use CloudWatch metrics",
    "isCorrect" : "false"
  }, {
    "id" : "4fdd02e134194f96b1702f98cc820a77",
    "option" : "Use CloudTrail.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nVPC Flow Log is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.\nFlow log data is stored using Amazon CloudWatch Logs.\nAfter you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs.\nFor more information on VPC Flow Logs, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html\nNote:\nThe question asks to monitor all traffic flowing in and out of EC2 instances.\nNow you have to launch the EC2 instance inside the VPC.\nAs there is no other option available to monitor IP traffic navigation, we use VPC Flow Logs.\nNow coming to the question - why not CloudTrail?\nBefore venturing into it, let's look into the types of log categories we have in AWS.\n1\nAWS Infrastructure Logs - AWS CloudTrail, Amazon VPC Flow Logs.\n2\nAWS Service Logs - Amazon S3, AWS Elastic Load Balancing, Amazon CloudFront, AWS Lambda, AWS Elastic Beanstalk, etc.,\n3\nHost-Based Logs - Messages, Security, NGINX/Apache/IIS, Windows Event Logs, Windows Performance Counters, etc.,\nAWS CloudTrail: it is used to record AWS API calls for your account like,\n- who made the API call?\n- when was the API call made?\n- what was the API call?\n- which resources were acted upon in the API call?\n- where were the API calls made from and made to?\nNOTE:\nAWS has launched a new feature called VPC Traffic Mirroring, which is used to capture and inspect network traffic at scale.To know more about this feature, please check the link below.\nhttps://aws.amazon.com/blogs/aws/new-vpc-traffic-mirroring/\n\nThe service that you would use to achieve monitoring of traffic flowing in and out of EC2 instances launched in a VPC is VPC Flow Logs. Therefore, the correct answer is option B.\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. With VPC Flow Logs, you can monitor the traffic going in and out of your EC2 instances and identify potential security threats or network issues.\nWhen you enable VPC Flow Logs for a VPC, it creates a flow log that captures information about each IP packet that traverses the network interfaces in the VPC. The flow log contains information such as the source and destination IP addresses, protocol, port, and the number of bytes and packets transferred.\nYou can then configure CloudWatch Logs to store the flow log data, which enables you to search, filter, and analyze the data using CloudWatch Logs insights. You can also set up CloudWatch Alarms to alert you when certain traffic patterns or thresholds are met.\nTrusted Advisor is a service that provides best practice recommendations for optimizing your AWS resources, and is not related to monitoring network traffic. CloudWatch metrics are used to monitor the performance of your AWS resources, such as EC2 instances, and are not related to monitoring network traffic either. CloudTrail, on the other hand, is used to monitor API activity in your AWS account, which is not directly related to monitoring network traffic going in and out of EC2 instances launched in a VPC.\n\n"
}, {
  "id" : 343,
  "question" : "A company is currently utilizing a Redshift cluster as its production warehouse.\nAs a cloud architect, you are tasked to ensure that disaster recovery is in place.\nWhich would be the best option in addressing this issue?\n",
  "answers" : [ {
    "id" : "3711822fcda843cc8fc359dcfc459a4f",
    "option" : "Take a copy of the underlying EBS volumes to S3 and then do Cross-Region Replication.",
    "isCorrect" : "false"
  }, {
    "id" : "10a3d2f594fc40c8b86139482e5c8f97",
    "option" : "Enable Cross-Region Snapshots for the Redshift Cluster.",
    "isCorrect" : "true"
  }, {
    "id" : "634bded3feec40e3958b5ffdb2e02fc7",
    "option" : "Create a CloudFormation template to restore the Cluster in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "168f6d664a7b4ab58b36fbdaecb6f80f",
    "option" : "Enable Cross Availability Zone Snapshots for the Redshift Cluster.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe below diagram shows that snapshots are available for Redshift clusters enabling them to be available in different regions.\nFor more information on managing Redshift Snapshots, please visit the following URL-\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html\n\n\nTo ensure disaster recovery for a production Redshift cluster, enabling cross-region snapshots would be the best option.\nOption A, taking a copy of the underlying EBS volumes to S3 and then doing cross-region replication, could be a viable option, but it is not as efficient as using cross-region snapshots. This approach requires more manual work and has a higher risk of data loss or corruption during the transfer.\nOption C, creating a CloudFormation template to restore the cluster in another region, would provide a way to recover the Redshift cluster in the event of a disaster, but it would require a longer recovery time as it involves rebuilding the entire infrastructure from scratch.\nOption D, enabling cross-availability zone snapshots for the Redshift cluster, is not a good option for disaster recovery because the snapshots would be stored in the same region as the cluster, and thus would not protect against a region-wide outage.\nTherefore, the best option for disaster recovery is to enable cross-region snapshots for the Redshift cluster. Cross-region snapshots provide a point-in-time copy of the cluster, including all data, metadata, and configuration settings, and store them in a different region than the production cluster. This approach allows for quick and efficient recovery of the cluster in the event of a disaster, minimizing the impact on the business.\n\n"
}, {
  "id" : 344,
  "question" : "You have an AWS RDS PostgreSQL database hosted in the Singapore region.\nYou need to ensure that the database is asynchronously copied to another one that can also share the read workload.\nWhat would be helpful to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "a12ddd06be3e422484aebcb8950bb8f2",
    "option" : "Enable Multi-AZ for the database",
    "isCorrect" : "false"
  }, {
    "id" : "521616d25f93480fa61b61c1fbebde7e",
    "option" : "Enable Read Replicas for the database",
    "isCorrect" : "true"
  }, {
    "id" : "b831257c271d4f5194f6d0cd7196286d",
    "option" : "Enable Asynchronous replication for the database",
    "isCorrect" : "false"
  }, {
    "id" : "44ea11780d8e43798ef5ff55ebe24846",
    "option" : "Enable manual backups for the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Documentation mentions the following.\nAmazon RDS Read Replicas enable you to create one or more read-only copies of your database instance within the same AWS Region or in a different AWS Region.\nUpdates made to the source database are then asynchronously copied to your Read Replicas.\nIn addition to providing scalability for read-heavy workloads, Read Replicas can be promoted to become a standalone database instance when needed.\nFor more information on Read Replicas, please visit the following URL-\nhttps://aws.amazon.com/rds/details/read-replicas/\nNote:\nWhen you enable Multi-AZ for the database, you enable synchronous replication rather than asynchronous replication mentioned in the question.\nWhen you create a Read Replica, you first specify an existing DB instance as the source.\nThen Amazon RDS takes a snapshot of the source instance and creates a read-only instance from the snapshot.\nAmazon RDS then uses the asynchronous replication method for the DB engine to update the Read Replica whenever there is a change to the source DB instance.\nYou can use the Read Replica promotion as a data recovery scheme if the source DB instance fails.\nFor more information, please click the link given below.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html\n\nTo ensure that the AWS RDS PostgreSQL database is asynchronously copied to another database that can also share the read workload, the best option is to enable Read Replicas for the database. This allows for a secondary copy of the database to be created in a different availability zone or even region.\nRead replicas are copies of the primary database instance that are updated asynchronously with the primary database. The primary database writes updates to its transaction logs, which are then shipped to the read replicas. The read replicas then apply these logs to their own databases. This allows for near-real-time data replication between the primary and read replica databases.\nRead replicas can be used to offload read traffic from the primary database instance, as well as provide high availability for read traffic. Read replicas can be promoted to become the new primary database instance in the event of a failure of the primary instance.\nMulti-AZ is another option for ensuring high availability of the primary database instance, but it is not necessary for fulfilling the requirement of asynchronously copying the database to another one that can share the read workload. Multi-AZ provides synchronous data replication between the primary and secondary database instances, but it is not intended to offload read traffic from the primary instance.\nAsynchronous replication is a feature that allows you to replicate a PostgreSQL database asynchronously to a standby instance, but it requires additional setup and configuration beyond what is provided by RDS. It is also not necessary for fulfilling the requirement of copying the database to another one that can share the read workload.\nManual backups are also not helpful for fulfilling this requirement, as they do not provide real-time or near-real-time data replication between databases.\n\n"
}, {
  "id" : 345,
  "question" : "Your current log analysis application takes more than four hours to generate a report of the top 10 users of your web application.\nYou have been asked to implement a system that can report this information in real-time.\nYou need to ensure that the report is always up to date, and handle increases in the number of requests to your web application.\nWhich of the following is a cost-effective option to fulfill these requirements?\n",
  "answers" : [ {
    "id" : "e82b5d46dde24720b93e5a857183f651",
    "option" : "Publish your data to CloudWatch Logs, and configure your application to Auto Scale to handle the load on demand.",
    "isCorrect" : "false"
  }, {
    "id" : "c0208f2ecd484cb5b100dd8064014abd",
    "option" : "Publish your log data to an Amazon S3 bucket. Use AWS CloudFormation to create an Auto Scaling group to scale your post-processing application which is configured to pull down your log files stored in Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "c172a043a7b341fb8a0652c348d50ed8",
    "option" : "Post your log data to an Amazon Kinesis data stream, and subscribe your log-processing application so that it is configured to process your logging data.",
    "isCorrect" : "true"
  }, {
    "id" : "19c2c7e0b9b344e4a0f111f39d4763b1",
    "option" : "Configure an Auto Scaling group to increase the size of your Amazon EMR cluster.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the below.\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, data lakes, and data warehouses, or build your own real-time applications using this data.\nAmazon Kinesis enables you to process and analyze data as it arrives and respond in real-time instead of having to wait until all your data is collected before the processing can begin.\nFor more information on AWS Kinesis, please see the below link-\nhttps://aws.amazon.com/kinesis/\n\nThe best option for this scenario is to use Amazon Kinesis to stream your log data and configure a log-processing application to consume and process the log data. Option C is the correct answer.\nExplanation:\nAmazon Kinesis is a fully managed service that makes it easy to collect, process, and analyze real-time streaming data. With Kinesis, you can ingest, buffer, and process streaming data in real-time, and then route it to other services such as Amazon S3, Amazon Redshift, or Elasticsearch. By using Kinesis, you can build applications that process real-time data feeds, such as log data.\nOption A suggests using CloudWatch Logs to publish log data and configure your application to auto-scale to handle the load on demand. While CloudWatch Logs can be used for log aggregation and analysis, it is not suitable for real-time processing of large volumes of data. Auto-scaling is a good option for handling varying loads, but it does not address the challenge of real-time processing.\nOption B suggests publishing log data to an S3 bucket and using CloudFormation to create an auto-scaling group to scale your post-processing application. This option can work, but it involves several steps and may not be as cost-effective as using Kinesis. Moreover, S3 is not suitable for real-time processing of streaming data.\nOption D suggests configuring an auto-scaling group to increase the size of your Amazon EMR cluster. Amazon EMR is a managed Hadoop framework for processing large amounts of data. While EMR can be used for real-time processing of data, it is not suitable for handling streaming data in real-time.\nTherefore, the best option is to use Amazon Kinesis to stream your log data and configure a log-processing application to consume and process the log data. By using Kinesis, you can process data in real-time and scale your application to handle increasing loads. Kinesis also provides a cost-effective solution for real-time data processing, as you only pay for the data you stream and process.\n\n"
}, {
  "id" : 346,
  "question" : "You have been hired as an AWS Architect in a global financial firm.\nThey provide daily consolidated reports to their clients for trades in stock markets.\nFor a large amount of data processing, they store daily trading transaction data in S3 buckets, which triggers the AWS Lambda function.\nThis function submits a new AWS Batch job in the Job queue.\nThese queues use EC2 compute resources with this customized AMI and Amazon ECS to complete the job.\nYou have been working on an application created using the above requirements.\nWhile performing a trial for the application, even though it has enough memory/CPU resources, the job is stuck in a Runnable state.\nWhich of the following checks would help to resolve the issue?\n",
  "answers" : [ {
    "id" : "82595693c1174ea5ac9540f6e120274c",
    "option" : "Ensure that AWS logs driver is configured on compute resources.",
    "isCorrect" : "true"
  }, {
    "id" : "8e4bf441ce334cfda736835602267627",
    "option" : "AWS Batch does not support customized AMI, use ECS-optimized AMI.",
    "isCorrect" : "false"
  }, {
    "id" : "1fec818803544a2cacd8d77de8254330",
    "option" : "Check dependencies for the job which holds the job in Runnable state.",
    "isCorrect" : "false"
  }, {
    "id" : "a62bf94ec1cf4f9f94ec30f5bb98214a",
    "option" : "Use only On-Demand EC2 instance in compute resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nIf your compute environment contains compute resources, but your jobs don't progress beyond the\nRUNNABLE.\nstatus, then there is something preventing the jobs from actually being placed on a compute resource.\nHere are some common causes for this issue:\nThe\nawslogs.\nlog driver isn't configured on your compute resources.\nInsufficient resources.\nNo internet access for compute resources.\nAmazon EC2 instance limit reached.\nOption A is correct as this is one of the reasons which is preventing the jobs from actually being placed on a compute resource.\nOption B is incorrect as AWS Batch supports both customized AMI and Amazon ECS-optimized AMI.\nThis is not a reason for Job being stuck in the Runnable state.\nOption C is incorrect as a Job moves into a Runnable state only after all dependencies are processed.\nIf there are any dependencies, Job stays in the Pending state, not the Runnable state.\nOption D is incorrect as Compute resource can be an On-Demand Instance or a Spot Instance.\nIn the question, it is given that the application has enough memory/CPU resources, so this is not a reason for Job being stuck in the Runnable state.\nFor more information on AWS Batch Job state &amp; troubleshooting, if a job is stuck in Runnable state, refer to the following URLs-\nhttps://docs.aws.amazon.com/batch/latest/userguide/job_states.html\nhttps://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#job_stuck_in_runnable\n\nThe issue that the AWS Architect is facing is that the job submitted to the AWS Batch queue is stuck in a \"Runnable\" state. This means that the job has been assigned to an EC2 instance, but the instance is unable to start the job.\nTo resolve this issue, the following checks can be performed:\nA. Ensure that AWS logs driver is configured on compute resources: This check is related to logging. AWS logs driver is a tool that can be used to send logs generated by Docker containers running on Amazon ECS or AWS Fargate to AWS services such as Amazon CloudWatch Logs. While it is important to have logs enabled for troubleshooting, this is unlikely to resolve the issue of the job being stuck in a \"Runnable\" state.\nB. AWS Batch does not support customized AMI, use ECS-optimized AMI: This check is related to the use of customized AMI for EC2 instances. It is important to note that AWS Batch supports the use of customized AMI. However, it is recommended to use the ECS-optimized AMI as it has been pre-configured with the necessary components for running Amazon ECS tasks. While this check may improve the performance of the job, it is unlikely to resolve the issue of the job being stuck in a \"Runnable\" state.\nC. Check dependencies for the job which holds the job in Runnable state: This check is related to the dependencies of the job. It is possible that the job is waiting for a resource or service that is not available. This check involves reviewing the dependencies of the job and verifying that all required resources and services are available. This check is likely to identify the root cause of the issue and help in resolving it.\nD. Use only On-Demand EC2 instance in compute resources: This check is related to the use of On-Demand EC2 instances for running the job. On-Demand instances are instances that are launched as needed and are billed by the hour. While it is important to use the appropriate EC2 instance type based on the workload requirements, it is unlikely that using On-Demand instances would resolve the issue of the job being stuck in a \"Runnable\" state.\nIn conclusion, the check that is most likely to help in resolving the issue of the job being stuck in a \"Runnable\" state is to review the dependencies of the job and verify that all required resources and services are available.\n\n"
}, {
  "id" : 347,
  "question" : "There is a requirement to load a lot of data from your on-premises network to AWS S3, bypassing the internet service.\nWhat can be used for this data transfer? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "7ceb48ec21164036b515843eca5d9462",
    "option" : "Data Pipeline",
    "isCorrect" : "false"
  }, {
    "id" : "6889cd1b4f7e410d9c8f6ea99d150997",
    "option" : "Direct Connect",
    "isCorrect" : "true"
  }, {
    "id" : "90a30a4967284724a7076d9444355ccd",
    "option" : "Snowball",
    "isCorrect" : "true"
  }, {
    "id" : "7e21e02028a94e5dbb4f1d3b7f34d24e",
    "option" : "AWS VPC Peering.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - B and C.\nAWS documentation mentions the following about the above services.\nWith a Snowball, you can transfer hundreds of terabytes or petabytes of data between your on-premises data centers and Amazon Simple Storage Service (Amazon S3)\nAWS Snowball uses Snowball appliances and provides powerful interfaces that you can use to create jobs, transfer data, and track your jobs' status to completion.\nBy shipping your data in Snowballs, you can transfer large amounts of data at a significantly faster rate than if you were transferring that data over the Internet, saving you time and money.\nAWS Direct Connect links your internal network to an AWS Direct Connect location over a standard 1-gigabit or 10-gigabit Ethernet fiber-optic cable.\nOne end of the cable is connected to your router, the other to an AWS Direct Connect router.\nWith this connection in place, you can create virtual interfaces directly to public AWS services (for example, Amazon S3) or Amazon VPC, bypassing Internet service providers in your network path.\nFor more information on Direct Connect, please refer to the below URL-\nhttp://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\nOption A is Incorrect because AWS Data Pipeline is a web service that you can use to automate data movement and transformation.\nHere, we are not transforming the data, and we are just moving the data from on-premises to S3.\nOption D is Incorrect because VPC Peering is used for the connection between two AWS VPCs.\nIt cannot transfer the data from on-premises to AWS S3.\nFor more information on AWS Snowball, please refer to the below URL:\nhttp://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html\n\nThe most efficient and secure way to transfer large amounts of data from an on-premises network to AWS S3, bypassing the internet service, is by using Direct Connect and Snowball.\nDirect Connect is a dedicated network connection between your on-premises network and AWS, bypassing the public internet. This provides a consistent, low-latency, and high-bandwidth connection, enabling faster and more secure data transfer. Direct Connect can be used for transferring data to any AWS service, including S3.\nSnowball is a physical device that can be used to transfer large amounts of data to and from AWS. It is a secure, tamper-resistant, and ruggedized appliance that is shipped to your location, where you can load your data onto it. Once the data is loaded, the Snowball is shipped back to AWS, where the data is imported into your S3 bucket. Snowball can transfer up to 80 terabytes of data in a single job and can be used to transfer data in areas with limited or no internet connectivity.\nData Pipeline is an AWS service that can be used to automate the movement and transformation of data between different AWS services. However, it is not designed for large-scale data transfer between on-premises networks and AWS, and it does not bypass the internet.\nAWS VPC peering is a way to connect two VPCs within AWS. It is not designed for connecting an on-premises network to AWS, and it does not bypass the internet.\nIn summary, for large-scale data transfer from an on-premises network to AWS S3, bypassing the internet service, Direct Connect and Snowball are the most efficient and secure options.\n\n"
}, {
  "id" : 348,
  "question" : "With a Redshift cluster in AWS, you are trying to use SQL Client tools from an EC2 Instance.\nBut you aren't able to connect to the Redshift Cluster.\nWhat must you do to ensure that you can connect to the Redshift Cluster from the EC2 Instance?\n",
  "answers" : [ {
    "id" : "6195dd1a97e449a8856672f6c851a9cc",
    "option" : "Install Redshift client tools on the EC2 Instance first.",
    "isCorrect" : "false"
  }, {
    "id" : "366a39f803ea4c9d9dfb1b9bd3071599",
    "option" : "Modify the Security Groups.",
    "isCorrect" : "true"
  }, {
    "id" : "9226ce37fcc54d4dbf0a42a6c525c766",
    "option" : "Use the AWS CLI instead of the Redshift client tools.",
    "isCorrect" : "false"
  }, {
    "id" : "7b9facf13e124e8ba4ac0b127d318c4c",
    "option" : "Modify the Route Table of the subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - B.\nAWS Documentation mentions the following.\nBy default, any cluster that you create is closed to everyone.\nIAM credentials only control access to the Amazon Redshift API-related resources: the Amazon Redshift console, Command Line Interface (CLI), API, and SDK.\nTo enable access to the cluster from SQL client tools via JDBC or ODBC, you use security groups:\nIf you are using the EC2-Classic platform for your Amazon Redshift cluster, you must use Amazon Redshift security groups.\nIf you are using the EC2-VPC platform for your Amazon Redshift cluster, you must use VPC security groups.\nFor more information on Amazon Redshift, please refer to the URL below.\nhttp://docs.aws.amazon.com/redshift/latest/mgmt/overview.html\n\nTo connect to a Redshift cluster from an EC2 instance, you need to follow the below steps:\nInstall the Redshift client tools on the EC2 instance: You need to install the Redshift client tools on the EC2 instance to connect to the Redshift cluster. You can install the tools using the following command:\nCopy code sudo yum install -y amazon-redshift-utils\nModify the Security Groups: You also need to ensure that the EC2 instance's security group allows inbound traffic from the Redshift cluster's security group on the appropriate port. By default, Redshift uses port 5439. So, you need to add an inbound rule to the security group that allows traffic from the EC2 instance's security group on port 5439. Use the correct endpoint: When connecting to the Redshift cluster, you need to use the correct endpoint. You can find the endpoint in the AWS Management Console by navigating to the Redshift cluster's details page. Use the correct credentials: You need to use the correct credentials to connect to the Redshift cluster. The credentials should have appropriate permissions to access the cluster. Modify the Route Table of the subnet: You do not need to modify the route table of the subnet to connect to the Redshift cluster from an EC2 instance.\nTherefore, the correct answer is A. Install Redshift client tools on the EC2 Instance first, and B. Modify the Security Groups.\n\n"
}, {
  "id" : 349,
  "question" : "You currently work for a company that is specialized in baggage management.\nGPS devices installed on all the baggages deliver the coordinates of the unit every 10 seconds.\nYou need to collect and analyze these coordinates in real-time from multiple sources.\nWhich tool should you use to collect the data in real-time for processing?\n",
  "answers" : [ {
    "id" : "c900591f98a64332b35f1bf011dd0ac5",
    "option" : "Amazon EMR",
    "isCorrect" : "false"
  }, {
    "id" : "6b88e335abf34e0b8589eff1a2b77271",
    "option" : "Amazon SQS",
    "isCorrect" : "false"
  }, {
    "id" : "a81dfef461ec4b25b1de404a7e90f2aa",
    "option" : "AWS Data Pipeline",
    "isCorrect" : "false"
  }, {
    "id" : "b66c89aea001466b9d1034e8a55c255d",
    "option" : "Amazon Kinesis.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThe AWS Documentation mentions the following.\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely insights and react quickly to new information.\nAmazon Kinesis offers key capabilities to cost-effectively process streaming data at any scale, along with the flexibility to choose the tools that best suit the requirements of your application.\nWith Amazon Kinesis, you can ingest real-time data such as video, audio, application logs, website clickstreams, and IoT telemetry data for machine learning, analytics, and other applications.\nAmazon Kinesis enables you to process and analyze data as it arrives and respond instantly instead of having to wait until all your data is collected before the processing can begin.\nFor more information on Amazon Kinesis, please visit the link below.\nhttps://aws.amazon.com/kinesis/\n\nThe best tool to collect and analyze real-time data from multiple sources in AWS is Amazon Kinesis.\nAmazon Kinesis is a fully managed, scalable, and highly available platform for collecting, processing, and analyzing real-time, streaming data. It allows you to collect and process large amounts of data from multiple sources, such as sensors, social media, logs, and other streaming sources, in real-time.\nIn the scenario given, GPS devices installed on all the baggage are delivering the coordinates of the unit every 10 seconds. These coordinates need to be collected and analyzed in real-time from multiple sources. Amazon Kinesis is the ideal tool for this task, as it can ingest, process, and analyze streaming data with sub-second latencies.\nWith Amazon Kinesis, you can create a data stream for each source of data, which will receive the data in real-time. You can then use Kinesis Data Analytics or Kinesis Data Firehose to process and analyze the data as it flows through the stream.\nKinesis Data Analytics allows you to analyze streaming data using SQL queries, while Kinesis Data Firehose can transform and load the data into other services, such as Amazon S3, Amazon Redshift, or Amazon Elasticsearch.\nIn summary, Amazon Kinesis is the best tool to collect and analyze real-time data from multiple sources in AWS, making it the ideal choice for the scenario given.\n\n"
}, {
  "id" : 350,
  "question" : "You are planning to host a web and MySQL database application in an AWS VPC.\nThe database should only be accessible by the web server.\nWhat would you change to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "11287df3a0774b7bb2d5943e01784b8a",
    "option" : "Environment variables",
    "isCorrect" : "false"
  }, {
    "id" : "67755bfbfbe442b098c59cf9ee330ffe",
    "option" : "AWS RDS Parameter Groups",
    "isCorrect" : "false"
  }, {
    "id" : "7120bc7b00c344c6b555de27b2d3c9a7",
    "option" : "Route Tables",
    "isCorrect" : "false"
  }, {
    "id" : "91bd43300a7143aab4cfebf379bf808a",
    "option" : "Security groups.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThe security group associated with the DB instance should allow port 3306 traffic from the EC2 instance.\nThe AWS Documentation additionally mentions the following.\nA security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nWhen you launch an instance in a VPC, you can assign a maximum of five security groups to the instance.\nSecurity groups act at the instance level, not the subnet level.\nTherefore, each instance in a subnet in your VPC could be assigned to a different set of security groups.\nIf you don't specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.For more information on VPC Security Groups, please visit the link below.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html\n\nTo fulfill the requirement of making the MySQL database only accessible by the web server in an AWS VPC, you would need to configure the security groups.\nA security group acts as a virtual firewall for your instances to control inbound and outbound traffic. You can configure a security group to allow traffic from specific sources to specific ports. In this scenario, you would create two security groups, one for the web server and another for the database server.\nFor the security group of the database server, you would create a rule to allow inbound traffic only from the security group of the web server. This means that only instances that belong to the web server security group can access the database server through the specified port.\nThe security group of the web server would also need to have outbound rules to allow traffic to the database server on the specified port.\nUsing environment variables is not the appropriate solution for this requirement as environment variables are used to store configuration values that can be accessed by the application code, but they do not provide security measures to limit access.\nAWS RDS Parameter Groups are also not relevant in this case, as RDS parameter groups are used to configure and manage database engine settings and are not related to security.\nRoute tables are used to define where network traffic is directed within a VPC or between a VPC and an external network. While they can be used to control traffic, they are not the best option for restricting access to a specific resource within a VPC.\n\n"
}, {
  "id" : 351,
  "question" : "A company has a requirement for block-level storage that should be able to store 800GB of data.\nAlso, encryption of the data is required.\nWhat can be used in this case?\n",
  "answers" : [ {
    "id" : "bbcc0947b1a34f9587099f5b03a2a7ba",
    "option" : "AWS EBS Volumes",
    "isCorrect" : "true"
  }, {
    "id" : "4f00ad2cf5bf4181bcaacce2dab39d29",
    "option" : "AWS S3",
    "isCorrect" : "false"
  }, {
    "id" : "262bc33dd9e1438e861769833974f4b3",
    "option" : "AWS Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "ea2d581ca36147328c70759c67011614",
    "option" : "AWS EFS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nFor block-level storage, consider EBS Volumes.\nOptions B and C are incorrect since they provide object-level storage.\nOption D is incorrect since this provides file-level storage.\nFor more information on EBS Volumes, please visit the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\n\nThe best option for this requirement would be AWS EBS Volumes (option A).\nAWS EBS (Elastic Block Store) Volumes provide block-level storage that can be attached to Amazon EC2 instances. It is a durable, high-performance, and cost-effective storage solution for applications that require persistent storage.\nAWS EBS Volumes offer several features that make it an ideal choice for this requirement. Firstly, EBS Volumes are able to store up to 16 terabytes of data, which is well beyond the required 800GB. This means that there will be plenty of room to store the required data, and even more if necessary.\nSecondly, AWS EBS Volumes support encryption of data at rest. This means that data stored on the EBS Volumes will be encrypted and secure, providing an added layer of protection to the data.\nAWS S3 (option B) is an object storage service that is best used for storing and retrieving large amounts of data. It is ideal for storing data such as images, videos, and backup files. However, it is not well suited for block-level storage, which is required in this case.\nAWS Glacier (option C) is a low-cost storage service designed for data archiving and long-term backup. It is not designed for block-level storage and is not recommended for this requirement.\nAWS EFS (option D) is a fully managed file system that can be mounted on Amazon EC2 instances. It is ideal for applications that require shared file storage. However, it is not well suited for block-level storage, which is required in this case.\nIn conclusion, the best option for this requirement would be AWS EBS Volumes as it meets all the requirements of block-level storage, and supports encryption of data at rest.\n\n"
}, {
  "id" : 352,
  "question" : "You are working as an AWS Architect for a media firm.\nThe firm has large text files that need to be converted into audio files.\nThey are using S3 buckets to store this text files. AWS Batch is used to process these files along with Amazon Polly.\nFor the compute environment, you have a mix of EC2 On-Demand &amp; Spot instances.\nCritical Jobs are required to be completed quickly, while non-critical Jobs can be scheduled during non-peak hours.\nWhile using AWS Batch, management wants a cost-effective solution with no performance impact.\nWhich of the following Job Queue can be selected to meet this requirement?\n",
  "answers" : [ {
    "id" : "f77426cb44d047e59bbd296a768fc643",
    "option" : "Create single Job Queue with EC2 On Demand instance having higher priority &amp; Spot Instance having lower priority.",
    "isCorrect" : "false"
  }, {
    "id" : "4006a527199948ce89412c27ae7e5c56",
    "option" : "Create multiple Job Queues with one Queue having EC2 On Demand instance &amp; having higher priority while another queue having Spot Instance &amp; lower priority.",
    "isCorrect" : "true"
  }, {
    "id" : "6b27755035c14c868d805b05a789e0ea",
    "option" : "Create multiple Job Queues with one Queue having EC2 On Demand instance &amp; having lower priority while another queue having Spot Instance &amp; higher priority.",
    "isCorrect" : "false"
  }, {
    "id" : "0641c31f7dac42f3a43e59f658ce7f73",
    "option" : "Create single Job Queue with EC2 On Demand instance having lower priority &amp; Spot Instance having higher priority.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nYou can create multiple Job queues with different priority &amp; mapped Compute environments to each Job queue.\nWhen Job queues are mapped to the same compute environment, queues with higher priority are evaluated first.\nOption A is incorrect as Multiple queues need to be created for each Job type.\nIn the requirement, critical Jobs will be processed using EC2 instance while low priority jobs will be using Job queue with Spot Instance.\nOption C is incorrect as Priority for a Job queue is selected in descending order.\nHigher priority Job queue is preferred first.\nOption D is incorrect as Multiple queues need to be created for each Job type.\nIn the requirement, critical Jobs will be processed using EC2 instance while low priority jobs will be using Job queue with Spot Instance.\nAlso, a higher priority Job queue is preferred first.\nFor more information on Job queues in AWS Batch, refer to the following URL-\nhttps://docs.aws.amazon.com/batch/latest/userguide/job_queue_parameters.html\n\nAWS Batch is a fully managed service that enables developers, scientists, and engineers to easily and efficiently run batch computing workloads of any scale on AWS. Batch jobs can be used to process large amounts of data or to perform complex calculations that take a long time to complete. Amazon Polly is a text-to-speech service that uses advanced deep learning technologies to synthesize speech that sounds like a human voice.\nTo meet the requirements of the media firm, a cost-effective solution is needed with no performance impact. The critical jobs need to be completed quickly, while non-critical jobs can be scheduled during non-peak hours. Since the firm is using a mix of EC2 On-Demand and Spot instances for the compute environment, AWS Batch can be used to process the text files into audio files.\nJob Queues are used to manage batch jobs in AWS Batch. A Job Queue is a collection of Jobs that share the same compute resources. Jobs in a Job Queue are processed in the order they are received. AWS Batch automatically selects the optimal instance types and sizes based on the resource requirements of the Jobs in the queue.\nOption A: Create single Job Queue with EC2 On-Demand instance having higher priority & Spot Instance having lower priority.\nThis option uses a single Job Queue with a mix of On-Demand and Spot instances. On-Demand instances are used for critical jobs that need to be completed quickly, while Spot instances are used for non-critical jobs that can be scheduled during non-peak hours. This option is not optimal as it does not take into account the fact that Spot instances can be interrupted at any time if the Spot price goes above the maximum bid price.\nOption B: Create multiple Job Queues with one Queue having EC2 On-Demand instance & having higher priority while another queue having Spot Instance & lower priority.\nThis option uses multiple Job Queues to manage the critical and non-critical jobs separately. On-Demand instances are used for critical jobs that need to be completed quickly, while Spot instances are used for non-critical jobs that can be scheduled during non-peak hours. This option is better than option A as it separates the critical and non-critical jobs into different Job Queues. However, it does not take into account the fact that Spot instances can be interrupted at any time if the Spot price goes above the maximum bid price.\nOption C: Create multiple Job Queues with one Queue having EC2 On-Demand instance & having lower priority while another queue having Spot Instance & higher priority.\nThis option uses multiple Job Queues to manage the critical and non-critical jobs separately. On-Demand instances are used for non-critical jobs that can be scheduled during non-peak hours, while Spot instances are used for critical jobs that need to be completed quickly. This option is better than option B as it takes into account the fact that Spot instances can be interrupted at any time if the Spot price goes above the maximum bid price. Using Spot instances for critical jobs may cause delays and impact the performance of the workload.\nOption D: Create single Job Queue with EC2 On-Demand instance having lower priority & Spot Instance having higher priority.\nThis option uses a single Job Queue with a mix of On-Demand and Spot instances. On-Demand instances are used for non-critical jobs that can be scheduled during non-peak hours, while Spot instances are used for critical jobs that need to be completed quickly. This option is the best choice as it separates the critical and non-critical jobs into different priorities, and uses On-Demand instances for non-critical jobs, which have a higher chance of being interrupted by Spot instances.\nIn conclusion, Option D is the best choice for meeting the media firm's requirements, as it separates the critical and non-critical jobs into different priorities and uses On-Demand\n\n"
}, {
  "id" : 353,
  "question" : "As the company's cloud administrator, you notice that one of the EC2 instances is frequently restarting.\nThere is a need to troubleshoot and analyze the system logs with an embedded metric format.\nWhat can be used in AWS to store and analyze the log files from the EC2 Instance?\n",
  "answers" : [ {
    "id" : "5cbdf9acabce476fb3560af9f942b779",
    "option" : "AWS SQS",
    "isCorrect" : "false"
  }, {
    "id" : "544cb455038047a48fbf638a27bf5d5e",
    "option" : "AWS S3",
    "isCorrect" : "false"
  }, {
    "id" : "6640e1a9fe5b42acb702cf8c95bcb428",
    "option" : "AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "e4df8eb9a30945e1b1f01624ad228f7f",
    "option" : "AWS CloudWatch Logs.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nYou can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, and other sources.\nFor more information on CloudWatch Logs, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\nRefer to page 638 on the below link:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/acw-ug.pdf\nNote: The question is not about compliance or auditing or tracking any kind of malicious activity or monitoring API calls in your account.\nIf that had been the case, we would have used CloudTrail as it provides info such as who made the request, when the request was made, the request, the response, etc.\nIn this question, we need cloud watch logs to store and analyze logs from EC2 to find why the instance is frequently restarting.\n\n\nThe correct answer is D. AWS CloudWatch Logs.\nExplanation:\nAWS CloudWatch Logs is a service that allows you to store, monitor, and access log files from AWS resources like EC2 instances, AWS Lambda functions, and more. CloudWatch Logs can collect and store logs from any source, and EC2 instances can easily stream their logs to CloudWatch Logs.\nIf an EC2 instance is frequently restarting, analyzing the system logs can help identify the root cause of the issue. CloudWatch Logs can be used to store and analyze logs in an embedded metric format, which provides a way to extract metric data from log files without having to manually parse them.\nCloudWatch Logs allows you to create metric filters to extract metric data from logs and store them as CloudWatch metrics. These metrics can then be used to set up alarms, create dashboards, or even trigger automated actions through CloudWatch Events.\nSQS (Simple Queue Service) is a messaging service used to decouple and scale microservices, applications, and distributed systems. It is not designed to store and analyze log files.\nS3 (Simple Storage Service) is a highly scalable object storage service designed for storing and retrieving any amount of data from anywhere. While S3 can be used to store log files, it does not provide any built-in log analysis capabilities.\nCloudTrail is a service that provides governance, compliance, operational auditing, and risk auditing of your AWS account. It logs all API calls made within your AWS account, but it is not designed to store and analyze log files from EC2 instances.\nIn conclusion, AWS CloudWatch Logs is the best option for storing and analyzing log files from an EC2 instance.\n\n"
}, {
  "id" : 354,
  "question" : "Your company migrated its production environment into AWS VPC 6 months ago.\nAs a cloud architect, you must revise the infrastructure and ensure that it is cost-effective in the long term.\nMore than 50 EC2 instances are up and running all the time to support the business operation.\nWhat can you do to lower the cost?\n",
  "answers" : [ {
    "id" : "40da6360c0c249b18fae00900fc17684",
    "option" : "Reserved instances",
    "isCorrect" : "true"
  }, {
    "id" : "d627fd44195a4fd8b865e4cef409e01e",
    "option" : "On-demand instances",
    "isCorrect" : "false"
  }, {
    "id" : "d558e98bb80d472fb2780a9936a020f4",
    "option" : "Spot instances",
    "isCorrect" : "false"
  }, {
    "id" : "4e82384fc8a34f21918c195ea8d281d2",
    "option" : "Regular instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nWhen you have instances that will be used continuously and throughout the year, the best option is to buy reserved instances.\nBy buying reserved instances, you actually allocate an instance for the entire year or the duration you specify with a reduced cost.\nTo understand more on reserved instances, please visit the below URL-\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\n\nAs a cloud architect, you must ensure that the infrastructure is cost-effective in the long term, and one of the ways to achieve this goal is to optimize the cost of running EC2 instances in AWS.\nThere are several pricing models for running EC2 instances in AWS, and each has its advantages and disadvantages.\nOn-demand instances (option B) are the most straightforward pricing model, where you pay for the compute capacity by the hour or second. This pricing model is flexible, as there are no long-term commitments or upfront payments, and it is suitable for short-term workloads with unpredictable traffic patterns. However, the cost of running on-demand instances can be high in the long term.\nReserved instances (option A) provide a significant discount compared to on-demand instances, as you commit to running the instance for a specific term, typically one or three years, in exchange for a lower hourly rate. This pricing model is ideal for workloads that have predictable traffic patterns and can commit to running the instance for a more extended period. However, reserved instances require upfront payment, and the capacity cannot be changed during the term.\nSpot instances (option C) provide the most significant cost savings, as they offer unused EC2 capacity at a heavily discounted price. The price of spot instances fluctuates based on supply and demand, and AWS can reclaim the instance with two minutes' notice if the capacity is needed. This pricing model is ideal for fault-tolerant, non-time-sensitive workloads that can handle interruptions or sudden termination of instances. However, spot instances are not suitable for workloads that cannot tolerate downtime or have strict SLAs.\nRegular instances (option D) do not exist as a distinct pricing model. Regular instances refer to on-demand instances that are not reserved or spot instances. Regular instances have the same pricing as on-demand instances, and they are suitable for workloads that do not require long-term commitments and do not have predictable traffic patterns.\nIn conclusion, to optimize the cost of running EC2 instances in AWS, you can use a combination of pricing models based on your workload's characteristics and requirements. For workloads that have predictable traffic patterns, you can use reserved instances, for non-time-sensitive workloads, you can use spot instances, and for short-term or unpredictable workloads, you can use on-demand instances or regular instances.\n\n"
}, {
  "id" : 355,
  "question" : "Your organization is building a collaboration platform for which they chose AWS EC2 for web and application servers and MySQL RDS instance as the database.\nDue to the nature of the traffic to the application, they would like to increase the number of connections to the RDS instance.\nHow could this be achieved?\n",
  "answers" : [ {
    "id" : "90cd5774830c4c31be94888db9dafa0d",
    "option" : "Login to RDS instance and modify database config file under /etc/mysql/my.cnf",
    "isCorrect" : "false"
  }, {
    "id" : "3fd515d8e434448397d4d839f4eeee9b",
    "option" : "Create a new parameter group, attach it to DB instance and change the setting.",
    "isCorrect" : "true"
  }, {
    "id" : "049c148c8ab24f1795706902c518ae6f",
    "option" : "Create a new option group, attach it to DB instance and change the setting.",
    "isCorrect" : "false"
  }, {
    "id" : "b50fd09d5cfa45a2b0a77dd670f5af7b",
    "option" : "Modify setting in default options group attached to DB instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nYou manage your DB engine configuration through the use of parameters in a DB parameter group.\nDB parameter groups act as a container for engine configuration values that are applied to one or more DB instances.\nA default DB parameter group is created if you create a DB instance without specifying a customer-created DB parameter group.\nEach default DB parameter group contains database engine defaults and Amazon RDS system defaults based on the engine, compute class, and allocated storage of the instance.\nYou cannot modify the parameter settings of a default DB parameter group.\nYou must create your own DB parameter group to change parameter settings from their default value.\nNote that not all DB engine parameters can be changed in a customer-created DB parameter group.\nIf you want to use your own DB parameter group, you simply create a new DB parameter group, modify the desired parameters, and modify your DB instance to use the new DB parameter group.\nAll DB instances that are associated with a particular DB parameter group get all parameter updates to that DB parameter group.\nFor more information, please visit the URL below-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html\n\n\nThe correct answer is B. Create a new parameter group, attach it to the DB instance, and change the setting.\nExplanation: Amazon RDS is a managed database service provided by AWS that makes it easy to set up, operate, and scale a relational database in the cloud. By default, the maximum number of allowed connections for an RDS instance depends on the instance class and ranges from a few hundred to several thousand. If the default limit is not enough, you can increase the maximum number of connections by modifying the value of the \"max_connections\" parameter in a custom parameter group and attaching it to the DB instance.\nOption A is incorrect because modifying the database config file directly is not recommended and may cause the database to become unstable or stop working altogether.\nOption C is incorrect because option groups are used to enable specific features or functionalities for a DB instance, not to change the maximum number of allowed connections.\nOption D is also incorrect because modifying the settings in the default options group is not recommended since it can affect other DB instances using the same group.\nTo summarize, the best practice to increase the maximum number of connections to an RDS instance is to create a new parameter group, modify the \"max_connections\" parameter, attach it to the DB instance, and then restart the instance to apply the changes.\n\n"
}, {
  "id" : 356,
  "question" : "You are working for a Pharma firm.\nYou are using S3 buckets to save a large amount of sensitive project documents for new medical research.\nYou need to ensure that all data at rest in these buckets are encrypted.\nAll the keys need to be managed by the in-house Security team.\nWhich of the following can be used as a best practice to encrypt all data securely?\n",
  "answers" : [ {
    "id" : "170a4d0af7ae4b59994307cd841e6bec",
    "option" : "Generate a data key using Customer managed CMK. Encrypt data with Plaintext data key &amp; delete Plaintext data key. Store Encrypted data key &amp; data in S3 buckets. For decryption, use CMK to decrypt the Encrypted data key into the Plaintext data key &amp; then decrypt data using the Plaintext data key.",
    "isCorrect" : "true"
  }, {
    "id" : "58f6714249fd429cb96cc68ee90f49f0",
    "option" : "Generate a data key using AWS-managed CMK. Encrypt data with Plaintext data key &amp; delete Plaintext data key. Store Encrypted data key &amp; data in S3 buckets. For decryption, use CMK to decrypt the Encrypted data key into the Plaintext data key &amp; then decrypt data using the Plaintext data key.",
    "isCorrect" : "false"
  }, {
    "id" : "be28fc16f3284e21a60c60fb9532a05f",
    "option" : "Generate a data key using Customer managed CMK. Encrypt data with Plaintext data key &amp; do not delete Plaintext data key. Store Encrypted data key &amp; data in S3 buckets. For decryption, use the Plaintext data key to decrypt data.",
    "isCorrect" : "false"
  }, {
    "id" : "2fc15e29df414ca8916c1da09a3ed10b",
    "option" : "Generate a data key using AWS-managed CMK. Encrypt data with Plaintext data key &amp; do not delete Plaintext data key. Store Encrypted data key &amp; data in S3 buckets. For decryption, use the Plaintext data key to decrypt data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nSince the In-house security team will do key Management, Customer Managed CMK needs to be used.\nCustomer-managed CMK will generate plain text Data Key &amp; encrypted Data Keys.\nAll project-related sensitive documents will be encrypted using these plain text Data Keys.\nAfter encryption, plain text Data keys need to be deleted to avoid any inappropriate use, and encrypted Data Keys and encrypted data are stored in S3 buckets.\nData keys = Plaintext Data Key and Encrypted Data Key.\nWhile decryption, encrypted Data Key is decrypted using Customer CMK into plain text Key, which is further used to decrypt documents.\nThis Envelope Encryption ensures that data is protected by a Data key, which is further protected by another key.\nOption B is incorrect.\nSince all keys need to manage by the in-house customer Security team, AWS-managed CMKs cannot be used.\nOption C is incorrect as it's not the best practice to save data key files in plain text format.\nAll plain text data keys should be deleted and only encrypted data keys need to be saved.\nOption D is incorrect since all keys need to be managed by the in-house customer Security team.\nAWS-managed CMKs cannot be used.\nAlso, all plain text data keys should be deleted and only encrypted data keys need to be saved.\nFor more information on AWS KMS, refer to the following URLs-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys.\nhttps://d0.awsstatic.com/whitepapers/aws-kms-best-practices.pdf.\n\nAs a best practice to encrypt all data at rest in S3 buckets, the answer is (A): Generate a data key using Customer managed CMK. Encrypt data with Plaintext data key & delete Plaintext data key. Store Encrypted data key & data in S3 buckets. For decryption, use CMK to decrypt the Encrypted data key into the Plaintext data key & then decrypt data using the Plaintext data key.\nHere's why:\nCustomer managed CMK (KMS) enables you to create, rotate, disable, or delete your encryption keys as needed. It gives you greater control over key management. Generate a data key using Customer managed CMK, which means that the key is managed by the in-house security team. Encrypt data with the generated Plaintext data key. Delete the Plaintext data key as soon as possible, so it's not stored in plain text. Store the Encrypted data key and data in S3 buckets. This ensures that the data is encrypted both in transit and at rest. For decryption, use CMK to decrypt the Encrypted data key into the Plaintext data key. Decrypt the data using the Plaintext data key.\nThe other options (B, C, and D) are not recommended because:\n(B) AWS-managed CMK is not managed by the in-house security team. This gives less control over key management. (C and D) The Plaintext data key is not deleted after encryption, which means that it is stored in plain text, which makes it vulnerable to unauthorized access.\n\n"
}, {
  "id" : 357,
  "question" : "A company is building a service using Amazon EC2 as a worker instance to process an uploaded audio file and generate a text file.\nYou must store both of these files in the same durable storage until the text file is retrieved.\nYou do not know what the storage capacity requirements are.\nWhich storage option is both cost-efficient and scalable?\n",
  "answers" : [ {
    "id" : "8ffdb1784d534fa09ce249c7770073fe",
    "option" : "Multiple Amazon EBS Volume with snapshots",
    "isCorrect" : "false"
  }, {
    "id" : "66e5f5e7ed2e43f1b956cc58d32f6a7b",
    "option" : "A single Amazon Glacier vault",
    "isCorrect" : "false"
  }, {
    "id" : "4141cc4bb9ad4c739b9346e4c844ae1b",
    "option" : "A single Amazon S3 bucket",
    "isCorrect" : "true"
  }, {
    "id" : "60d2e6472bd54d77b999ecf3310c9cc6",
    "option" : "Multiple instance stores.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon S3 is the best storage option for this.\nIt is durable and highly available.\nFor more information on Amazon S3, please refer to the below URL-\nhttps://aws.amazon.com/s3/\n\nThe most cost-efficient and scalable storage option for this scenario would be a single Amazon S3 bucket (option C). Here's why:\nScalability: Amazon S3 is highly scalable and can handle an unlimited amount of data. This means that as your storage needs grow, you can easily scale up your storage capacity without worrying about running out of space. Cost-Efficiency: Amazon S3 is a highly cost-effective storage solution. You only pay for the storage you use, and there are no upfront costs or minimum fees. Additionally, S3 offers a tiered pricing structure where you pay less per GB for higher levels of storage. Durable Storage: Amazon S3 is designed to provide highly durable storage for your data. By default, S3 stores data across multiple Availability Zones (AZs) within a region to ensure high availability and durability. Security: Amazon S3 offers a variety of security features to help protect your data, including server-side encryption, access control lists (ACLs), and bucket policies.\nOn the other hand, the other options have their drawbacks:\nA. Multiple Amazon EBS Volume with snapshots: This option involves using multiple Amazon EBS volumes to store the uploaded audio and generated text files. While this option is scalable and provides durable storage, it can quickly become costly as you need to pay for each EBS volume and the associated snapshots.\nB. A single Amazon Glacier vault: This option involves using Amazon Glacier, which is a low-cost storage solution designed for long-term data archiving. While Glacier is highly cost-effective, it is not suitable for storing data that needs to be frequently accessed, such as the generated text file in this scenario.\nD. Multiple instance stores: This option involves using the local instance stores on the EC2 instances to store the uploaded audio and generated text files. However, instance stores are not durable and data stored on them can be lost if the instance fails or is terminated. Additionally, instance stores are not scalable and the storage capacity is limited to the instance type.\n\n"
}, {
  "id" : 358,
  "question" : "You are working as an AWS developer for an online multiplayer game start-up company.\nElasticache with Redis is used for gaming leaderboards to provide low latency for online games.\nRedis clusters are deployed within a dedicated VPC in the us-east-1 region. Last week, due to configuration changes in Redis Clusters, the gaming application was impacted for two hours.\nTo avoid such incidents, you have been requested to plan for secure access to all the new clusters.\nWhat would you prefer to use for secure access to Redis Clusters while accessing from EC2 instance, initialized in different VPC in the us-east-1 region? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "161c8322c28d46f1ab9d95d4dd691030",
    "option" : "Use Redis AUTH with in-transit encryption disabled for clusters.",
    "isCorrect" : "false"
  }, {
    "id" : "9b3008403b944355ad92dc4c06e2f0e9",
    "option" : "Create a Transit Gateway to have connectivity between 2 VPCâ€™s.",
    "isCorrect" : "true"
  }, {
    "id" : "b983a94814ea4b779a790aa2d66f4d58",
    "option" : "Use Redis AUTH with in-transit encryption, enabled for clusters.",
    "isCorrect" : "true"
  }, {
    "id" : "26b8733257e04ca49d5db786d6518002",
    "option" : "Create an Amazon VPN connection between 2 VPCs.",
    "isCorrect" : "false"
  }, {
    "id" : "f2d3751de93e4e3093f2d08665689fe5",
    "option" : "Use Redis AUTH with At-Rest encryption, enabled for clusters.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B, C.\nTo use Redis AUTH that will require users to provide a password before accessing Redis Cluster, in-transit encryption needs to be enabled on the cluster while creating the cluster.\nFor accessing Redis Cluster from EC2 instance in different VPCs from the same region, a Transit Gateway can be established between 2 VPCs.\nOption A is incorrect.\nFor Redis AUTH, clusters must be enabled with in-transit encryption during initial deployment.\nOption B is correct.\nTransit Gateway is suitable to access Redis Clusters from EC2 instance in VPC created in different regions.\nTransit Gateway is more secure because it reduces the security footprint available between the VPCs.\nRefer link: https://aws.amazon.com/transit-gateway/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc.\nOption D is incorrect as VPN Connections will be required to access the Redis Cluster from on-prem servers.\nOption E is incorrect.\nFor Redis AUTH, clusters must be enabled with in-transit encryption during initial deployment, not At-Rest encryption.\nFor more information on Authentication with Redis &amp; Accessing Redis Clusters from a different VPC, refer to the following URLs-\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/auth.html\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-vpc-accessing.html\n\nAs an AWS developer for an online multiplayer game start-up company, you have been requested to plan for secure access to all the new Redis Clusters to avoid any incidents like the one that occurred last week. The Redis clusters are deployed within a dedicated VPC in the us-east-1 region, and you need to allow secure access to the Redis clusters from an EC2 instance initialized in a different VPC in the same region. There are two correct options that you can choose to ensure secure access to the Redis clusters while accessing from an EC2 instance in a different VPC. These options are:\nB. Create a Transit Gateway to have connectivity between 2 VPC's: A transit gateway is a network transit hub that you can use to interconnect your VPCs or your on-premises networks. It enables you to create a hub-and-spoke network topology that simplifies network management and minimizes the number of connections required to connect VPCs and on-premises networks. In this case, creating a Transit Gateway will allow secure communication between the VPCs and also help to reduce operational overheads. This is because you do not need to create and manage a VPN connection between VPCs, but only one connection from each VPC to the transit gateway. This ensures secure access to the Redis clusters from an EC2 instance in a different VPC.\nD. Create an Amazon VPN connection between 2 VPCs: Another option to ensure secure access to Redis clusters while accessing from an EC2 instance initialized in a different VPC is to create an Amazon VPN connection between the two VPCs. VPN connection provides secure communication between VPCs and allows you to extend your on-premises network to the Amazon VPC. This approach can be more complex and costly than using a transit gateway, but it allows you to have more control over the network configuration and security.\nThe other options are not suitable for ensuring secure access to Redis clusters. Redis AUTH with in-transit encryption disabled for clusters (Option A) is not secure, and it can expose Redis clusters to the risk of unauthorized access. Redis AUTH with At-Rest encryption, enabled for clusters (Option E) provides encryption only at rest and not during transit, so it will not ensure secure access from an EC2 instance in a different VPC. Redis AUTH with in-transit encryption enabled for clusters (Option C) provides encryption during transit but does not ensure secure access to Redis clusters.\n\n"
}, {
  "id" : 359,
  "question" : "Your company is utilizing CloudFront to distribute its media content to multiple regions.\nUsers frequently access the content.\nAs a cloud architect, which of the following options would help you improve the system's performance?\n",
  "answers" : [ {
    "id" : "985297d7e3dc4eb4ae5da012cedd17d8",
    "option" : "Change the origin location from an S3 bucket to an EL.",
    "isCorrect" : "false"
  }, {
    "id" : "5cbaceffee6e408389333258e57f0afa",
    "option" : "Use a faster Internet connection.",
    "isCorrect" : "false"
  }, {
    "id" : "21e6f56290eb4d9683ecc6f93e096a16",
    "option" : "Increase the cache expiration time.",
    "isCorrect" : "true"
  }, {
    "id" : "b22c66f8f1fb45cfb49842a068ff64ef",
    "option" : "Create an \"invalidation\" for all your objects, and recache them.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nYou can control how long your objects stay in a CloudFront cache before CloudFront forwards another request to your origin.\nReducing the duration allows you to serve dynamic content.\nIncreasing the duration means your users get better performance because your objects are more likely to be served directly from the edge cache.\nA longer duration also reduces the load on your origin.\nFor more information on CloudFront cache expiration, please refer to the following link-\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html\n\nThe correct answer for improving the performance of the CloudFront system for distributing media content to multiple regions is option C: Increase the cache expiration time.\nCloudFront is a content delivery network (CDN) that can distribute content to multiple regions worldwide, thereby improving the performance and reliability of web applications and content. CloudFront caches the content in edge locations, which are located closer to the users accessing the content, thus reducing the latency and improving the performance.\nWhen a user requests content from a CloudFront distribution, the request is first routed to the edge location closest to the user, which then checks whether it has the requested content in its cache. If the content is not present in the cache or if the cache has expired, the request is forwarded to the origin server (S3 bucket or EC2 instance) to fetch the content. This process can add to the latency and reduce the performance.\nBy increasing the cache expiration time, we can instruct CloudFront to keep the content in its cache for a longer duration. This would result in fewer requests being forwarded to the origin server, thereby reducing the latency and improving the performance. However, we should be careful while increasing the cache expiration time, as it could lead to stale content being served to the users.\nOption A, changing the origin location from an S3 bucket to an EC2 instance, may not necessarily improve the performance, as it depends on various factors such as the size of the content, the network bandwidth, and the availability of the origin server.\nOption B, using a faster internet connection, is not relevant in this scenario, as CloudFront already utilizes a high-speed network to distribute content to edge locations.\nOption D, creating an invalidation for all objects and recaching them, would result in CloudFront fetching fresh content from the origin server and recaching them. However, this process could add to the latency and result in a temporary decrease in performance. It should be used only when there is a need to invalidate the cached content, such as when there is a change in the content or when we want to remove stale content from the cache.\n\n"
}, {
  "id" : 360,
  "question" : "Your supervisor has instructed you to devise a disaster recovery model for the resources in the AWS account.\nThe key requirement while devising the solution is to ensure that the cost is at the minimum.\nWhich disaster recovery mechanism would you employ in such a scenario?\n",
  "answers" : [ {
    "id" : "38957b8e9cb2457ba6f3741478c25455",
    "option" : "Backup and Restore",
    "isCorrect" : "true"
  }, {
    "id" : "ecd7fa10656f446faf99e2083e573eab",
    "option" : "Pilot Light",
    "isCorrect" : "false"
  }, {
    "id" : "f0ba6cd7b2ee4c29903a1795a2210f23",
    "option" : "Warm standby",
    "isCorrect" : "false"
  }, {
    "id" : "9611910aba3b456a9b1e0d8a6545d0c4",
    "option" : "Multi-Site.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nSince the cost needs to be at the minimum, the best option is to back up all the resources and then perform a restore in the event of a disaster.\nFor more information on disaster recovery, please refer to the below link-\nhttps://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/\nhttps://aws.amazon.com/disaster-recovery/\n\nWhen designing a disaster recovery (DR) solution for AWS resources, there are various mechanisms available to choose from, depending on the business requirements, RTOs, RPOs, and cost constraints.\nA backup and restore mechanism involves making regular backups of the data and storing it in a safe location. In the event of a disaster, the backed-up data can be restored to the production environment. This solution is cost-effective but might take a longer time to recover, depending on the size of the data and the recovery point objective (RPO).\nA pilot light DR solution involves maintaining a minimal version of the production environment that can be quickly scaled up in the event of a disaster. This solution requires fewer resources than the production environment, thus reducing costs, but the recovery time objective (RTO) may still be longer than desired.\nA warm standby solution involves maintaining a partially scaled production environment, which can be scaled up as necessary. This solution is more expensive than the pilot light model but offers faster recovery times.\nA multi-site DR solution involves replicating the production environment in another AWS region or even in an on-premises data center. This solution offers the fastest recovery times, but it's also the most expensive, as it requires resources in multiple locations.\nConsidering the key requirement to minimize costs, the best option would be the pilot light model, which allows for a quick recovery while minimizing resource usage and costs. However, it's essential to evaluate the RTO and RPO requirements of the business and ensure that the pilot light model aligns with those requirements.\n\n"
}, {
  "id" : 361,
  "question" : "An application consists of the following architecture. a.\nEC2 Instances are in multiple AZ's behind an ELB.b.\nThe EC2 Instances are launched via an Auto Scaling Group. c.\nThere is a NAT instance used so that instances can download updates from the internet. Due to the high bandwidth being consumed by the NAT instance, it has been decided to use a NAT Gateway.\nHow could this be implemented?\n",
  "answers" : [ {
    "id" : "cfe5d2120f4642c6be6324b98bffd5f7",
    "option" : "Use NAT Instances along with the NAT Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "80b10e3e962f4476881156a20ed43952",
    "option" : "Host the NAT instance in the private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "62aacaeafd7342c985335e98fe7a4f01",
    "option" : "Migrate from NAT Instance to a NAT Gateway and host the NAT Gateway in the public subnet.",
    "isCorrect" : "true"
  }, {
    "id" : "d066bda3025b49bfaf6543b92b237093",
    "option" : "Host the NAT gateway in the private subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOne can simply start using the NAT Gateway service and stop using the deployed NAT instances.\nBut you need to ensure that the NAT Gateway is deployed in the public subnet.\nFor more information on migrating to a NAT Gateway, please visit the following URL-\nhttps://aws.amazon.com/premiumsupport/knowledge-center/migrate-nat-instance-gateway/\n\nThe correct answer is C: Migrate from NAT Instance to a NAT Gateway and host the NAT Gateway in the public subnet.\nExplanation:\na) EC2 Instances are in multiple AZ's behind an ELB: This architecture provides high availability and fault tolerance to the application by distributing the EC2 instances across multiple Availability Zones (AZs). Elastic Load Balancing (ELB) ensures that traffic is distributed evenly across the EC2 instances in the AZs.\nb) The EC2 Instances are launched via an Auto Scaling Group: An Auto Scaling Group automatically scales the number of EC2 instances in response to changes in demand. This ensures that the application can handle spikes in traffic and that there are always enough resources available to serve users.\nc) There is a NAT instance used so that instances can download updates from the internet: By default, EC2 instances in a private subnet cannot access the internet directly. A Network Address Translation (NAT) instance is used to allow instances to access the internet for tasks such as downloading updates. The NAT instance is launched in a public subnet, and traffic is routed through it.\nDue to the high bandwidth being consumed by the NAT instance, it has been decided to use a NAT Gateway. NAT Gateway is a managed AWS service that provides better availability, higher bandwidth, and requires less administration compared to NAT instances. A NAT Gateway scales automatically up to 45 Gbps.\nTo implement a NAT Gateway, the following steps can be taken:\nMigrate from NAT Instance to a NAT Gateway: The first step is to migrate from NAT instances to a NAT Gateway. This can be done by creating a new NAT Gateway and updating the routing tables of the private subnets to use the NAT Gateway instead of the NAT instance. Host the NAT Gateway in the public subnet: The NAT Gateway needs to be hosted in a public subnet to allow it to communicate with the internet. The public subnet should have a route to an internet gateway. Update the routing tables of the private subnets: The routing tables of the private subnets should be updated to use the NAT Gateway as the default route for traffic destined for the internet.\nOption A is incorrect because there is no need to use both NAT instances and NAT Gateway.\nOption B is incorrect because NAT instances should be hosted in a public subnet, not a private subnet.\nOption D is incorrect because NAT Gateway needs to be hosted in a public subnet to allow it to communicate with the internet.\n\n"
}, {
  "id" : 362,
  "question" : "A company has an application hosted in AWS.\nThis application consists of EC2 Instances that sit behind an ELB.\nThe following are the requirements from an administrative perspective. a) Must be able to collect and analyze logs about ELB's performance. b) Ensure that notifications are sent when the latency goes beyond 10 seconds. What should be used to achieve this requirement? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "7a2913cca4d44020ab84120f91b49850",
    "option" : "Use CloudWatch for monitoring.",
    "isCorrect" : "true"
  }, {
    "id" : "8654dcd185af4c099ee94d4e2881966d",
    "option" : "Enable VPC Flow logs and then investigate the logs whenever there is an issue.",
    "isCorrect" : "false"
  }, {
    "id" : "5e57454e313e46148bc72437391d2a5d",
    "option" : "Enable the logs on the ELB with Latency Alarm that sends an email and then investigate the logs whenever there is an issue.",
    "isCorrect" : "true"
  }, {
    "id" : "90f841216b494de5adefa2befb4ff0e5",
    "option" : "Use CloudTrail to monitor whatever metrics need to be monitored.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and C.\nWhen you use CloudWatch metrics for an ELB, you can get some read requests and latency out of the box.\nFor more information on using CloudWatch with the ELB, please visit the following URL-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-cloudwatch-metrics.html\nElastic Load Balancing provides access logs that capture detailed information about requests sent to your load balancer.\nEach log contains information on when the request was received, the client's IP address, latencies, request paths, and server responses.\nYou can use these access logs to analyze traffic patterns and to troubleshoot issues.\nFor more information on using ELB logs, please visit the following URL-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html\nOption B is INCORRECT because using VPC flow logs, we cannot pinpoint the issues related to ELB performance.\nOption D is INCORRECT because CloudTrail is used only for monitoring API activities on AWS resources.\n\nFrom the given requirements, there are two aspects that need to be addressed: collecting and analyzing logs about ELB's performance, and sending notifications when latency exceeds 10 seconds. Two options need to be selected to address these requirements.\nOption A: Use CloudWatch for monitoring.\nCloudWatch is a monitoring and management service offered by AWS that enables users to collect and track metrics, collect and monitor log files, and set alarms. CloudWatch can be used to monitor the performance of AWS resources, including EC2 instances and ELBs.\nTo collect and analyze logs about ELB's performance, CloudWatch can be used to collect ELB access logs. These logs contain information about each request that is sent to the ELB, including the time taken to process the request. CloudWatch can also be used to monitor the latency of the ELB, which is the time taken to process a request from the client to the back-end instances. Using CloudWatch, users can set up alarms that notify them when the latency exceeds a certain threshold, such as 10 seconds, as required by the scenario.\nTherefore, Option A is a suitable option for addressing both of the given requirements.\nOption C: Enable the logs on the ELB with Latency Alarm that sends an email and then investigate the logs whenever there is an issue.\nThis option suggests enabling ELB logs and setting up a latency alarm to notify administrators via email when the latency exceeds 10 seconds. While this option could work, it is not as efficient as using CloudWatch, as it requires manual investigation of logs whenever there is an issue. Additionally, the logs generated by ELB may not provide as much detail as the logs collected by CloudWatch.\nOption B: Enable VPC Flow logs and then investigate the logs whenever there is an issue.\nVPC Flow Logs are a feature of Amazon VPC that captures information about the IP traffic going to and from network interfaces in a VPC. While this option could provide visibility into the network traffic between the ELB and the EC2 instances, it is not directly related to the ELB's performance or latency. Moreover, it does not address the need to set up alarms when the latency exceeds 10 seconds.\nOption D: Use CloudTrail to monitor whatever metrics need to be monitored.\nCloudTrail is a service that provides event history of AWS account activity. It records account activity made by users, services, or applications within an AWS account. While CloudTrail can provide visibility into AWS account activity, it is not suitable for monitoring the performance or latency of an ELB.\nTherefore, options A and C are the most suitable options for addressing the requirements given in the scenario.\n\n"
}, {
  "id" : 363,
  "question" : "Your company would like to leverage the AWS storage option and integrate it with the current on-premises infrastructure.\nThere is a requirement of low latency access to the entire dataset and backup.\nWhich of the following options would be best suited for this scenario?\n",
  "answers" : [ {
    "id" : "bd7cf5d395e645caa2f11ccf1ba78fd9",
    "option" : "Configure the Simple Storage Service.",
    "isCorrect" : "false"
  }, {
    "id" : "cc4decb9f05b4f36a94b415f573323f8",
    "option" : "Configure Storage Gateway Cached Volume.",
    "isCorrect" : "false"
  }, {
    "id" : "39b8b79148354631b8d4206b6d73d6c9",
    "option" : "Configure Storage Gateway Stored Volume.",
    "isCorrect" : "true"
  }, {
    "id" : "1bcef0e5ca504cc8833ec86e2731a49c",
    "option" : "Configure Amazon Glacier.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect because S3 is not used to provide low latency data access by integrating with the on-premises data center.\nOption B is incorrect because cached volumes do not provide low latency to all the data.\nOption C is correct because it provides low latency to all the data and maintains backup in AWS.\nOption D is incorrect because Amazon Glacier is used to achieve data for a longer period of time, not related to the required scenario.\nCached volumes - Cached volumes offer substantial cost savings on primary storage and minimize the need to scale your storage on-premises.\nYou also retain low-latency access to your frequently accessed data.\nStored volumes - Stored volumes provide your on-premises applications with low-latency access to their entire datasets.\nAt the same time, they provide durable, offsite backups.\nWith stored volumes, you maintain your volume storage on-premises in your data center.\nThis solution is ideal if you want to keep data locally on-premises, because you need to have low-latency access to all your data, and also to maintain backups in AWS.\nReference:\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/backing-up-volumes.html\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html\n\nFor this scenario, option B, \"Configure Storage Gateway Cached Volume,\" would be the best suited option.\nStorage Gateway is a hybrid cloud storage service that enables your on-premises applications to use AWS cloud storage. The service provides seamless and secure integration between an organization's on-premises IT environment and AWS storage infrastructure.\nOption A, \"Configure the Simple Storage Service (S3),\" is a highly scalable object storage service that is suitable for storing and retrieving large amounts of data. While S3 is an excellent option for backup and archival purposes, it may not be ideal for low latency access to the entire dataset.\nOption C, \"Configure Storage Gateway Stored Volume,\" would be suitable if the primary use case is for on-premises applications to directly access the entire dataset stored on AWS storage. However, it may not be the best option for low latency access as data is stored in the cloud.\nOption D, \"Configure Amazon Glacier,\" is a low-cost storage service designed for long-term data archival. While it can provide low-cost backup storage, it may not be suitable for low latency access to the entire dataset.\nStorage Gateway Cached Volume provides a low-latency on-premises cache of frequently accessed data that is stored in AWS. The cached data is stored on-premises and synchronized with the data stored in AWS. This option allows for low latency access to the frequently accessed data while still providing backup and archival storage in AWS.\n\n"
}, {
  "id" : 364,
  "question" : "An IT company has a set of EC2 Instances hosted in a VPC.\nThey are hosted in a private subnet.\nThese instances now need to access resources stored in an S3 bucket.\nThe traffic should not traverse the internet.\nThe addition of which of the following would help to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "5a212547b7114832acfb0a1ab1019455",
    "option" : "VPC Endpoint",
    "isCorrect" : "true"
  }, {
    "id" : "1901940ff5df4ac5b1b1b5f78f4cd752",
    "option" : "NAT Instance",
    "isCorrect" : "false"
  }, {
    "id" : "63fec564ebe940038e62c0ab607956b1",
    "option" : "NAT Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "6c79606701ea43a292d77131160c61ff",
    "option" : "Internet Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nA VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.\nInstances in your VPC do not require public IP addresses to communicate with resources in the service.\nTraffic between your VPC and the other service does not leave the Amazon network.\nFor more information on AWS VPC endpoints, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html\n\nTo allow EC2 instances in a private subnet to access resources stored in an S3 bucket, we need to establish a connection between the two resources without sending traffic over the internet. This can be achieved by using a VPC endpoint.\nA VPC endpoint is a service that allows you to privately connect your VPC to supported AWS services without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. A VPC endpoint uses a private IP address from your VPC's IP address range, eliminating the need for NAT gateways, VPN connections, or internet gateways to access S3.\nA NAT instance or NAT gateway could be used to enable instances in a private subnet to access the internet, but in this case, we need to access S3 without sending traffic over the internet, so these options are not suitable.\nAn internet gateway provides internet access to resources within a VPC, which is not required in this scenario. Therefore, option D is not the correct answer.\nTherefore, the correct answer is A. VPC endpoint.\n\n"
}, {
  "id" : 365,
  "question" : "You need to host a set of web servers and database servers in an AWS VPC.\nWhat would be the best practice in designing a multi-tier infrastructure?\n",
  "answers" : [ {
    "id" : "1b1a325fbb734d3794fb955d0a675b7a",
    "option" : "Use a public subnet for the web tier and a public subnet for the database layer.",
    "isCorrect" : "false"
  }, {
    "id" : "7030bf27e1aa4075b61b203edc47e1a4",
    "option" : "Use a public subnet for the web tier and a private subnet for the database layer.",
    "isCorrect" : "true"
  }, {
    "id" : "41e8dfa4872746b5a6f57b62c425c015",
    "option" : "Use a private subnet for the web tier and a private subnet for the database layer.",
    "isCorrect" : "false"
  }, {
    "id" : "aafca228d70f40e88f644fd5782d7388",
    "option" : "Use a private subnet for the web tier and a public subnet for the database layer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe ideal setup ensures that the web server is hosted in the public subnet so that users on the internet can access it.\nThe database server can be hosted in the private subnet.\nThe below diagram from AWS Documentation shows how this can be set up.\nFor more information on public and private subnets in AWS, please visit the following URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\n\n\nWhen designing a multi-tier infrastructure on AWS VPC, it's best to follow the principle of least privilege, which means granting the minimum required permissions to each component of the infrastructure. Based on this principle, the best practice for hosting a set of web servers and database servers in an AWS VPC is to use a public subnet for the web tier and a private subnet for the database layer. Therefore, option B is the correct answer.\nHere's why:\nPublic Subnet: A public subnet is a subnet that has a route to the internet through an Internet Gateway (IGW). By placing the web servers in a public subnet, you can make them accessible to the internet, and users can connect to them using public IP addresses. Private Subnet: A private subnet, on the other hand, is a subnet that does not have a direct route to the internet. By placing the database servers in a private subnet, you can prevent them from being directly accessible from the internet.\nThis separation of the web and database layers provides additional security, as it prevents any potential attacker from directly accessing the database servers. The web servers in the public subnet can communicate with the database servers in the private subnet using a NAT (Network Address Translation) Gateway or a Bastion Host, which acts as a secure gateway to access the private subnet.\nIn summary, the best practice for designing a multi-tier infrastructure on AWS VPC is to use a public subnet for the web tier and a private subnet for the database layer, which provides additional security and follows the principle of least privilege.\n\n"
}, {
  "id" : 366,
  "question" : "An IT company wants to secure its resources in its AWS Account.\nWhich of the following options would secure data at rest and in transit in AWS? (SELECT THREE)\n",
  "answers" : [ {
    "id" : "2385fb3962f444b0ad12c1fb6acfa71a",
    "option" : "Encrypt all EBS volumes attached to EC2 Instances.",
    "isCorrect" : "true"
  }, {
    "id" : "d2e31977c46a4bc7be0515e3be07a17d",
    "option" : "Use Server-Side Encryption for S3.",
    "isCorrect" : "true"
  }, {
    "id" : "ad608b7a0d0e4969b70a89a4859dca52",
    "option" : "Use SSL/HTTPS when using the Elastic Load Balancer.",
    "isCorrect" : "true"
  }, {
    "id" : "5224dc51d0b44859afe5f8b08181dacd",
    "option" : "Use IOPS Volumes when working with EBS Volumes on EC2 Instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers - A, B and C.\nAWS documentation mentions the following.\nAmazon EBS encryption offers you a simple encryption solution for your EBS volumes without the need for you to build, maintain, and secure your own key management infrastructure.\nWhen you create an encrypted EBS volume and attach it to a supported instance type, the following types of data are encrypted.\nData at rest inside the volume.\nAll data moving between the volume and the instance.\nAll snapshots created from the volume\nData protection refers to protecting data while in transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3 data centers)\nYou can protect data in transit by using SSL or by using client-side encryption.\nYou have the following options of protecting data at rest in Amazon S3.\nUse Server-Side Encryption - You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you download the objects.\nUse Client-Side Encryption - You can encrypt data client-side and upload the encrypted data to Amazon S3\nIn this case, you manage the encryption process, the encryption keys, and related tools.\nYou can create a load balancer that uses the SSL/TLS protocol for encrypted connections (also known as SSL offload)\nThis feature enables traffic encryption between your load balancer and the clients who initiate HTTPS sessions and connections between your load balancer and your EC2 instances.\nFor more information on securing data at rest, please refer to the below link-\nhttps://d0.awsstatic.com/whitepapers/aws-securing-data-at-rest-with-encryption.pdf\n\nSure, I can provide you with a detailed explanation of the options that would secure data at rest and in transit in AWS.\nA. Encrypt all EBS volumes attached to EC2 Instances: Encrypting EBS volumes attached to EC2 instances will ensure that the data stored on those volumes is protected at rest. This means that even if an unauthorized user gains access to the underlying physical storage, they will not be able to read the data without the encryption key. Therefore, encrypting EBS volumes is an essential step in securing data at rest in AWS.\nB. Use Server-Side Encryption for S3: Amazon S3 (Simple Storage Service) is a popular storage service in AWS, used to store and retrieve any amount of data from anywhere on the web. Enabling server-side encryption on S3 buckets ensures that data is encrypted at rest, while also providing control over who can access the encrypted data. S3 offers several options for server-side encryption, including SSE-S3, SSE-KMS, and SSE-C.\nC. Use SSL/HTTPS when using the Elastic Load Balancer: The Elastic Load Balancer (ELB) is a service that automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses. Using SSL/HTTPS with ELB ensures that data is encrypted in transit between the clients and the load balancer, as well as between the load balancer and the targets. This adds an additional layer of security to your applications and helps prevent unauthorized access to sensitive data.\nD. Use IOPS Volumes when working with EBS Volumes on EC2 Instances: Input/Output Operations Per Second (IOPS) is a measure of the maximum number of read/write operations that can be performed on an EBS volume in a given time frame. Using IOPS volumes with EBS volumes on EC2 instances can improve performance and throughput while also ensuring that data is written securely to the storage.\nTherefore, options A, B, and C would secure data at rest and in transit in AWS, while option D relates to performance rather than security.\n\n"
}, {
  "id" : 367,
  "question" : "Your company currently has a set of EC2 Instances running a web application that sits behind an Elastic Load Balancer.\nYou also have an Amazon RDS instance which is accessible from the web application.\nYou have been asked to ensure that this architecture is self-healing in nature.\nWhat would fulfill this requirement? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "5fd721e342ce4117bc7949bb6987ae75",
    "option" : "Use CloudWatch metrics to check the utilization of the web layer. Use Auto Scaling Group to scale the web instances accordingly based on the CloudWatch metrics.",
    "isCorrect" : "true"
  }, {
    "id" : "2315ada138904a839acb9c17a72a6052",
    "option" : "Use CloudWatch metrics to check the utilization of the database servers. Use Auto Scaling Group to scale the database instances accordingly based on the CloudWatch metrics.",
    "isCorrect" : "false"
  }, {
    "id" : "f72aac41579549edb65afd9548d91bfc",
    "option" : "Utilize the Read Replica feature for the Amazon RDS layer.",
    "isCorrect" : "false"
  }, {
    "id" : "15e7e9e9d7be4091a747e193b8a1a670",
    "option" : "Utilize the Multi-AZ feature for the Amazon RDS layer.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers - A and D.\nThe following diagram from AWS showcases a self-healing architecture where you have a set of EC2 servers as a Web server launched by an Auto Scaling Group.\nAWS Documentation mentions the following.\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads.\nWhen you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates it to a standby instance in a different Availability Zone (AZ)\nEach AZ runs on its own physically distinct, independent infrastructure and is engineered to be highly reliable.\nIn case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora) so that you can resume database operations as soon as the failover is complete.\nSince the endpoint for your DB Instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention.\nFor more information on Multi-AZ RDS, please refer to the below link-\nhttps://aws.amazon.com/rds/details/multi-az/\n\n\nTo ensure self-healing architecture, we need to detect failures and automatically recover them without human intervention. In the given scenario, we can achieve this by utilizing the following two options:\nOption A: Use CloudWatch metrics to check the utilization of the web layer. Use Auto Scaling Group to scale the web instances accordingly based on the CloudWatch metrics.\nThis option focuses on scaling the web layer automatically based on the traffic load. We can set up CloudWatch metrics to monitor the utilization of the web layer, such as CPU utilization, network traffic, or HTTP 5xx errors. When the metrics cross a certain threshold, we can configure an Auto Scaling Group to automatically launch new EC2 instances to handle the increased traffic. Additionally, we can configure the Auto Scaling Group to terminate instances when the traffic load decreases.\nBy automatically scaling the web layer, we can ensure that our web application can handle the increased traffic load without downtime. It also ensures that the web application is highly available and fault-tolerant.\nOption D: Utilize the Multi-AZ feature for the Amazon RDS layer.\nThis option focuses on providing fault tolerance and high availability to the RDS layer. Multi-AZ is a feature that automatically replicates the primary RDS instance to a standby instance in a different availability zone (AZ). The standby instance is continuously synchronized with the primary instance, so it can take over in case the primary instance fails.\nBy utilizing the Multi-AZ feature, we can ensure that our RDS instance is highly available and fault-tolerant. If the primary instance fails, the standby instance can take over within minutes, and our application can continue to function without interruption.\nOption B and C are not correct:\nOption B: Use CloudWatch metrics to check the utilization of the database servers. Use Auto Scaling Group to scale the database instances accordingly based on the CloudWatch metrics.\nScaling the database layer is not a recommended option since it can cause data consistency issues, and it is not a scalable solution for a large number of users. Therefore, we should not use an Auto Scaling Group to scale the database instances.\nOption C: Utilize the Read Replica feature for the Amazon RDS layer.\nRead Replicas are used to offload read-only traffic from the primary RDS instance to a replica instance. They are not suitable for self-healing since they do not provide fault tolerance or high availability. Additionally, Read Replicas cannot be used for write operations, so they cannot replace the primary RDS instance in case of failure. Therefore, we should not utilize Read Replica for self-healing.\n\n"
}, {
  "id" : 368,
  "question" : "Your company has a set of EC2 Instances that access data objects stored in an S3 bucket.\nYour IT Security department is concerned about this architecture's security and wants you to implement the following. 1) Ensure that the EC2 Instance securely accesses the data objects stored in the S3 bucket. 2) Prevent accidental deletion of objects. What would be helpful to fulfill the requirements of the IT Security department? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "d4cc21c5379e461ebd2ae419349ecc53",
    "option" : "Create an IAM user and ensure the EC2 Instances use the IAM user credentials to access the bucket data.",
    "isCorrect" : "false"
  }, {
    "id" : "093570b0a55e4dd781a58340c34ad85b",
    "option" : "Create an IAM Role and ensure the EC2 Instances use the IAM Role to access the bucket data.",
    "isCorrect" : "true"
  }, {
    "id" : "be0bd597b2684f0a8659faa5f6005418",
    "option" : "Use S3 Cross-Region Replication to replicate the objects so that the integrity of data is maintained.",
    "isCorrect" : "false"
  }, {
    "id" : "8cbefef788d64426877c32e8d6e29cd8",
    "option" : "Use an S3 bucket policy that ensures that MFA Delete is set on the objects in the bucket.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers - B and D.\nAWS Documentation mentions the following.\nIAM roles are designed to securely make API requests from your instances without requiring you to manage the security credentials that the applications use.\nInstead of creating and distributing your AWS credentials, you can delegate permission to make API requests using IAM roles.\nFor more information on IAM Roles, please refer to the link below.\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\nMFA Delete can be used to add another layer of security to S3 Objects to prevent accidental deletion of objects.\nFor more information on MFA Delete, please refer to the link below.\nhttps://aws.amazon.com/blogs/security/securing-access-to-aws-using-mfa-part-3/\n\nThe two options that would fulfill the requirements of the IT Security department are:\nA. Create an IAM user and ensure the EC2 Instances use the IAM user credentials to access the bucket data. B. Create an IAM Role and ensure the EC2 Instances use the IAM Role to access the bucket data.\nExplanation:\nA. Creating an IAM user and ensuring that the EC2 instances use the IAM user credentials to access the S3 bucket data will ensure secure access. IAM users allow you to create and manage AWS identities with access to AWS services and resources. You can grant IAM users permissions to access the S3 bucket and its objects by creating an IAM policy with the required permissions. IAM users are created within your AWS account and can be used to access AWS resources programmatically or via the AWS Management Console.\nB. Creating an IAM role and ensuring that the EC2 instances use the IAM role to access the S3 bucket data is another way to provide secure access. IAM roles are similar to IAM users, but they are not associated with a specific user. IAM roles allow you to grant permissions to entities that you define, such as an EC2 instance or an AWS service. You can create an IAM role with permissions to access the S3 bucket and its objects and then associate the role with the EC2 instance. This way, the EC2 instance will assume the IAM role's permissions and access the S3 bucket securely.\nC. Using S3 Cross-Region Replication is not directly related to the security requirements of the IT Security department. Cross-region replication allows you to replicate objects across different AWS regions for data durability and availability purposes. While it can help maintain data integrity, it is not a security feature that prevents unauthorized access or deletion of objects.\nD. Using an S3 bucket policy that ensures that MFA Delete is set on the objects in the bucket can prevent accidental deletion of objects, but it does not directly address the requirement to ensure secure access. MFA Delete requires that the user has both their credentials and an MFA device to delete objects, which can help prevent accidental deletions. However, it is not a security feature that prevents unauthorized access to objects or ensures secure access from EC2 instances.\nTherefore, options A and B are the correct answers.\n\n"
}, {
  "id" : 369,
  "question" : "You have a requirement to get a snapshot of the current configuration of resources in your AWS Account.\nWhich service can be used for this purpose?\n",
  "answers" : [ {
    "id" : "0367f8df2cba4e2b96b7e36fffe44668",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "false"
  }, {
    "id" : "e97d90d984a04cc99ffc66588560c72f",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "3a96cc2bef1b491eb5bba11b6a8d3048",
    "option" : "AWS Config",
    "isCorrect" : "true"
  }, {
    "id" : "c5d80cc3375543aaa446115d07aded33",
    "option" : "AWS IAM.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Documentation mentions the following.\nWith AWS Config, you can do the following.\nEvaluate your AWS resource configurations for desired settings.\nGet a snapshot of the current configurations of the supported resources that are associated with your AWS account.\nRetrieve configurations of one or more resources that exist in your account.\nRetrieve historical configurations of one or more resources.\nReceive a notification whenever a resource is created, modified or deleted.\nView relationships between resources.\nFor example, you might want to find all resources that use a particular security group.\nFor more information on AWS Config, please visit the below URL-\nhttp://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\n\nThe correct answer is C. AWS Config.\nAWS Config is a fully managed service that provides you with an inventory of your AWS resources, as well as detailed information about the configuration of those resources. It continuously monitors and records changes to the configuration of your resources, and provides you with a history of those changes.\nWith AWS Config, you can assess, audit, and evaluate the overall compliance of your AWS resources with your organization's policies and standards. You can also use it to troubleshoot and track the relationships between resources, and to identify any potential issues that might arise due to changes in the configuration of your resources.\nAWS Trusted Advisor is a service that provides recommendations to optimize your AWS infrastructure in terms of cost optimization, performance, security, and fault tolerance. It does not provide a snapshot of your current resource configuration.\nAWS CodeDeploy is a service that automates software deployments to a variety of compute services, such as Amazon EC2 instances, Lambda functions, and on-premises servers. It is not used for monitoring or reporting on resource configuration.\nAWS IAM is a service that enables you to manage access to AWS resources. It is not used for monitoring or reporting on resource configuration.\n\n"
}, {
  "id" : 370,
  "question" : "Your company is hosting an application in AWS.\nThe application is read-intensive and consists of a set of web servers and AWS RDS.\nIt has been noticed that the response time of the application increases due to the load on the AWS RDS instance.\nWhich of the following measures can be taken to scale the data tier? (SELECT TWO.)\n",
  "answers" : [ {
    "id" : "e5b51d7290034bd78cef568e5765bd69",
    "option" : "Create Amazon DB Read Replicas. Configure the application layer to query the Read Replicas for query needs.",
    "isCorrect" : "true"
  }, {
    "id" : "9e48c8c418864071805ff55cf9381ab3",
    "option" : "Use Auto Scaling to scale out the database tier.",
    "isCorrect" : "false"
  }, {
    "id" : "b6d659fee6044fed9bae37c0181084a7",
    "option" : "Use SQS to cache the database queries.",
    "isCorrect" : "false"
  }, {
    "id" : "ee8672e1e606444a930180f3f13ab2db",
    "option" : "Use ElastiCache in front of your Amazon RDS DB to cache common queries.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers - A and D.\nAWS documentation mentions the following.\nAmazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances.\nThis replication feature makes it easy to elastically scale out beyond a single DB Instance's capacity constraints for read-heavy database workloads.\nYou can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.\nRead replicas can also be promoted when needed to become standalone DB instances.\nFor more information on AWS RDS Read Replica's, please visit the URL below:\nhttps://aws.amazon.com/rds/details/read-replicas/\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud.\nThe service improves web applications' performance by allowing you to retrieve information from fast, managed, in-memory data stores instead of relying entirely on slower disk-based databases.\nFor more information on AWS ElastiCache, please visit the URL below.\nhttps://aws.amazon.com/elasticache/\n\nThe given scenario involves a read-intensive application hosted in AWS, consisting of web servers and AWS RDS. It has been observed that the response time of the application increases due to the load on the RDS instance. In this case, the data tier can be scaled using the following measures:\nA. Create Amazon DB Read Replicas: This solution involves creating read replicas of the RDS instance to offload the read traffic from the primary instance. Read replicas are asynchronous copies of the primary instance that can be used to serve read requests. By configuring the application layer to query the read replicas for read requests, the load on the primary instance can be reduced. This approach improves read scalability, availability, and reduces the response time. However, it doesn't help in scaling write operations.\nD. Use ElastiCache in front of your Amazon RDS DB: ElastiCache is a fully managed in-memory data store that can be used to cache frequently accessed data. By using ElastiCache in front of the RDS instance, common queries can be cached, reducing the load on the RDS instance. This approach improves both read and write scalability, availability, and reduces the response time.\nB. Use Auto Scaling to scale out the database tier: Auto Scaling can be used to add or remove RDS instances based on the demand. However, this approach may not be effective for read-intensive workloads as adding more RDS instances may not improve read performance. Also, it requires additional management overhead, and the cost may increase as the number of instances increases.\nC. Use SQS to cache the database queries: SQS is a message queuing service that can be used to decouple and scale distributed systems. However, using SQS to cache the database queries may not be effective as it introduces additional latency and may not be suitable for real-time applications.\nIn summary, the most appropriate measures for scaling the data tier in a read-intensive application are creating Amazon DB read replicas and using ElastiCache in front of the RDS instance.\n\n"
}, {
  "id" : 371,
  "question" : "Your company is planning on hosting a set of EC2 Instances in AWS.\nThe Instances would be configured in a way that one will be used as a web tier and the other as a database (EC2 Hosted)\nThe web tier should be exposed to the Internet in the Public Subnet and Database is in Private Subnet in the same VPC with the default configuration.\nWhat configuration needs to be done in order to let Web Server communicate with Database Server?\n",
  "answers" : [ {
    "id" : "d844a99123f14cc6865e750db7a464f9",
    "option" : "Change the main route tables to have the desired routing between the subnets",
    "isCorrect" : "false"
  }, {
    "id" : "f6774ba93564427f9459043979e5b074",
    "option" : "Ensure that the Security Groups have the required rules defined to allow traffic",
    "isCorrect" : "true"
  }, {
    "id" : "acaba75fe6554e04bb05314f29d2b09e",
    "option" : "Ensure that all instances have a public IP for communication",
    "isCorrect" : "false"
  }, {
    "id" : "e6e35575ef98437bb4e378854bc0506c",
    "option" : "Ensure that all subnets are defined as public subnets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nA security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nWhen you launch an instance in a VPC, you can assign up to five security groups to the instance.\nSecurity groups act at the instance level, not the subnet level.\nTherefore, each instance in a subnet in your VPC could be assigned to a different set of security groups.\nIf you don't specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.\nMain route table.\nThe first entry is the default entry for local routing in the VPC; this entry enables the instances in the VPC to communicate with each other.\nDestination.\nTarget.\n10.0.0.0/16\nlocal.\nOption A is invalid since the main route table will have the required rules to route traffic between subnets in a VPC (By default)\nNo change is required there.\nRefer below URL for more details,\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html#VPC_Scenario2_Routing\nOption C is invalid since the instances would communicate with each other on the private IP.\nThe primary reason to use the Private IP of an EC2 instance is to route the traffic internally within your VPC.If you use the private IP to communicate, traffic will stay within the VPC, it will not be routed out, the routing table will route it internally.\nOption D is invalid since the database should be in the private subnet and not the public subnet.\nThis question asks for communication between subnets.\nFor more information on Security Groups, please visit the below URL:\nhttp://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.htm\n\nThe correct answer is B - Ensure that the Security Groups have the required rules defined to allow traffic.\nIn the given scenario, the EC2 instances are configured in such a way that one is used as a web tier and the other as a database (EC2 hosted). The web tier is exposed to the internet in the Public Subnet, and the database is in the Private Subnet in the same VPC with the default configuration.\nTo allow the web server to communicate with the database server, the following steps need to be taken:\nStep 1: Create Security Groups Create two security groups, one for the web tier and the other for the database tier. The security group for the web tier should allow incoming traffic on port 80 or 443 (depending on your configuration), and the security group for the database tier should allow incoming traffic on the database port (for example, port 3306 for MySQL or port 5432 for PostgreSQL).\nStep 2: Associate Security Groups Associate the security groups with the appropriate instances. The security group for the web tier should be associated with the web server instance, and the security group for the database tier should be associated with the database server instance.\nStep 3: Update Security Group Rules Update the security group rules to allow traffic between the web server and the database server. In the security group for the web tier, add an outbound rule to allow traffic to the security group for the database tier on the database port. In the security group for the database tier, add an inbound rule to allow traffic from the security group for the web tier on the database port.\nBy following these steps, the web server will be able to communicate with the database server securely.\nNow let's take a look at the other answer options and see why they are not correct:\nA. Change the main route tables to have the desired routing between the subnets This is not required because the instances are already in the same VPC, and the default configuration of the VPC should allow communication between instances in different subnets.\nC. Ensure that all instances have a public IP for communication This is not required because the web server needs a public IP to be accessible from the internet, but the database server should not have a public IP address since it should only be accessible from the web server.\nD. Ensure that all subnets are defined as public subnets. This is not required because the web server should be in a public subnet, but the database server should be in a private subnet for security reasons.\n\n"
}, {
  "id" : 372,
  "question" : "You work for a company that has a set of EC2 Instances.\nThere is an internal requirement to create another instance in another availability zone.\nOne of the EBS volumes from the current instance needs to be moved from one of the older instances to the new instance.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "466340c679a942b9b9fbedc6589a73de",
    "option" : "Detach the volume and attach to an EC2 instance in another AZ.",
    "isCorrect" : "false"
  }, {
    "id" : "9245f6777da04151984dede6912a0abf",
    "option" : "Create a new volume in the other AZ and specify the current volume as the source.",
    "isCorrect" : "false"
  }, {
    "id" : "09d1c807b5044711a9605e46b455e108",
    "option" : "Create a snapshot of the volume and then create a volume from the snapshot in the other AZ",
    "isCorrect" : "true"
  }, {
    "id" : "ec176c1dee7b4c4a93e97c0c83d941c5",
    "option" : "Create a new volume in the AZ and do a disk copy of contents from one volume to another.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nFor a volume to be available in another availability zone, you need to create a snapshot from the volume.\nThen in the snapshot from creating a volume, you can then specify the new availability zone accordingly.\nThe EBS Volumes attached to the EC2 Instance will always have to remain in the same availability zone as the EC2 Instance.\nA possible reason for this could be because EBS Volumes are present outside of the host machine, and instances have to be connected over the network.\nIf the EBS Volumes are present outside the Availability Zone, there can be potential latency issues and subsequent performance degradation.\nWhat one can do in such a scenario is to get the Snapshot of the EBS Volume.\nSnapshot sequentially captures the state of your EBS Volume.\nYou can create an EBS Volume from this snapshot in your desired Availability Zone and attach it to your new Instance.\nLater you can detach the volume from the older instance and delete then.\nOption A is invalid because the Instance and Volume have to be in the same AZ to be attached to the instance.\nAfter all, we have to specify AZ while creating Volume.\nOption B is invalid because there is no way to specify a volume as a source.\nOption D is invalid because the Diskcopy would be a tedious process.\nFor more information on snapshots, please visit the below URLs-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/ebs-detaching-volume.html\n\n\nTo move an EBS volume from one EC2 instance to another in a different availability zone, you have several options.\nOption A: Detach and Attach You can detach the EBS volume from the existing EC2 instance, and then attach it to a new EC2 instance in another availability zone. To do this, follow these steps:\nStop the existing EC2 instance. Go to the EC2 console, select the volume that needs to be moved, and choose \"Detach Volume.\" Create a new EC2 instance in the other availability zone. Go to the EC2 console, select the new instance, and choose \"Attach Volume.\" Select the volume that you detached from the old instance and attach it to the new instance.\nOption B: Create a New Volume You can create a new EBS volume in the other availability zone and copy the contents of the existing volume to the new one. To do this, follow these steps:\nCreate a new EBS volume in the other availability zone. Go to the EC2 console, select the new volume, and choose \"Create Snapshot.\" Go to the EC2 console, select the snapshot, and choose \"Create Volume.\" Select the new volume and choose \"Attach Volume\" to attach it to the new EC2 instance.\nOption C: Create a Snapshot You can create a snapshot of the existing EBS volume and then create a new volume from the snapshot in the other availability zone. To do this, follow these steps:\nGo to the EC2 console, select the volume that needs to be moved, and choose \"Create Snapshot.\" Go to the EC2 console, select the snapshot, and choose \"Create Volume.\" Create a new EC2 instance in the other availability zone. Go to the EC2 console, select the new instance, and choose \"Attach Volume.\" Select the new volume and attach it to the new instance.\nOption D: Disk Copy You can create a new EBS volume in the other availability zone and do a disk copy of the contents from the existing volume to the new one. To do this, follow these steps:\nCreate a new EBS volume in the other availability zone. Create a temporary EC2 instance in the same availability zone as the existing volume. Attach both volumes to the temporary instance. Use a disk copy utility to copy the contents of the existing volume to the new one. Detach both volumes from the temporary instance. Create a new EC2 instance in the other availability zone. Attach the new volume to the new instance.\nOverall, the best approach depends on your specific requirements and constraints. However, Option A and Option C are typically the most straightforward and least complex.\n\n"
}, {
  "id" : 373,
  "question" : "Your team has developed an application and now needs to deploy that application onto an EC2 Instance.\nThis application interacts with a DynamoDB table.\nWhich of the following is the correct and MOST SECURE way to ensure that the application interacts with the DynamoDB table?\n",
  "answers" : [ {
    "id" : "81b1560db327455d9c299fda87f6307b",
    "option" : "Create a role which has the necessary permissions and can be assumed by the EC2 instance.",
    "isCorrect" : "true"
  }, {
    "id" : "da44ccca3f8d4510804444fe72731d55",
    "option" : "Use the API credentials from an EC2 instance. Ensure the environment variables areupdated with the API access keys.",
    "isCorrect" : "false"
  }, {
    "id" : "cac90ad3eee74c5b8e8b32627ba3d416",
    "option" : "Use the API credentials from a bastion host. Make the application on the EC2 Instance send requests via the bastion host.",
    "isCorrect" : "false"
  }, {
    "id" : "39242351c0d7438395f06524b48fa0f9",
    "option" : "Use the API credentials from a NAT Instance. Make the application on the EC2 Instance send requests via the NAT Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nIAM roles are designed in such a way that your applications can securely make API requests from your instances without requiring you to manage the security credentials that the applications use.\nOptions B, C, and D are invalid because it is not secure to use API credentials from any EC2 instance.\nThe API credentials can be tampered with.\nHence it is not the ideal secure way to make API calls.\nFor more details on AWS Credentials, please refer below URLs-\nhttps://aws.amazon.com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey_API\nFor more information on IAM roles for EC2, please refer below URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\n\nThe most secure way to ensure that an application running on an EC2 instance interacts with a DynamoDB table is to use AWS Identity and Access Management (IAM) roles. IAM roles allow you to grant permissions to entities that you trust, such as EC2 instances, without the need for any hardcoded API credentials.\nOption A, \"Create a role which has the necessary permissions and can be assumed by the EC2 instance,\" is the correct answer. This involves creating an IAM role that has the necessary permissions to access the DynamoDB table, and then assigning that role to the EC2 instance. When the EC2 instance is launched, it can assume the role, and any API calls made by the application running on the instance will automatically inherit the permissions granted to the role.\nOption B, \"Use the API credentials from an EC2 instance. Ensure the environment variables are updated with the API access keys,\" is not as secure as option A because it requires the storage of API access keys on the EC2 instance itself. This approach increases the risk of access key exposure, and also requires you to manually update environment variables on the EC2 instance.\nOption C, \"Use the API credentials from a bastion host. Make the application on the EC2 Instance send requests via the bastion host,\" is not as secure as option A either. This approach involves using a bastion host as an intermediary between the EC2 instance and the DynamoDB table. While the bastion host can be used to restrict access to the DynamoDB table, it also introduces additional complexity and potential points of failure.\nOption D, \"Use the API credentials from a NAT Instance. Make the application on the EC2 Instance send requests via the NAT Instance,\" is also not as secure as option A. This approach involves using a NAT instance as an intermediary between the EC2 instance and the DynamoDB table. While this can be used to restrict access to the DynamoDB table, it also introduces additional complexity and potential points of failure.\nIn summary, the most secure way to ensure that an application running on an EC2 instance interacts with a DynamoDB table is to use IAM roles. This approach avoids the need for hardcoded API credentials, reduces the risk of access key exposure, and simplifies the management of permissions.\n\n"
}, {
  "id" : 374,
  "question" : "You are using Amazon RDS as a relational database for your web application in AWS.\nAll your data stored in Amazon RDS is encrypted using AWS KMS.\nEncrypting this data is handled by a separate team of 4 users (User A, B, C, &amp; D) in the Security Team.\nThey have created 2 CMK's for the encryption of data.\nDuring the annual Audit, Auditors raised concerns for access to these CMK's for each user.\nSecurity Team has the following IAM Policy &amp; Key Policy set for AWS KMS. Â· CMK1 is created by AWS KMS API &amp; has a default Key policy. Â· CMK2 is the default key policy created by AWS Management console &amp; allows User D.Â· User C has IAM Policy denying all action for CMK1 while allowing for CMK2. Â· User A &amp; User B has IAM Policy allowing access to CMK1 while denying access to CMK2. Â· User D has an IAM policy allowing full access to AWS KMS. Which of the following is the correct statement for access each user has for AWS KMS CMK?\n",
  "answers" : [ {
    "id" : "273879bd4fe1443eaf9ec2c39583442a",
    "option" : "User A &amp; B can use the only CMK1, user C cannot use CMK1, while user D can use both CMK1 &amp; CMK2.",
    "isCorrect" : "true"
  }, {
    "id" : "a1e183dac6fe4a2588eceef98cf02311",
    "option" : "User A &amp; B can use CMK1&amp; CMK2, user C can use only CMK2, while user D can use both CMK1 &amp; CMK2.",
    "isCorrect" : "false"
  }, {
    "id" : "ffb2b8b5b74445ee9a4ae729ff21a9f7",
    "option" : "User A &amp; B can use CMK1, user C can use CMK1 &amp; CMK2, while user D can use both CMK1 &amp; CMK2.",
    "isCorrect" : "false"
  }, {
    "id" : "707000e399ee4b5e9ec0ca10fd0b2bfd",
    "option" : "User A &amp; B can use only CMK1, user C can use only CMK2, while user D cannot use both CMK1 &amp; CMK2.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAccess to AWS KMS CMK is a combination of both Key policy &amp; IAM policy.\nIAM Policy should grant access to a user for AWS KMS.\nAt the same time, Key Policy is used to control access to CMK in AWS KMS.\nOption B is incorrect as User A &amp; B do not have the IAM policy to access CMK2.\nOption C is incorrect as User C is not allowed to access CMK1.\nOption D is incorrect as User D has an IAM policy &amp; Key Policy to use both CMK1 &amp; CMK2.\nFor more information on determining access to AWS KMS CMK, refer to the following URL-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/determining-access.html\n\nThe correct answer is option B: User A & B can use CMK1 & CMK2, user C can use only CMK2, while user D can use both CMK1 & CMK2.\nLet's break down the policies assigned to each user and the key policies assigned to each CMK:\nCMK1: Created by AWS KMS API with default key policy. CMK2: Created with the default key policy in AWS Management Console.\nIAM Policies:\nUser A & B: Allowed access to CMK1, denied access to CMK2. User C: Denied all actions for CMK1, allowed access to CMK2. User D: Allowed full access to AWS KMS.\nKey Policies:\nCMK1: Default key policy. CMK2: Default key policy allows User D.\nBased on the above information, we can see that:\nUser A and B can access CMK1 according to their IAM policies. User C is denied all actions for CMK1 according to their IAM policy, but they are allowed to access CMK2. User D has full access to AWS KMS, so they can access both CMK1 and CMK2.\nTherefore, the correct answer is option B: User A & B can use CMK1 & CMK2, user C can use only CMK2, while user D can use both CMK1 & CMK2.\n\n"
}, {
  "id" : 375,
  "question" : "Your company is building container-based applications.\nCurrently, they use Kubernetes for their on-premises docker based orchestration.\nThey want to use AWS and preferably not manage the infrastructure for the underlying orchestration service.\nWhich of the following could be used for this purpose?\n",
  "answers" : [ {
    "id" : "98dab5d59ecb4debb1ae408cad20147e",
    "option" : "AWS DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "d8b6c1664a314d94b224571d06f6d9d7",
    "option" : "AWS ECS with EC2 launch type",
    "isCorrect" : "false"
  }, {
    "id" : "4d66eeda21454dbbafdb38a3a97234a5",
    "option" : "AWS EC2 with Amazon EKS",
    "isCorrect" : "false"
  }, {
    "id" : "cbc21ce1f224400a9634f350c8a92545",
    "option" : "AWS Fargate with Amazon EKS.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nAmazon EKS integrates Kubernetes with AWS Fargate by using controllers that are built by AWS using the upstream, extensible model provided by Kubernetes.\nThese controllers run as part of the Amazon EKS managed Kubernetes control plane and are responsible for scheduling native Kubernetes pods onto Fargate.\nOption A is incorrect since this is a fully managed NoSQL database, not suitable for this scenario.\nOption B is incorrect because you have to manage Kubernetes infrastructure, EC2 will add maintenance overhead for the company.\nOption C is incorrect since this would add maintenance overhead for the company, and the question mentions that the company does not want to manage the infrastructure.\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate.html\nhttps://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html\n\nThe correct answer for the question is option D: AWS Fargate with Amazon EKS.\nExplanation:\nKubernetes is an open-source container orchestration system used to automate the deployment, scaling, and management of containerized applications. AWS provides two main managed Kubernetes services, Amazon Elastic Kubernetes Service (Amazon EKS) and Elastic Kubernetes Service for Amazon Elastic Container Service (EKS for ECS).\nOption A: AWS DynamoDB is a managed NoSQL database service. It is not related to container orchestration and cannot be used for the purpose mentioned in the question.\nOption B: AWS ECS with EC2 launch type is a managed container orchestration service that uses Amazon EC2 instances as the underlying infrastructure. Although ECS is a good option for container orchestration, it is not a Kubernetes-based service and cannot be used for on-premises Kubernetes-based orchestration.\nOption C: AWS EC2 with Amazon EKS is a managed Kubernetes service that allows the user to manage the infrastructure for the underlying orchestration service. In other words, the user is responsible for managing the underlying EC2 instances that run the Kubernetes control plane and worker nodes. This option is not preferred by the company, as they do not want to manage the infrastructure.\nOption D: AWS Fargate with Amazon EKS is a managed Kubernetes service that removes the need to manage the underlying infrastructure. With Fargate, the user only needs to define the compute resources required by their containerized applications, and AWS will handle the rest. This option is the best fit for the company, as they do not want to manage the underlying infrastructure.\nIn summary, option D, AWS Fargate with Amazon EKS, is the best fit for the company's requirements.\n\n"
}, {
  "id" : 376,
  "question" : "Your company is looking at decreasing the amount of time it takes to build servers that are deployed as EC2 Instances.\nThese Instances always have the same type of software installed as per the security standards.\nAs an architect, what would you recommend to decrease the server build time?\n",
  "answers" : [ {
    "id" : "1795fb269f504bdca680f96e06e2d1ba",
    "option" : "Look at creating snapshots of EBS Volumes",
    "isCorrect" : "false"
  }, {
    "id" : "be97134bd41340eba56ec7b20d1ba53e",
    "option" : "Create the same master copy of the EBS volume",
    "isCorrect" : "false"
  }, {
    "id" : "9f75527bbd7e4a30bc93ef0a5df8cf9d",
    "option" : "Create a custom AMI",
    "isCorrect" : "true"
  }, {
    "id" : "a8ad95f1824a4ba1b4c64c3444b204f3",
    "option" : "Create a base profile.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAn Amazon Machine Image (AMI) provides the information required to launch an instance, which is a virtual server in the cloud.\nYou must specify a source AMI when you launch an instance.\nYou can launch multiple instances from a single AMI when you need multiple instances with the same configuration.\nYou can use different AMIs to launch instances when you need instances with different configurations.\nYou can launch an instance from an existing AMI, customize the instance, and then save this updated configuration as a custom AMI.\nInstances launched from this new custom AMI include the customizations that you made when you created the AMI.\nOptions A and B are incorrect since they cannot create a master copy of the instance.\nOption D is incorrect because creating a profile will not assist.\nFor more information on AMI's, please visit the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nAs an architect, to decrease the server build time for EC2 instances that always have the same type of software installed as per the security standards, I would recommend creating a custom Amazon Machine Image (AMI).\nAn Amazon Machine Image is a pre-configured virtual machine image that is used to create EC2 instances. It contains the operating system, application server, and applications needed for the server to function. By creating a custom AMI, you can pre-configure the software and settings needed for your specific requirements, which will reduce the time it takes to launch an EC2 instance.\nTo create a custom AMI, you would first launch an EC2 instance with the desired configuration, install the necessary software and applications, and then create an AMI from that instance. The custom AMI can then be used to launch additional instances with the same configuration and software.\nCreating snapshots of EBS volumes (Option A) is also a viable solution, but it requires launching a new instance from the snapshot and then configuring the software and settings on the new instance, which may take more time compared to launching instances from a custom AMI.\nCreating a master copy of the EBS volume (Option B) is similar to creating snapshots of EBS volumes, but it may require additional steps to configure the software and settings on the new instance.\nCreating a base profile (Option D) is not a common AWS terminology, but it might refer to creating a baseline configuration of EC2 instances that can be used as a starting point for new instances. However, this option may not be as efficient as creating a custom AMI, as it still requires configuring the software and settings on each new instance launched.\nTherefore, the most efficient solution to decrease the server build time for EC2 instances with the same type of software installed as per the security standards is to create a custom AMI.\n\n"
}, {
  "id" : 377,
  "question" : "Which of the following is a limitation of the Elastic Fabric Adapter (EFA)?\n",
  "answers" : [ {
    "id" : "fc36ce3ae71244fcb229f16537a11d79",
    "option" : "EFA provides all of the functionality of an ENA (Elastic Network Adapter).",
    "isCorrect" : "false"
  }, {
    "id" : "41003d95b9914859a960592ef7b99dfa",
    "option" : "The EFA must be a member of a security group that allows only inbound traffic to the security group itself.",
    "isCorrect" : "true"
  }, {
    "id" : "711d96c9f3a142ea8d52c1a72d149b97",
    "option" : "OS-bypass functionality is supported by EFA, not by EN.",
    "isCorrect" : "false"
  }, {
    "id" : "dd380183e65543769f8aa03f968241b7",
    "option" : "OS-bypass capabilities of EFAs are not supported on Windows instances.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: B and D.\nAn Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate High-Performance Computing (HPC) and machine learning applications.\nOption A is incorrect because it is a true feature of EFA.\nAn EFA is an Elastic Network Adapter (ENA) with added capabilities.\nIt provides all of the functionality of an ENA, with additional OS-bypass functionality.\nOption B is correct because it is one of the limitations of EFA (Elastic Fabric Adapter).\nOption C is incorrect because OS-bypass functionality is supported only by EFA.Option D is correct because it is also a limitations.\nThe OS-bypass capabilities of EFAs are not supported on Windows instances.\nIf you attach an EFA to a Windows instance, the instance functions as an Elastic Network Adapter, without the added EFA capabilities.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html\n\nThe Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that provides low-latency, high-bandwidth connectivity between instances. It is designed to provide high-performance networking capabilities for demanding HPC (High-Performance Computing) workloads.\nOut of the given options, the correct answer is D - OS-bypass capabilities of EFAs are not supported on Windows instances.\nThe EFA provides all the functionality of an Elastic Network Adapter (ENA), as mentioned in option A, and also supports OS-bypass functionality, which enables direct communication between the application and the network interface card, bypassing the operating system's networking stack. This provides low-latency communication and high bandwidth between instances, making it an excellent choice for HPC workloads.\nOption B is incorrect as it describes a security group configuration and does not relate to any specific limitation of EFA.\nOption C is incorrect as it is the opposite of the actual limitation of EFA. EFA supports OS-bypass functionality, while ENA does not.\nOption D is the correct answer as EFA OS-bypass capabilities are not supported on Windows instances. This is because Windows OS does not currently provide the required interfaces for EFA to support OS-bypass functionality on Windows instances.\nIn summary, the correct answer is D - OS-bypass capabilities of EFAs are not supported on Windows instances.\n\n"
}, {
  "id" : 378,
  "question" : "You are designing the following application in AWS.\nUsers will use the application to upload videos and images.\nThe files will then be picked up by a worker process for further processing.\nWhich of the below services should be used in the design of the application.\nChoose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "d7eeb65a8de848689140190f12446320",
    "option" : "AWS Simple storage service for storing the videos and images",
    "isCorrect" : "true"
  }, {
    "id" : "a76f37a76eb149b09e7cc6c19ec68f6b",
    "option" : "AWS Glacier for storing the videos and images",
    "isCorrect" : "false"
  }, {
    "id" : "aee39ed401c04e088024c234e630b98a",
    "option" : "AWS SNS for distributed processing of messages by the worker process",
    "isCorrect" : "false"
  }, {
    "id" : "4da5302845924556bd07c31a96a84826",
    "option" : "AWS SQS for distributing messages to the worker process.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThe AWS Documentation mentions the following.\nAmazon Simple Storage Service is storage for the Internet.\nIt is designed to make web-scale computing easier for developers.\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\nSQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.\nUsing SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\nBased on S3 Bucket events, you can trigger the SQS Queue.\nYou can go in Bucket properties and select the event and choose SQS Queue.\nOption A is CORRECT because S3 is a highly available service to store videos and images.\nOption B is incorrect since this is used for archive storage.\nOption C is incorrect since this is used as a notification service.\nOption D is CORRECT because the SQS queue can store the upload events for the worker process to pick up as messages.\nFor more information on S3, please visit the below URL:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/Welcome.html\nFor more information on SQS, please visit the below URL:\nhttps://aws.amazon.com/sqs/\n\n\nThe recommended AWS services for the given application are:\nA. AWS Simple Storage Service (S3) for storing the videos and images D. AWS Simple Queue Service (SQS) for distributing messages to the worker process.\nExplanation:\nAWS S3 is an object storage service that allows storing and retrieving large amounts of data, including videos and images. It provides high durability, availability, and scalability. S3 also supports features such as versioning, lifecycle policies, and access control. Uploading the videos and images to S3 allows for easy management and access by the worker process for further processing.\nAWS Glacier is a low-cost storage service for data archiving and long-term backup. However, it is not suitable for storing frequently accessed data such as videos and images that require fast access.\nAWS SNS is a publish/subscribe messaging service that allows sending messages to multiple subscribers. It is suitable for broadcasting messages to multiple subscribers, but it does not provide message queuing capabilities, which is necessary for ensuring reliable delivery of messages to the worker process.\nAWS SQS is a fully managed message queuing service that allows decoupling the components of an application by providing a reliable and scalable messaging system. The worker process can consume messages from an SQS queue, which provides a reliable way of distributing messages to the worker process and helps in scaling the processing of messages as needed.\nTherefore, the recommended services for this application are AWS S3 for storing videos and images and AWS SQS for distributing messages to the worker process.\n\n"
}, {
  "id" : 379,
  "question" : "Your development team has created a web application in a subnet that needs to be tested.\nYou need to advise the IT admin team on how they should configure the VPC to ensure the application can be accessed from the Internet.\nWhich of the following components would be part of the design? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "f0113acdcf2e48a3bb78bc91d9ee3aa4",
    "option" : "An Internet gateway attached to the VP.",
    "isCorrect" : "true"
  }, {
    "id" : "a4553e28370d431d96770ef8fada9b97",
    "option" : "A NAT gateway attached to the VP.",
    "isCorrect" : "false"
  }, {
    "id" : "827f71334c52486bbf2e4c32afca9f15",
    "option" : "Custom Route table entry added for the Internet gateway",
    "isCorrect" : "true"
  }, {
    "id" : "c0cd0ae336794ea0aa7914184112de17",
    "option" : "All instances launched with a public IP.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, C and D.\nThis scenario's configuration includes a virtual private cloud (VPC) with a single public subnet and an internet gateway to enable communication over the internet.\nAn internet gateway.\nThis connects the VPC to the internet and other AWS services.\nA custom route table is associated with the subnet.\nThe route table entries enable instances in the subnet to use IPv4 to communicate with other instances in the VPC, and communicate directly over the internet.\nA subnet, that is associated with a route table and that has a route to an internet gateway, is known as a public subnet.\nInstances receive a Public IP address so that it is reachable from outside the VPC.\nThis IP address might change if the instance stops and starts.\nAlternatively, you can use an Elastic IP Address that remains static.\nOnly use a Public IP/Elastic IP address when communicating with the instance from outside the VPC.Option B is incorrect since this should be used to communicate instances in the private subnet to the Internet.\nFor more information on public subnets and the VPC, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario1.html\n\n\nTo make the web application accessible from the Internet, the following components should be included in the VPC design:\nAn Internet Gateway: An internet gateway is a horizontally scalable, redundant, and highly available VPC component that allows communication between instances in a VPC and the internet. It acts as a gateway between the VPC and the internet, and provides a target in the VPC route tables for internet-routable traffic. An internet gateway is required to allow the web application to be accessed from the internet. Subnet with a public IP address: The subnet in which the web application is deployed should have a public IP address. Instances launched in a public subnet have direct internet access and can be accessed from the internet. Route table with a default route to the internet gateway: A custom route table needs to be created and added to the subnet. This route table should have a default route pointing to the internet gateway. By default, all subnets in a VPC are associated with the main route table, which has a local route for all VPC CIDR blocks and no internet gateway route. Hence, a custom route table should be created, and a default route should be added to it, pointing to the internet gateway. This ensures that the web application can be accessed from the internet.\nHence, the correct options are A, C, and D. A NAT gateway is not required for this scenario because it is used to provide internet access to instances in a private subnet. However, in this scenario, the instance needs to be accessed from the internet, so it should be launched in a public subnet with a public IP address.\n\n"
}, {
  "id" : 380,
  "question" : "Your company is planning to deploy an application that will consist of a web and database tier.\nThe database tier should not be accessible from the Internet.\nHow would you design the networking part of the application? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "445333adcc7b4b13be48e929e1177c33",
    "option" : "A public subnet for the web tier",
    "isCorrect" : "true"
  }, {
    "id" : "bfb035efe9344b9abe14ddfb3415d5a5",
    "option" : "A private subnet for the web tier",
    "isCorrect" : "false"
  }, {
    "id" : "2e8e8455fc8447a38216bee11ff08230",
    "option" : "A public subnet for the database tier",
    "isCorrect" : "false"
  }, {
    "id" : "c2305309b41547f99adfa9c7a26a0273",
    "option" : "A private subnet for the database tier.",
    "isCorrect" : "true"
  } ],
  "explanations" : "\n\nAnswer - A and D.\nThe below diagram from the AWS Documentation shows the design of a web and database tier.\nOption B is incorrect since users will not access the web application if it is placed in a private subnet.\nOption C is incorrect since the question mentions that the database should not be accessible from the internet.\nFor more information on private and public subnets and the VPC, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Scenario2.html\n\n\nThe best approach to design the networking part of the application that consists of a web and database tier, where the database tier should not be accessible from the internet, is to use private and public subnets in a Virtual Private Cloud (VPC) on Amazon Web Services (AWS).\nA subnet is a range of IP addresses in your VPC where you can launch Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Relational Database Service (Amazon RDS) instances, or AWS Lambda functions.\nA public subnet is a subnet that has a direct route to the internet gateway. Instances launched in a public subnet can have public IP addresses assigned to them, which makes them directly accessible from the internet. Therefore, it is not suitable to launch the database tier in a public subnet because it would expose it to the internet.\nOn the other hand, a private subnet is a subnet that does not have a direct route to the internet gateway. Instances launched in a private subnet cannot have public IP addresses assigned to them, which makes them not directly accessible from the internet. Therefore, launching the database tier in a private subnet is the best approach because it is not directly exposed to the internet.\nHence, the two answers that should be selected are B (A private subnet for the web tier) and D (A private subnet for the database tier).\nHere is a high-level diagram of the recommended VPC design:\nlua Copy code          VPC   +---------------+   |    Internet   |   +---------------+           |           |    +------------+    |  NAT Gateways   |    +------------+           |           |   +---------------------+   | Private Subnet 1  |   | (Web Tier)        |   |                     |   | +----------------+ |   | | EC2 instances | |   | +----------------+ |   +---------------------+           |           |   +-----------------------+   | Private Subnet 2    |   | (Database Tier) |   |                         |   | +------------------+ |   | | RDS instances | |   | +------------------+ |   +-----------------------+\nIn this design, the web tier is launched in a private subnet, and the database tier is also launched in a separate private subnet. A Network Address Translation (NAT) Gateway is used to provide internet access to the instances in the private subnet for software updates, patches, and external API calls, etc.\nNote that in this design, the instances in the private subnet do not have a public IP address, and they can communicate with each other using their private IP addresses. The security group rules can be configured to allow communication between the web and database tiers as required, while the network access control lists (ACLs) can be used to control inbound and outbound traffic at the subnet level.\n\n"
}, {
  "id" : 381,
  "question" : "You are creating several EBS Volumes for the EC2 Instances hosted in your company's AWS account.\nThe company has asked you to ensure that the EBS volumes are available if the entire region experiences an outage.\nHow would you accomplish this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "a3533b635ff343039c64bff6c4c1a153",
    "option" : "Configure Amazon Storage Gateway with EBS volumes as the data source and store the backups on premise through the storage gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "b2026f6453ec48dcb32185779c22a2cb",
    "option" : "Create snapshots of the EBS Volumes.",
    "isCorrect" : "true"
  }, {
    "id" : "2a29eb342ad449b1a947fda25f4e38b7",
    "option" : "Ensure the snapshots are made available in another availability zone.",
    "isCorrect" : "false"
  }, {
    "id" : "e86e1412d7ff4bcaa20c00aa1f09250a",
    "option" : "Ensure the snapshots are made available in another region.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe AWS Documentation mentions the following.\nYou can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.\nSnapshots are incremental backups, which means that only the blocks on the device have changed after your most recent snapshot is saved.\nThis minimizes the time required to create the snapshot and saves on storage costs by not duplicating data.\nWhen you delete a snapshot, only the data unique to that snapshot is removed.\nEach snapshot contains all of the information needed to restore your data (from the moment when the snapshot was taken) to a new EBS volume.\nOption A is incorrect since you have to make use of EBS snapshots.\nOption C is incorrect since the snapshots need to be made available in another region for such a huge disaster.\nIt may be rare for the whole AWS region to go down.\nBut it could cause massive permanent damage if we don't plan for it.\nFor more information on EBS snapshots, please visit the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\n\nTo ensure that the EBS volumes are available in case of an entire region outage, two possible solutions are:\nB. Create snapshots of the EBS Volumes: EBS snapshots are a point-in-time copy of the EBS volume, which can be used to create a new volume or restore an existing one in the same or different availability zone or region. Snapshots are stored in Amazon S3, which provides durability and availability across multiple availability zones within a region. Therefore, if an entire region experiences an outage, the EBS snapshots would still be available in the S3 storage and could be used to create new volumes in a different region or availability zone.\nD. Ensure the snapshots are made available in another region: To ensure that the EBS volumes are available even if the entire region experiences an outage, it is recommended to copy the EBS snapshots to another region. AWS provides an automated way to copy EBS snapshots across regions using Amazon EC2 Copy Snapshot API, AWS CLI or AWS Management Console. This way, if the primary region goes down, the EBS snapshots stored in the secondary region could be used to create new volumes and resume operations without any data loss.\nA. Configure Amazon Storage Gateway with EBS volumes as the data source and store the backups on premise through the storage gateway: This option is not a recommended solution to ensure EBS volumes availability in case of an entire region outage. Amazon Storage Gateway is a hybrid cloud storage service that enables on-premises applications to use AWS cloud storage, including EBS volumes. However, if the entire region experiences an outage, the on-premises storage would be unavailable as well, so this solution wouldn't provide a robust disaster recovery strategy.\nC. Ensure the snapshots are made available in another availability zone: Although creating EBS snapshots in another availability zone can help to recover EBS volumes if the primary availability zone fails, it doesn't provide a comprehensive solution to ensure EBS volumes' availability in case of an entire region outage. Since availability zones are physically located within the same region, they share the same underlying infrastructure and may be impacted by the same outage events.\n\n"
}, {
  "id" : 382,
  "question" : "You are planning to host a static website on EC2 Instances.\nYou need to ensure that the environment is highly available and scalable to meet demand.\nWhich of the below aspects can be used to create a highly available environment.\nChoose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "29a8255bd2ba41308bd9ad05ab87e172",
    "option" : "Auto Scaling group",
    "isCorrect" : "true"
  }, {
    "id" : "39ff1911cb1f4488884c50ecb68a91f3",
    "option" : "Elastic Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "64640a98a38546ac8f67f4ad812fb999",
    "option" : "SQS queue",
    "isCorrect" : "false"
  }, {
    "id" : "4b43b677be164ab2acf7cf7e80ac8030",
    "option" : "Multiple Availability Zones.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, B and D.\nThe diagram below shows an example of a highly available architecture for hosting EC2 Instances.\nHere you have the.\nELB is placed in front of the users which helps in directing the traffic to the EC2 Instances.\nThe EC2 Instances which are placed as part of an AutoScaling Group.\nThen you have multiple subnets that are mapped to multiple availability zones.\nThe solution is to create several instances across several availability zones and to use an elastic load balancer to distribute the traffic and Auto Scaling group to scale the instances.\nThis way, even if an instance fails, you already have other ones available.\nAWS recommends this solution as they have an SLA of 99.95% for their instance in an AZ.\nBy putting in several AZs, you can have 100% availability.\nFor a static web site, the SQS is not required to build such an environment.\nIf you have a system such as an order processing system, which has that sort of queuing of requests, then that could be a candidate for using SQS Queues.\nFor more information on high availability, please visit the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html#scale-and-load-balance-prerequisites\n\n\nTo create a highly available and scalable environment for a static website hosted on EC2 instances, the following aspects can be used:\nAuto Scaling group: An Auto Scaling group can be used to automatically scale the EC2 instances based on the traffic or demand. It allows you to automatically add or remove EC2 instances in response to changes in demand. This ensures that the environment is highly available and can handle varying levels of traffic without any downtime. Elastic Load Balancer (ELB): An ELB can be used to distribute incoming traffic across multiple EC2 instances. This helps to balance the load and ensure that no single EC2 instance is overwhelmed with traffic. ELBs can also detect unhealthy instances and automatically route traffic to healthy instances, further increasing the availability of the environment. Multiple Availability Zones: Deploying EC2 instances in multiple Availability Zones (AZs) can provide high availability and fault tolerance. AZs are geographically distinct data centers within a region, each with independent power, cooling, and networking. By deploying EC2 instances in multiple AZs, you can ensure that your website remains available even if an entire AZ goes down. SQS queue: SQS is a message queuing service that enables decoupling and scalability of microservices, serverless functions and distributed systems. SQS can be used to scale the processing of messages or tasks within your application.\nIn summary, using Auto Scaling group, Elastic Load Balancer, and Multiple Availability Zones can help you create a highly available and scalable environment for hosting a static website on EC2 instances.\n\n"
}, {
  "id" : 383,
  "question" : "A global media firm is using AWS CodePipeline as an automation service for releasing new features to customers.\nAll the codes are uploaded in the Amazon S3 bucket.\nChanges in files stored in the S3 bucket should trigger AWS CodePipeline that will further initiate AWS Elastic Beanstalk for deploying additional resources.\nWhat is the additional requirement that should be configured to trigger CodePipeline in a faster way?\n",
  "answers" : [ {
    "id" : "fc6878c6ba5f4bd68074fb5cd96c426a",
    "option" : "Enable periodic checks and create a Webhook which triggers pipeline once S3 bucket is updated.",
    "isCorrect" : "false"
  }, {
    "id" : "7db1571319194a819c3247c4dfb8967c",
    "option" : "Disable periodic checks, create an Amazon CloudWatch Events rule &amp; AWS CloudTrail trail.",
    "isCorrect" : "true"
  }, {
    "id" : "ff4b36016a8c460e9bf8b7851b7a704a",
    "option" : "Enable periodic checks, create an Amazon CloudWatch Events rule &amp; AWS CloudTrail trail.",
    "isCorrect" : "false"
  }, {
    "id" : "5bfd4b5ae49242128c126b2899b5f751",
    "option" : "Disable periodic checks and create a Webhook which triggers pipeline once S3 bucket is updated.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nTo automatically trigger pipeline with changes in the source S3 bucket, Amazon CloudWatch Events rule &amp; AWS CloudTrail trail must be applied.\nWhen there is a change in the S3 bucket, events are filtered using AWS CloudTrail &amp; then Amazon CloudWatch events are used to trigger the start of the pipeline.\nThis default method is faster &amp; periodic checks should be disabled to have events-based triggering of CodePipeline.\nYou can use the following tools to monitor your CodePipeline pipelines and their resources:\nAmazon CloudWatch Events - Use Amazon CloudWatch Events to detect and react to pipeline execution state changes (for example, send an Amazon SNS notification or invoke a Lambda function).\nAWS CloudTrail - Use CloudTrail to capture API calls made by or on behalf of CodePipeline in your AWS account and deliver the log files to an Amazon S3 bucket.\nYou can choose to have CloudWatch publish Amazon SNS notifications when new log files are delivered so you can take quick action.\nConsole and CLI - you can use the CodePipeline console and CLI to view details about the status of a pipeline or a particular pipeline execution.\nOption A is incorrect as Webhooks are used to trigger pipeline when the source is GitHub repository.\nAlso, the periodic check will be a slower process to trigger CodePipeline.\nOption C is incorrect as Periodic checks are not a faster way to trigger CodePipeline.\nOption D is incorrect as Webhooks are used to trigger pipeline when the source is GitHub repository.\nFor more information on Automatically Triggering Pipeline, refer to the following URL:\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html\n\nIn this scenario, the requirement is to trigger AWS CodePipeline as soon as any changes are made in the files stored in the Amazon S3 bucket. So, instead of relying on periodic checks, we need to set up a mechanism that can detect changes in the S3 bucket and trigger AWS CodePipeline accordingly.\nOption A suggests enabling periodic checks and creating a webhook to trigger the pipeline once the S3 bucket is updated. This approach can work, but it may not be the most efficient way to trigger the pipeline as it may take time for the periodic checks to occur.\nOption B suggests disabling periodic checks and creating an Amazon CloudWatch Events rule and AWS CloudTrail trail. This option is incorrect because disabling periodic checks will prevent the pipeline from being triggered in a timely manner when changes are made to the S3 bucket.\nOption C suggests enabling periodic checks, creating an Amazon CloudWatch Events rule and AWS CloudTrail trail. This is the correct answer as enabling periodic checks will allow AWS CodePipeline to check the S3 bucket for changes on a regular basis. Additionally, creating an Amazon CloudWatch Events rule and AWS CloudTrail trail will allow for real-time detection of changes in the S3 bucket, ensuring that the pipeline is triggered as soon as any changes are made.\nOption D suggests disabling periodic checks and creating a webhook to trigger the pipeline once the S3 bucket is updated. This option is incorrect because disabling periodic checks will prevent the pipeline from being triggered in a timely manner when changes are made to the S3 bucket.\nIn summary, the correct answer is option C - enable periodic checks, create an Amazon CloudWatch Events rule and AWS CloudTrail trail. This approach will ensure that AWS CodePipeline is triggered as soon as changes are made to the S3 bucket, resulting in faster deployment of additional resources using AWS Elastic Beanstalk.\n\n"
}, {
  "id" : 384,
  "question" : "You have a requirement to host a web-based application with high availability.\nSo you create an Elastic Load Balancer and place the Auto Scaling group behind the Elastic Load Balancer.\nYou need to ensure that users only access the application via the DNS name of the load balancer instead of the EC2 public IP addresses.\nHow would you design the network part of the application? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "cdbb27598fc64d79b4a72350eeeec9a6",
    "option" : "Configure 2 public subnets for the Elastic Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "47bbd5a7b18a4d22881a3ae0e3881a13",
    "option" : "Configure 2 private subnets for the Elastic Load Balancer",
    "isCorrect" : "false"
  }, {
    "id" : "33eabd71f1494e4fad11ca19b86fe7d7",
    "option" : "Configure 2 public subnets for the Auto Scaling group",
    "isCorrect" : "false"
  }, {
    "id" : "56ad0c69f1234dc0800c4fe3467605f3",
    "option" : "Configure 2 private subnets for the Auto Scaling group.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nThe AWS Documentation mentions the following.\nUse Case: A load balancer, two public subnets, two private subnets, two NAT Gateways,\nThe NAT Gateway goes into both public subnets (Public-Subnet-A, Public-Subnet-B).\nThe EC2 instances are launched in private subnets across two AZs (Private-Subnet-A, Private-Subnet-B).\nThe Route Table Private-Subnet-A points to the NAT Gateway in Public-Subnet-A.The Route Table Private-Subnet-B points to the NAT Gateway in Public-Subnet-B.If one of the AZs should fail, then the EC2 instances in the remaining private subnet will still communicate with the Internet because they have their own NAT Gateway in that AZ.\nOption B is incorrect since the ELB needs to be placed in the public subnet to access the Internet.\nOption C is incorrect because the Auto Scaling groups should be placed in the private subnets for security and the question also requires that users cannot access the application through the public IP addresses of the instances.\nFor more information on an example to use the Load balancer, please visit the below URL-\nhttps://aws.amazon.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/\n\nTo host a web-based application with high availability, it is a best practice to use Elastic Load Balancer (ELB) and Auto Scaling group together. An ELB distributes incoming traffic across multiple instances in multiple Availability Zones (AZs) for the same region to achieve high availability. Auto Scaling group helps to automatically adjust the number of instances running in response to changes in demand for the application.\nTo ensure that users only access the application via the DNS name of the load balancer instead of the EC2 public IP addresses, we need to design the network part of the application properly.\nOption A: Configure 2 public subnets for the Elastic Load Balancer An ELB needs to be deployed in public subnets to allow access from the internet. By configuring 2 public subnets for the ELB, we can achieve high availability in case of failure in one subnet. However, this option alone does not ensure that users only access the application via the DNS name of the load balancer.\nOption B: Configure 2 private subnets for the Elastic Load Balancer It is not recommended to deploy an ELB in private subnets as it requires internet access for incoming traffic. However, if we use a VPN or Direct Connect to access the VPC, then we can deploy the ELB in private subnets. This option alone does not ensure that users only access the application via the DNS name of the load balancer.\nOption C: Configure 2 public subnets for the Auto Scaling group An Auto Scaling group needs to be deployed in private subnets to ensure that the instances are not exposed to the internet. By configuring 2 public subnets for the Auto Scaling group, we cannot achieve this goal. Also, this option alone does not ensure that users only access the application via the DNS name of the load balancer.\nOption D: Configure 2 private subnets for the Auto Scaling group. This is the correct option. To ensure that the instances are not exposed to the internet, an Auto Scaling group needs to be deployed in private subnets. By configuring 2 private subnets for the Auto Scaling group, we can achieve high availability in case of failure in one subnet. Also, we can use a NAT Gateway to allow instances to connect to the internet for updates and patches. Finally, by placing the ELB in the public subnets, we can ensure that users only access the application via the DNS name of the load balancer.\nTherefore, the correct answers are B and D.\n\n"
}, {
  "id" : 385,
  "question" : "You are working as an AWS Architect for a retail company using AWS EC2 instance for a web application.\nThe company is using Provisioned IOPS SSD EBS volumes to store all product database. This is a critical database &amp; you need to ensure appropriate backups are accomplished every 12 hours.\nAlso, you need to ensure that storage space is optimally used for storing all these snapshots removing all older files.\nWhich of the following can help to meet this requirement with the least management overhead?\n",
  "answers" : [ {
    "id" : "314643c3f8964881999b695d4dbe46c1",
    "option" : "Manually create snapshots &amp; delete old snapshots for EBS volumes as this is a critical data.",
    "isCorrect" : "false"
  }, {
    "id" : "0fc9388197a64a4e95d4803936bb1bf1",
    "option" : "Use Amazon CloudWatch events to initiate AWS Lambda which will create snapshot of EBS volumes along with deletion of old snapshots.",
    "isCorrect" : "false"
  }, {
    "id" : "3ba34f9985e844a2a75d03db28f7dc1e",
    "option" : "Use Amazon Data Lifecycle Manager to schedule EBS snapshots and delete old snapshots as per retention policy.",
    "isCorrect" : "true"
  }, {
    "id" : "924abff29ac245858b41243664ff4714",
    "option" : "Use Third party tool to create snapshot of EBS volumes along with deletion of old snapshots.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon Data Lifecycle Manager can be used for creation, retention &amp; deletion of EBS snapshots.\nIt protects critical data by initiating backup of Amazon EBS volumes at selected intervals, along with storing &amp; deletion of old snapshots to save storage space &amp; cost.\nOption A is incorrect as this will result in additional admin work &amp; there can be a risk of losing critical data due to manual errors.\nOption B is incorrect as for this we will need to do additional config changes in CloudWatch &amp; AWS Lambda.\nAlso, AWS Lambda would need a script to create snapshots which can be an overhead.\nOption D is incorrect as this will result in an additional cost to maintain a third-party software.\nFor more information on Automating Amazon EBS Snapshot Lifecycle, refer to the following URL,\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\n\nThe best option to meet the requirement of creating backups every 12 hours and managing storage space optimally for a critical database is to use Amazon Data Lifecycle Manager (DLM) to schedule EBS snapshots and delete old snapshots as per the retention policy. Option C is the correct answer.\nExplanation: Option A is not a scalable solution because it requires manual intervention, which is not feasible when managing multiple instances or volumes. It is also prone to human errors.\nOption B, using Amazon CloudWatch events to trigger an AWS Lambda function to create snapshots and delete old snapshots, is possible but not the optimal solution. It requires more management overhead, as it involves creating and managing the Lambda function, configuring the CloudWatch event trigger, and ensuring that the function is running correctly.\nOption D, using a third-party tool, is not recommended because it adds another layer of complexity and cost, and it may not integrate well with AWS services.\nOption C, using Amazon Data Lifecycle Manager (DLM), is the best option because it is a fully managed service that automates the process of creating EBS snapshots according to a defined schedule and retention policy. DLM allows you to create policies that can define the interval between snapshots, the number of snapshots to retain, and the time to delete older snapshots. With DLM, you can automate the backup process, reduce the risk of human errors, and optimize storage space.\nIn conclusion, option C, using Amazon Data Lifecycle Manager, is the best option to ensure appropriate backups are accomplished every 12 hours, manage storage space optimally, and minimize management overhead.\n\n"
}, {
  "id" : 386,
  "question" : "You work as an architect for a consulting company.\nThe consulting company normally creates the same set of resources for their clients.\nThey want some way of building templates, which can then be used to deploy the resources to the AWS accounts for the various clients.\nAlso, your team needs to be ensured that they have control over the infrastructure.\nWhich of the following service can help fulfill this requirement?\n",
  "answers" : [ {
    "id" : "5bcb58085187444ea34300ed26ca73c7",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "27df995846c74ba685f919fa9fcb760e",
    "option" : "Custom AMI",
    "isCorrect" : "false"
  }, {
    "id" : "0ec8774dc0a64a98a11ba06843208cf7",
    "option" : "AWS Cloudformation",
    "isCorrect" : "true"
  }, {
    "id" : "6d8a0d8ae0e642818fafbd4ce7e6a02c",
    "option" : "EBS Snapshots.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS.\nYou create a template that describes all the AWS resources you want (like Amazon EC2 instances or Amazon RDS DB instances)\nAWS CloudFormation takes care of provisioning and configuring those resources for you.\nElastic Beanstalk is intended to make developers' lives easier.\nCloudFormation is intended to make systems engineers' lives easier.\nElastic Beanstalk is a PaaS layer on top of AWS's IaaS services which abstracts away the underlying EC2 instances, Elastic Load Balancers, Auto Scaling groups, etc.\nThis makes it a lot easier for developers, who don't want to deal with all the systems stuff, to get their application quickly deployed on AWS.\nWith Elastic Beanstalk, you don't need to understand how any of the underlying magic works.\nCloudFormation, on the other hand, doesn't automatically do anything.\nIt's simply a way to define all the resources needed for deployment in a huge JSON file.\nOption A could be a valid choice.\nBut it has been clearly asked in question that the team needs to have control over the infrastructure.\nOption B is invalid because Custom AMI will help create an Image for EC2 Instances, not for all the resources.\nOption D is invalid because EBS Snapshot is a copy of your Volume used for EC2 Instance.\nFor more information on CloudFormation, please visit the below URL-\nhttp://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\n\nThe service that can help fulfill the requirement of creating templates to deploy resources to AWS accounts for various clients, while ensuring control over infrastructure is AWS CloudFormation.\nAWS CloudFormation is a service that allows you to model and provision AWS resources as a code, using a declarative template format. This enables you to manage and automate the deployment of your infrastructure in a repeatable and predictable way. CloudFormation templates are written in JSON or YAML format, and describe the resources that need to be created, their configuration, and any dependencies between them.\nIn this scenario, the consulting company can create a CloudFormation template that contains all the resources that they normally create for their clients. This template can then be used to deploy the same set of resources to different AWS accounts for different clients, without the need for manual intervention. This approach can save time and reduce the chance of human errors.\nIn addition, CloudFormation allows for version control and can be integrated with source code management tools such as Git. This provides the consulting company with greater control over the infrastructure and the ability to track changes made to the templates over time.\nTherefore, the correct answer to the question is C. AWS CloudFormation.\nTo summarize, AWS CloudFormation is a powerful tool that can help manage and automate the deployment of AWS resources using a declarative template format. It enables organizations to create templates that can be used to deploy the same set of resources to different AWS accounts in a repeatable and predictable way.\n\n"
}, {
  "id" : 387,
  "question" : "You work as an architect for a company.\nAn application will be deployed on a set of EC2 instances in a private subnet of VPC.\nYou need to ensure that IT administrators can securely administer the instances in the private subnet.\nHow can you accomplish this?\n",
  "answers" : [ {
    "id" : "8ee9c1541a124f05827dcf874d445156",
    "option" : "Create a NAT gateway, ensure SSH access is provided to the NAT gateway. Access the Instances via the NAT gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "48442bf1a0af4f69baa4c6101bdc6c32",
    "option" : "Create a NAT instance in a public subnet, ensure SSH access is provided to the NAT instance. Access the Instances via the NAT instance.",
    "isCorrect" : "false"
  }, {
    "id" : "a39d08f32b4d4f81aaf1c8b493784b76",
    "option" : "Create a bastion host in the private subnet. Make IT admin staff use this as a jump server to the backend instances.",
    "isCorrect" : "false"
  }, {
    "id" : "acd8bc670f5d46c8bcf85373fa925717",
    "option" : "Create a bastion host in the public subnet. Make IT admin staff use this as a jump server to the backend instances.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nA bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet.\nBecause of its exposure to potential attacks, a bastion host must minimize the chances of penetration.\nFor example, you can use a bastion host to mitigate the risk of allowing SSH connections from an external network to the Linux instances launched in a private subnet of your Amazon Virtual Private Cloud (VPC).\nOption A is invalid because you would not allow the inbound SSH access via the NAT gateway.\nOption B is invalid because you would not allow the inbound access via the NAT instance.\nOption C is incorrect since the bastion host needs to be in the public subnet.\nFor more information on bastion hosts, please visit the below URL-\nhttps://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/\n\nThe correct answer to this question is option C: Create a bastion host in the private subnet. Make IT admin staff use this as a jump server to the backend instances.\nExplanation: When an application is deployed in a private subnet of a VPC, the instances are not accessible from the internet. However, it is necessary to provide secure access for IT administrators to manage the instances.\nOption A is incorrect because it suggests creating a NAT gateway to access the instances. Although a NAT gateway provides internet access to instances in a private subnet, it does not provide secure access for IT administrators.\nOption B is incorrect because it suggests creating a NAT instance in a public subnet. NAT instances are not recommended because they require maintenance and do not provide high availability.\nOption D is incorrect because it suggests creating a bastion host in the public subnet. This approach exposes the bastion host to the internet, making it vulnerable to attacks.\nOption C is the correct answer because it suggests creating a bastion host in the private subnet. A bastion host is a secure gateway that provides access to the backend instances. By creating a bastion host in the private subnet, IT administrators can securely access the backend instances without exposing them to the internet. The IT administrators can connect to the bastion host using SSH and then use the bastion host as a jump server to access the backend instances.\nIn summary, option C is the most secure and recommended approach for providing secure access to instances in a private subnet of a VPC.\n\n"
}, {
  "id" : 388,
  "question" : "You work as an architect for a company.\nAn application is going to be deployed on a set of EC2 instances in a VPC.\nThe Instances will be hosting a web application.\nYou need to design the security group to ensure that users have the ability to connect from the Internet via HTTPS.\nWhich of the following needs to be configured for the security group?\n",
  "answers" : [ {
    "id" : "02de73cdcf3447b78aa59fabd90255af",
    "option" : "Allow Inbound access on port 443 for 0.0.0.0/0",
    "isCorrect" : "true"
  }, {
    "id" : "845f9442b001405a8235f0af3ce4b195",
    "option" : "Allow Outbound access on port 443 for 0.0.0.0/0",
    "isCorrect" : "false"
  }, {
    "id" : "83f753f2e073475e833207ca43cda697",
    "option" : "Allow Inbound access on port 80 for 0.0.0.0/0",
    "isCorrect" : "false"
  }, {
    "id" : "712a4b99c7784f5bb8c243f0ba562e92",
    "option" : "Allow Outbound access on port 80 for 0.0.0.0/0",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nA security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nWhen you launch an instance in a VPC, you can assign five security groups to the instance.\nSecurity groups act at the instance level, not the subnet level.\nTherefore, each instance in a subnet in your VPC could be assigned to a different set of security groups.\nIf you don't specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.AWS Security groups are stateful.\nIt means that you do not need to open the outbound for responses - open only inbound for requests.\nIf you think your instances will be sending requests to certain IPs (for example: to upgrade/install a package), then you need to open the IP/port for that request.\nBy default, it is open for all traffic.\nOption B is incorrect since security groups are stateful.\nYou don't need to define the rule for outbound traffic.\nOptions C and D are incorrect since you need to ensure access for HTTPS.\nHence you should not configure rules for port 80.\nFor more information on security groups, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html\n\nThe correct answer is A. Allow Inbound access on port 443 for 0.0.0.0/0.\nExplanation:\nTo enable users to connect to a web application running on EC2 instances via HTTPS, the security group associated with the instances should be configured to allow inbound access on port 443.\nPort 443 is the default port used for HTTPS connections, and it's used to secure communication between the web server and the client browser. Therefore, allowing inbound access on port 443 for 0.0.0.0/0 (which means any IP address) will enable users to connect to the web application securely over the internet.\nOption B, allowing outbound access on port 443, is not necessary for HTTPS connections to work. Outbound access is required for the instances to communicate with other services on the internet, but this is not relevant to the requirement of enabling HTTPS access from the internet.\nOption C, allowing inbound access on port 80, is incorrect because port 80 is used for unsecured HTTP connections, and it does not encrypt the data exchanged between the web server and the client browser. Moreover, the question specifically asks for HTTPS access, which uses port 443.\nOption D, allowing outbound access on port 80, is also irrelevant to the requirement of enabling HTTPS access from the internet. Outbound access on port 80 is needed if the web application running on the instances needs to communicate with other services on the internet over unsecured HTTP connections.\n\n"
}, {
  "id" : 389,
  "question" : "Your company runs an automobile reselling company that has a popular online store on AWS.\nThe application sits behind an Auto Scaling group and requires new instances of the Auto Scaling group to identify their public and private IP addresses.\nWhich of the following is the correct AWS option to identify the IP addresses?\n",
  "answers" : [ {
    "id" : "354104f8d8c5402b8c011b04ba9057eb",
    "option" : "By using Ipconfig for windows or Ifconfig for Linux.",
    "isCorrect" : "false"
  }, {
    "id" : "3c19cc04356e4f57b5cdb5d4e0e5a8ca",
    "option" : "By using a CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "7f97cace1c6e493d952d510fb8f10c14",
    "option" : "Using a Curl or Get Command to get the latest meta-data from http://169.254.169.254/latest/meta-data/",
    "isCorrect" : "true"
  }, {
    "id" : "16b49281734c41fca52419ed4ca77a16",
    "option" : "Using a Curl or Get Command to get the latest user-data from http://169.254.169.254/latest/user-data/",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nTo get the private and public IP addresses, you can run the following commands on the running instance.\nhttp://169.254.169.254/latest/meta-data/local-ipv4\nhttp://169.254.169.254/latest/meta-data/public-ipv4\nOption A is partially correct, but it is an overhead when you already have the service running in AWS.\nOption B is incorrect because CloudTrail is used for tracking the API activities of a resource.\nOption D is incorrect because user-data cannot get the IP addresses.\nFor more information on instance metadata, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html\n\nThe correct option to identify the public and private IP addresses of new instances in an Auto Scaling group on AWS is C - Using a Curl or Get Command to get the latest meta-data from http://169.254.169.254/latest/meta-data/.\nExplanation:\nAmazon EC2 instances come with metadata that can be accessed by making a request to a special IP address - http://169.254.169.254/. This IP address is available to every EC2 instance, and it provides a way for the instance to learn about its own metadata regardless of the operating system or network configuration.\nThe metadata contains a wealth of information about the instance, including its instance ID, instance type, hostname, public and private IP addresses, security groups, and more. To access the metadata, you can use a command-line tool like Curl or a programming language like Python to make an HTTP request to the metadata endpoint.\nTo get the public and private IP addresses of a new instance in an Auto Scaling group, you can make an HTTP GET request to http://169.254.169.254/latest/meta-data/public-ipv4 and http://169.254.169.254/latest/meta-data/local-ipv4, respectively. These requests will return the instance's public and private IP addresses as plain text.\nOption A is incorrect because Ipconfig and Ifconfig are commands to display network interface information on the local machine, not EC2 instances.\nOption B is incorrect because CloudTrail is a service that logs AWS API calls and can be used for auditing and security purposes. It does not provide information about individual EC2 instances.\nOption D is incorrect because user-data is information that can be passed to an instance when it is launched, typically in the form of a shell script or cloud-init script. It does not provide information about the instance's IP addresses.\n\n"
}, {
  "id" : 390,
  "question" : "You have been designing a CloudFormation template that creates one elastic load balancer fronting two EC2 instances.\nWhich section of the template should you edit so that the load balancer's DNS is returned upon creating the stack?\n",
  "answers" : [ {
    "id" : "29940db7dc0346d4ac598f9c824eb4d4",
    "option" : "Resources",
    "isCorrect" : "false"
  }, {
    "id" : "596d1a0dfc9644c38a9aed63895336e9",
    "option" : "Parameters",
    "isCorrect" : "false"
  }, {
    "id" : "33242878b9c04de293fd96d7e0ccb7db",
    "option" : "Outputs",
    "isCorrect" : "true"
  }, {
    "id" : "e251886bec684471ba2a19f5d3e72e68",
    "option" : "Mappings.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nAnswer - C.\nThe below example shows a simple CloudFormation template.\nIt creates an EC2 instance based on the AMI - ami-d6f32ab5\nWhen the instance is created, it will output the AZ in which it is created.\n{\n\"Resources\": {\n\"MyEC2Instance\": {\n\"Type\": \"AWS::EC2::Instance\",\n\"Properties\": {\n\"ImageId\": \"ami-d6f32ab5\"\n}\n}\n},\n\"Outputs\": {\n\"Availability\": {\n\"Description\": \"The Instance ID\",\n\"Value\":\n{ \"Fn::GetAtt\" : [ \"MyEC2Instance\", \"AvailabilityZone\" ]}\n}\n}\n}\nOption A is incorrect because this is used to define the main resources in the template.\nOption B is incorrect because this is used to define parameters that can be taken during template deployment.\nOption D is incorrect because this is used to map key-value pairs in a template.\nTo understand more on CloudFormation, please visit the URL-\nhttps://aws.amazon.com/cloudformation/\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-getatt.html\n\nThe correct answer is C. Outputs.\nIn an AWS CloudFormation template, the Resources section is where you define the AWS resources you want to create, such as EC2 instances, load balancers, and databases. The Parameters section is where you define the parameters that your users can pass to your CloudFormation stack at runtime, such as instance types, security groups, and key pairs. The Mappings section is where you define a set of named keys and associated values that you can later reference in your CloudFormation template.\nThe Outputs section is where you can define the values that you want to be returned when the CloudFormation stack is created. These outputs can be used by other resources, by other CloudFormation stacks, or by external systems. For example, you might want to return the DNS name of an elastic load balancer so that your users can use it to access your application.\nTo return the DNS name of an elastic load balancer in your CloudFormation template, you would define an output in the Outputs section. Here's an example:\nyaml Copy code Outputs:   LoadBalancerDNS:     Description: The DNS name of the load balancer     Value: !GetAtt MyLoadBalancer.DNSName\nIn this example, the output is named LoadBalancerDNS and has a description that explains what it is. The Value property uses the intrinsic function !GetAtt to retrieve the DNS name of the MyLoadBalancer resource, which is the elastic load balancer you created in the Resources section.\nWhen you create the CloudFormation stack, the LoadBalancerDNS output will be returned along with any other outputs you define. You can then use this output in other parts of your CloudFormation template, or pass it to external systems as needed.\n\n"
}, {
  "id" : 391,
  "question" : "A company has a set of VPCs defined in AWS.\nThey need to connect this to their on-premises network.\nThey need to ensure that all data is encrypted in transit.\nWhich of the following would you use to connect the VPCs to the on-premises networks?\n",
  "answers" : [ {
    "id" : "22486fbd2dfb47c2a1f00de18456d290",
    "option" : "VPC Peering",
    "isCorrect" : "false"
  }, {
    "id" : "ec030a2664784ec68438221da3027d3e",
    "option" : "VPN connections",
    "isCorrect" : "true"
  }, {
    "id" : "98412ba6c5204576b21f4fe0958ac4a1",
    "option" : "AWS Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "3df08c4882a146f6a798443f60db2568",
    "option" : "Placement Groups.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nBy default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network.\nYou can enable access to your remote network from your VPC by attaching a virtual private gateway to the VPC, creating a custom route table, updating your security group rules, and creating an AWS managed VPN connection.\nVPN connection encrypts the traffic, whereas Direct Connect does not encrypt your traffic that is in transit.\nTo encrypt the data in transit that traverses AWS Direct Connect, you must use the transit encryption options for that service.\nOption A is incorrect because this is used to connect multiple VPCs together.\nOption C is incorrect because this does not encrypt traffic in connections between AWS VPCs and the On-premises network.\nOption D is incorrect because this is used for low latency access between EC2 Instances.\nFor more information on AWS VPN connections and Direct Connect, please visit the below URLs-\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPC_VPN.html#concepts\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/encryption-in-transit.html\n\nTo connect a company's VPCs in AWS to their on-premises network, there are different methods available such as VPC peering, VPN connections, AWS Direct Connect, and Placement Groups. Among these options, VPN connections and AWS Direct Connect are the two most suitable solutions to ensure all data is encrypted in transit.\nVPN Connections: VPN (Virtual Private Network) connections use encrypted tunnels over the public internet to connect the company's VPCs to their on-premises network. The VPN connection is established through the internet gateway and encrypted with IPSec. VPN connections are secure and reliable for connecting remote sites and have lower setup costs compared to AWS Direct Connect. AWS VPNs can be configured to provide redundancy and high availability.\nAWS Direct Connect: AWS Direct Connect is a dedicated network connection between the company's on-premises network and AWS. The connection is established through a private network connection provided by AWS Direct Connect partners. AWS Direct Connect provides a dedicated, private connection, which makes it a more secure and reliable option than VPN connections. Data is encrypted in transit using AES 256 encryption. AWS Direct Connect provides consistent network performance, and it is suitable for high bandwidth and mission-critical applications.\nVPC Peering: VPC Peering allows connecting two VPCs within the same region or different regions. However, it is not recommended for connecting on-premises networks. VPC peering also doesn't provide encryption of data in transit.\nPlacement Groups: Placement Groups are used to control the placement of instances within an availability zone to achieve low latency and high throughput. They are not related to connecting VPCs to on-premises networks and do not provide any encryption.\nTherefore, in this scenario, the most suitable options to connect VPCs to the on-premises network while ensuring data encryption in transit would be either VPN connections or AWS Direct Connect.\n\n"
}, {
  "id" : 392,
  "question" : "A company wants to host a selection of MongoDB instances.\nThey are expecting a high load and want to achieve high performance.\nAs an architect, you need to ensure that the right storage is used to host the MongoDB database.\nWhich of the following would you incorporate as the underlying storage layer?\n",
  "answers" : [ {
    "id" : "77dfaf6ced23481b8b34a710ea398a29",
    "option" : "Provisioned IOPS SSD",
    "isCorrect" : "true"
  }, {
    "id" : "09e7c862e337455a802ae45dca47bc0c",
    "option" : "General Purpose SSD",
    "isCorrect" : "false"
  }, {
    "id" : "b06fc4c626234442b5756fae24470d42",
    "option" : "Throughput Optimized HDD",
    "isCorrect" : "false"
  }, {
    "id" : "288621aae38349798212da9a83ee70a6",
    "option" : "Cold HDD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe below snapshot from the AWS Documentation shows the different volume types and why Provisioned IOPS is the most ideal for this requirement.\nAlso, Provisioned IOPS is recommended for NoSQL and Relational Databases.\nBecause of what is mentioned in the documentation as the ideal storage type, the other options are invalid.\nFor more information on the different EBS volume types, please visit the below URL-\nhttps://aws.amazon.com/ebs/details/\n\n\nWhen selecting storage for a MongoDB instance, it is essential to consider the expected workload, the type of operations, and the performance requirements. Here are the explanations of the storage options listed in the question and their suitability for hosting MongoDB databases:\nA. Provisioned IOPS SSD (Solid State Drive): This storage option offers consistent high-performance I/O operations per second (IOPS) with low latency. Provisioned IOPS SSD is recommended for high-performance applications that require low latency and high throughput for both read and write operations, such as large-scale MongoDB deployments with a high number of transactions per second (TPS). It provides the necessary level of IOPS required to support write-heavy workloads and provides the necessary low-latency access for read-heavy workloads. Therefore, provisioned IOPS SSD is an excellent storage option for MongoDB.\nB. General Purpose SSD: This storage option provides a balance between cost and performance for workloads that require low to moderate I/O operations. However, it is not recommended for high-performance applications or for applications that require consistent low latency, such as MongoDB. Hence, it may not be an ideal storage option for hosting a high-performance MongoDB database.\nC. Throughput Optimized HDD (Hard Disk Drive): This storage option is designed for workloads that require high throughput for sequential read/write operations. However, it may not be suitable for high-performance applications with low-latency requirements, such as MongoDB. MongoDB requires low-latency access to storage, especially for write-intensive workloads, and HDDs are known to have higher latency than SSDs. Therefore, this storage option may not be the best fit for hosting MongoDB databases.\nD. Cold HDD: This storage option is designed for infrequently accessed data or archival data. It has the lowest cost per GB but offers the lowest performance and lowest I/O operations per second (IOPS) compared to other storage options. It is not recommended for high-performance applications or workloads that require frequent access to data. Therefore, this storage option may not be suitable for hosting a high-performance MongoDB database.\nIn conclusion, the most suitable storage option for hosting a high-performance MongoDB database is Provisioned IOPS SSD.\n\n"
}, {
  "id" : 393,
  "question" : "A customer needs corporate IT governance and cost oversight of all AWS resources consumed by its divisions.\nEach division has its own AWS account and there is a need to ensure that the security policies are kept in place at the Account Level.\nHow can you achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "c18eeab763c34aac846bce7f103391be",
    "option" : "Use AWS organizations",
    "isCorrect" : "true"
  }, {
    "id" : "d2cf3a20428e485684d815aece53cf6c",
    "option" : "Club all divisions under a single account instead",
    "isCorrect" : "false"
  }, {
    "id" : "7512ae98b532499aa5ed9e88e735bff3",
    "option" : "Use IAM Policies to segregate access",
    "isCorrect" : "false"
  }, {
    "id" : "7fb8182715404fc3b7c22e46829c25ff",
    "option" : "Use Service control policies.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nWith AWS Organizations, you can centrally manage policies across multiple AWS accounts without having to use custom scripts and manual processes.\nFor example, you can apply service control policies (SCPs) across multiple AWS accounts that are members of an organization.\nSCPs allow you to define which AWS service APIs can and cannot be executed by AWS Identity and Access Management (IAM) entities (such as IAM users and roles) in your organization's member AWS accounts.\nSCPs are created and applied from the master account, which is the AWS account that you used when you created your organization.\nOption B is incorrect since the question mentions that you need to use separate AWS accounts.\nOption C is incorrect since you need to use service control policies.\"AWS IAM doesn't provide the facility to define access permissions to that minute level i.e., which AWS service APIs can and cannot be executed by IAM entities.\"\nFor more information on how to use service control policies, please visit the below URL.\nhttps://aws.amazon.com/blogs/security/how-to-use-service-control-policies-in-aws-organizations/\n\nTo achieve corporate IT governance and cost oversight of all AWS resources consumed by its divisions while ensuring that the security policies are kept in place at the Account Level, you can use the following solutions:\nA. Use AWS Organizations: AWS Organizations helps in centrally managing multiple AWS accounts. It allows you to consolidate billing and set up a hierarchy of accounts for the organization. You can apply policies to the entire organization or specific accounts, and these policies can govern various aspects of the AWS resources, such as networking, security, and compliance. With AWS Organizations, you can easily apply security policies to all AWS accounts, automate account creation and management, and consolidate billing and payment.\nD. Use Service Control Policies: Service Control Policies (SCPs) are another way to manage AWS resources at scale. SCPs allow you to set permissions and restrictions on specific services or actions across multiple AWS accounts in your organization. SCPs are applied to an AWS Organizations organizational unit (OU) or an account, and the policies can be used to prevent users from performing actions that violate security policies or incur unexpected costs. SCPs can be used to enforce security and compliance requirements across all AWS accounts in an organization, ensuring that all divisions are following the same security policies.\nTherefore, the correct answers to the question are A and D. By using AWS Organizations and Service Control Policies, you can achieve corporate IT governance, cost oversight, and security policies at the Account Level for all AWS resources consumed by the divisions.\n\n"
}, {
  "id" : 394,
  "question" : "Your company has a set of EC2 Instances hosted on the AWS Cloud.\nAs an architect, you have been told to ensure that if the status of any of the instances is related to failure, then the instances should restart automatically.\nHow can you achieve this in the most efficient way possible?\n",
  "answers" : [ {
    "id" : "646339940fa54819aa8c61f3f3222e88",
    "option" : "Create CloudWatch alarms that stop and start the instance based off of status check alarms.",
    "isCorrect" : "true"
  }, {
    "id" : "b305d87ae74148bb865e66766737eb80",
    "option" : "Write a script that queries the EC2 API for each instance status check.",
    "isCorrect" : "false"
  }, {
    "id" : "3478039d6f85480c89be4402113297ad",
    "option" : "Write a script that periodically shuts down and starts instances based on certain stats.",
    "isCorrect" : "false"
  }, {
    "id" : "f04db2b6706a475487351d6a1b038f15",
    "option" : "Implement a third-party monitoring tool.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nUsing Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances.\nYou can use the stop or terminate actions to help you save money when you no longer need an instance to be running.\nYou can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.\nAll other options are possible but would be an extra maintenance overhead.\nFor more information on using alarm actions, please refer to the below link-\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html\n\nThe most efficient way to ensure that EC2 instances restart automatically when their status is related to failure is to create CloudWatch alarms that stop and start the instance based on status check alarms.\nCloudWatch is a monitoring and management service provided by AWS that allows users to collect and track metrics, collect and monitor log files, and set alarms. CloudWatch provides various metrics related to EC2 instances, including the status check metrics. When an EC2 instance fails a status check, CloudWatch can trigger an alarm that can then perform an action, such as stopping and starting the instance.\nCreating a CloudWatch alarm to monitor the status of an EC2 instance is a simple process. First, the user needs to create a metric filter to filter out the desired metric. In this case, the metric filter should filter out the status check metric for the EC2 instance. Next, the user can create an alarm based on the metric filter. The alarm can then be configured to perform an action, such as stopping and starting the instance, when the alarm is triggered.\nWriting a script that queries the EC2 API for each instance status check, or periodically shutting down and starting instances based on certain stats, are possible solutions but are not as efficient as creating CloudWatch alarms. Writing a script requires additional maintenance and monitoring, and is prone to errors. Additionally, these methods may not be as flexible as CloudWatch alarms, which allow for more complex conditions to trigger the action. Implementing a third-party monitoring tool may also be an option, but may require additional costs and setup time. Therefore, creating CloudWatch alarms is the most efficient way to achieve this goal.\n\n"
}, {
  "id" : 395,
  "question" : "You are working for a financial institute using AWS cloud infrastructure.\nAll project-related data is uploaded to Amazon EFS.\nThis data is retrieved from an on-premises data center connecting to VPC via AWS Direct Connect.\nYou need to ensure that all client access to EFS is encrypted using TLS 1.2 to adhere to the security team's latest security guidelines.\nWhich of the following is a cost-effective recommended practice for securing data in transit while accessing data from Amazon EFS?\n",
  "answers" : [ {
    "id" : "2e02bcf499584c838fcd0092f6295313",
    "option" : "Use EFS mount helper to encrypt data in transit.",
    "isCorrect" : "true"
  }, {
    "id" : "a502980ae2194cadaa971ee2e77a0a55",
    "option" : "Use stunnel to connect to Amazon EFS &amp; encrypt traffic in transit.",
    "isCorrect" : "false"
  }, {
    "id" : "e26cbe0a82f9458ba643c0dadefd0272",
    "option" : "Use third-party tool to encrypt data in transit.",
    "isCorrect" : "false"
  }, {
    "id" : "d5d01e91ed5a4b7f8159b7f40a1435fb",
    "option" : "Use NFS client to encrypt data in transit.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nWhile mounting Amazon EFS, if encryption of data in transit is enabled, the EFS Mount helper initializes the client Stunnel process to encrypt data in transit.\nEFS Mount helper uses TLS 1.2 to encrypts data in transit.\nOption B is incorrect as using Stunnel for encryption of data in transit will work fine, but additional admin work is needed to download &amp; install Stunnel for each mount.\nOption C is incorrect as using a third-party tool will be a costly option.\nOption D is incorrect as the NFS client can't be used to encrypt data in transit.\nThe amazon-efs-utils package can be used, which consists of an EFS mount helper.\nFor more information on encrypting of data in transit for EFS, refer to the following URL-\nhttps://docs.aws.amazon.com/efs/latest/ug/encryption-in-transit.html#encrypt-mount\n\nTo ensure that client access to Amazon EFS is encrypted using TLS 1.2, the recommended practice is to use the EFS mount helper to encrypt data in transit. This is a cost-effective solution because it is built into Amazon EFS and does not require the use of any third-party tools.\nThe EFS mount helper is a tool that simplifies the process of mounting Amazon EFS file systems on Amazon EC2 instances. It also enables encryption of data in transit using TLS 1.2. When using the EFS mount helper, all traffic between the EC2 instance and EFS is encrypted using TLS 1.2. This ensures that all data transmitted between the EC2 instance and EFS is protected from unauthorized access.\nUsing stunnel to connect to Amazon EFS and encrypt traffic in transit is not a recommended practice because it is a third-party tool that requires additional configuration and maintenance. It may also incur additional costs. Similarly, using a third-party tool to encrypt data in transit is not necessary because Amazon EFS provides built-in encryption.\nUsing NFS client to encrypt data in transit is also not a recommended practice because NFS is not a secure protocol and does not provide encryption. Although it is possible to use NFS over SSH or VPN, this is not a cost-effective solution and is more complex than using the EFS mount helper.\nIn conclusion, the recommended practice for securing data in transit while accessing data from Amazon EFS is to use the EFS mount helper to encrypt data in transit. This is a cost-effective solution that ensures data transmitted between the EC2 instance and EFS is protected from unauthorized access using TLS 1.2 encryption.\n\n"
}, {
  "id" : 396,
  "question" : "Your company has a set of resources defined in AWS.\nThese resources consist of applications hosted on EC2 Instances.\nData is stored on EBS volumes and S3\nThe company mandates that all data should be encrypted at rest.\nHow can you achieve this? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "49ed401b1b434e89a3e430b0eb78755b",
    "option" : "Enable SSL with the underlying EBS volumes.",
    "isCorrect" : "false"
  }, {
    "id" : "cf356dfd6f3d4c6693dd6af3ae304bdd",
    "option" : "Enable EBS Encryption.",
    "isCorrect" : "true"
  }, {
    "id" : "5616af335d3d48e4ad654414fb00d74a",
    "option" : "Make sure that data is transmitted from S3 via HTTPS.",
    "isCorrect" : "false"
  }, {
    "id" : "7e030917cbd34959b406471cf7fbebfb",
    "option" : "Enable S3 server-side Encryption.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe AWS Documentation mentions the following.\nAmazon EBS encryption offers a simple encryption solution for your EBS volumes without the need to build, maintain, and secure your own key management infrastructure.\nServer-side encryption protects data at rest.\nServer-side encryption with Amazon S3-managed encryption key (SSE-S3) uses strong multi-factor encryption.\nOptions A and C are incorrect since they have to do with encryption of data in transit, but not at rest.\nFor more information on EBS Encryption, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\nFor more information on S3 server-side encryption, please refer to the below link-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html\n\nTo achieve the requirement of encrypting data at rest for resources in AWS, two options are available:\nEnable EBS Encryption: Amazon Elastic Block Store (EBS) encryption provides encryption for EBS volumes. When the EBS volume is encrypted, the data stored on the EBS volume is also encrypted at rest. This means that even if an unauthorized person gains access to the EBS volume, they cannot read the data without the decryption key. EBS encryption uses AWS Key Management Service (KMS) to manage encryption keys. To enable EBS encryption, follow the below steps: Create a new AWS KMS key or use an existing one. Create a new encrypted EBS volume using this key or encrypt an existing EBS volume. Attach the encrypted EBS volume to the EC2 instance. Enable S3 server-side Encryption: Amazon S3 Server-Side Encryption provides encryption for data stored in S3. Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) provides automatic server-side encryption using the AES-256 encryption algorithm. When data is uploaded to S3, it is encrypted using a unique key and then decrypted when accessed. To enable S3 server-side encryption, follow the below steps: Enable encryption when uploading the data to S3 or configure an S3 bucket to enable default encryption for all objects. Specify SSE-S3 as the encryption type. Amazon S3 encrypts the object using encryption keys managed by AWS KMS.\nTherefore, the correct answers for achieving the requirement of encrypting data at rest are: Enable EBS Encryption and Enable S3 server-side Encryption.\n\n"
}, {
  "id" : 397,
  "question" : "Your company has a web application hosted in AWS that makes use of an Application Load Balancer.\nYou need to ensure that the web application is protected from web-based attacks such as cross-site scripting etc. Which of the following implementation steps can help protect web applications from common security threats from the outside world?\n",
  "answers" : [ {
    "id" : "aa85b2facfbd4c08a92bd6e29f9a8c65",
    "option" : "Place a NAT instance in front of the web application to protect against attacks.",
    "isCorrect" : "false"
  }, {
    "id" : "a630b577ced94574b7a7fbea1b815be5",
    "option" : "Use the WAF service in front of the web application.",
    "isCorrect" : "true"
  }, {
    "id" : "5d0fb4e03d3646948ae1ab1e2ec9a5c6",
    "option" : "Place a NAT gateway in front of the web application to protect against attacks.",
    "isCorrect" : "false"
  }, {
    "id" : "01352bb8fb594f93a2d8bc7cbaebdfde",
    "option" : "Place the web application in front of a CDN service instead.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAWS WAF is a web application firewall that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.\nAWS WAF gives you control over which traffic to allow or block your web applications by defining customizable web security rules.\nYou can use AWS WAF to create custom rules that block common attack patterns, such as SQL injection or cross-site scripting, and rules designed for your specific application.\nOptions A and C are incorrect because these are used to allow instances in your private subnet to communicate with the internet.\nOption D is incorrect since this is ideal for content distribution and good when you have DDoS attacks, but the WAF should be used for concentrated types of web attacks.\nFor more information on AWS WAF, please refer to the below link-\nhttps://aws.amazon.com/waf/\n\nThe correct answer is B. Use the WAF service in front of the web application.\nExplanation: The AWS WAF (Web Application Firewall) is a web application firewall that helps protect web applications from common web exploits that can affect the availability, compromise the security, or consume excessive resources of your web application.\nBy placing the WAF in front of your web application, it can monitor and filter the traffic coming to your web application, and block any malicious requests. The WAF supports a range of customizable rules to block common attack patterns such as SQL injection, cross-site scripting, and more.\nThe other options listed are not appropriate for protecting against web-based attacks: A. Placing a NAT instance or NAT gateway in front of the web application can help with network address translation but will not provide any protection against web-based attacks. D. A CDN service can help with delivering content to users more quickly, but it does not provide any security benefits.\nIn summary, using the AWS WAF in front of your web application is the best solution for protecting against common web-based attacks.\n\n"
}, {
  "id" : 398,
  "question" : "Your supervisor asks you to create a decoupled application whose process includes dependencies on EC2 instances.\nIn the application you would be using Polling Strategy to trigger messages once the defined criteria are fulfilled.\nWhich of the following would you include in the architecture?\n",
  "answers" : [ {
    "id" : "0d17967d8c25493eab74e4c72e247c31",
    "option" : "An SQS queue as the messaging component between the Instances and servers",
    "isCorrect" : "true"
  }, {
    "id" : "a5c9181542604197aa7cb0af208fe99b",
    "option" : "An SNS topic as the messaging component between the Instances and servers",
    "isCorrect" : "false"
  }, {
    "id" : "6f8f31fb2677460dbdc4c32a7208d318",
    "option" : "An Elastic Load balancer to distribute requests to your EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "48efe4f27d6b4746a6bb74e1bd9edfcc",
    "option" : "Route 53 resource records to route requests based on failure.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications.\nSQS eliminates the complexity and overhead associated with managing and operating message-oriented middleware and empowers developers to focus on differentiating work.\nUsing SQS, you can send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\nSQS is a distributed queuing system.\nMessages are NOT pushed to receivers.\nReceivers have to poll or pull messages from SQS.\nSNS is a distributed publish-subscribe system.\nMessages are pushed to subscribers as and when publishers send them to SNS.\nOption B is incorrect since this is a push-based notification service.\nOption C is incorrect since there is no mention in the question of adding any fault tolerance.\nOption D is incorrect since there is no mention in the question of adding any failure detection.\nFor more information on AWS SQS, please refer to the below link-\nhttps://aws.amazon.com/sqs/\n\nFor creating a decoupled application with dependencies on EC2 instances, and using Polling Strategy to trigger messages, the best option to include in the architecture would be an SQS queue as the messaging component between the Instances and servers.\nHere's a detailed explanation of why this is the correct option:\nSQS queue: Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables decoupling and scaling of microservices, distributed systems, and serverless applications. It allows you to decouple and scale the components of a cloud application by reliably and asynchronously transferring data between them. Polling Strategy: Polling is a method of repeatedly checking for changes or updates in a system. Polling Strategy is used to trigger messages once the defined criteria are fulfilled. For example, you might poll an SQS queue to check if there are any messages waiting to be processed. Dependencies on EC2 instances: Amazon Elastic Compute Cloud (EC2) is a web service that provides resizable compute capacity in the cloud. EC2 instances can be used to run applications or services that are part of a larger distributed system. These instances can be configured to send messages to an SQS queue, which can then be polled by other components in the system. Messaging component: In a distributed system, components communicate with each other by sending messages. An SQS queue can be used as the messaging component between EC2 instances and servers. EC2 instances can send messages to the queue, which can then be processed by other components in the system. SNS topic: Amazon Simple Notification Service (SNS) is a fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. However, SNS is not the best option for a decoupled application with dependencies on EC2 instances because it is primarily designed for push-based, event-driven architectures. Elastic Load balancer: An Elastic Load Balancer (ELB) is a service that distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. ELB is not the best option for a decoupled application with dependencies on EC2 instances because it does not provide message queuing or polling capabilities. Route 53 resource records: Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. It can be used to route traffic to various resources, such as EC2 instances, based on DNS records. However, Route 53 is not the best option for a decoupled application with dependencies on EC2 instances because it does not provide message queuing or polling capabilities.\nIn summary, an SQS queue is the best option for a decoupled application with dependencies on EC2 instances and using Polling Strategy to trigger messages. It provides reliable and asynchronous transfer of messages between components in the system, which helps to decouple and scale the application.\n\n"
}, {
  "id" : 399,
  "question" : "Your company has a set of VPC's.\nThere is now a requirement to establish communication across the Instances in the VPC's.\nYour supervisor has asked you to implement the VPC peering connection.\nWhich of the following considerations would you keep in mind for VPC peering? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "242676c0766840cca895bf2ccb0a08fc",
    "option" : "Ensuring that the VPCs don`t have overlapping CIDR blocks.",
    "isCorrect" : "false"
  }, {
    "id" : "fe7a2a33cefc47669fcc66582aa9c922",
    "option" : "The transitive peering is not supported.",
    "isCorrect" : "false"
  }, {
    "id" : "6211ff88c2144d568bc8a3f010c37ced",
    "option" : "Ensuring that the VPCs only have public subnets for communication.",
    "isCorrect" : "false"
  }, {
    "id" : "39fe3bb197be448893b0dc843548f97f",
    "option" : "Ensuring that the VPCs are created in the same region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nSure, I'd be happy to help!\nVPC peering is a feature that allows you to connect two VPCs together so that instances in one VPC can communicate directly with instances in the other VPC. This can be useful in a variety of situations, such as when you want to set up a hub-and-spoke network topology or when you want to share resources between VPCs.\nNow let's take a look at the considerations for VPC peering:\nA. Ensuring that the VPCs don't have overlapping CIDR blocks: This is a very important consideration for VPC peering. CIDR (Classless Inter-Domain Routing) blocks are used to assign IP addresses to resources in your VPC. If the CIDR blocks of the two VPCs overlap, it can cause IP address conflicts and prevent the VPC peering from functioning correctly. Therefore, it's important to ensure that the CIDR blocks of the VPCs don't overlap before creating a VPC peering connection.\nB. The transitive peering is not supported: This is another important consideration for VPC peering. Transitive peering is a scenario where VPC A is peered with VPC B and VPC B is peered with VPC C, but VPC A is not directly peered with VPC C. In other words, VPC peering does not support transitive relationships. If you need to connect multiple VPCs, you'll need to create a separate VPC peering connection between each pair of VPCs that need to communicate.\nC. Ensuring that the VPCs only have public subnets for communication: This is not a requirement for VPC peering. VPC peering allows communication between instances in private subnets as well as public subnets. However, it's important to note that communication between private subnets across VPC peering connections requires additional configuration, such as setting up route tables and network address translation (NAT).\nD. Ensuring that the VPCs are created in the same region: This is a requirement for VPC peering. VPC peering connections can only be created between VPCs in the same region. If you need to connect VPCs in different regions, you'll need to use a different solution, such as VPN or AWS Direct Connect.\nSo, the correct answers for this question are A and B. You need to ensure that the VPCs don't have overlapping CIDR blocks and that transitive peering is not supported.\n\n"
}, {
  "id" : 400,
  "question" : "You have been instructed to establish a successful site-to-site VPN connection from your on-premises network to the VPC (Virtual Private Cloud)\nAs an architect, which of the following pre-requisites should you ensure to establish the site-to-site VPN connection? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "b7e3271bea8c4817add6f5d80b7a5d38",
    "option" : "The main route table to route traffic through a NAT instance",
    "isCorrect" : "false"
  }, {
    "id" : "9a9431bc8b01415c9d75914afddd5ec6",
    "option" : "A public IP address on the customer gateway for the on-premises network",
    "isCorrect" : "true"
  }, {
    "id" : "48febd489d734c7dbec93ed0dc71c47a",
    "option" : "A virtual private gateway attached to the VPC",
    "isCorrect" : "true"
  }, {
    "id" : "cdc0e4af3d0649d090a7cb3ef9094e0b",
    "option" : "An Elastic IP address to the Virtual Private Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThis is mentioned in the AWS Documentation.\nOption A is incorrect since the NAT instance is not required to route traffic via the VPN connection.\nOption D is incorrect since the Virtual Private Gateway is managed by AWS.\nFor more information on VPN connections, please refer to the below link-\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html\n\n\nTo establish a successful site-to-site VPN connection from your on-premises network to the VPC (Virtual Private Cloud), the following prerequisites should be ensured:\nA virtual private gateway attached to the VPC: A virtual private gateway is the VPN concentrator on the AWS side of the VPN connection. It enables communication between your VPC and your on-premises network through the VPN connection. You need to attach a virtual private gateway to your VPC before setting up a site-to-site VPN connection. A public IP address on the customer gateway for the on-premises network: Your on-premises network should have a public IP address on its customer gateway. This public IP address is used to identify your on-premises network in the VPN connection. Without a public IP address, your on-premises network cannot establish a VPN connection with your VPC.\nIn addition to the above, you also need to have the following configurations in place:\nAn Internet Gateway (IGW) attached to the VPC: An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in your VPC and the internet. An IGW must be attached to the VPC before setting up a VPN connection. A security group that allows inbound and outbound traffic: You need to configure a security group to allow inbound and outbound traffic to and from the VPC. The security group should allow traffic from the on-premises network to the VPC and vice versa.\nTherefore, options C and B are the correct prerequisites to establish a site-to-site VPN connection from your on-premises network to the VPC. Option A is incorrect because routing traffic through a NAT instance is not a requirement for establishing a VPN connection. Option D is incorrect because you do not need an Elastic IP address to the Virtual Private Gateway to establish a VPN connection. However, you might use an Elastic IP address to the Virtual Private Gateway for specific use cases.\n\n"
}, {
  "id" : 401,
  "question" : "Your company wants to enable encryption of services such as S3 and EBS volumes so that the data it maintains is encrypted at rest.\nThey want to have complete control over the keys and the entire lifecycle around the keys.\nMorever, the company needs to create and manage the hardware security modules that store the encryption keys.\nHow can you accomplish this?\n",
  "answers" : [ {
    "id" : "9b4711ed03224f7fa2eb47bcadf24cdc",
    "option" : "Use the AWS CloudHSM",
    "isCorrect" : "true"
  }, {
    "id" : "bec14b8aea3c4facbae37c4af33dc32b",
    "option" : "Use the KMS service",
    "isCorrect" : "false"
  }, {
    "id" : "5695aa7a2f7a4a1aad1c686a4fa390b3",
    "option" : "Enable S3 server-side encryption",
    "isCorrect" : "false"
  }, {
    "id" : "391a2c32e2d24affaa9773b8f8e33643",
    "option" : "Enable EBS Encryption with the default KMS keys.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThis is mentioned in the AWS Documentation.\nAWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily generate and use your own encryption keys on the AWS Cloud.\nWith CloudHSM, you can manage your own encryption keys using FIPS 140-2 Level 3 validated HSMs.\nKMS is a shared hardware tenancy - your keys are in their own partition of an encryption module shared with other AWS customers, each with their own isolated partition.\nCloud HSM gives you your own hardware module, so the most likely reason to choose Cloud HSM is if you had to ensure your keys were isolated on their own encryption module.\nOptions B, C, and D are incorrect since they have shared hardware tenancy.\nFor more information on cloud HSM and Encryption Tools, please refer to the below URL.\nhttps://aws.amazon.com/cloudhsm/\nhttps://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-toplevel.html\n\nTo accomplish this requirement, option A (Use the AWS CloudHSM) is the best solution. AWS CloudHSM is a cloud-based hardware security module that allows you to create and use your own encryption keys. It gives you complete control over your keys and the entire lifecycle around them.\nCloudHSM provides secure key storage and management using dedicated hardware security modules (HSMs) within the AWS infrastructure. It is designed to meet strict security and compliance requirements, including FIPS 140-2 Level 3 and Common Criteria EAL4+. CloudHSM also integrates with other AWS services, including S3 and EBS, to provide encryption at rest.\nOption B (Use the KMS service) is not the best solution because although KMS allows you to create and manage your own keys, it does not provide the same level of control over the key lifecycle as CloudHSM. Additionally, KMS keys are stored within the AWS infrastructure, which may not meet the company's requirements.\nOption C (Enable S3 server-side encryption) and option D (Enable EBS Encryption with the default KMS keys) do not meet the requirement of having complete control over the keys and the entire lifecycle around them. With both of these options, AWS manages the encryption keys, which may not meet the company's requirements.\nIn summary, to meet the company's requirements of having complete control over the keys and the entire lifecycle around them, while providing encryption at rest for services like S3 and EBS volumes, the best solution is to use the AWS CloudHSM.\n\n"
}, {
  "id" : 402,
  "question" : "A company wants to implement a data store in AWS.\nThe data store needs to have the following requirements 1) Completely managed by AWS 2) Ability to store JSON objects efficiently 3) Scale based on demand Which of the following would you use as the data store?\n",
  "answers" : [ {
    "id" : "fc6dd337f9404ac98ef9b453ead0f5c0",
    "option" : "AWS Redshift",
    "isCorrect" : "false"
  }, {
    "id" : "dd9cf834ca6349f898785cfa9b5d712e",
    "option" : "AWS DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "624ce44a9cbe4fd798019a2f104309ec",
    "option" : "AWS Aurora",
    "isCorrect" : "false"
  }, {
    "id" : "292b9a44d674478fa8f5ff092a31d5d4",
    "option" : "AWS Glacier.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAmazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.\nDynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don't have to worry about hardware provisioning, setup, and configuration, replication, software patching, or cluster scaling.\nIt is ideal for storing JSON based objects.\nWith DynamoDB On-Demand, capacity planning is a thing of the past.\nYou don't specify read and write capacity at all-you pay only for the usage of your DynamoDB tables.\nI pay more when I have more usage, which means I'm delivering more value to my customers.\nPreviously, you had to set read and write throughput capacity on your DynamoDB tables.\nThis specified how many and how large of reads and writes you could make on your table in any given second.\nRead and write capacity units were charged by the hour, and your requests would be throttled if you exceeded your provisioned capacity in any given second.\nOption A is incorrect since this is normally used to host a data warehousing solution.\nOption C is incorrect since this doesn't provide scaling on-demand.\nOption D is incorrect since this is used for archive storage.\nFor more information on DynamoDB, please refer to the below link.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\nhttps://aws.amazon.com/blogs/aws/amazon-dynamodb-on-demand-no-capacity-planning-and-pay-per-request-pricing/\n\nThe correct answer is B. AWS DynamoDB.\nHere's a breakdown of each option and why they do or don't meet the requirements:\nA. AWS Redshift: AWS Redshift is a data warehousing service that is designed for analytical workloads, not transactional workloads. It is not a completely managed service, as the customer is responsible for managing backups, snapshots, and security. Redshift is also not well suited for storing JSON objects efficiently.\nB. AWS DynamoDB: DynamoDB is a NoSQL database that is designed for transactional workloads. It is a completely managed service and can store JSON objects efficiently. DynamoDB can scale based on demand and can handle very large datasets.\nC. AWS Aurora: Aurora is a relational database that is designed for transactional workloads. It is a completely managed service and can scale based on demand. However, it may not be the best choice for storing JSON objects efficiently.\nD. AWS Glacier: Glacier is a storage service that is designed for data archiving and long-term backup. It is not a completely managed service, as the customer is responsible for managing backups, snapshots, and security. It is also not well suited for storing JSON objects efficiently.\nIn conclusion, based on the requirements mentioned, AWS DynamoDB would be the most suitable option for the data store as it meets all the requirements mentioned.\n\n"
}, {
  "id" : 403,
  "question" : "A company has set up some EC2 Instances in a VPC with the default Security group and NACL settings.\nThey want to ensure that the IT admin staff can connect to the EC2 Instance via SSH.\nAs an architect, what would you ask the IT admin team to do to ensure that they can connect to the EC2 Instance from the Internet? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "6b783e4f1e15441cbf65cde2b2abfcbc",
    "option" : "Ensure that the Instance has a Public or Elastic IP.",
    "isCorrect" : "true"
  }, {
    "id" : "0d6dc32ed165466f8b8530e899d90ac1",
    "option" : "Ensure that the Instance has a Private IP.",
    "isCorrect" : "false"
  }, {
    "id" : "60af425179ab4ddfa92a42a151dde0be",
    "option" : "Ensure to modify the Security groups.",
    "isCorrect" : "true"
  }, {
    "id" : "6b17af81c7b34e00bd45e78c73dd9e32",
    "option" : "Ensure to modify the NACL rules.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS Documentation mentions the following.\nTo enable access to or from the internet for instances in a VPC subnet, you must do the following.\nAttach an Internet gateway to your VPC.Ensure that your subnet's route table points to the internet gateway.\nEnsure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).\nEnsure to add an inbound rule to allow traffic from SSH with source 0.0.0.0/0\nBy default, all outbound traffic is allowed.\nOption B is incorrect since the Private IP will always be created and would not connect from the internet.\nIf you use the private IP to communicate, traffic will stay within the VPC.\nIt will not be routed out.\nThe routing table will route it internally.\nIf you use the public IP to communicate, traffic will go out to the internet (through the internet gateway) and return to your VPC.Instances receive a Public IP address so that it is reachable from outside the VPC.\nThis IP address might change if the instance stops and starts.\nAlternatively, you can use an Elastic IP Address that remains static.\nOption D is incorrect since the default NACL rules will allow all traffic.\nFor more information on exposing VPC resources to the Internet, please refer to the below link-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Internet_Gateway.html\n\nTo enable the IT admin staff to connect to the EC2 instance via SSH from the internet, the following steps should be taken:\nEnsure that the Instance has a Public or Elastic IP: If the instance is located in a private subnet, it will not have a public IP address, which means that it cannot be accessed from the internet directly. In this case, an Elastic IP can be assigned to the instance, which will provide a static, public IP address that can be used to connect to the instance. Modify the Security Group settings: By default, the security group associated with the EC2 instance will not allow inbound traffic to port 22 (used for SSH). Therefore, the IT admin staff must add a new inbound rule to the security group to allow traffic from their IP address range to port 22. This will ensure that the SSH traffic can reach the instance from their location. Ensure to modify the NACL rules: By default, the Network Access Control Lists (NACLs) associated with the VPC will allow all inbound and outbound traffic. However, if the NACLs have been modified to block any inbound or outbound traffic, then the IT admin staff may not be able to connect to the instance via SSH. In this case, the NACLs need to be modified to allow inbound SSH traffic from the IT admin staff's IP address range.\nTherefore, the correct answers are A and C - Ensure that the Instance has a Public or Elastic IP and Ensure to modify the Security groups.\n\n"
}, {
  "id" : 404,
  "question" : "Your company has a set of EBS volumes and a set of adjoining EBS snapshots.\nThey want to minimize the costs for the underlying EBS snapshots.\nWhich of the following approaches provides the lowest cost for Amazon Elastic Block Store snapshots while giving you the ability to restore data fully?\n",
  "answers" : [ {
    "id" : "20e836fc47af4665805e5cfd640e3dcc",
    "option" : "Maintain two snapshots: the original snapshot and the latest incremental snapshot.",
    "isCorrect" : "false"
  }, {
    "id" : "2fe80e53542b4cb48a97bf6dce961e00",
    "option" : "Maintain a volume snapshot; subsequent snapshots will overwrite one another",
    "isCorrect" : "false"
  }, {
    "id" : "2031b27358274140b7f1ac27d6d6b6c4",
    "option" : "Maintain a single snapshot: the latest snapshot is both Incremental and complete.",
    "isCorrect" : "true"
  }, {
    "id" : "a11152fe8a2e426bb545355e69308ced",
    "option" : "Maintain the most current snapshot, archive the original and incremental to Amazon Glacier.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nYou can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.\nSnapshots are incremental backups, which means that only the blocks on the device have changed after your most recent snapshot is saved.\nThis minimizes the time required to create the snapshot and saves on storage costs by not duplicating data.\nWhen you delete a snapshot, only the data unique to that snapshot is removed.\nEach snapshot contains all of the information needed to restore your data (from the moment when the snapshot was taken) to a new EBS volume.\nFor more information on EBS Snapshots, please refer to the below link-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\n\nWhen it comes to managing EBS snapshots, the goal is to strike a balance between cost and data integrity. AWS provides several approaches to accomplish this, each with its own tradeoffs. Here's a breakdown of the options provided in the question:\nA. Maintain two snapshots: the original snapshot and the latest incremental snapshot. This approach involves creating an initial snapshot of the volume and then only taking incremental snapshots of the changes since the last snapshot. However, this approach requires the original snapshot to be retained, which can increase costs. In addition, multiple snapshots may need to be restored to fully recover the volume, which can be time-consuming.\nB. Maintain a volume snapshot; subsequent snapshots will overwrite one another This approach involves taking snapshots of the volume at regular intervals, with each snapshot overwriting the previous one. While this is a straightforward and inexpensive approach, it lacks the ability to restore data from a specific point in time. It also leaves the volume vulnerable to data corruption or loss if an issue arises between snapshot intervals.\nC. Maintain a single snapshot: the latest snapshot is both Incremental and complete. This approach involves taking a snapshot of the volume at regular intervals, but each snapshot is a complete snapshot of the volume rather than an incremental one. This ensures that data can be restored from any point in time, but can be more costly and time-consuming to manage than incremental snapshots.\nD. Maintain the most current snapshot, archive the original and incremental to Amazon Glacier. This approach involves taking regular snapshots of the volume and retaining only the most current snapshot on EBS, while archiving the original and incremental snapshots to Amazon Glacier for long-term storage. While this approach can significantly reduce EBS costs, it can be complex to manage, and restoring data can take longer due to the need to retrieve data from Glacier.\nOverall, each approach has its own advantages and tradeoffs, so it's important to consider the specific requirements and constraints of your use case when selecting an EBS snapshot management strategy.\n\n"
}, {
  "id" : 405,
  "question" : "You are using a c5.large EC2 Instance with one 300GB EBS General purpose SSD volume to host a relational database.\nYou noticed that the read/write capacity of the database needs to be increased.\nWhich of the following approaches can help achieve this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "c08c98bf07164f36831696578210cdfa",
    "option" : "Use a larger EC2 Instance Type.",
    "isCorrect" : "true"
  }, {
    "id" : "184cb62013d74d298c92559762a30a69",
    "option" : "Enable Multi-AZ feature for the database.",
    "isCorrect" : "false"
  }, {
    "id" : "133b5b2ac32c47feb3f2d119758bcb6d",
    "option" : "Consider using Provisioned IOPS Volumes.",
    "isCorrect" : "true"
  }, {
    "id" : "305a2ee1eb0d4f20b6ce5ec4c8f3d613",
    "option" : "Put the database behind an Elastic Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe below snapshot from the AWS Documentation shows the different volume types and why Provisioned IOPS is the most ideal for this requirement.\nAlso, consider using a larger instance size for better processing capabilities based on EBS Bandwidth.\nOption B is incorrect since the Multi-AZ feature is only for high availability.\nOption D is incorrect since this would not alleviate the high number of read/write of the database.\nFor more details on EBS Volume Types and EC2 Instance Types, please refer below URLs-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html#EBSVolumeTypes_piops\nhttps://aws.amazon.com/ec2/instance-types/\n\n\nSure, I can explain each option and how it can help increase the read/write capacity of the database:\nA. Use a larger EC2 Instance Type: Upgrading the instance type to a larger one with more compute and memory resources can help increase the read/write capacity of the database. With a larger instance, you will have access to more CPU and memory resources, which can be beneficial for handling larger workloads.\nB. Enable Multi-AZ feature for the database: The Multi-AZ feature provides enhanced availability and data durability for your database by replicating data to a standby instance in a different Availability Zone (AZ). By enabling this feature, you can offload some of the read traffic to the standby instance, thereby increasing the overall read capacity of the database.\nC. Consider using Provisioned IOPS Volumes: Provisioned IOPS volumes are designed for high-performance workloads, such as database applications. They offer predictable and consistent I/O performance, which can help increase the read/write capacity of the database. By using Provisioned IOPS volumes, you can provision the amount of IOPS you need for your workload, ensuring that you have the necessary resources to handle high-volume workloads.\nD. Put the database behind an Elastic Load Balancer: Putting the database behind an Elastic Load Balancer can help distribute the read/write workload across multiple instances, thereby increasing the overall read/write capacity of the database. The load balancer can help direct traffic to the instances based on the current load and capacity of each instance.\nIn conclusion, the two options that can help achieve an increase in the read/write capacity of the database are:\nUse a larger EC2 Instance Type Consider using Provisioned IOPS Volumes\n\n"
}, {
  "id" : 406,
  "question" : "Your company has a set of AWS RDS Instances.\nYour management has asked you to disable Automated backups to save on cost.\nWhen you disable automated backups for AWS RDS, what are you compromising on?\n",
  "answers" : [ {
    "id" : "7fa9f26c050a47009cbcecd827852c3d",
    "option" : "Nothing,you are actually saving resources on aws",
    "isCorrect" : "false"
  }, {
    "id" : "c315e3a01e364cd8af08aacc3a9f8dab",
    "option" : "You are disabling the point-in-time recovery.",
    "isCorrect" : "true"
  }, {
    "id" : "16b1cabfa1794076969c118858f66045",
    "option" : "Nothing really, you can still take manual backups.",
    "isCorrect" : "false"
  }, {
    "id" : "5490f1782b584d228f26f6f29f38ca68",
    "option" : "You cannot disable automated backups in RDS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAmazon RDS creates a storage volume snapshot of your DB instance, backing up the entire DB instance, not just individual databases.\nYou can set the backup retention period when you create a DB instance.\nIf you don't set the backup retention period, Amazon RDS uses a default period retention period of one day.\nYou can modify the backup retention period; valid values are 0 (for no backup retention) to a maximum of 35 days.\nAutomatic Backups are taken daily when we specify the point in time recovery feature that enables the recovery of the database at any point in time.\nAWS applies the transaction logs to the most appropriate DB backup.\nAt the same time, DB snapshots are a manual thing where we user manually triggers the backup and then restores it from the desired time period.\nYou will also specifically see AWS mentioning the risk of not allowing automated backups.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.Disabling\nImportant.\nWe highly discourage disabling automated backups because it disables point-in-time recovery.\nIf you disable and then re-enable automated backups, you can only restore starting from the time you re-enabled automated backups.\nManual snapshots are user-initiated backups of your instance stored in Amazon S3 that are kept until you explicitly delete them.\nYou can create a new instance from a database snapshot whenever you desire.\nAlthough database snapshots serve operationally as full backups, you are billed only for incremental storage use.\nBecause of the risk which is clearly mentioned in the AWS Documentation, all other options are incorrect.\nFor more information on Automated backups, please visit.\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html\n\nWhen you disable automated backups for AWS RDS, you are compromising on the ability to perform point-in-time recovery.\nPoint-in-time recovery allows you to restore your database to a specific point in time within a specified retention period. This is a useful feature in case of accidental data deletion or database corruption. Automated backups are the basis of point-in-time recovery, and if you disable automated backups, you lose the ability to restore your database to a specific point in time.\nWhile you can still take manual backups, they do not provide the same level of granularity as automated backups. Manual backups can only be taken at specific points in time and are not automatically scheduled, so you may miss critical recovery points if you rely solely on manual backups.\nAdditionally, disabling automated backups for RDS does save some AWS resources and reduce costs, but it also increases the risk of data loss and downtime if you do not have a proper backup and recovery plan in place. Therefore, it's important to carefully evaluate the cost-benefit of disabling automated backups and ensure that you have a robust backup and recovery strategy in place.\n\n"
}, {
  "id" : 407,
  "question" : "A company is planning on setting up a web-based application.\nThey need to ensure that users across the world have the ability to view the pages from the web site with the least amount of latency.\nHow can you accomplish this?\n",
  "answers" : [ {
    "id" : "d13ae4e0326f40189f73f333bc27d156",
    "option" : "Use Route 53 with latency-based routing",
    "isCorrect" : "false"
  }, {
    "id" : "1f68df0ad05e4817bf0f5f0e5ea95d47",
    "option" : "Place a cloudfront distribution in front of the web application",
    "isCorrect" : "true"
  }, {
    "id" : "2c98116e96ac4c318e0ae15990d17dbf",
    "option" : "Place an Elastic Load balancer in front of the web application",
    "isCorrect" : "false"
  }, {
    "id" : "55bdfc961f9f4908887e93192acad8b3",
    "option" : "Place an Elastic Cache in front of the web application.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon CloudFront is a global content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to your viewers with low latency and high transfer speeds.\nCloudFront is integrated with AWS - including physical locations that are directly connected to the AWS global infrastructure, as well as software that works seamlessly with services including AWS Shield for DDoS mitigation, Amazon S3, Elastic Load Balancing or Amazon EC2 as origins for your applications, and Lambda@Edge to run custom code close to your viewers.\nCloudFront vs Route 53\nCloudFront will distribute your content over 100+ edge location which will decrease your response time with low latency and save your cost as well.\nIt will deliver the content from the nearest location.\nCloudFront: It is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users.\nThe content is cached at the edge location (data center)\nIn CloudFront, you specify the distribution from where the content needs to be served.\nRoute53 is a DNS service and is an origin of data.\nThe term Origin is a term for where the original data resides before it is cached in the CDN (CloudFront)\nIt redirects the original content rather than caching.\nOption A can be correct but the least amount of latency will not be there.\nOption C is incorrect since this is used for fault tolerance for the web application.\nOption D is incorrect since this is used for caching requests in front of a database layer.\nFor more information on AWS CloudFront, please visit.\nhttps://aws.amazon.com/cloudfront/\n\nTo ensure that users across the world have the ability to view the pages from the web site with the least amount of latency, we can follow these steps:\nUse Route 53 with latency-based routing: Route 53 is a DNS service provided by AWS. With latency-based routing, Route 53 automatically routes traffic to the region that provides the lowest latency for the user. This helps in reducing the latency for the users and improves their experience. Latency-based routing uses Amazon's latency data to determine the best endpoint for the user. Therefore, this option is a good choice to minimize the latency for users across the world. Place a CloudFront distribution in front of the web application: CloudFront is a content delivery network (CDN) provided by AWS. It caches content in edge locations located across the world. When a user requests a page, CloudFront serves it from the nearest edge location, reducing the latency. This option is useful when the web application has a large number of static resources like images, videos, etc., which can be cached by CloudFront. Place an Elastic Load Balancer in front of the web application: Elastic Load Balancer (ELB) is a load balancer service provided by AWS. It distributes incoming traffic across multiple EC2 instances. By distributing the load, ELB reduces the latency and ensures high availability of the application. This option is useful when the web application has multiple EC2 instances serving the requests. Place an Elastic Cache in front of the web application: Elastic Cache is an in-memory caching service provided by AWS. It caches frequently accessed data in memory, reducing the number of requests to the database. By reducing the database load, Elastic Cache reduces the latency for users. This option is useful when the web application is database-intensive and has a high read rate.\nIn summary, the best option to minimize the latency for users across the world is to use Route 53 with latency-based routing. CloudFront can be used to cache static resources, ELB can be used to distribute the load across multiple EC2 instances, and Elastic Cache can be used to cache frequently accessed data.\n\n"
}, {
  "id" : 408,
  "question" : "A company wants to design a new serverless website with customers from many countries.\nFor a better user experience, customers from different countries should get customized contents from the website. How would you design this solution cost-effectively?\n",
  "answers" : [ {
    "id" : "c81c4a0bc1c04be3b1ca384b6f8c0606",
    "option" : "Use Amazon Route 53 geo-based routing to route traffic to the AWS Region and generate a geography-specific URL.",
    "isCorrect" : "false"
  }, {
    "id" : "a7c861796197438292cfa116651c5537",
    "option" : "Use a Network load balancer to route traffic to the AWS Region and replicate setup in each AWS region.",
    "isCorrect" : "false"
  }, {
    "id" : "a7128934bf9a4ff5bc4ae0f12ff20e66",
    "option" : "Use a Lambda function to identify the country and use an RDS database to return a customized URL for the customer.",
    "isCorrect" : "false"
  }, {
    "id" : "c8f2bc94799742a4817abaa7625c0ce2",
    "option" : "Use Lambda@Edge to customize content based on geography that CloudFront delivers and generate geography-specific URL.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer : D.\nOption A is incorrect as replicating the setup in each AWS Region would increase Operational overhead and cost.\nOption B is incorrect network load balancer to route traffic is a feasible solution since NLB are regional and can not be used to route traffic across AWS regions.\nOption C is incorrect because this is not the most cost-effective solution and RDS is not severless.\nOption D CORRECT.\nLambda@Edge function does provide the capability to customize content.\nLambda@Edge allows users to run their own Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS Regions closer to the viewer.\nLambda functions run in response to CloudFront events, without provisioning or managing servers.\nSo, it meets Serverless requirements.\nLambda@Edge can change CloudFront requests and responses like the above diagram.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-redirecting-examples\n\n\nTo design a serverless website that can provide customized content for users from different countries, we need to consider a few factors such as latency, scalability, and cost-effectiveness.\nOption A: Use Amazon Route 53 geo-based routing to route traffic to the AWS Region and generate a geography-specific URL.\nAmazon Route 53 is a highly available and scalable DNS service provided by AWS. With geo-based routing, we can route traffic to different AWS regions based on the location of the user. This ensures that users are served from the nearest AWS region, reducing latency and improving performance.\nIn addition, we can generate a geography-specific URL using Route 53, so that users can access the website using a URL that is customized for their country. This option is cost-effective as we don't need to replicate the setup in each region, and we only pay for the resources used in the regions where the website is accessed.\nOption B: Use a Network load balancer to route traffic to the AWS Region and replicate setup in each AWS region.\nThis option involves setting up a network load balancer in each AWS region where the website will be accessed. The load balancer routes traffic to the nearest AWS region, improving performance and reducing latency.\nHowever, replicating the setup in each region can be expensive, as we need to maintain and pay for resources in each region. This option may not be cost-effective for a small-scale website with limited traffic.\nOption C: Use a Lambda function to identify the country and use an RDS database to return a customized URL for the customer.\nIn this option, we use a Lambda function to identify the location of the user and retrieve a customized URL from an RDS database. The Lambda function can be triggered by an API Gateway, which receives the user's request.\nThis option is cost-effective as we only pay for the resources used by the Lambda function and the RDS database. However, it may not be as scalable as the other options, as the Lambda function may experience latency when accessing the RDS database.\nOption D: Use Lambda@Edge to customize content based on geography that CloudFront delivers and generate geography-specific URL.\nLambda@Edge is a service provided by AWS that allows us to run Lambda functions at the edge locations of the CloudFront CDN. This option involves using Lambda@Edge to customize the content delivered by CloudFront based on the user's location.\nWe can also generate a geography-specific URL using Lambda@Edge, so that users can access the website using a URL that is customized for their country.\nThis option is cost-effective as we only pay for the resources used by the Lambda function at the edge locations. It is also highly scalable, as the content customization and URL generation happen at the edge locations, reducing latency.\nOverall, the best option for a cost-effective serverless website that provides customized content for users from different countries is Option A, which uses Amazon Route 53 geo-based routing to route traffic to the nearest AWS region and generate a geography-specific URL.\n\n"
}, {
  "id" : 409,
  "question" : "A company is hosting their website on a cluster of web servers behind a public-facing load balancer.\nThe web application interacts with an AWS RDS database.\nIt has been noticed that a set of similar types of queries is causing a performance bottleneck at the database layer.\nWhich of the following architecture additions can help alleviate this issue?\n",
  "answers" : [ {
    "id" : "3444b765a3d7476b9a515a08efe4235e",
    "option" : "Deploy ElastiCache in front of the web servers.",
    "isCorrect" : "false"
  }, {
    "id" : "86683faa383b4823829d1e35f858164a",
    "option" : "Deploy ElastiCache in front of the database servers.",
    "isCorrect" : "true"
  }, {
    "id" : "c3b4217e18dd49638ff929c5536d63da",
    "option" : "Deploy Elastic Load balancer in front of the web servers.",
    "isCorrect" : "false"
  }, {
    "id" : "3047a69b1d674b76a22dce2fd480a031",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon ElastiCache offers fully managed Redis and Memcached.\nSeamlessly deploy, operate, and scale popular open source compatible in-memory data stores.\nBuild data-intensive apps or improve your existing apps' performance by retrieving data from high throughput and low latency in-memory data stores.\nOption A is incorrect since the database is having issues.\nHence you need to ensure that ElastiCache is placed in front of the database servers.\nOption C is incorrect since there is an issue with the database servers.\nSo, we don't need to add anything for the web servers.\nOption D is incorrect since this is used for the high availability of the database.\nFor more information on ElastiCache, please visit the URL-\nhttps://aws.amazon.com/elasticache/\n\nThe best solution to alleviate performance bottlenecks at the database layer is to add caching to the web application. When caching is added, the web application will not need to perform repeated queries to the database, thus reducing the load on the database and improving the overall performance of the application.\nOption A - Deploy ElastiCache in front of the web servers: This option is incorrect as ElastiCache is a caching service that can be used to cache frequently accessed data to improve application performance. However, deploying it in front of the web servers will not help with the database query performance bottleneck.\nOption B - Deploy ElastiCache in front of the database servers: This option is closer to the correct answer. Deploying ElastiCache in front of the database servers can help alleviate the performance bottleneck by caching frequently accessed data, thus reducing the load on the database servers. This can improve the response time and scalability of the application.\nOption C - Deploy Elastic Load Balancer in front of the web servers: This option is not directly related to the database query performance bottleneck. Elastic Load Balancer is a service that helps distribute incoming traffic to multiple web servers, increasing availability and scalability. While it can help with performance, it does not directly address the database query performance issue.\nOption D - Enable Multi-AZ for the database: This option is also incorrect, as Multi-AZ is a feature that provides high availability for the database by creating a secondary replica in a different availability zone. It does not directly address the performance bottleneck at the database query level.\nTherefore, the correct answer to alleviate the performance bottleneck at the database layer is to deploy ElastiCache in front of the database servers (Option B).\n\n"
}, {
  "id" : 410,
  "question" : "A company is hosting its website on a cluster of web servers behind a public-facing load balancer.\nThe web application has interfaces with an AWS RDS database.\nThe management has specified that the database needs to be available in case of a hardware failure on the primary database.\nThe secondary needs to be made available in the least amount of time.\nWhich of the following would you opt for?\n",
  "answers" : [ {
    "id" : "ccbd5327c66d46ad996ca14bf002eae8",
    "option" : "Made a snapshot of the database",
    "isCorrect" : "false"
  }, {
    "id" : "7c77aab26390418b928d4a5197526970",
    "option" : "Enabled Multi-AZ failover",
    "isCorrect" : "true"
  }, {
    "id" : "d4c0ae81895f45c880882239a70c10a2",
    "option" : "Increased the database instance size",
    "isCorrect" : "false"
  }, {
    "id" : "ff665e778bc848be850c71d520738dd5",
    "option" : "Created a read replica.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads.\nWhen you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ)\nEach AZ runs on its own physically distinct, independent infrastructure, and is engineered to be highly reliable.\nIn case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora)\nThus you can resume database operations as soon as the failover is complete.\nOptions A and D are incorrect since even though they can be used to recover a database, it would just take more time than just enabling Multi-AZ.\nOption C is incorrect since this will not help the cause.\nFor more information on Multi-AZ, please visit the URL-\nhttps://aws.amazon.com/rds/details/multi-az/\n\nFor high availability and fault tolerance, Amazon RDS offers multiple options. In this scenario, the requirement is to have a secondary database available in the least amount of time in case of hardware failure on the primary database.\nOption A: Taking a snapshot of the database is a backup method that can be used to restore data, but it will not help in achieving high availability or reducing the time required to recover from a failure. So this option is not suitable for this scenario.\nOption B: Multi-AZ (Availability Zone) deployment can be used to create a standby replica of the primary database in another availability zone. In case of a hardware failure or any other disruption in the primary database, the failover process is automatically handled by Amazon RDS, and the secondary database is promoted to become the new primary database. This option provides high availability and reduces the recovery time to seconds, making it a suitable choice for this scenario.\nOption C: Increasing the database instance size can improve the performance of the database, but it will not help in achieving high availability or reducing the recovery time in case of a hardware failure. Therefore, this option is not the best choice for this scenario.\nOption D: A read replica is a copy of the primary database that can be used to offload read traffic from the primary database. It does not provide automatic failover, so it is not suitable for this scenario.\nTherefore, the best option for achieving high availability and reducing the recovery time in case of a hardware failure on the primary database is to enable Multi-AZ deployment (Option B).\n\n"
}, {
  "id" : 411,
  "question" : "Your company is using different AWS resources and services.\nAs an architect, you have to instruct the operations department on which service they can use to trigger AWS Lambda functions based on AWS events.\nWhich of the following would you recommend?\n",
  "answers" : [ {
    "id" : "5ccf5ac2d85540d9882ff5f6d2736bdf",
    "option" : "AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "e5cd8fba9e1a4e48a33a08cf496a798c",
    "option" : "AWS CloudWatch Event",
    "isCorrect" : "true"
  }, {
    "id" : "a0f26e10247c436498ed14f10535df34",
    "option" : "AWS SQS",
    "isCorrect" : "false"
  }, {
    "id" : "fc8c1262bd9c4470921109b76cea12fa",
    "option" : "AWS CloudWatch Log.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon CloudWatch is a monitoring and management which collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications and services that run on AWS, and on-premises servers.\nIt captures real-time events which can be further used to trigger AWS Lambda.\nAWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account.\nActions taken by a user, role, or an AWS service are recorded as events in CloudTrail.\nEvents include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.\nIt doesn't have the capability to trigger anything.\nFor more details on the process, refer below URL-\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/events/Create-CloudWatch-Events-Rule.html\nOption C is incorrect since this is used to working with messages in the queue.\nOption D is incorrect CloudWatch Log cannot trigger the Lambda function based on real-time events.\nThere is some delay for the logs to be sent to CloudWatch Logs.\nFor more information on AWS CloudWatch, please visit the below URL-\nhttps://aws.amazon.com/cloudwatch/\n\nAs an AWS Solutions Architect, if you need to instruct the operations department on which service they can use to trigger AWS Lambda functions based on AWS events, the recommended option would be AWS CloudWatch Events.\nAWS CloudWatch Events is a fully managed event-routing service that enables you to match events produced by various AWS services, including Lambda functions, to different targets such as SNS, SQS, Lambda functions, Kinesis data streams, and more.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. It can be used to track user activity and API usage, but it does not provide direct support for triggering Lambda functions based on events.\nAWS Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. While it can be used to store and process messages, it does not directly support triggering Lambda functions based on events.\nAWS CloudWatch Logs is a fully managed log storage and monitoring service that enables you to collect, monitor, and analyze logs from various sources. It can be used to track and analyze Lambda function logs, but it does not provide direct support for triggering Lambda functions based on events.\nTherefore, the recommended option for triggering AWS Lambda functions based on AWS events is AWS CloudWatch Events.\n\n"
}, {
  "id" : 412,
  "question" : "A company is planning to store its files from their on-premises location onto the Simple Storage service.\nAfter a period of 3 months, they want to archive the files, since they would be rarely used.\nWhich of the following would be the right way to preserve this requirement?\n",
  "answers" : [ {
    "id" : "23ba1ebee5c044eca4f377ae819a6aeb",
    "option" : "Use an EC2 instance with EBS volumes. After a period of 3 months, keep on taking snapshots of the data.",
    "isCorrect" : "false"
  }, {
    "id" : "a9e8d2d4f7df42119026f3eb87c926c6",
    "option" : "Store the data on S3 and then use Lifecycle policies to transfer the data to Amazon Glacier",
    "isCorrect" : "true"
  }, {
    "id" : "953eb67bd6494a8695493a317dd9ac04",
    "option" : "Store the data on Amazon Glacier and then use Lifecycle policies to transfer the data to Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "29fde691a2a540428a61c81a880aefe3",
    "option" : "Use an EC2 instance with EBS volumes. After a period of 3 months , keep on taking copies of the volume using Cold HDD volume type.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nTo manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their lifecycle.\nA lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects.\nThere are two types of actions.\nTransition actions-Define when objects transition to another storage class.\nFor example, you might choose to transition objects to the STANDARD_IA storage class 30 days after you created them or archive objects to the GLACIER storage class one year after creating them.\nExpiration actions-Define when objects expire.\nAmazon S3 deletes expired objects on your behalf.\nOptions A and D are incorrect since using EBS volumes is not the right storage option for this sort of requirement.\nOption C is incorrect since the files should be initially stored in S3.\nFor more information on AWS S3 Lifecycle policies, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\n\nThe best option for preserving the requirement to store files from an on-premises location on the Simple Storage Service (S3) and archive them after three months would be to use S3 and Lifecycle policies to transfer the data to Amazon Glacier. Therefore, the correct answer is B.\nExplanation: Option A: Using an EC2 instance with EBS volumes and taking snapshots of the data would not be a suitable solution for this requirement. EBS snapshots are used for backup and disaster recovery purposes, and they do not provide an efficient way to archive data that is rarely used. Additionally, EC2 instances require ongoing management and maintenance, which would not be necessary with S3.\nOption B: Storing the data on S3 and using Lifecycle policies to transfer the data to Amazon Glacier would be the most appropriate solution for this requirement. S3 provides a scalable and durable storage solution for storing files, and Lifecycle policies can be used to automatically transition the files to lower-cost storage tiers, such as Amazon Glacier. Amazon Glacier is a cost-effective archival storage service that is designed for data that is rarely accessed. By using S3 and Lifecycle policies, the company can reduce its storage costs while still maintaining access to its archived data.\nOption C: Storing the data on Amazon Glacier and using Lifecycle policies to transfer the data to S3 would not be the best solution for this requirement. While Glacier is designed for archival storage, it is not suitable for frequently accessed data. If the company needs to access its data frequently, storing it on Glacier would result in high retrieval costs. Additionally, Glacier does not provide the same level of durability and availability as S3.\nOption D: Using an EC2 instance with EBS volumes and taking copies of the volume using Cold HDD volume type is not the best solution for this requirement. Similar to Option A, this solution requires ongoing management and maintenance of the EC2 instance, which is not necessary with S3. Additionally, copying the entire volume every three months would be time-consuming and inefficient.\n\n"
}, {
  "id" : 413,
  "question" : "A company has a workflow that sends video files from their on-premise system to AWS for transcoding.\nThey use EC2 worker instances that pull transcoding jobs from SQS.\nAs an architect, you need to design how the SQS service would be used in this architecture to achieve high throughput.\nWhich of the following is the ideal way in which the SQS service should be used?\n",
  "answers" : [ {
    "id" : "aa1b45fdf0f0498aab45926eabb4f2d2",
    "option" : "SQS should be used to guarantee high throughput because of the order of messages.",
    "isCorrect" : "false"
  }, {
    "id" : "6160085404074013bfa27e68e0ac779b",
    "option" : "SQS should be used to synchronously manage the transcoding output.",
    "isCorrect" : "false"
  }, {
    "id" : "49aba42a23974972b7a6ac33b6dbaeae",
    "option" : "SQS should be used to check the health of the worker instances.",
    "isCorrect" : "false"
  }, {
    "id" : "4fd223d23da8413f93d974bc66673347",
    "option" : "SQS should be used to facilitate horizontal scaling.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAmazon Simple Queue Service (Amazon SQS) offers a secure, durable, and available hosted queue that lets you integrate and decouple distributed software systems and components.\nOption A is incorrect since the ordering of messages won't help to achieve high throughput.\nOptions B and C are incorrect since these are not the responsibility of the SQS queue.\nFor more information on AWS SQS Horizontal Scaling, please visit the below URL-\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-throughput-horizontal-scaling-and-batching.html#horizontal-scaling\n\nThe ideal way to use the Amazon Simple Queue Service (SQS) in this architecture is D. SQS should be used to facilitate horizontal scaling.\nSQS is a distributed message queuing service that enables decoupling of distributed systems and applications. In this case, SQS is used to decouple the on-premise system from the EC2 worker instances that perform the transcoding jobs. This decoupling is essential to achieve high throughput as it allows the worker instances to pull transcoding jobs from SQS at their own pace.\nHorizontal scaling is a technique used to increase the number of instances of a component in the system to improve performance and throughput. In this architecture, the worker instances can be horizontally scaled to handle more transcoding jobs.\nSQS provides a scalable and reliable way of managing the transcoding jobs between the on-premise system and the worker instances. By using SQS to facilitate horizontal scaling, the architecture can handle a higher number of transcoding jobs without putting any undue stress on the worker instances.\nTherefore, the ideal way to use the SQS service in this architecture is to facilitate horizontal scaling of the worker instances, thereby achieving high throughput.\n\n"
}, {
  "id" : 414,
  "question" : "You're an architect for your company.\nYour IT admin staff needs access to newly created EC2 Instances for administrative purposes.\nWhich of the following needs to be done to ensure that the IT admin staff can successfully connect via port 22 on to the EC2 Instances?\n",
  "answers" : [ {
    "id" : "e34687ad689e4a5c95c6709f8c750207",
    "option" : "Adjust Security Group to permit egress traffic over TCP port 443 from your IP.",
    "isCorrect" : "false"
  }, {
    "id" : "00d619881d4d4c65ba72f035e5ba7792",
    "option" : "Configure the IAM role to permit changes to security group settings.",
    "isCorrect" : "false"
  }, {
    "id" : "3aee09cec3b248a5b2d30fc14b9a82e3",
    "option" : "Modify the instance security group to allow ingress of ICMP packets from your IP.",
    "isCorrect" : "false"
  }, {
    "id" : "8063cf4be2af4dda9ec20f505a7e38a3",
    "option" : "Adjust the instanceâ€™s Security Group to permit ingress traffic over port 22.",
    "isCorrect" : "true"
  }, {
    "id" : "d196a2ebcac7497398092b01abf9f764",
    "option" : "Apply the most recently released Operating System security patches.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - D.\nA security group acts as a virtual firewall that controls the traffic for one or more instances.\nWhen you launch an instance, you associate one or more security groups with the instance.\nYou add rules to each security group that allows traffic to or from its associated instances.\nFor connecting via SSH on EC2, you need to ensure that port 22 is open on the security group for the EC2 instance.\nOption A is wrong because port 443 is for HTTPS and not for SSH.\nOption B is wrong because the IAM role is not pertinent to security groups.\nOption C is wrong because this is relevant to ICMP and not SSH.\nOption E is wrong because it does not matter what patches are there on the system.\nFor more information on EC2 Security groups, please visit the URL-\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html\n\nThe correct answer for this question is D. Adjust the instance's Security Group to permit ingress traffic over port 22.\nExplanation: When EC2 instances are created, they are launched within a security group that acts as a virtual firewall controlling traffic to and from the instance. To allow IT admin staff to connect to the newly created EC2 instances via port 22, we need to adjust the security group associated with those instances to permit ingress traffic on port 22.\nOption A, adjusting the Security Group to permit egress traffic over TCP port 443 from your IP, would only allow outbound traffic from your IP address to the instances over port 443, which is not relevant to the requirement of allowing IT admin staff to connect via port 22.\nOption B, configuring the IAM role to permit changes to security group settings, allows users to modify the security group settings, but it does not specifically address the requirement of allowing inbound traffic on port 22.\nOption C, modifying the instance security group to allow ingress of ICMP packets from your IP, would only allow ICMP packets from your IP address to the instances, and not allow IT admin staff to connect to the instances via port 22.\nOption E, applying the most recently released Operating System security patches, is not relevant to the requirement of allowing IT admin staff to connect to the instances via port 22.\nTherefore, the correct answer is D, adjusting the instance's Security Group to permit ingress traffic over port 22.\n\n"
}, {
  "id" : 415,
  "question" : "Your company is running a photo-sharing website.\nCurrently, all the photos are stored in S3\nAt some point, the company finds out that other sites have been linking to your site's photos, which causes loss to your business.\nYou need to implement a solution for the company to mitigate this issue.\nWhich of the following would you look at implementing?\n",
  "answers" : [ {
    "id" : "0a4a901ca9434a43a497dbb70eb55f8c",
    "option" : "Remove public read access and use signed URLs with expiry dates.",
    "isCorrect" : "true"
  }, {
    "id" : "0c627e16624348a3bbd9413f68cdcf16",
    "option" : "Use CloudFront distributions for static content.",
    "isCorrect" : "false"
  }, {
    "id" : "e0e70edd2bb347d7b121f559374edcc5",
    "option" : "Block the IPs of the offending websites in Security Groups.",
    "isCorrect" : "false"
  }, {
    "id" : "8e33fd9b45f4409cb5e6bbfa77da8fb2",
    "option" : "Store photos on an EBS volume of the web server.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nA pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permission to access that object.\nThat is, if you receive a pre-signed URL to upload an object, you can upload the object only if the creator of the pre-signed URL has the necessary permissions to upload that object.\nOption B is incorrect since CloudFront is only used to distribute content across edge or region locations.\nIt is not used for restricting access to content.\nOption C is incorrect since Blocking IP's is challenging because they are dynamic in nature, and you will not know which sites are accessing your main site.\nOption D is incorrect since Storing photos on EBS volume is not a good practice or architecture approach for an AWS Solution Architect.\nFor more information on serving private content, please visit the URL-\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\n\nThe correct answer in this scenario would be A. Remove public read access and use signed URLs with expiry dates.\nExplanation:\nThe problem that the company is facing is that other websites are directly linking to their photos, resulting in loss of business. This means that the photos on the company's S3 bucket are publicly accessible, which should not be the case. There are two things that the company can do to mitigate this issue:\nRemove Public Read Access: The company should remove public read access to their S3 bucket so that photos can only be accessed by authorized users. This can be done by updating the S3 bucket policy or access control list (ACL) to deny public access. Use Signed URLs with Expiry Dates: Even after removing public read access, the company may still need to provide access to some users, such as website visitors who are authorized to view the photos. In this case, the company should use signed URLs with expiry dates to provide temporary access to these users. This can be done by generating signed URLs that have a limited time validity, and then providing these URLs to authorized users.\nOption B, using CloudFront distributions for static content, is not directly related to the problem of unauthorized access to the company's photos. However, using CloudFront can improve the website's performance by caching content closer to users.\nOption C, blocking IPs of the offending websites in Security Groups, is not an effective solution as it is easy for offending websites to change their IP addresses, and it may also block legitimate users who happen to use the same IP addresses.\nOption D, storing photos on an EBS volume of the web server, is not a recommended solution for storing photos as it can be costly and difficult to scale. S3 is a more cost-effective and scalable solution for storing and managing large amounts of data.\n\n"
}, {
  "id" : 416,
  "question" : "You have been hired as a consultant for a company to implement their CI/CD processes.\nThey currently use an on-premises deployment of Chef for their configuration management on servers.\nYou need to advise them on what they can use on AWS to leverage their existing capabilities.\nWhich of the following service would you recommend?\n",
  "answers" : [ {
    "id" : "793f40e89564434681a990a6c4e20e71",
    "option" : "Amazon Simple Workflow Service",
    "isCorrect" : "false"
  }, {
    "id" : "807ac5ce4a4e487d9f66b3ce41b1e2ff",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "c88bd40413e845298c1ac147ed71bf07",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "d64368f2d6ef40dc9e7935430a3a25e3",
    "option" : "AWS OpsWorks.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.\nChef and Puppet are automation platforms that allow you to use code to automate your servers' configurations.\nOpsWorks let you use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or on-premises compute environments.\nOpsWorks have three offerings, AWS Opsworks for Chef Automate, AWS OpsWorks for Puppet Enterprise, and AWS OpsWorks Stacks.\nAll of the other options are incorrect since the only tool which works effectively with the Chef Configuration management tool is AWS OpsWorks.\nFor more information on AWS Opswork, please visit the URL-\nhttps://aws.amazon.com/opsworks/\n\nBased on the scenario provided, the service that would best fit the requirements of the company is AWS OpsWorks.\nAWS OpsWorks is a configuration management service that enables the management of Chef and Puppet-powered servers. It provides a complete DevOps experience for managing applications on AWS. OpsWorks automates the deployment, configuration, and management of applications, so that developers can focus on writing code instead of managing infrastructure.\nSince the company already uses an on-premises deployment of Chef, OpsWorks would be the most logical choice for them to leverage their existing capabilities. OpsWorks supports Chef recipes and cookbooks, enabling them to easily migrate their existing Chef infrastructure to AWS. With OpsWorks, they can manage their configuration management at scale and automate the deployment of new servers and applications.\nAmazon Simple Workflow Service (SWF) is a service that coordinates tasks across distributed applications. SWF can be used to build applications with tasks that are long-running, asynchronous, and require coordination between multiple components.\nAWS Elastic Beanstalk is a Platform-as-a-Service (PaaS) offering that simplifies the process of deploying, managing, and scaling web applications. Elastic Beanstalk supports multiple programming languages, including Java, .NET, PHP, Node.js, Python, Ruby, and Go.\nAWS CloudFormation is a service that provides a way to create and manage a collection of AWS resources as a single unit, called a stack. CloudFormation templates are used to define the desired state of the resources and the relationships between them.\nIn summary, based on the scenario provided, the service that would best fit the company's requirements is AWS OpsWorks because it is a configuration management service that supports Chef and Puppet-powered servers, allowing them to easily migrate their existing Chef infrastructure to AWS.\n\n"
}, {
  "id" : 417,
  "question" : "You are working as an AWS consultant for a banking institute.\nThey have deployed a digital wallet platform for clients using multiple EC2 instances in the us-east-1 region.\nThe application establishes a secure encrypted connection between clients &amp; EC2 instances for each transaction using custom TCP port 5810. Due to the increasing popularity of this digital wallet, they observe load on backend servers resulting in delay in transaction.\nFor security purpose, all client IP address accessing this application should be preserved &amp; logged.\nThe technical team of the banking institution is looking for a solution that will address this delay &amp; also proposed solution should be compatible with millions of transactions done simultaneously.\nWhich of the following is a recommended option to meet this requirement?\n",
  "answers" : [ {
    "id" : "4f8c1b6aba194d5d886dba13034c6300",
    "option" : "Use Network Load Balancers with SSL certificate. Configure TLS Listeners on this NLB with custom security policy consisting of protocols &amp; ciphers.",
    "isCorrect" : "false"
  }, {
    "id" : "325c4b66d5904098b5a9194ffd17a545",
    "option" : "Use Network Load Balancers with SSL certificate. Configure TLS Listeners on this NLB with default security policy consisting of protocols &amp; ciphers.",
    "isCorrect" : "true"
  }, {
    "id" : "b847e45d083e437495393b9e9e9f50f0",
    "option" : "Use Network Load Balancers with SSL certificate. Configure TLS Listeners on this NLB with default security policy consisting of protocols &amp; TCP port 5810.",
    "isCorrect" : "false"
  }, {
    "id" : "378ae8bf227b4c8a8df6c442fd614c07",
    "option" : "Use Network Load Balancers with SSL certificate. Configure TLS Listeners on this NLB with custom security policy consisting of protocols &amp; TCP port 5810.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nNetwork Load Balancer can terminate TLS connections instead of back end instance, reducing the load on this instance.\nWith Network Load Balancers, millions of simultaneous sessions can be established with no impact on latency along with preserving client IP address.\nTo negotiate TLS connections with clients, NLB uses a security policy that consists of protocols &amp; ciphers.\nOption A is incorrect as Network Load Balancers does not support a custom security policy.\nOption C is incorrect as Network Load Balancers should consist of security policies comprising of Protocols &amp; Ciphers.\nOption D is incorrect as Network Load Balancers do not support custom security policy, and security policies should comprise protocols &amp; ciphers.\nFor more information on Security Policies for TLS termination on Network Load Balancers, refer to the following URL-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html\n\nThe recommended option to meet the requirement of reducing delays in transactions and preserving client IP address while logging is to use Network Load Balancers (NLB) with SSL certificate and configuring TLS Listeners on the NLB.\nOption A is the correct answer, as it includes configuring a custom security policy consisting of protocols and ciphers.\nExplanation: Network Load Balancers (NLBs) are designed to handle millions of simultaneous connections and provide high availability and low latency. By using NLBs, traffic can be distributed evenly among multiple EC2 instances, thus reducing the load on each instance and improving the overall performance of the application.\nTo ensure security, SSL certificates can be used to establish a secure connection between clients and the NLB. This ensures that all communication between the client and the application is encrypted and secure.\nTLS (Transport Layer Security) Listeners can be configured on the NLB to accept incoming traffic on a specific port (in this case, TCP port 5810). By using a custom security policy consisting of protocols and ciphers, the NLB can ensure that only secure connections are established and can block any insecure connections.\nPreserving client IP address while logging is important for security and compliance purposes. By using NLBs with TCP protocol, client IP addresses can be preserved and logged, allowing the banking institution to track and monitor all transactions.\nOption A is the correct answer as it provides a custom security policy that can be tailored to the specific requirements of the banking institution, while still ensuring the highest level of security for all transactions. The other options do not provide custom security policies, and some options do not include TCP port 5810, which is required for this application.\n\n"
}, {
  "id" : 418,
  "question" : "You work as an architect for a company.\nThere is a requirement for an application to be deployed on a set of EC2 Instances.\nThese would be part of a compute cluster that requires low inter-node latency.\nWhich of the following would you use for this requirement?\n",
  "answers" : [ {
    "id" : "ccd7a45fc93c4aa2ab2a86bba4b9ec07",
    "option" : "Multiple Availability Zones",
    "isCorrect" : "false"
  }, {
    "id" : "84364accc44047dcbd01e343c8c78285",
    "option" : "AWS Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "2529a27f119745e0a9bdf374f790c32a",
    "option" : "EC2 Dedicated Instances",
    "isCorrect" : "false"
  }, {
    "id" : "17bb888698a547b6addd7af010fdb152",
    "option" : "Cluster placement Groups",
    "isCorrect" : "true"
  }, {
    "id" : "c3e9ba17cf694166970b6cfcfb4cb8c6",
    "option" : "VPC private subnets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nAmazon Web Services' solution to reduce latency between instances involves the use of placement groups.\nAs the name implies, a placement group is just that -- a group.\nAWS instances that exist within a common availability zone can be grouped into a placement group.\nGroup members can communicate with one another in a way that provides low latency and high throughput.\nCluster Placement groups are recommended for applications that benefit from low network latency, high network throughput, or both.\nThe majority of the network traffic is between the instances in the group.\nTo provide the lowest latency and the highest packet-per-second network performance for your placement group, choose an instance type that supports enhanced networking.\nBecause of what is mentioned in the documentation, all other options are incorrect.\nFor more information on AWS placement groups, please visit the URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nTo achieve low inter-node latency for an application deployed on a set of EC2 instances, the best option would be to use Cluster Placement Groups.\nCluster Placement Groups are logical groupings of EC2 instances within a single Availability Zone (AZ) that are designed to provide low network latency and high network throughput. When instances are launched within a cluster placement group, they are placed within a single rack or group of racks that are in close physical proximity to each other.\nThis provides a low-latency, high-bandwidth network connection between the instances, making them ideal for high-performance computing and data-intensive workloads. It is important to note that Cluster Placement Groups are limited to a single Availability Zone, and cannot span multiple Availability Zones.\nMultiple Availability Zones (option A) are used to provide high availability and fault tolerance by distributing instances across multiple AZs. While this can improve the overall availability of an application, it does not necessarily address the requirement for low inter-node latency.\nAWS Direct Connect (option B) is a dedicated network connection between an on-premises data center and AWS. While this can provide a reliable and secure connection, it does not necessarily improve inter-node latency within an EC2 cluster.\nEC2 Dedicated Instances (option C) provide dedicated hardware for instances, which can improve security and compliance for certain workloads. However, it does not address the requirement for low inter-node latency within an EC2 cluster.\nVPC private subnets (option E) are used to provide isolation and security for instances within a VPC. While this is an important security feature, it does not necessarily address the requirement for low inter-node latency within an EC2 cluster.\nTherefore, the correct answer is D, Cluster Placement Groups.\n\n"
}, {
  "id" : 419,
  "question" : "Your company stores a large set of files in Amazon S3\nThey need to ensure that if any new files are added to an S3 bucket, an event notification would be sent to the IT admin staff.\nWhich of the following could be used to fulfill this requirement? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "5028eea105bb402480848d6d717640e9",
    "option" : "Create an SNS topic",
    "isCorrect" : "true"
  }, {
    "id" : "5ee27919975a42018f31ae88aa2af0cb",
    "option" : "Create an SQS queue",
    "isCorrect" : "false"
  }, {
    "id" : "d055eba2a9054285aa8f91de8996e6ab",
    "option" : "Add an event notification to the S3 bucket",
    "isCorrect" : "true"
  }, {
    "id" : "5ed43614146040ff8a239ee950a24098",
    "option" : "Add an event notification to the S3 object.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS Documentation mentions the following.\nThe Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket.\nTo enable notifications, you must first add a notification configuration identifying the events you want Amazon S3 to publish, and the destinations where you want Amazon S3 to send the event notifications.\nGo in S3 bucket properties, and events.\nThen choose the relevant event, and select send to as SNS topic.\nOption B is incorrect since you need to create an SNS topic that could be used to send an email to multiple IT administrators.\nOption D is incorrect since the event notification needs to be placed on the bucket and not the object.\nNOTE:\nOptions C and D are different.\nOption C: Add an event notification to the S3 bucket.\nOption D: Add an event notification to the S3 object.\nFor more information on AWS S3 notifications, please visit the URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html\n\n\nTo fulfill the requirement of sending an event notification to IT admin staff when a new file is added to an S3 bucket, we have two possible solutions:\nAdd an event notification to the S3 bucket: Amazon S3 supports event notifications that can be configured to trigger notifications when specific events occur in your S3 bucket. We can use the Amazon S3 console, AWS SDKs, or Amazon S3 REST API to configure the events that we want to monitor and the actions that we want Amazon S3 to take when those events occur. In this case, we can create an S3 bucket event notification for the s3:ObjectCreated:Put event type that triggers the notification to the IT admin staff. The notification can be sent via email, SMS, or other supported means. Create an SNS topic: Amazon SNS (Simple Notification Service) is a fully managed pub/sub messaging service that can be used to send notifications from the cloud to various endpoints such as email, SMS, HTTP endpoints, or even other AWS services such as SQS (Simple Queue Service). In this scenario, we can create an SNS topic and subscribe the IT admin staff to the topic. Then, we can configure an S3 bucket event notification to send a message to the SNS topic whenever a new file is added to the bucket. The SNS topic will then forward the message to all the subscribed endpoints, including the IT admin staff.\nBoth solutions are valid, but the best choice may depend on the specific use case and requirements. If we want to send notifications to multiple endpoints or integrate with other AWS services, SNS may be a better option. If we only need to notify a single endpoint or don't want to introduce an additional service, configuring the S3 bucket event notification directly may be sufficient.\nCreating an SQS queue is not a suitable option in this case as it is a message queue service and is typically used for decoupling and scaling microservices, not for event notifications.\n\n"
}, {
  "id" : 420,
  "question" : "Your company is planning to migrate code written in C# from their on-premises infrastructure onto AWS.\nThey want to ensure to limit the amount of maintenance that would be required for the underlying infrastructure.\nWhich of the following would they choose for hosting the code base?\n",
  "answers" : [ {
    "id" : "3eff8691a8e54b528662b38a2fe543df",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "a91b6ef27f0a49e09f3a9be64723d4ea",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "92df790e43ee4bb4aaa8d7ed4a4cfda4",
    "option" : "AWS ECS",
    "isCorrect" : "false"
  }, {
    "id" : "850934d4aa074cbfa8f129ac7dadc17d",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers.\nAWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.\nYou pay only for the compute time you consume.\nThere is no charge when your code is not running.\nWith AWS Lambda, you can run code for virtually any type of application or backend service - all with zero administration.\nWith Lambda, you don't need to build, secure, or maintain a container.\nYou must be worried about the code.\nEC2\n- is simply a remote (virtual) machine.\nECS stands for Elastic Container Service.\nECS is basically a logical grouping of\nEC2\nInstances.\nTechnically, ECS is a mere configuration for the efficient use and management of your\nEC2\ninstance(s) resources i.e.\nstorage, memory, CPU, etc.\nTo simplify it further, if you have launched an Amazon ECS with no\nEC2\ninstances added to it, it's good for nothing.\nSo you can't do anything about it.\nECS.\nmakes sense only once one (or more)\nEC2\ninstances are added to it.\nOptions B and C are incorrect since here you would need to manage the underlying servers.\nOption D is incorrect since this is a messaging service.\nFor more information on AWS Lambda, please visit the URL-\nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html\n\nWhen it comes to hosting C# code on AWS, there are a few different options available. However, based on the requirement to limit maintenance, the best option would be AWS Lambda (Option A).\nAWS Lambda is a serverless computing service, which means that there are no servers to manage or maintain. Instead, you simply upload your code and Lambda takes care of everything else. You only pay for the compute time that your code actually uses, so there is no need to worry about provisioning or managing any infrastructure. Additionally, AWS Lambda is highly scalable, so it can handle large numbers of requests without any additional setup.\nAWS EC2 (Option B) is a traditional compute service that provides virtual machines that can be configured and managed by the user. This means that there would be more maintenance required, as you would need to manage and maintain the underlying infrastructure yourself. While EC2 does offer more flexibility and customization options than Lambda, it requires a higher level of expertise to manage.\nAWS ECS (Option C) is a container orchestration service that allows you to run and manage Docker containers on AWS. While ECS can provide a high level of scalability and flexibility, it also requires more maintenance than Lambda. You would need to manage the underlying infrastructure, as well as the containers themselves.\nAWS SQS (Option D) is a message queue service that allows you to decouple and scale microservices, distributed systems, and serverless applications. While SQS can be useful in certain scenarios, it is not a hosting option for C# code.\nIn conclusion, based on the requirement to limit maintenance, AWS Lambda (Option A) is the best option for hosting C# code on AWS.\n\n"
}, {
  "id" : 421,
  "question" : "A company has an AWS account that contains three VPCs (Dev, Test, and Prod) in the same region.\nThere is a requirement to ensure that instances in the Development and Test VPC's can access resources in the Production VPC.\nThere should be minimal efforts with minimal administrative overhead.\nWhich of the following would be the ideal way to get this in place?\n",
  "answers" : [ {
    "id" : "35e90d485476446d8143edd4778428c4",
    "option" : "Create an AWS Direct Connect connection between the Development, Test VPC to the Production VP.",
    "isCorrect" : "false"
  }, {
    "id" : "79222d5c2dd6458d897963ba9256e9b7",
    "option" : "Create a separate VPC peering connection from Development to Production and from Test to the Production VP.",
    "isCorrect" : "true"
  }, {
    "id" : "70cfe98014c5452cbcea52c28a94fa4d",
    "option" : "Create a VPN connection between the Development, Test VPC to the Production VP.",
    "isCorrect" : "false"
  }, {
    "id" : "68a72474c0d04faa9bfe2618e85ae3dd",
    "option" : "Create a VPC peering connection between the Development to the Production VPC and from Development to the Test VP.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nOptions A and C are incorrect since these options will NOT involve minimal efforts or minimal overhead.\nHence you need to choose VPC peering.\nOptions D is incorrect since the VPC Peering configuration mentioned would be invalid.\nWe need access from Production to Test and NOT from \"Development to Test\" as given in the option.\nYou need VPC Peering Configuration between Dev to Prod and Test to Prod.\n\"A VPC peering connection is a networking connection between two VPCs that enable you to route traffic between them using private IPv4 addresses or IPv6 addresses.\nInstances in either VPC can communicate with each other as if they are within the same network.\nYou can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account.\nThe VPCs can be in different regions (also known as an inter-region VPC peering connection).\"\nFor more information on VPC peering, please visit the URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/PeeringGuide/Welcome.html\n\n\nTo allow instances in the Development and Test VPC to access resources in the Production VPC, there are a few options to consider:\nA. AWS Direct Connect: AWS Direct Connect is a dedicated network connection between an on-premises data center and AWS. It provides a high-speed, low-latency connection that can be used to access AWS resources. In this case, Direct Connect would not be the best option because all three VPCs are already in AWS. Additionally, Direct Connect has a high setup cost and ongoing fees, making it more expensive than other options.\nB. VPC peering connections: VPC peering allows direct communication between two VPCs. It's a good option for connecting VPCs within the same region, as it provides low-latency, high-bandwidth communication without going over the public internet. In this case, creating separate VPC peering connections from the Development VPC and Test VPC to the Production VPC would be a good option. It provides direct communication between the VPCs, and the setup and ongoing maintenance is minimal.\nC. VPN connections: A VPN connection is a secure, encrypted tunnel between two networks. It's a good option for connecting VPCs that are in different regions or on-premises data centers. However, in this case, all three VPCs are in the same region, and VPN connections have higher latency and lower bandwidth than VPC peering.\nD. VPC peering connections between Development and Production VPC, and Development and Test VPC: Creating VPC peering connections between the Development VPC and Production VPC, as well as between the Development and Test VPC, would not allow direct communication between the Test VPC and Production VPC, which is a requirement in this case. Additionally, it would create a more complex network topology, increasing administrative overhead.\nTherefore, option B (creating separate VPC peering connections from the Development VPC and Test VPC to the Production VPC) would be the ideal way to get this in place, as it provides direct communication between the VPCs with minimal administrative overhead.\n\n"
}, {
  "id" : 422,
  "question" : "You are designing the application architecture for a company.\nThe architecture will consist of a web tier that will be hosted on EC2 Instances placed behind an Elastic Load Balancer.\nWhich of the following would help you to decide the instance type of the EC2 instances?\n",
  "answers" : [ {
    "id" : "133dede34ad247d0986377c46a2ea1ea",
    "option" : "Determine the required I/O performance.",
    "isCorrect" : "false"
  }, {
    "id" : "dc646859282044478724b6c6586f1db5",
    "option" : "Determine the minimum memory requirements.",
    "isCorrect" : "false"
  }, {
    "id" : "74b132e077524333b06a74becfd001bb",
    "option" : "Determine the storage capabilities required for the application.",
    "isCorrect" : "false"
  }, {
    "id" : "d92833b5fab742d49161a99dc1d97d9b",
    "option" : "All the above options.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe instance type determines the hardware of the host computer used for your instance.\nEach instance type offers different compute, memory, and storage capabilities, and is grouped in an instance family based on these capabilities.\nFor more information on EC2 Instance types, please visit the URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-types.html\n\nWhen designing the application architecture for a company, it is important to consider various factors to ensure optimal performance, reliability, and scalability of the system. In this scenario, the architecture will consist of a web tier that will be hosted on EC2 Instances placed behind an Elastic Load Balancer. One important decision that needs to be made is selecting the appropriate EC2 instance type.\nThere are various factors that can help in deciding the instance type of the EC2 instances, and the options provided in the question are:\nA. Determine the required I/O performance. B. Determine the minimum memory requirements. C. Determine the storage capabilities required for the application. D. All the above options.\nLet's discuss each option in more detail:\nA. Determine the required I/O performance: Input/output (I/O) performance is an important consideration when selecting an EC2 instance type, especially for applications that require high I/O operations, such as databases, analytics, and machine learning. In such cases, it is recommended to select an instance type that provides high I/O performance, such as instances that use NVMe SSD or local instance storage. Therefore, determining the required I/O performance is a key factor in deciding the EC2 instance type.\nB. Determine the minimum memory requirements: Memory is another important consideration when selecting an EC2 instance type. Applications that require a large amount of memory, such as in-memory databases or caching systems, should use instance types with a high memory capacity. In addition, some applications may require a certain minimum amount of memory to function properly. Therefore, determining the minimum memory requirements is another important factor in deciding the EC2 instance type.\nC. Determine the storage capabilities required for the application: Storage requirements can also impact the selection of the EC2 instance type. Applications that require high storage capacity should use instance types with larger storage volumes or that support network-attached storage (NAS). In addition, some applications may require specific storage types, such as solid-state drives (SSDs) or magnetic disks. Therefore, determining the storage capabilities required for the application is another important factor in deciding the EC2 instance type.\nD. All the above options: As all the options mentioned above are important factors in deciding the EC2 instance type, selecting \"All the above options\" is also a valid answer.\nIn conclusion, when designing the application architecture for a company, it is important to consider various factors when selecting the appropriate EC2 instance type. The factors mentioned in the options provided in the question - I/O performance, memory requirements, and storage capabilities - are all important considerations that can help in making the decision.\n\n"
}, {
  "id" : 423,
  "question" : "Your company has a requirement to host an application in AWS that requires access to a NoSQL database.\nBut there are no human resources available who can take care of the database infrastructure.\nIn addition to this, the database should have the capability to scale automatically based on demand and also have high availability.\nWhich of the following databases would you use for this purpose?\n",
  "answers" : [ {
    "id" : "9c7d5359b8094fa6b83da4c12e943058",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "823063e8b69d4f27ac6a12d6e28b4862",
    "option" : "ElasticMap Reduce",
    "isCorrect" : "false"
  }, {
    "id" : "6002465ee8544eb0bcd2f086951ecee8",
    "option" : "Amazon RDS",
    "isCorrect" : "false"
  }, {
    "id" : "36430755b0d04ca48c5a6f808a359ed6",
    "option" : "Amazon Aurora.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon DynamoDB is a nonrelational database that delivers reliable performance at any scale.\nIt's a fully managed, multi-region, multi-master database that provides consistent single-digit millisecond latency, and offers built-in security, backup and restore, and in-memory caching.\nWith DynamoDB On-Demand, capacity planning is a thing of the past.\nYou don't specify read and write capacity at all-you pay only for the usage of your DynamoDB tables.\nThis fits perfectly with the Lambda and Serverless model-I pay more when I have more usage, which means I'm delivering more value to my customers.\nPreviously, you had to set read and write throughput capacity on your DynamoDB tables.\nThis specified how many and how large of reads and writes you could make on your table in any given second.\nRead and write capacity units were charged by the hour, and your requests would be throttled if you exceeded your provisioned capacity in any given second.\nOption B is invalid since this is used for Big Data.\nOption C is invalid since here you still have to partially manage the infrastructure.\nOption D is invalid since this would allow you to host MySQL compatible databases.\nFor more information on DynamoDB, please visit the URL.\nhttps://aws.amazon.com/dynamodb/\n\nIn this scenario, we need a NoSQL database that is easy to manage and can scale automatically based on demand while maintaining high availability, without requiring human resources to manage the infrastructure.\nOption A: DynamoDB DynamoDB is a NoSQL database service that is fully managed by AWS, which means it requires no human intervention to manage the infrastructure. It is designed for high scalability and availability, and it can automatically scale to accommodate increasing demand. It also provides built-in features like encryption, backup and restore, and global tables for multi-region deployments. Therefore, it is a suitable option for this scenario.\nOption B: ElasticMap Reduce ElasticMap Reduce is a service that allows you to process vast amounts of data using Apache Hadoop and Apache Spark. It is not a database service and does not provide a NoSQL database. Therefore, it is not a suitable option for this scenario.\nOption C: Amazon RDS Amazon RDS is a managed database service that provides relational databases such as MySQL, PostgreSQL, and Oracle. It does not provide a NoSQL database like DynamoDB. Although it is easy to manage and can scale automatically, it may not be the best choice for this scenario because it does not meet the NoSQL requirement.\nOption D: Amazon Aurora Amazon Aurora is a managed database service that provides MySQL and PostgreSQL-compatible databases. It is designed for high performance, scalability, and availability. Although it is not a NoSQL database, it provides some NoSQL-like features such as auto-scaling and high availability. However, it may not be the best choice for this scenario because it does not meet the NoSQL requirement.\nTherefore, option A: DynamoDB is the best choice for this scenario because it is a fully managed NoSQL database service that provides scalability, availability, and ease of management without requiring human resources to manage the infrastructure.\n\n"
}, {
  "id" : 424,
  "question" : "You are working as an AWS consultant for an online grocery store.\nThey are using a two-tier web application with web-servers hosted in VPC's at us-east-1 region &amp; on-premise data-center.\nNetwork Load balancer is configured in the front end to distribute traffic between these servers.\nAll traffic between clients &amp; servers is encrypted.\nThey are looking for an alternate solution to terminate the TLS connection on this Network Load balancer to reduce load on back-end servers. This store's management team has engaged you to suggest a solution for certificate management used in case of TLS termination.\nWhich of the following is a preferred secure option to provision &amp; store certificates to be used along with Network Load Balancer for terminating TLS?\n",
  "answers" : [ {
    "id" : "f40b46c3d39b41e6b1c4ee3a7e748860",
    "option" : "Use multiple certificates per TLS listener &amp; If a hostname provided by a client matches multiple certificates in the certificate list. The load balancer selects all of the certificates.",
    "isCorrect" : "false"
  }, {
    "id" : "e9154dedf19243b9a243c1780bdce2fd",
    "option" : "Use TLS tools to generate a new certificate &amp; upload in AWS Certificate Manager.",
    "isCorrect" : "false"
  }, {
    "id" : "5d70dd10d4ea4f19a683aee859178a44",
    "option" : "Use a single certificate per TLS listener provided by AWS Certificate Manager.",
    "isCorrect" : "true"
  }, {
    "id" : "475ac43040a943beb2a465a149f63e81",
    "option" : "Use a single certificate with 4096 bits RSA keys for higher security.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nNetwork Load Balancer requires one certificate per TLS connection to encrypt traffic between client &amp; NLB and forward decrypted traffic to target servers.\nUsingAWS Certificate Manager is a preferred option, as these certificates are automatically renewed on expiry.\nOption A is incorrect as Network Load Balancer uses a smart certificate selection algorithm to support Server Name Indication (SNI)\nIf the hostname provided by a client matches a single certificate in the certificate list, the load balancer selects this certificate.\nIf a hostname provided by a client matches multiple certificates in the certificate list, the load balancer selects the best certificate that the client can support.\nRefer section \"Certificate List\" under the link: https://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html.\nOption B is incorrect as this will increase admin work.\nAlso, you will need to monitor the expiry dates of certificates &amp; renew these certificates before expiration.\nOption D is incorrect as Network Load Balancer does not support certificates with RSA bits higher than 2048 bits.\nFor more information on certificates for TLS termination on Network Load Balancers, refer to the following URL-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/network/create-tls-listener.html\n\nIn this scenario, the client is using a two-tier web application, with web servers hosted in VPCs in the us-east-1 region and on-premises data centers. A Network Load Balancer (NLB) is configured in the front end to distribute traffic between these servers, and all traffic between clients and servers is encrypted.\nThe client is looking for an alternate solution to terminate the TLS connection on the NLB to reduce the load on the back-end servers. To implement this, the consultant needs to suggest a solution for certificate management used in case of TLS termination.\nTLS termination refers to the process of decrypting the TLS traffic at the NLB, instead of forwarding the encrypted traffic to the backend servers. This approach reduces the load on the backend servers, as they no longer need to handle the overhead of encryption and decryption. However, this also means that the NLB needs to be configured with the appropriate SSL/TLS certificates.\nOption A: Use multiple certificates per TLS listener & If a hostname provided by a client matches multiple certificates in the certificate list. The load balancer selects all of the certificates. This option involves using multiple certificates per TLS listener. If the hostname provided by a client matches multiple certificates in the certificate list, the load balancer selects all of the certificates. While this approach provides flexibility, it may not be the most secure option, as it increases the risk of certificate spoofing attacks.\nOption B: Use TLS tools to generate a new certificate & upload in AWS Certificate Manager. This option involves using TLS tools to generate a new certificate and uploading it to the AWS Certificate Manager. This approach provides a higher level of security, as it allows the client to generate and manage their own certificates. However, this may be more complex and time-consuming, as the client needs to have the necessary expertise to generate and manage the certificates.\nOption C: Use a single certificate per TLS listener provided by AWS Certificate Manager. This option involves using a single certificate per TLS listener provided by the AWS Certificate Manager. This approach provides a simple and secure way to manage SSL/TLS certificates, as the AWS Certificate Manager provides a centralized management interface for all certificates. However, this approach may not be suitable for clients who require more control over their SSL/TLS certificates.\nOption D: Use a single certificate with 4096 bits RSA keys for higher security. This option involves using a single certificate with 4096 bits RSA keys for higher security. While this approach provides a higher level of security, it may also increase the processing overhead on the backend servers, as they need to handle the higher encryption key length.\nIn conclusion, option C - using a single certificate per TLS listener provided by the AWS Certificate Manager - is the preferred secure option for provisioning and storing certificates to be used with the Network Load Balancer for terminating TLS. It provides a simple and secure way to manage SSL/TLS certificates, and the AWS Certificate Manager provides a centralized management interface for all certificates.\n\n"
}, {
  "id" : 425,
  "question" : "Your company is planning on moving to the AWS Cloud.\nOne of the Web applications will be launched on a set of EC2 Instances.\nYou need to ensure that the architecture is fault-tolerant and highly available.\nWhich of the following would be considered during the design process.\nChoose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "86115a451f09437e8685d025c96f0863",
    "option" : "Have a Single Availability Zone for the databases",
    "isCorrect" : "false"
  }, {
    "id" : "4808935e30e243e38fb0e56872b470a4",
    "option" : "Use a load balancer in front of the EC2 Instances",
    "isCorrect" : "true"
  }, {
    "id" : "e805c531a9724c1a8f08e786f0e00031",
    "option" : "Ensure that the EC2 Instances are spread across multiple availability zones",
    "isCorrect" : "true"
  }, {
    "id" : "14f38858794a4bf4a76ef2a73012cd26",
    "option" : "Ensure that the EC2 Instances are spread across a single availability zone for better maintenance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThis is clearly mentioned in the AWS Documentation.\nWhat Is Elastic Load Balancing?\nElastic Load Balancing distributes incoming application or network traffic across multiple targets, such as Amazon EC2 instances, containers, and IP addresses, in multiple Availability Zones.\nElastic Load Balancing scales your load balancer as traffic to your application changes over time and can automatically scale to the vast majority of workloads.\nLoad Balancer Benefits.\nA load balancer distributes workloads across multiple compute resources, such as virtual servers.\nUsing a load balancer increases the availability and fault tolerance of your applications.\nOption A is invalid because it will not increase the availability and fault tolerance of your applications.\nOption D is invalid because you need to ensure that the Instances are spread across multiple availability zones.\nFor more information on load balancing and availability of EC2 Instances, please visit the URL-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-increase-availability.html\n\nTo ensure fault-tolerant and highly available architecture for the Web application on EC2 instances, the following two design considerations should be taken into account:\nUse a load balancer in front of the EC2 instances: Load balancers distribute incoming traffic across multiple EC2 instances. By distributing the traffic, the load balancer ensures that no single instance receives too much traffic, and the overall workload is shared across all the instances. In case one of the EC2 instances fails, the load balancer will automatically redirect traffic to the other available instances. This ensures that the application remains available and responsive even if one or more instances fail. AWS provides a load balancing service called Elastic Load Balancer (ELB) that can be used for this purpose. Ensure that the EC2 instances are spread across multiple availability zones: An availability zone (AZ) is an isolated location within an AWS region, where resources can be deployed. Deploying EC2 instances across multiple AZs ensures that the application remains available even if an entire AZ goes down due to a natural disaster or other unforeseen events. In such cases, the load balancer will redirect traffic to instances in other AZs, ensuring high availability and fault tolerance. To achieve this, Auto Scaling Groups can be used to automatically launch instances in multiple AZs. Additionally, data storage for the application, such as databases, should also be deployed across multiple AZs to ensure data availability in case of an AZ failure.\nThe following options are incorrect: A. Have a Single Availability Zone for the databases: Deploying databases in a single availability zone can result in a single point of failure, and any outage in that AZ could lead to the loss of data and application downtime. Hence, it is not a recommended practice. D. Ensure that the EC2 Instances are spread across a single availability zone for better maintenance: Deploying all the EC2 instances in a single AZ may result in lower maintenance efforts but also results in a single point of failure. If the AZ fails, then the application becomes unavailable, resulting in downtime. Hence, it is not a recommended practice.\n\n"
}, {
  "id" : 426,
  "question" : "An organization produces different types of data on-premises.\nThis organization would like to migrate these data to the AWS cloud for analysis purposes. What combination of services will be used to achieve this?\n",
  "answers" : [ {
    "id" : "616ab0881dd04c24a45711f34aaa1c81",
    "option" : "On premises data &gt; Amazon S3 &gt; AWS Glue &gt;&gt; Amazon Athena &gt; Amazon Quicksight",
    "isCorrect" : "true"
  }, {
    "id" : "a3bfa695ed704a9d916049b68c559b4e",
    "option" : "On premises data &gt; Amazon S3 &gt; AWS Glue &gt;&gt; S3 Select &gt; Amazon Quicksight",
    "isCorrect" : "false"
  }, {
    "id" : "5cc949114bae4c479aff386c95b000dc",
    "option" : "On premises data &gt; Amazon S3 &gt; AWS Glue &gt;&gt; Glacier Select &gt; Amazon Quicksight",
    "isCorrect" : "false"
  }, {
    "id" : "cef7c26348e9409787376dd417d793c0",
    "option" : "On premises data &gt; Amazon S3 &gt; AWS Glue &gt;&gt; Amazon OpenSearch &gt; Amazon Quicksight.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nThe key point is to migrate and analyze data.\nOption A is CORRECT, where the data from on-premise is being migrated to the S3 and using AWS Glue ETL jobs data will be transformed.\nAWS Glue crawlers will create the schema in the glue data catalog that can be used to analyze the data further using Amazon Athena, an interactive query service and good for analyzing data.\nOption B is incorrect as S3 Select is a SQL-like query that is used for filtering objects on S3\nThis cannot be used for large-scale analysis of data.\nOption C is incorrect as Glacier Select is a SQL-like query used for filtering objects on Glacier.\nThis cannot be used for large-scale analysis of data.\nOption D is incorrect because the Amazon OpenSearch service is used to search the data, not for analysis.\nReference:\nhttps://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/\n\n\nThe correct answer is A. On premises data > Amazon S3 > AWS Glue >> Amazon Athena > Amazon Quicksight.\nExplanation:\nOn-premises data: The organization already has data stored in their on-premises system. Amazon S3: Amazon Simple Storage Service (S3) is a highly scalable object storage service. The on-premises data will be migrated to Amazon S3. AWS Glue: AWS Glue is a fully managed ETL (Extract, Transform, and Load) service that can be used to prepare and transform the data for analysis. AWS Glue is used to create a data catalog, extract, transform and load data from various sources such as RDBMS, S3, etc. Amazon Athena: Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Once the data is transformed using AWS Glue, it will be queried using Amazon Athena. Amazon Quicksight: Amazon Quicksight is a business intelligence service that allows users to create interactive dashboards, visualizations, and perform ad-hoc analysis. The data from Amazon Athena will be used in Amazon Quicksight to perform analysis.\nSo, the correct sequence of services used to migrate on-premises data to AWS cloud for analysis purposes is: On premises data > Amazon S3 > AWS Glue >> Amazon Athena > Amazon Quicksight.\n\n"
}, {
  "id" : 427,
  "question" : "You are working for a start-up firm, working on a POC project, in which multiple EC2 instances are launched for an internal project to check Web application performance.\nYou need to pre-warm EC2 instances by initiating them into the desired mode &amp; then moving to Hibernate state.\nYou are looking for IP address changes when EC2 instances move from Running state to Hibernate and back to Running state.\nWhich of the following statements is correct?\n",
  "answers" : [ {
    "id" : "8f470ac0287f4fadae379a57b10ce8b2",
    "option" : "Both Public IPv4 and Private IPv4 addresses are assigned with new IP addresses, whereas any IPv6 address is retained.",
    "isCorrect" : "false"
  }, {
    "id" : "e502a1824ac94bc4a3bbb582af8b2b1d",
    "option" : "Only Public IPv4 addresses are assigned with new IP addresses, whereas Private IPv4 and any IPv6 address are retained.",
    "isCorrect" : "true"
  }, {
    "id" : "245468b7d2784b809bd16dc670d7951c",
    "option" : "Only IPv6 addresses are assigned with new IP addresses, whereas both Private IPv4 and Public IPv4 addresses are retained.",
    "isCorrect" : "false"
  }, {
    "id" : "d509ad0437ab413abcb4fcbb28ee52e0",
    "option" : "All IP addresses assigned to the EC2 instances are released &amp; new IP addresses are allocated to EC2 instances post hibernation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe instance retains its private IPv4 and any IPv6 addresses when hibernated and started.\nAWS releases the public IPv4 address and assigns a new one when you start it.\nOption A is incorrect as when the EC2 instance is hibernated &amp; restarted, there is no change in the private IPv4 address assigned to the EC2 instance.\nOption C is incorrect as when the EC2 instance is hibernated &amp; restarted, there is no change in the public IPv6 address assigned to the EC2 instance.\nOption D is incorrect as when the EC2 instance is hibernated &amp; restarted, there is a change in the public IPv4 address assigned to the EC2 instance while no change in the private IPv4 &amp; IPv6 address assigned to the EC2 instance.\nFor more information on IP Address for EC2 in Hibernate state, refer to the following URL,\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Hibernate.html#instance_hibernate\n\nThe correct answer is B. Only Public IPv4 addresses are assigned with new IP addresses, whereas Private IPv4 and any IPv6 address are retained.\nWhen an EC2 instance is stopped, it loses its associated public IP address (if one is assigned) and receives a new one when it's started again. However, the instance retains its private IP address and any associated IPv6 address.\nWhen an EC2 instance is in hibernation, its in-memory state is saved to the instance store or to Amazon Elastic Block Store (EBS) volumes, and the instance is put into a stopped state. When the instance is resumed from hibernation, it's started from the saved state, and its public IP address is replaced with a new one.\nIn this scenario, the EC2 instances need to be pre-warmed by initiating them into the desired mode and then moving them to hibernate state. This means that the instances will be started and stopped to warm them up, and then put into hibernation mode. When they are resumed from hibernation, their public IP addresses will be assigned new addresses, while their private IPv4 and any IPv6 addresses will be retained.\nTherefore, option B is the correct answer.\n\n"
}, {
  "id" : 428,
  "question" : "As a solutions architect, it is your job to design for high availability and fault tolerance.\nCompany-A is utilizing Amazon S3 to store large amounts of file data.\nYou need to ensure that the files are still available in the case of an entire region facing an outage due to a natural disaster.\nHow can you achieve this?\n",
  "answers" : [ {
    "id" : "9cebb01c039046c7b20ea543e085ccee",
    "option" : "Copy the S3 bucket to an EBS optimized backed EC2 instance",
    "isCorrect" : "false"
  }, {
    "id" : "fa896f1dd1d94451ae324ccfc9987033",
    "option" : "Amazon S3 is highly available and fault tolerant by design and requires no additional configuration",
    "isCorrect" : "false"
  }, {
    "id" : "3c14f1d4dc0640cda2de999b1eaf1b33",
    "option" : "Enable Cross-Region Replication for the bucket",
    "isCorrect" : "true"
  }, {
    "id" : "bdc5fd119b23491e9cd4c0ba0fc1296c",
    "option" : "Enable versioning for the bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nCross-region replication is a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions.\nWe refer to these buckets as source bucket and destination bucket.\nDifferent AWS accounts can own these buckets.\nAWS services are designed with DR considerations in mind.\nS3, for example, achieves 99.999999999% durability and 99.99% availability by redundantly storing data across multiple AZs within a region.\nIt may be rare for the whole AWS region to go down, but it could cause massive permanent damage if we don't plan for it; this is when S3 Cross-Region Replication (CRR) solution comes into play.\nOption A is invalid because this is not the right way to take backups of an S3 bucket.\nOption B is invalid because S3 will ensure that objects are available in multiple availability zones but not across regions in case of a disaster.\nOption D is invalid because versioning can only help from accidental deletion of objects but not from disaster recovery.\nFor more information on Cross-Region Replication, please visit the URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html\nNOTE:\nMost organizations try to implement High Availability (HA) instead of DR to guard them against any downtime of services.\nIn the case of HA, we ensure there exists a fallback mechanism for our services.\nThe service that runs in HA is handled by hosts running in different availability zones but the same geographical region.\nHowever, this approach does not guarantee that our business will be up and running in case the entire region goes down.\nDR takes things to a completely new level, wherein you need to recover from a different region that's separated by over 250 miles.\nOur DR implementation is an Active/Passive model, meaning that we always have minimum critical services running in different regions.\nStill, a major part of the infrastructure is launched and restored when required.\n\nOption B is incorrect. While it is true that Amazon S3 is designed for high availability and fault tolerance, it is not immune to an entire region outage.\nOption A is also incorrect as it suggests copying the data to an EC2 instance. While this may work in some cases, it introduces additional complexity and potential failure points.\nOption C is the correct answer. Enabling Cross-Region Replication for the S3 bucket ensures that the data is replicated to a secondary region, allowing for seamless access to the data in case of an entire region outage. This can be achieved by creating a new S3 bucket in a secondary region and configuring the replication settings on the primary bucket to replicate all data to the secondary bucket.\nOption D, enabling versioning, is not relevant to the scenario as it does not address the issue of data availability in case of an entire region outage. Versioning allows for multiple versions of an object to be stored in an S3 bucket, which can be useful for data backup and recovery purposes, but it does not provide redundancy across regions.\nIn summary, the best solution for ensuring high availability and fault tolerance for large amounts of file data stored in Amazon S3 is to enable Cross-Region Replication for the S3 bucket.\n\n"
}, {
  "id" : 429,
  "question" : "Your company currently has a set of virtual servers that need to be migrated to the AWS Cloud.\nThese Instances are normally 70% utilized and committed to run for a minimum of three years.\nAs a solutions architect which of the following Instance pricing model would you suggest?\n",
  "answers" : [ {
    "id" : "9149cd37908a4a039d2eeda292dc1eb8",
    "option" : "Reserved instances",
    "isCorrect" : "true"
  }, {
    "id" : "6e879aca89b24a3ea314257f4a67376d",
    "option" : "On-demand instances",
    "isCorrect" : "false"
  }, {
    "id" : "a04539024b614d53a59f8aa48fcbb319",
    "option" : "Spot instances",
    "isCorrect" : "false"
  }, {
    "id" : "1d10c90795a2419e949b46f6e90f3615",
    "option" : "Regular instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following on the different instance pricing options.\nAmazon EC2 provides the following purchasing options to enable you to optimize your costs based on your needs:\nOn-Demand Instances - Pay, by the second, for the instances that you launch.\nReserved Instances - Purchase, at a significant discount, instances that are always available, for a term from one to three years.\nSpot Instances - Request unused EC2 instances, which can lower your Amazon EC2 costs significantly.\nReserved Instances provide you with a significant discount (up to 75%) compared to On-Demand instance pricing.\nSince there is a commitment to run for a minimum period of three years, going for the \"Reserved Instances\" would minimise the cost.\nOn-demand can be used but Reserved Instances are most cost-efficient in the given scenario.\nOption B is incorrect because Reserved Instances are more effective.\nOption C is incorrect because in Spot Instances Instance there is no commitment.\nAs soon as the Bid price exceeds the Spot price, a user gets the Instance.\nIn an On-demand Instance, a user has to pay the On-demand rate specified by Amazon.\nOnce they have bought the Instance they have to use it by paying that rate.\nIn Spot Instance, once the Spot price exceeds the Bid price, Amazon will shut the instance.\nThe benefit to the user is that they will not be charged for the partial hour in which the Instance was taken back from them.\nOption D is incorrect because there is nothing like Regular Instances in AWS.\nFor more information on instance pricing options, please visit the URL.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-purchasing-options.html\n\nAs a solutions architect, for virtual servers that need to be migrated to AWS Cloud, and are normally 70% utilized and committed to run for a minimum of three years, the best Instance pricing model to suggest would be Reserved Instances.\nReserved Instances (RIs) provide significant savings over On-demand instances by committing to a certain instance configuration for a specified period, typically one or three years. This pricing model provides a significant cost advantage over on-demand instances, especially for long-term workloads that have predictable and consistent usage patterns.\nOn-demand instances, as the name suggests, are priced at an hourly rate and can be started and stopped as needed, which makes them ideal for short-term workloads or workloads that have unpredictable usage patterns.\nSpot instances, on the other hand, provide a cost-effective option for running workloads that are flexible in terms of their start and end times, as the pricing is based on demand and supply of the available capacity in the market.\nRegular instances refer to the standard pricing model for EC2 instances where customers pay for the compute resources on an hourly basis.\nGiven the scenario described, Reserved Instances provide the most cost-effective option for long-term workloads with predictable and consistent usage patterns, as they offer a significant discount compared to on-demand instances, and provide a fixed rate for the duration of the commitment.\n\n"
}, {
  "id" : 430,
  "question" : "Your company currently has a set of EC2 Instances hosted on the AWS Cloud.\nThere is a requirement to ensure the restart of instances if a CloudWatch metric goes beyond a certain threshold.\nAs a solutions architect, how would you ask the IT admin staff to implement this?\n",
  "answers" : [ {
    "id" : "8b6422504c6d4547b31715d5abcd47f1",
    "option" : "Look at the Cloudtrail logs for events and then restart the Instance based on the events.",
    "isCorrect" : "false"
  }, {
    "id" : "554fc72699a24effbf9b6a9169c43643",
    "option" : "Create a CloudWatch metric which looks into the instance threshold, and assign this metric against an alarm to reboot the instance.",
    "isCorrect" : "true"
  }, {
    "id" : "33da2a89a377442692a29cb46ce98ccb",
    "option" : "Create a CLI script that restarts the server at certain intervals.",
    "isCorrect" : "false"
  }, {
    "id" : "941c4de826014fb5974fe629ba845fde",
    "option" : "Use the AWS Config utility on the EC2 Instance to check for metrics and restart the server.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nUsing Amazon CloudWatch alarm actions, you can create alarms that automatically stop, terminate, reboot, or recover your EC2 instances.\nYou can use the stop or terminate actions to help you save money when you no longer need an instance to be running.\nYou can use the reboot and recover actions to automatically reboot those instances or recover them onto new hardware if a system impairment occurs.\nOption A is incorrect because CloudTrail logs will provide event details and not metrics.\nOption C is incorrect because we want to restart Instance as we reach a certain threshold.\nBut this way, it will keep on restarting the Instance even without any threshold reach.\nOption D is incorrect because AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\nFor more information on using alarm actions, please visit the URL-\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html\n\nAs a solutions architect, you would ask the IT admin staff to implement the following steps to ensure the restart of instances if a CloudWatch metric goes beyond a certain threshold:\nB. Create a CloudWatch metric which looks into the instance threshold, and assign this metric against an alarm to reboot the instance.\nExplanation: CloudWatch is a monitoring service provided by AWS that monitors various AWS resources, such as EC2 instances, databases, and more. CloudWatch provides metrics for resources, which are data points that represent the performance of resources over time. These metrics can be used to set up alarms that trigger actions based on specific conditions.\nTo implement the requirement of restarting instances if a CloudWatch metric goes beyond a certain threshold, you need to create a CloudWatch metric that monitors the performance of the EC2 instances. For example, you can create a metric that monitors the CPU utilization of the EC2 instance.\nOnce you have created the CloudWatch metric, you can then set up an alarm based on that metric. The alarm can be configured to trigger an action, such as restarting the instance, when the metric goes beyond a certain threshold. For example, you can set up an alarm that triggers a restart action when the CPU utilization of the instance goes beyond 80%.\nTo create an alarm based on a CloudWatch metric, you need to follow these steps:\nOpen the CloudWatch console. Select the metric you want to monitor. Choose the Create Alarm button. Configure the alarm threshold and actions.\nBy following these steps, you can ensure that your EC2 instances are automatically restarted if a CloudWatch metric goes beyond a certain threshold. This helps to ensure the availability and performance of your applications running on the EC2 instances.\n\n"
}, {
  "id" : 431,
  "question" : "You have a read-intensive application hosted in AWS.\nThe application is currently using the MySQL RDS feature in AWS.\nThe CloudWatch metrics show high-read throughput on the database and are causing performance issues on the database.\nWhich of the following can be used to increase the read throughput on the MySQL database?\n",
  "answers" : [ {
    "id" : "c1390d6e75ce49e2a1d03643f849cfce",
    "option" : "Enable the Multi-AZ on the MySQL RDS.",
    "isCorrect" : "false"
  }, {
    "id" : "9c625225b9784a59925b1d55a7b0479f",
    "option" : "Use Cold Storage Volumes for the MySQL RDS.",
    "isCorrect" : "false"
  }, {
    "id" : "11ac15dfb56f4b539cc8f16becbf45f1",
    "option" : "Enable Read Replica.",
    "isCorrect" : "true"
  }, {
    "id" : "6a83a61b01164df79ec6e82f19b799de",
    "option" : "Use SQS to queue up the reads.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS documentation mentions the following on Read Replica.\nAmazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances.\nThis replication feature makes it easy to elastically scale out beyond a single DB Instance's capacity constraints for read-heavy database workloads.\nYou can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.\nRead replicas can also be promoted when needed to become standalone DB instances.\nOption A is invalid since this is used for fault tolerance for the database.\nOption B is invalid since this is not the ideal storage mechanism for databases that require high read throughput.\nOption D is invalid since SQS is used as a decoupling component and would not be ideal for reducing the reads on the database.\nFor more information on Read Replica, please visit the below URL-\nhttps://aws.amazon.com/rds/details/read-replicas/\n\nThe option that can be used to increase the read throughput on the MySQL database is to enable Read Replica, which is answer C.\nRead Replica is an AWS RDS feature that enables you to create a replica of your database instance. This replica can be used to offload the read traffic from your primary instance, thereby increasing the read throughput. The Read Replica is an exact copy of your primary instance, which is asynchronously replicated from the primary instance. Read replicas can be used to perform read-intensive operations such as reporting, analytics, and backups, without affecting the performance of the primary instance.\nThe other options listed are not relevant for increasing the read throughput of the MySQL database.\nEnabling Multi-AZ on the MySQL RDS is used for high availability and redundancy purposes, and it does not increase the read throughput.\nUsing Cold Storage Volumes for the MySQL RDS is used to store data that is rarely accessed, and it does not affect the read throughput.\nUsing SQS to queue up the reads is not applicable to RDS instances. SQS is a messaging service that enables decoupling and scalability between components of an application. It is not designed for increasing read throughput on a database.\nIn summary, to increase the read throughput on the MySQL RDS instance in AWS, you should enable Read Replica.\n\n"
}, {
  "id" : 432,
  "question" : "You are working as an AWS Architect for a software company.\nYou are working on a new project which involves an application deployed on twenty C5 EC2 On-demand Instances with Elastic IP attached to each instance.\nDuring peak hours, when you are initiating new instances, a considerable delay is observed.\nYou perform a pilot test for the option of initiating these Instances and hibernating so that during peak hours, these instances could be quickly launched. It works fine during the pilot phase.\nYou are recommending this option to be implemented in production.\nThe management team is concerned about the pricing of many EC2 instances in the Hibernate state.\nWhat is considered to calculate the pricing for an EC2 instance in the Hibernate state?\n",
  "answers" : [ {
    "id" : "32a1a58f81814120815e2de96a6a7443",
    "option" : "Elastic IP address and EBS volumes attached to EC2 Instance",
    "isCorrect" : "true"
  }, {
    "id" : "4552051c5ec64ec595a6e483058e8779",
    "option" : "Total Compute capacity per hour, Elastic IP address and EBS volumes attached to EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "d67c0f43735b4faa9c43e4a23c88055f",
    "option" : "Total Compute capacity per hour and EBS volumes attached to EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "d605d543ea6e4fe18a11fdb273b6a1a0",
    "option" : "Total Compute capacity per hour &amp; Elastic IP address attached to EC2 Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer -A.\nWhen an EC2 instance is in the Hibernate state, you pay only for the EBS volumes and Elastic IP Addresses attached to it.\nOptions B, C, and D are incorrect because when an EC2 instance is in a hibernate state, compute capacity charges are not applicable.\nThe charges are only applicable for the EBS volumes and Elastic IP Addresses attached to it.\nFor more information on pricing for an EC2 instance in Hibernate state, refer to the following URL-\nhttps://aws.amazon.com/blogs/aws/new-hibernate-your-ec2-instances/\n\nWhen an EC2 instance is hibernated, it is in a stopped state, and the contents of its memory are saved to the attached EBS storage. The hibernation feature is useful in scenarios where you want to quickly start an instance with its previously saved state. The hibernation feature is supported for specific instance types, including the C5 instance type.\nWhen an instance is in a hibernated state, you are not charged for the instance's running time or instance-hour usage. Instead, you are charged for the EBS storage that is attached to the instance and the Elastic IP address that is associated with the instance.\nTherefore, the correct answer to the question is option A: Elastic IP address and EBS volumes attached to the EC2 instance. These are the only components that are charged when an EC2 instance is in a hibernated state.\nOption B is incorrect because it includes \"Total Compute capacity per hour,\" which is not charged when an EC2 instance is in a hibernated state. Option C is incorrect because it only includes \"Total Compute capacity per hour\" and \"EBS volumes attached to the EC2 instance,\" but it does not include the Elastic IP address, which is also charged when an EC2 instance is in a hibernated state. Option D is incorrect because it only includes \"Total Compute capacity per hour\" and \"Elastic IP address,\" but it does not include the EBS volumes attached to the EC2 instance, which are also charged when an EC2 instance is in a hibernated state.\n\n"
}, {
  "id" : 433,
  "question" : "Your company has started hosting its databases on the Amazon RDS.\nAs an architect, they have requested you to advise the IT admin staff on monitoring the underlying databases.\nNotifications should be sent to IT admin staff if any issues are detected.\nWhich AWS services can accomplish these requirements? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "36dacee39df341a496e440d7b8992298",
    "option" : "Amazon Simple Email Service",
    "isCorrect" : "false"
  }, {
    "id" : "b7490d74ceb14852bc0484ca4e72c3d2",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "true"
  }, {
    "id" : "0867f25b02c748b39148bb76842ed821",
    "option" : "Amazon Simple Queue Service (SQS)",
    "isCorrect" : "false"
  }, {
    "id" : "b4e6616672974e31a799617b51e310af",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "ee489129fc70415b88b4f34bfb6f323b",
    "option" : "Amazon Simple Notification Service (SNS)",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and E.\nThe AWS Documentation mentions the following.\nYou can monitor DB instances using Amazon CloudWatch, which collects and processes raw data from Amazon RDS into readable, near real-time metrics.\nThese statistics are recorded for a period of two weeks.\nFor that, you can access historical information and gain a better perspective on how your web application or service is performing.\nOption A is invalid since this an email service and not a notification service.\nOption C is invalid since this is a queuing service.\nOption D is invalid since this is a domain name service.\nFor more information on monitoring databases, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Monitoring.html\n\nThe two AWS services that can accomplish the requirements of monitoring and sending notifications for databases hosted on Amazon RDS are Amazon CloudWatch and Amazon Simple Notification Service (SNS).\nAmazon CloudWatch is a monitoring service that provides a comprehensive view of resources and applications running on AWS. CloudWatch can monitor RDS instances and capture a wide range of performance metrics, such as CPU utilization, memory usage, network traffic, and disk I/O. These metrics can be used to set alarms that trigger notifications to IT admin staff when predefined thresholds are breached.\nTo set up CloudWatch monitoring for an RDS instance, you can use the CloudWatch console or AWS CLI. You can also use CloudWatch Logs to capture and analyze log data generated by RDS instances.\nAmazon Simple Notification Service (SNS) is a notification service that can send messages or alerts to various endpoints, such as email addresses, SMS text messages, and mobile push notifications. You can use SNS to set up alerts and notifications based on CloudWatch alarms that trigger when RDS performance metrics exceed predefined thresholds.\nTo set up SNS notifications for CloudWatch alarms, you can use the SNS console or AWS CLI. You can specify one or more endpoints that should receive the notifications, such as email addresses or phone numbers. You can also customize the notification messages and formats.\nIn summary, using Amazon CloudWatch and Amazon Simple Notification Service (SNS) together can provide comprehensive monitoring and notification capabilities for RDS databases hosted on AWS. CloudWatch captures RDS performance metrics, while SNS delivers notifications based on predefined thresholds.\n\n"
}, {
  "id" : 434,
  "question" : "Your company has started hosting its data store on AWS by using the Simple Storage service.\nThey are storing files that are downloaded by users frequently.\nAfter 3 months, the files need to be transferred to archive storage since they are not used beyond this point.\nWhich of the following could be used to manage this requirement effectively?\n",
  "answers" : [ {
    "id" : "d36cf2e8768648aa99b6929d4919e0e4",
    "option" : "Transfer the files via scripts from S3 to Glacier after a period of 3 months.",
    "isCorrect" : "false"
  }, {
    "id" : "da17dd9fac02420c847724f73a48cc60",
    "option" : "Use Lifecycle policies to transfer the files onto Glacier after a period of 3 months.",
    "isCorrect" : "true"
  }, {
    "id" : "80c1f6ab488e4f5f9b116ee786a86e45",
    "option" : "Use Lifecycle policies to transfer the files onto Cold HDD after a period of 3 months.",
    "isCorrect" : "false"
  }, {
    "id" : "6b4c50e2020249b298a64cbbf3ea4049",
    "option" : "Create a snapshot of the files in S3 after a period of 3 months.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nTo manage your objects so that they are stored cost-effectively throughout their lifecycle, you need to configure their lifecycle.\nA lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects.\nThere are two types of actions:\nTransition actions-Define when objects transition to another storage class.\nFor example, you might choose to transition objects to the STANDARD_IA storage class 30 days after you created them or archive objects to the GLACIER storage class one year after creating them.\nExpiration actions-Define when objects expire.\nAmazon S3 deletes expired objects on your behalf.\nThe lifecycle expiration costs depend on when you choose to expire objects.\nOption A is invalid since there is already the option of lifecycle policies.\nOption C is invalid since lifecycle policies are used to transfer to Glacier or S3-Infrequent Access.\nOption D is invalid since snapshots are used for EBS volumes.\nFor more information on S3 lifecycle policies, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\n\nThe most effective way to manage this requirement is to use Lifecycle policies in Amazon S3 to automatically transfer the files to a lower-cost storage class after a period of 3 months. This is because Amazon S3 offers multiple storage classes that provide different levels of durability, availability, performance, and cost.\nOption A is not the best approach because it involves transferring the files manually using scripts, which is a time-consuming and error-prone process. In addition, it is difficult to keep track of which files have been transferred and which files have not.\nOption B is the correct answer because Lifecycle policies allow you to automate the process of transferring objects to another storage class or deleting them after a specified period of time. You can set up a policy that transfers the files to Amazon S3 Glacier after 3 months of inactivity, which is a lower-cost storage class designed for data archiving and long-term backup.\nOption C is not recommended because Cold HDD is a storage class optimized for infrequently accessed data that requires high throughput and low cost. It is not suitable for archiving data that is not accessed beyond a certain point, as it is not designed for long-term storage.\nOption D is also not recommended because creating a snapshot of the files in S3 after 3 months would not transfer them to a lower-cost storage class. Snapshots are point-in-time copies of a volume that can be used to restore data, but they do not provide a way to archive data in a cost-effective manner.\nIn conclusion, the best way to manage this requirement is to use Lifecycle policies to transfer the files to Amazon S3 Glacier after a period of 3 months. This approach is cost-effective, scalable, and easy to automate.\n\n"
}, {
  "id" : 435,
  "question" : "Your company plans to set up a VPC with private and public subnets and host EC2 Instances in the subnet.\nIt has to be ensured that instances in the private subnet can download updates from the internet.\nWhich of the following needs to be part of the architecture for this requirement?\n",
  "answers" : [ {
    "id" : "5f1c592c60244bcf9975f901cf19bf40",
    "option" : "WAF",
    "isCorrect" : "false"
  }, {
    "id" : "b348f7d73bb34db0ab995471c0510e4b",
    "option" : "Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "1124d9948d1a446d87e3fd1452e9c4f9",
    "option" : "NAT Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "84166d9b95ab4eba8339f30d4d2a28f0",
    "option" : "VPN.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nYou can use a network address translation (NAT) gateway to enable instances in a private subnet to connect to the internet or other AWS services.\nBut you prevent the internet from initiating a connection with those instances.\nOption A is invalid since this is a web application firewall.\nOptions B and D are invalid since they are used to connect on-premises infrastructure to AWS VPCs.\nFor more information on NAT gateway, please visit the below URL-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html\n\nThe correct answer to the question is C. NAT Gateway.\nTo understand why, let's break down the options and their relevance to the question.\nA. WAF (Web Application Firewall) is used to protect web applications from common web exploits such as SQL injection, cross-site scripting, and others. However, it's not related to the requirement of allowing instances in the private subnet to download updates from the internet.\nB. Direct Connect is a service that establishes a dedicated network connection from an on-premises environment to AWS. It's not relevant to the question as it doesn't address the requirement of allowing instances in the private subnet to download updates from the internet.\nC. NAT (Network Address Translation) Gateway is a managed AWS service that enables instances in a private subnet to connect to the internet or other AWS services while preventing inbound connections from the internet. It acts as a gateway between the private subnet and the internet by translating the private IP addresses of instances in the private subnet to a public IP address. NAT Gateway is the correct answer to the question because it allows instances in the private subnet to download updates from the internet while also providing a layer of security by preventing direct inbound connections.\nD. VPN (Virtual Private Network) is a secure and encrypted connection that allows remote users to access an organization's internal network over the internet. VPN is not relevant to the question as it doesn't address the requirement of allowing instances in the private subnet to download updates from the internet.\nIn conclusion, the correct answer is C. NAT Gateway, as it allows instances in the private subnet to download updates from the internet while providing a layer of security by preventing direct inbound connections.\n\n"
}, {
  "id" : 436,
  "question" : "Your company is planning on the following architecture for their application. A set of EC2 Instances hosting the web part of the application. A relational database for the backend. A Load balancer for distribution of traffic. A NAT gateway for routing traffic from the database server to the Internet. Which of the following architecture ensures high availability across all components?\n",
  "answers" : [ {
    "id" : "a754944cbd1a44faa64c19e63d991880",
    "option" : "A Load balancer with one public subnet, one private subnet. The EC2 Instances placed in one Availability Zone. RDS with Multi-AZ Enabled. NAT Gateway in one availability zone.",
    "isCorrect" : "false"
  }, {
    "id" : "f3cc33ed6f41479485296e6e52bd416e",
    "option" : "A Load balancer with 2 public subnets, 2 private subnets. The EC2 Instances placed across 2 Availability Zones. RDS with Multi-AZ Enabled. NAT Gateways in each availability zone.",
    "isCorrect" : "true"
  }, {
    "id" : "8c29a88a1d1646a7871d6c005b814856",
    "option" : "A Load balancer with 2 public subnets, 2 private subnets. The EC2 Instances placed in 2 Availability Zones. RDS with Multi-AZ Enabled. NAT Gateway in one availability zone.",
    "isCorrect" : "false"
  }, {
    "id" : "db01b45e9c344a4aa99a2271e13764bd",
    "option" : "A Load balancer with 2 public subnets, 2 private subnets. The EC2 Instances placed in one Availability Zone. RDS with Multi-AZ Enabled. NAT Gateway in one availability zone.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nIn a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone.\nRunning a DB instance with high availability can enhance availability during planned system maintenance and help protect your databases against DB instance failure and Availability Zone disruption.\nOption A is incorrect because High Availability is not ensured.\nWe recommend that you enable multiple Availability Zones.\nWith this configuration, if one Availability Zone becomes unavailable or has no healthy targets, the load balancer can continue to route traffic to the healthy targets in another Availability Zone.\nOption B is Correct because High Availability is ensured.\nIf one of the AZs should fail, then the EC2 instances in the remaining private subnet will still be able to communicate with the Internet because they have their own NAT Gateway in the same AZ.\nOptions C and D are incorrect because High Availability is not ensured as either if we have EC2 in a single AZ or multiple AZ.\nWe have NAT Gateway in single AZ is a cause for not ensuring High Availability.\nHowever, if there is a failure with Availability Zone A (rare, but can happen), then the NAT Gateway is not reachable from Private-Subnet-B.\nThus, the system may be impacted even though it runs across two AZs or a single AZ.\nFor more information on Elastic Load Balancing, Multi-AZ, and NAT gateway, please refer to the below URLs:-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\n\nThe correct answer is B. A Load balancer with 2 public subnets, 2 private subnets. The EC2 Instances placed across 2 Availability Zones. RDS with Multi-AZ Enabled. NAT Gateways in each availability zone.\nExplanation:\nIn order to ensure high availability across all components, we need to have redundancy and fault tolerance built into the architecture. This includes having multiple instances of each component spread across different Availability Zones (AZs) and enabling Multi-AZ for the database.\nHere's why each option is right or wrong:\nOption A: This architecture places all EC2 instances and the NAT Gateway in one Availability Zone, which means they are not redundant. If that AZ goes down, the entire application will go down. RDS with Multi-AZ enabled provides redundancy, but only for the database.\nOption B: This architecture includes two public and two private subnets spread across two Availability Zones. The EC2 instances are also spread across both AZs, providing redundancy. RDS with Multi-AZ enabled provides redundancy for the database, and NAT Gateways in each AZ ensure fault tolerance.\nOption C: This architecture is similar to Option A, with all EC2 instances placed in two Availability Zones but only one NAT Gateway in one AZ. This means there is no fault tolerance for the NAT Gateway.\nOption D: This architecture has multiple subnets spread across two AZs, but all EC2 instances are in one AZ and there is only one NAT Gateway in one AZ. This means there is no redundancy or fault tolerance for the EC2 instances or the NAT Gateway.\nTherefore, the correct option is B, as it provides the necessary redundancy and fault tolerance for all components of the architecture.\n\n"
}, {
  "id" : 437,
  "question" : "Your company has an AWS account and a lot of resources defined in the Frankfurt region.\nThey want to use an API monitoring service to track the calls to the resources in their account.\nWhich of the following should be used for this purpose?\n",
  "answers" : [ {
    "id" : "fd5994854a8a4424b9fb8b7e1972c615",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "053ca9f12292454b9321e45332a4efa6",
    "option" : "AWS CloudTrail",
    "isCorrect" : "true"
  }, {
    "id" : "5fbe01b468b84cf1a308c654d5e7386d",
    "option" : "AWS CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "ca8e95ae72fa49b599896e954463b5d1",
    "option" : "AWS Opswork.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect because AWS Config is a fully managed service that provides you with a resource inventory, configuration history, and configuration change notifications to enable security and governance.\nYou can also discover existing AWS resources, export a complete inventory of your AWS resources with all configuration details, and determine how a resource was configured at any point in time.\nOption B is correct because this is an API monitoring service and using CloudTrail.\nYou can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nCloudTrail provides the event history of your AWS account activity, including actions taken through the AWS management console, AWS SDKs, command-line tools, and other AWS services.\nThis event history simplifies security analysis, resource change tracking, and troubleshooting.\nBesides, you can use CloudTrail to detect unusual activity in your AWS accounts.\nOption C is incorrect because this is a metric and logging service.\nOption D is incorrect because it is used to deploy stacks of resources.\nCloudWatch and Config serve distinct use cases for monitoring and complements each other from the AWS ecosystem.\nConfig is typically used for auditing and compliance purposes across organizations to verify whether AWS resource changes being made are per compliance rules.\nCloudWatch is designed to provide performance information about AWS resources such as EC2, Lambda, etc.\nDevelopers can use information from CloudWatch to identify bottlenecks in applications or workflows.\nCloudWatch will help you send alerts when CPU /Memory utilization reaches a certain threshold and browse metrics associated with CPU/Network to identify operational and security issues.\nFor more information on AWS CloudTrail, please refer to the below URL-\nhttps://aws.amazon.com/cloudtrail/\n\nThe correct answer for this scenario is C. AWS CloudWatch.\nAWS CloudWatch is a monitoring service that allows you to monitor your AWS resources and the applications you run on AWS. It provides a unified view of your AWS resources, services, and applications that run on AWS and on-premises servers. CloudWatch can be used to collect and track metrics, collect and monitor log files, and set alarms. CloudWatch provides data and actionable insights to monitor your applications, respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health.\nIn the given scenario, the company wants to use an API monitoring service to track the calls to the resources in their account. AWS CloudWatch can be used to collect and track metrics related to API calls, such as the number of requests, latency, and error rates. You can create custom metrics to monitor the specific APIs that are critical to your business, and set up alarms to alert you when the metrics breach certain thresholds.\nAWS Config is a service that enables you to assess, audit, and evaluate the configuration of your AWS resources. It provides a detailed inventory of your AWS resources and the relationships between them, as well as a history of configuration changes. AWS Config can help you ensure compliance with regulatory standards, security policies, and operational best practices.\nAWS CloudTrail is a service that enables you to monitor, log, and retain account activity related to actions across your AWS infrastructure. It provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nAWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. It automates the deployment, configuration, and management of applications and infrastructure across AWS and on-premises environments.\nIn summary, for API monitoring purposes in the given scenario, AWS CloudWatch is the most appropriate service to use.\n\n"
}, {
  "id" : 438,
  "question" : "A Solutions architect has been asked to design an order processing e-commerce application on AWS where synchronous transaction processing is not required.\nSome of the synchronous order processing tasks might take a long time and require integrations with external applications. How can this be achieved?\n",
  "answers" : [ {
    "id" : "caf2de950aa04ca19c3f7ba6d6632cab",
    "option" : "Use AWS Simple Workflow Service to design activities coordination between different components of the application.",
    "isCorrect" : "true"
  }, {
    "id" : "5af26f561b0d493ebe3487afe7cdd644",
    "option" : "Use Step functions to design activities coordination between different components of the application.",
    "isCorrect" : "false"
  }, {
    "id" : "d58fcb7f7bb64b8fae08cb4ea0518b94",
    "option" : "Use API Gateway and Lambda to design the application.",
    "isCorrect" : "false"
  }, {
    "id" : "d99b0390a927423497570ad277617b38",
    "option" : "Use SQS to coordinate the activities of the application.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer : A.\nDifferent between Steps function and SWF.\nStep Functions.\nSWF.\nGood for any new serverless application where coordination is required between various components using visual workflow.\nNeed external signals to intervene in processes, OR Good if there are child processes and those require passing signals to parents.\nEasy to use while developing application.\nMore complex while developing application but complete control of orchestration logic.\nUses declarative JAON to write state machine.\nNeed to write decider program (programing of your choice) to separate activities between steps or use AWS flow framework.\nServerless, lower admin overhead.\nUses servers.\nShort running workflows.\nLong-running workflows,\nMostly used for synchronous tasks.\nMostly used for asynchronous tasks.\nNew AWS Service, less complex applications.\nLegacy application, Complex decisions (custom decide application)\nIntegrate with AWS Mechanical Turk.\nThe main point is to design an application where some tasks can take longer than expected.\nFor example, address validation.\nAlso, this application should support integration with other AWS services.\nOption A is CORRECTsince this SWF is the right fit for long-running tasks.\nSWF can manage activities in an async way and support integration with other AWS services.\nSee the difference table for more details.\nSWF helps manage tasks sequentially or in a parallel way, and this is a must required for an order processing application.\nOption B is incorrect because the Step function is not good for long-running activities.See the difference table for more details.\nOption C is incorrect because managing workflow with Lambda and API gateway would not be the right solution.\nOption D is incorrect because SQS is queue management.\nSQL enables decoupling applications.\n\n\nThe requirement is to design an order processing e-commerce application on AWS where synchronous transaction processing is not required. This means that the application can handle long-running tasks and can integrate with external applications asynchronously. To achieve this, we can use AWS services that provide asynchronous messaging and coordination capabilities.\nOption A - Use AWS Simple Workflow Service (SWF) to design activities coordination between different components of the application: AWS SWF is a fully managed workflow service that provides the coordination and execution of distributed applications. It allows you to coordinate the execution of different components of an application and manage their state. However, AWS SWF is a complex service and requires you to write code to implement the workflows. Moreover, SWF has been replaced by AWS Step Functions.\nOption B - Use Step functions to design activities coordination between different components of the application: AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and microservices using visual workflows. It allows you to define, run, and debug workflows that integrate with AWS services, such as AWS Lambda and AWS Batch. AWS Step Functions also provides built-in error handling and retry logic, making it a more robust solution than SWF.\nOption C - Use API Gateway and Lambda to design the application: This option involves building the application using a combination of AWS Lambda and API Gateway. You can use Lambda functions to perform the order processing tasks, and API Gateway to create a RESTful API that exposes the Lambda functions as HTTP endpoints. However, this approach does not provide coordination between the different components of the application, and you would need to implement this manually.\nOption D - Use SQS to coordinate the activities of the application: Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. You can use SQS to store messages that represent the order processing tasks and have the different components of the application read from the queue asynchronously. SQS provides durable storage and automatically scales to handle the load, making it a robust solution for this use case.\nOverall, the most suitable option for the given requirements is Option B - Use Step functions to design activities coordination between different components of the application, as it provides a fully managed, visual workflow solution with built-in error handling and retry logic. Option D - Use SQS to coordinate the activities of the application, can also be a viable solution, depending on the specific use case and requirements.\n\n"
}, {
  "id" : 439,
  "question" : "A company is planning to host an application with the below architecture. Â· A lambda function that reads the metadata of objects from an S3 bucket. Â· The Lambda function then stores the metadata in DynamoDB and AWS RDS - MySQL. Which of the following needs to be in place to ensure the above architecture is high available?\n",
  "answers" : [ {
    "id" : "724664c6d3a544b4b4ecec9225ae544b",
    "option" : "Enable Cross Region Replication for the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "12d78ea01450498f89d3c88ca595e638",
    "option" : "Enable Lambda functions in Multiple Availability Zones.",
    "isCorrect" : "false"
  }, {
    "id" : "7bcb14fbbd6b4fcd806d763e8fbbe4ff",
    "option" : "Enable Multi-AZ for the MySQL database.",
    "isCorrect" : "true"
  }, {
    "id" : "43532f882b614937b5d9700ef6d7cc0b",
    "option" : "Enable Auto-Scaling for the DynamoDB table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOption C is correct because, in a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone.\nThe primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.\nRunning a DB instance with high availability can enhance availability during planned system maintenance and help protect your databases against DB instance failure and Availability Zone disruption.\nOption A is invalid because the S3 service is already a highly available service within a particular region.\nAlso, Amazon S3 gives any developer access to the same highly scalable, highly available, fast, inexpensive data storage infrastructure that Amazon uses to run its own global network of web sites.\nThe S3 Standard storage class is designed for 99.99% availability, the S3 Standard-IA storage class is designed for 99.9% availability, the S3 One Zone-IA storage class is designed for 99.5% availability, and the S3 Glacier and S3 Glacier Deep Archive class are designed for 99.99% availability and SLA of 99.9%\nAll of these storage classes are backed by the Amazon S3 Service Level Agreement.\nOptions B is invalid because AWS Lambda is already a highly available service in AWS.\nRefer below document.\nhttps://aws.amazon.com/lambda/features/\nOption D is invalid because High Availability is about availability; AS is about performance (usually throughput), also DynamoDB is high available by default.\nHigh Availability focuses on maintaining the liveness of the system in the presence of server or network failures.\nAt the same time, Auto Scaling means adding more resources when demand increases.\nRefer to the below document for DynamoDB reliability.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\nFor more information on RDS Multi-AZ, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html\n\nTo ensure high availability for the given architecture, we need to consider potential points of failure and design the system in a way that can tolerate them. Here are the explanations for each option:\nA. Enable Cross-Region Replication for the S3 bucket: Cross-Region Replication can help ensure high availability and durability by replicating the S3 bucket's objects to another region automatically. This way, if the primary region becomes unavailable, the objects are still accessible from the replicated region. However, this option doesn't directly address the high availability of the Lambda function and the databases.\nB. Enable Lambda functions in Multiple Availability Zones: AWS Lambda automatically deploys functions across multiple Availability Zones (AZs) within a region for high availability. This option can help ensure that the Lambda function continues to run even if one of the AZs becomes unavailable.\nC. Enable Multi-AZ for the MySQL database: Multi-AZ deployment in Amazon RDS provides high availability and durability by automatically replicating the database to a standby instance in a different AZ. In the event of a planned or unplanned outage, Amazon RDS automatically switches to the standby instance to minimize downtime. This option can help ensure that the MySQL database remains available even if the primary instance fails.\nD. Enable Auto-Scaling for the DynamoDB table: DynamoDB Auto Scaling automatically adjusts the read and write capacity of a DynamoDB table based on traffic patterns. This option can help ensure that the table can handle sudden spikes in traffic and remains available during periods of high demand.\nBased on the above explanations, options B, C, and D can all contribute to ensuring high availability for the architecture. However, enabling Cross-Region Replication for the S3 bucket may not be sufficient to ensure the high availability of the entire architecture. Therefore, the best answer would be a combination of options B, C, and D.\n\n"
}, {
  "id" : 440,
  "question" : "A company has a set of EC2 Instances hosting a revenue generating applications.\nSome of the data on the root EBS volumes are critical to retain.\nHence it has to be ensured that even after the instances are terminated, the EBS volumes will remain intact.\nWhich of the following needs to be done to ensure this requirement can be met?\n",
  "answers" : [ {
    "id" : "30263e71f0fa43748efba8be6f95fc60",
    "option" : "Enable the DisableApiTermination for the EC2 Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "02fc4a08ecb9474faf5698b47e7aea7b",
    "option" : "Make the attribute of DeleteOnTermination for the EBS volume to false.",
    "isCorrect" : "true"
  }, {
    "id" : "ef1c95f2306740c6b1382789d787109f",
    "option" : "Run the command, (aws ec2 delete-volume â€“f) in AWS Command Line Interface to disable deletion of volume on terminating Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "793aed751079400ea60dffccaf256dd7",
    "option" : "Run the command ( aws ec2 modify-instance-attribute\\--block-device-mappings `DeviceName=/dev/sda1,Ebs={DeleteOnTermination-false} ) in AWS Command Line Interface to disable deletion of volume on terminating Instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nOption B is correct because When an instance is terminated, Amazon Elastic Compute Cloud (Amazon EC2) uses the value of the DeleteOnTermination attribute for each root EBS volume to determine whether to preserve or delete the volume when the instance is terminated.\nBy default, the DeleteOnTermination attribute for an instance's root volume is set to true, but it is set to false for all other volume types.\nTo preserve the root volume when an instance is terminated, change the DeleteOnTermination attribute for the root volume to false.\nOption A is invalid since the flag needs to be set on the EBS volume.\nOption C is invalid because (-f) no such parameter exists as per AWS Documentation, and this command is used to delete the volume.\nSee the below correct syntax to delete a volume using CLI.\naws ec2 delete-volume --volume-id vol-049df61146cXXXX.\nOption D is invalid because command syntax is not correct, (=) should be used, but (-) is used, see the actual syntax below.\naws ec2 modify-instance-attribute\\ --instance-id i-1234567890abcdef0 \\ --block-device-mappings 'DeviceName=/dev/sda1,Ebs={DeleteOnTermination=false}'\nIn AWS Documentation, the format is given in JSON, so if you follow that, you have to specify the JSON in a file mapping.json.\n[{\\\"DeviceName\\\": \\\"/dev/sda1\\\",\\\"Ebs\\\":{\\\"DeleteOnTermination\\\":false}}]\nAnd run the command as below.\naws ec2 modify-instance-attribute --block-device-mappings.\nfile://mapping.json.\nIf you don't want to follow this, another syntax provided above is also correct.\nFor more information on the Delete on termination flag, please refer to the below URLs-\nhttps://aws.amazon.com/premiumsupport/knowledge-center/deleteontermination-ebs/\nhttps://docs.aws.amazon.com/cli/latest/reference/ec2/modify-instance-attribute.html\n\nThe correct answer to the question is option B: Make the attribute of DeleteOnTermination for the EBS volume to false.\nWhen an EC2 instance is terminated, by default, all its associated EBS volumes are also terminated, which means that all the data stored on those volumes is lost. However, if you want to retain the data on the EBS volumes even after the instance is terminated, you can modify the DeleteOnTermination attribute of the EBS volume to false.\nThis can be done in the following steps:\nOpen the Amazon EC2 console. In the navigation pane, choose \"Volumes\". Select the EBS volume that you want to modify. Choose \"Actions\", and then choose \"Modify Volume Attribute\". In the \"Modify Volume Attribute\" dialog box, clear the \"Delete on Termination\" check box. Choose \"Save\".\nThis will ensure that the EBS volume will not be deleted when the associated EC2 instance is terminated.\nNow let's take a look at the other options and why they are incorrect:\nOption A: Enable the DisableApiTermination for the EC2 Instance. This option will prevent the termination of the EC2 instance, but it will not prevent the deletion of the associated EBS volumes.\nOption C: Run the command, (aws ec2 delete-volume -f) in AWS Command Line Interface to disable deletion of volume on terminating Instances. This command will not disable the deletion of EBS volumes on instance termination, but instead, it will forcefully delete the EBS volume, which is the opposite of what the question is asking for.\nOption D: Run the command ( aws ec2 modify-instance-attribute--block-device-mappings 'DeviceName=/dev/sda1,Ebs={DeleteOnTermination-false} ) in AWS Command Line Interface to disable deletion of volume on terminating Instances. This command will modify the instance attribute of the block device mappings to retain the EBS volume when the instance is terminated. However, this command is not the best solution to meet the requirement of retaining the EBS volume, as it modifies the instance attribute instead of modifying the EBS volume attribute directly.\n\n"
}, {
  "id" : 441,
  "question" : "A company has a set of EC2 Instances hosted in a VPC.\nThe IT Security department has specified that they need to ensure they get a list of IP addresses for all sources making requests to the EC2 Instances.\nWhich one of the following could help achieve this requirement?\n",
  "answers" : [ {
    "id" : "a914af1f034b4f11aee44f8c35f806a1",
    "option" : "AWS VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "55375c7d363a43a5bc4efb65e93d2332",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "15ff673fd8e64fd7ac7becf3fb796f47",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "f2b0923293f7451f93d33bebe37e239e",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nThe correct answer is A. AWS VPC Flow Logs.\nExplanation: AWS VPC (Virtual Private Cloud) Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. VPC Flow Logs can help you troubleshoot network connectivity issues, monitor network activity, and analyze traffic patterns.\nIn this scenario, the IT Security department needs to ensure that they get a list of IP addresses for all sources making requests to the EC2 Instances. By enabling VPC Flow Logs for the VPC hosting the EC2 instances, you can capture information about the IP traffic going to and from the EC2 instances. You can then use this information to determine the source IP addresses of the requests made to the EC2 instances.\nAWS Cloudwatch (option B) is a monitoring service for AWS resources and applications. While it can be used to monitor EC2 instances, it does not provide the level of detail required to identify the source IP addresses of requests to the EC2 instances.\nAWS CloudFormation (option C) is a service that helps you model and set up your AWS resources so you can spend less time managing those resources and more time focusing on your applications. It is not directly related to capturing IP addresses of sources making requests to EC2 instances.\nAWS Trusted Advisor (option D) is a service that provides guidance and recommendations to help you optimize your AWS infrastructure. It is not directly related to capturing IP addresses of sources making requests to EC2 instances.\nTherefore, the correct answer is A. AWS VPC Flow Logs.\n\n"
}, {
  "id" : 442,
  "question" : "A company has many Amazon S3 buckets across many different AWS accounts.\nA company has a new compliance and audit requirement where all the S3 buckets server access log should be collected and centralized into an Audit AWS account. How would you design this solution?\n",
  "answers" : [ {
    "id" : "209ab1e6e2014f60aa4ab9c7b30eea8c",
    "option" : "Collect all S3 buckets server access logs in separate S3 buckets per account. Enable replication in the S3 server access log buckets to copy the logs to a centralized destination S3 bucket in the Audit account.",
    "isCorrect" : "false"
  }, {
    "id" : "ea4a1b74f22d45f58c4a1d796be3fc8b",
    "option" : "Directly enable the server access logs for the S3 buckets in all the AWS accounts to a centralized destination S3 bucket in the Audit account.",
    "isCorrect" : "false"
  }, {
    "id" : "4bbb3e076ce54dacb4d14a0be9af8a2e",
    "option" : "Collect all S3 buckets server access logs in one S3 bucket per account. Enable replication in the S3 server access log buckets to copy the logs to a centralized destination S3 bucket in the Audit account.",
    "isCorrect" : "true"
  }, {
    "id" : "7ebbdbf33e0a46f293809c3b085725c8",
    "option" : "Collect all S3 buckets server access logs in one S3 bucket per account. In the Audit AWS account, create a Lambda function to get the new logs from these S3 buckets and save the files in a centralized S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer : C.\nOption A is incorrect because in each AWS account, you only need an S3 bucket to save the server access logs.\nOption B is incorrect because for the server access logs, you can only configure the target S3 bucket in the same AWS account.\nOption C is CORRECT because it meets all the requirements for the solution.\nAll the S3 server access logs from each AWS account have been replicated to the audit account.\nOption D is incorrect because the Lambda function is not required.\nYou can directly enable replication to copy the logs to the S3 bucket in the Audit account.\n\n\nThe best solution for this requirement is option C, which involves collecting all S3 buckets server access logs in one S3 bucket per account and enabling replication in the S3 server access log buckets to copy the logs to a centralized destination S3 bucket in the Audit account.\nOption A is not optimal because it involves collecting all S3 buckets server access logs in separate S3 buckets per account, which can be difficult to manage and maintain. It also requires enabling replication in each of these separate S3 buckets, which can add complexity to the solution.\nOption B is not ideal because it involves enabling the server access logs for the S3 buckets in all the AWS accounts directly to a centralized destination S3 bucket in the Audit account, which can be difficult to manage and may cause issues with access and security.\nOption D is also not optimal because it involves creating a Lambda function to get the new logs from the S3 buckets and save the files in a centralized S3 bucket, which can add unnecessary complexity to the solution.\nOption C is the best solution because it involves collecting all S3 buckets server access logs in one S3 bucket per account, which is easier to manage and maintain. This also allows for replication to be enabled in each of the S3 server access log buckets to copy the logs to a centralized destination S3 bucket in the Audit account, which simplifies the solution and ensures that all logs are being collected in one location.\nOverall, the solution in option C is the most efficient and effective way to meet the compliance and audit requirement of collecting and centralizing all S3 buckets server access logs.\n\n"
}, {
  "id" : 443,
  "question" : "Your company has just started using the AWS RDS service.\nThey have an application making requests to a MySQL instance on this service.\nDue to the sudden surge of high requests, you need to ensure that the backup activities on the database do not interfere with the database's normal operation.\nWhich of the following would help in this requirement?\n",
  "answers" : [ {
    "id" : "64303546ae884368a7f41d20aea1d8e5",
    "option" : "Ensure that the underlying instance type RDS instance is using General Purpose SSD storage. This type of storage will have minimal impact on such operations.",
    "isCorrect" : "false"
  }, {
    "id" : "4cbe57763032477a9b9fbef1c8788c47",
    "option" : "Ensure that the underlying instance type RDS instance is using Enhanced Networking. This type of setting will have minimal impact on such operations.",
    "isCorrect" : "false"
  }, {
    "id" : "5c74e92b2ce542738dda9ef0b72d7485",
    "option" : "Ensure that the Multi-AZ feature has been enabled for the underlying RDS Instance.",
    "isCorrect" : "true"
  }, {
    "id" : "7e3096760a9a44daa847237bc9573e1c",
    "option" : "Ensure that cross-region replication is enabled for the underlying RDS Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOption C is correct because, in a Multi-AZ deployment, Amazon RDS automatically provisions and maintains a synchronous standby replica in a different Availability Zone.\nThe primary DB instance is synchronously replicated across Availability Zones to a standby replica to provide data redundancy, eliminate I/O freezes, and minimize latency spikes during system backups.\nRunning a DB instance with high availability can enhance availability during planned system maintenance and help protect your databases against DB instance failure and Availability Zone disruption.\nPoints to remember for Multi-AZ deployments:\nNo failover occurs for DB operations: long-running queries, deadlocks, or database corruption errors.\nWe know that during the backups, for instance, taking snapshots, there is usually an I/O consumption that takes place.\nTo avoid this when using a multi-AZ enabled RDS database engine, create a backup on the standby instance.\nWith automated backups, I/O activity is no longer suspended on your primary during your preferred backup window since backups are taken from the standby.\nOptions A and B are incorrect because, using General Purpose SSD Storage or using Enhanced networking, our backup activities will interfere with normal database operation.\nOption D is incorrect, Cross-region replication is a time-consuming process.\nDepending on the amount of data in the databases, this process can take hours to complete.\nReferences:\nhttps://aws.amazon.com/rds/faqs/\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html\n\nThe correct answer is C. Ensure that the Multi-AZ feature has been enabled for the underlying RDS Instance.\nExplanation: Amazon RDS (Relational Database Service) is a managed database service provided by Amazon Web Services (AWS) that supports several popular database engines, including MySQL, PostgreSQL, Oracle, and Microsoft SQL Server. RDS provides a scalable and highly available database solution for applications running in the AWS cloud.\nWhen it comes to managing backups in RDS, there are several options available. Amazon RDS provides automated backups, which allow you to recover your database to any point in time within a retention period. These backups do not impact the performance of your database, as they are taken in the background and use Amazon S3 for storage.\nTo ensure that the backup activities do not interfere with the normal operation of your database, it is recommended to enable the Multi-AZ feature for your RDS instance. Multi-AZ (Availability Zone) deployment provides automatic failover to a standby instance in case of a primary instance failure. This feature ensures that your database is always available, even during maintenance windows and backup activities. It also provides the additional benefit of automatic backups being taken from the standby instance, which further reduces the impact on the primary instance's performance.\nOption A is incorrect because choosing General Purpose SSD storage for the RDS instance has no direct impact on the backup activities. General Purpose SSD storage is recommended for most workloads as it provides a balance of price and performance.\nOption B is incorrect because Enhanced Networking is a setting that provides higher network performance for certain instance types. It does not have any direct impact on backup activities.\nOption D is incorrect because cross-region replication is a feature that allows you to replicate data from one RDS instance to another in a different region. It does not have any direct impact on backup activities.\n\n"
}, {
  "id" : 444,
  "question" : "A company has an application that needs to be hosted on an EC2 Instance.\nThe general amount of throughput data per volume will be in the range of 400-500 MiB/s from the application.\nWhich of the following should be used as the storage type for the underlying EC2 Instance in a Cost-effective manner?\n",
  "answers" : [ {
    "id" : "bfbdfe4bede74bee9df7f4cebc250fd1",
    "option" : "EBS - General Purpose SSD",
    "isCorrect" : "false"
  }, {
    "id" : "402b2641cbc54b6992568a317d04cff2",
    "option" : "EBS - Provisioned IOPS SSD",
    "isCorrect" : "false"
  }, {
    "id" : "ce1d54367b71414bbde4b53c36c961b0",
    "option" : "EBS - Throughput Optimized HDD",
    "isCorrect" : "true"
  }, {
    "id" : "087d0c01858143579461fa5efa30eebe",
    "option" : "EBS - Cold HDD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nWhen you want high throughput, you should choose using the Throughput Optimized EBS volume.\nThe below snapshot from the AWS Documentation shows the features of the different types of volumes.\nAs per the above document,\nOption A, B and D stands invalid.\nFor more information on the EBS volume types, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\n\n\nFor an application that requires high throughput, it's important to choose a storage option that can handle the amount of data required by the application.\nOut of the given options, the most cost-effective storage type for this particular use case would be the \"EBS - Throughput Optimized HDD\" (option C).\nThe Throughput Optimized HDD volume type is designed to provide low-cost, high-performance storage for frequently accessed, throughput-intensive workloads such as big data, data warehousing, and log processing. It provides up to 500 MiB/s of throughput per volume, which falls within the range required by the application.\nOption A (EBS - General Purpose SSD) and option B (EBS - Provisioned IOPS SSD) are solid-state drive (SSD) storage types, which are generally more expensive than HDD storage types. While they offer faster I/O performance, they might not be as cost-effective for workloads with high throughput requirements.\nOption D (EBS - Cold HDD) is a lower-cost HDD storage type that is intended for infrequently accessed data. It does not offer the required level of throughput required by the application.\nIn summary, for a cost-effective option for an EC2 instance hosting an application with high throughput requirements, EBS - Throughput Optimized HDD (option C) is the best option among the given choices.\n\n"
}, {
  "id" : 445,
  "question" : "A company has set up its application in AWS.\nIt consists of a web tier hosted on a set of EC2 Instances.\nThese instances interact with a MongoDB database server located in a private subnet.\nThe web tier also interacts with many service-based applications in the private subnet.\nA NAT Instance is being used to route traffic from the instances in the private subnet to the Internet.\nThe IT Administrative team is now getting Cloudwatch alerts that the NAT Instance is going beyond its threshold value for Network Activity.\nWhich of the following would you advise to increase the performance of this architecture?\n",
  "answers" : [ {
    "id" : "ead424e0e8694eee9c39d3a027ec6640",
    "option" : "Place the database server and application servers in the public subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "cdfa21b693bb4f6b8233a04ca7cae60d",
    "option" : "Place the NAT instance closer to the database servers by placing them in the private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "2874ef1e7ce0487c8818d3ebd9f61752",
    "option" : "Use the NAT gateway service instead of the NAT Instance.",
    "isCorrect" : "true"
  }, {
    "id" : "15a858127e684b6286cf196159ffdfc1",
    "option" : "Use a VPN connection for the Instances in the private subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe below snapshot from the AWS Documentation shows a partial comparison of the NAT Instance and NAT Gateway.\nYou should consider using the NAT gateway for higher bandwidth requirements.\nOption A is incorrect since you should not change the database or application servers' architecture since this would result in security issues.\nOption B is incorrect since this would still alleviate the current network issue.\nOption D is incorrect since the NAT instance should be used to route traffic to the Internet from the Instances in the private subnet.\nFor more information on the comparison between NAT Instances and the NAT gateway, please refer to the below URL-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\n\n\nThe scenario describes a web application hosted on EC2 instances that interact with a MongoDB database in a private subnet. Additionally, the web tier interacts with various other services in the private subnet, and a NAT instance is used to route traffic from the instances in the private subnet to the Internet. The IT administrative team is receiving Cloudwatch alerts indicating that the NAT instance is exceeding its threshold value for network activity. To increase the performance of this architecture, we need to identify the best solution from the options provided:\nA. Place the database server and application servers in the public subnet: This option is not recommended as it would expose the database and application servers to the public Internet, which could potentially compromise their security. Additionally, if the instances are moved to the public subnet, the NAT instance would still be needed to route traffic from the private subnet to the Internet, and this could further exacerbate the existing performance issue.\nB. Place the NAT instance closer to the database servers by placing them in the private subnet: This option is not ideal as it does not address the root cause of the problem, which is the network activity exceeding the threshold value. Additionally, placing the NAT instance in the private subnet may cause issues with connectivity to the Internet, and could make it more difficult to manage the instance.\nC. Use the NAT gateway service instead of the NAT Instance: This option is the recommended solution. NAT Gateway is a fully managed service that provides better scalability, availability, and performance than a NAT instance. It eliminates the need for a dedicated NAT instance, which reduces management overhead and simplifies the architecture. By using NAT Gateway, the network activity can be distributed across multiple NAT Gateway instances, which helps to increase performance and reduce the likelihood of exceeding threshold values.\nD. Use a VPN connection for the Instances in the private subnet: This option is not directly related to the performance issue with the NAT instance. VPN connections are typically used to provide secure access to resources in a private subnet over the Internet, but they can be more complex to set up and manage than NAT Gateway.\nIn summary, the best solution to increase the performance of the architecture would be to use the NAT gateway service instead of the NAT Instance. This approach would provide better scalability, availability, and performance while reducing management overhead and simplifying the architecture.\n\n"
}, {
  "id" : 446,
  "question" : "Your company is currently hosting a long-running heavy load application on its On-premise environment.\nThe company has developed this application in-house.\nConsulting companies then use this application via API calls, and each API call may take half an hour to finish.\nYou now need to consider moving this application to AWS.\nWhich of the following services would be best suited in the architecture design, which would also help deliver a cost-effective solution? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "bb3a7829a40b4238890d18ceb06b5028",
    "option" : "AWS Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "544a3e4f821b4ba88877bd4c9ed2e87e",
    "option" : "AWS API Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "1d972467c8ea4c28a84e9d96621c93a0",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "33c0768642844847aecbfb5df73bf8f9",
    "option" : "AWS EC2",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nOption A might be a valid choice, but the question specifies heavy load application, leading to a need for time-out of API greater than 15min.\nAs per AWS documentation, AWS Lambda can handle a max time-out of up to 15 minutes.\nIn this case, the application may take more time to run.\nOption B is correct because Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale.\nWith a few clicks in the AWS Management Console, you can create an API that acts as a â€œfront doorâ€ for applications to access data, business logic, or functionality from your back-end services, such as workloads running on Amazon Elastic Compute Cloud (Amazon EC2), code running on AWS Lambda, or any web application.\nOption C is incorrect since this is a configuration service available from AWS.\nOption D is correct because EC2 would fit for using API calls for the application.\nFor more information on AWS EC2 and the API gateway, please refer to the below URLs-\nhttps://aws.amazon.com/api-gateway/\nhttps://aws.amazon.com/ec2\n\nBased on the scenario described, the company's application appears to be a long-running, heavy-load application that is used by external clients via API calls. The API calls take up to half an hour to complete, indicating that the application requires significant compute resources to process each request. Moving this application to AWS could provide several benefits, including cost-effective scalability, better performance, and availability.\nOut of the given options, the two services that are best suited for this scenario are AWS Lambda and AWS API Gateway. Here is a detailed explanation of each option and why they are suitable:\nAWS Lambda: AWS Lambda is a serverless computing service that allows you to run your code without having to provision or manage servers. With Lambda, you can write your code in several programming languages such as Java, Python, and Node.js, and then upload it to AWS Lambda. Once uploaded, AWS Lambda automatically scales your application in response to incoming requests, without requiring you to configure or manage any infrastructure.\nIn the given scenario, AWS Lambda could be used to host the application's code and handle API requests. With Lambda, you can set the amount of memory allocated to your function, which determines the amount of CPU and network resources that your function can use. This can be adjusted according to the load on the application to ensure that it can handle requests efficiently. Since Lambda charges based on the number of requests and the amount of compute resources used, it can be a cost-effective solution.\nAWS API Gateway: AWS API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. With API Gateway, you can create APIs that act as a \"front door\" for your application, providing a unified interface for clients to access your application's functionality.\nIn the given scenario, AWS API Gateway could be used to manage the application's API calls. API Gateway provides several features such as request throttling, caching, and authentication, which can be used to improve the performance and security of the application. Additionally, API Gateway can integrate with several AWS services, such as Lambda, to create a serverless architecture that can scale automatically based on incoming requests. Since API Gateway charges based on the number of API requests and the amount of data transferred, it can be a cost-effective solution.\nAWS Config and AWS EC2 are not suitable for this scenario. AWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. It is used for compliance, auditing, and security purposes, but not for hosting applications. AWS EC2 is a virtual machine service that allows you to create and manage virtual machines in the cloud. While it can be used to host applications, it requires you to provision and manage servers, which can be costly and time-consuming. Additionally, EC2 does not offer the same scalability benefits as a serverless solution such as AWS Lambda.\n\n"
}, {
  "id" : 447,
  "question" : "Your company is planning on the following architecture for their application. Â· A set of EC2 Instances hosting the web part of the application. Â· A relational database for the backend Â· A Load balancer for distribution of traffic There is critical nature of the data stored on the underlying EBS volumes attached to the EC2 Instances.\nAs a Solutions Architect of the Company, your supervisor has asked you to follow best backup practices to ensure data is available in another region for disaster recovery purposes.\nWhich of the following would you consider complying with this requirement?\n",
  "answers" : [ {
    "id" : "504de97af09a4d4eb88b90f8f2d90813",
    "option" : "Create a copy of the volume in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "6a2f50a0cb8e46558b85ff2f507c3f58",
    "option" : "Create a snapshot of the volume in another region.",
    "isCorrect" : "false"
  }, {
    "id" : "f300327caa684e0183aba2370b1ab047",
    "option" : "Create a snapshot. Copy the snapshot to the new region.",
    "isCorrect" : "true"
  }, {
    "id" : "ae70be890499428f99f9ac08e474f023",
    "option" : "Create a copy of the volume. Copy the volume to the new region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation showcases the use cases of EBS snapshots.\nUse Cases.\nGeographic expansion: Launch your applications in a new region.\nMigration: Move an application to a new region, to enable better availability and to minimize cost.\nDisaster recovery: Back up your data and logs across different geographical locations at regular intervals.\nIn case of disaster, you can restore your applications using point-in-time backups stored in the secondary region.\nThis minimizes data loss and recovery time.\nEncryption: Encrypt a previously unencrypted snapshot, change the key with which the snapshot is encrypted, or, for encrypted snapshots that have been shared with you, create a copy that you own to restore a volume from it.\nData retention and auditing requirements: Copy your encrypted EBS snapshots from one AWS account to another to preserve data logs or other files for auditing or data retention.\nUsing a different account helps prevent accidental snapshot deletions and protects you if your main AWS account is compromised.\nOptions A and D are incorrect since you need to create a snapshot.\nOption B is incorrect since you cannot directly create a snapshot in another region.\nFor more information on EBS Snapshot copy, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-copy-snapshot.html\n\nTo comply with the requirement of ensuring data availability in another region for disaster recovery purposes, the best backup practice is to create a snapshot of the EBS volume in another region. The correct answer is option B: Create a snapshot of the volume in another region.\nA snapshot is a point-in-time copy of an EBS volume. It captures the data on the volume at the time the snapshot is taken and stores it in Amazon S3. Snapshots are incremental, meaning that only the blocks that have changed since the last snapshot are saved. This makes them an efficient way to back up data, as only the changes need to be stored, saving storage costs.\nCreating a snapshot of the volume in another region ensures that the data is available in another location in case of a disaster that affects the original region. This is because snapshots are stored in Amazon S3, which is a highly durable and reliable storage service designed for long-term storage of data. By creating a snapshot in another region, you can restore the data from the snapshot if the original region is unavailable.\nOption A: Create a copy of the volume in another region, is not the best backup practice because it involves creating a complete copy of the EBS volume and storing it in another region. This can be expensive and time-consuming, as it requires copying all the data on the volume. Additionally, it does not provide the same level of incremental backup as snapshots, which only store the changes to the data.\nOption C: Create a snapshot. Copy the snapshot to the new region, involves creating a snapshot and then copying it to another region. While this is a valid backup practice, it is not as efficient as creating a snapshot directly in the new region, as it involves an additional step of copying the snapshot.\nOption D: Create a copy of the volume. Copy the volume to the new region, is not the best backup practice because it involves creating a complete copy of the EBS volume and storing it in another region. This can be expensive and time-consuming, as it requires copying all the data on the volume. Additionally, it does not provide the same level of incremental backup as snapshots, which only store the changes to the data.\n\n"
}, {
  "id" : 448,
  "question" : "An application consists of a fleet of EC2 Instances.\nThese Instances are launched in the Oregon (us-west-2) region which consists of 3 availability zones.\nThis application needs 6 Instances running at all times.\nAs an architect, you need to distribute the instances so that the application could still maintain its capacity if any one of the availability zone goes down.\nAlso, you need to ensure that the cost is kept to a minimum? Which of the following configurations would you consider?\n",
  "answers" : [ {
    "id" : "23aa635fb7fe4a86a4d45c0bce7aceaf",
    "option" : "6 Instances running in us-west-2a, 6 Instances running in us-west-2b, 6 Instances running in us-west-2c",
    "isCorrect" : "false"
  }, {
    "id" : "612f722d6aab417fa2b8478c966465a2",
    "option" : "3 Instances running in us-west-2a, 3 Instances running in us-west-2c",
    "isCorrect" : "false"
  }, {
    "id" : "b2e531d1bb6944c18a3a91ebfab20163",
    "option" : "6 Instances running in us-west-2a, 3 Instances running in us-west-2b, 3 Instances running in us-west-2c",
    "isCorrect" : "false"
  }, {
    "id" : "aefa3969ff004d8fa9a4766c858d4184",
    "option" : "3 Instances running in us-west-2a, 3 Instances running in us-west-2b, 3 Instances running in us-west-2c.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nSo now, let's look at.\nOption A.If any availability zone goes down, we will have a total of 12 instances running.\nThis is an additional 6 over the requirement of the question and will result in a higher cost.\nSo now, let's look at Option.\nB.If the availability zone us-west-2a goes down, then you will have only 3 instances running.\nBecause the other 3 instances are running in the us-east-2c region.\nSo now, let's look at Option.\nC.If either us-west-2b or us-west-2c availability zone goes down, we will have a total of 9 instances running.\nThis is an additional 3 over the requirement of the question and will result in a higher cost.\nSo now, let's look at Option.\nD.If either us-east-2a or us-west-2b or us-west-2c availability zone goes down, there will be a total of 6 instances running, which is what we need.\nFor more information on Regions and Availability zones, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html\n\nTo ensure high availability and fault tolerance, it is recommended to distribute the instances across multiple availability zones in a region. If any one of the availability zones goes down, the application will still be able to operate with the remaining instances in the other availability zones.\nIn this scenario, the application needs 6 instances running at all times. To ensure high availability and fault tolerance, it is recommended to have at least 2 instances running in each availability zone. Therefore, options A and D are the most suitable because they both have 6 instances distributed across all 3 availability zones.\nOption B only has 3 instances running in 2 availability zones, which does not provide enough redundancy in case one of the availability zones goes down. Option C has 6 instances running in one availability zone, which could create a single point of failure if that availability zone goes down.\nTherefore, option A and D are the most suitable. However, since the question asks for the option that would keep costs to a minimum, option D would be the most cost-effective as it only requires a total of 9 instances instead of 18 in option A.\nOption D is the best configuration as it has 3 instances running in each availability zone. This would provide high availability and fault tolerance while keeping costs to a minimum.\n\n"
}, {
  "id" : 449,
  "question" : "You have a set of EC2 Instances in a custom VPC.\nYou have installed a web application and need to ensure that only HTTP and HTTPS traffic is allowed into the instance.\nWhich of the following would you consider for this requirement?\n",
  "answers" : [ {
    "id" : "c4815bae8929494e80eba89d8a644665",
    "option" : "Add a security group rule to allow HTTP and HTTPS Traffic.",
    "isCorrect" : "true"
  }, {
    "id" : "eb527b05c0714a85ad8e0425831b7c85",
    "option" : "Add a security group rule to an explicit DENY all traffic and a default allow on HTTP and HTTPS Traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "c95f6601662749a18cb380aa737b321d",
    "option" : "Add a security group rule to deny explicit traffic on HTTP and HTTPS Traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "16618449c18a4d4da4adfb35c4fe57c9",
    "option" : "Add a security group rule to allow all traffic.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A\nOption A is correct because we need to specify the allowed traffic in the security group, i.e., HTTP and HTTPS Traffic must be allowed from all sources.No inbound traffic is allowed by default.\nBy adding security group rules, you can specify which traffic you want to allow.\nThis is essentially a whitelist.\nOptions B is incorrect since, by default, nothing is allowed, and in the Security group, we can't specify what is denied.\nWe don't have any deny option in Security Groups.\nOption C is incorrect because we can specify what is allowed in the security group but not what is denied.\nIf you want to deny explicitly, you should use the Network Access control list.\nOption D is incorrect since this would be a security issue.\nFor more information on VPC Security groups, please refer to the below URL-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\n\nThe correct answer is A. Add a security group rule to allow HTTP and HTTPS Traffic.\nExplanation:\nA security group acts as a virtual firewall that controls the traffic for one or more instances. It allows you to specify inbound and outbound traffic to your instances.\nTo allow only HTTP and HTTPS traffic into the instance, we need to add a security group rule that allows inbound traffic on ports 80 (HTTP) and 443 (HTTPS).\nOption A is the correct answer because it allows only the necessary traffic, HTTP and HTTPS, and denies all other traffic.\nOption B is incorrect because it explicitly denies all traffic before allowing HTTP and HTTPS, which means that no other traffic can be allowed even if it is necessary.\nOption C is also incorrect because it denies all explicit traffic on HTTP and HTTPS, which means that even the necessary traffic for the web application would be denied.\nOption D is incorrect because it allows all traffic, including unnecessary and potentially harmful traffic.\nTherefore, the correct approach is to add a security group rule that allows HTTP and HTTPS traffic.\n\n"
}, {
  "id" : 450,
  "question" : "A company has an application defined with the following architecture. A fleet of EC2 Instances which are used to accept video uploads from users. A fleet of EC2 Instances which are used to process the video uploads. Which of the following would help architect an operationally excellent architecture?\n",
  "answers" : [ {
    "id" : "74ec4161ef2a4c9791b99515d3849b30",
    "option" : "Create an SQS queue to store the information for Video uploads. Spin up the processing servers via an Autoscaling Group. Ensure the Group scales based on the Memory utilization of the underlying processing servers.",
    "isCorrect" : "false"
  }, {
    "id" : "cc3e6b43f7084c089baa7d6eb76a628c",
    "option" : "Create an SQS queue to store the information for Video uploads. Spin up the processing servers via an Autoscaling Group. Ensure the Group scales based on the size of the queue.",
    "isCorrect" : "true"
  }, {
    "id" : "69592a4a8f174c91ab00de18f051990b",
    "option" : "Create an SNS topic to store the information for Video uploads. Spin up the processing servers via an Autoscaling Group. Ensure the Group scales based on the Memory utilization of the underlying processing servers.",
    "isCorrect" : "false"
  }, {
    "id" : "97e90df2e76643f3894260fe498a35ff",
    "option" : "Create an SNS topic to store the information for Video uploads. Spin up the processing servers via an Autoscaling Group. Ensure the Group scales based on the size of the queue messages.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThis architecture is also given in the AWS Documentation.\nOption A is incorrect.\nThe ideal approach is to scale the instances based on the size of the queue.\nOptions C and D are incorrect since you should be using SQS queues.\nSNS topics are used for notification purposes.\nFor more information on using SQS queues for Autoscaling, please refer to the below URL.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html\nAs per AWS,\nYou can use the number of messages stored in an SQS queue as an indicator of the amount of work waiting in line for eventual processing within an Auto Scaling Group comprised of a variable number of EC2 instances.\nEach SQS queue reports a number of metrics to CloudWatch at five minute intervals, including\nApproximateNumberOfMessagesVisible.\nIf your workload is spikey in nature, you may want to build an application that can respond more quickly to changes in the size of the queue.\nMemory utilization metrics are custom metrics.For this, you need to install a Cloudwatch agent on the EC2 instances and need to aggregate the dimensions.\nHowever, AWS already has a well-defined architecture based on SQS Queuelength being used for Autoscaling EC2 instances.\nFor more information, please refer to the link:\nhttps://aws.amazon.com/blogs/aws/auto-scaling-with-sqs/\n\n\nTo architect an operationally excellent architecture for a video processing application that uses a fleet of EC2 instances for video uploads and processing, we need to ensure high availability, scalability, and fault tolerance. One way to achieve this is by using Amazon SQS (Simple Queue Service) and Autoscaling Group.\nSQS is a highly scalable and fully managed message queuing service that enables decoupling of components in a distributed application, improving the overall system resilience. Autoscaling Group is a service that enables you to automatically scale Amazon EC2 instances based on predefined conditions.\nOption A is the correct answer. It suggests creating an SQS queue to store the information for video uploads and spinning up the processing servers via an Autoscaling Group. The Autoscaling Group should scale based on the Memory utilization of the underlying processing servers. This architecture enables us to handle a surge in traffic by automatically scaling the number of instances handling the processing workload. The SQS queue ensures that the video uploads are not lost even if the processing servers are down, and the processing workload can be resumed once the servers are back up.\nOption B is incorrect because the Autoscaling Group should not scale based on the size of the queue. The size of the queue does not necessarily reflect the processing workload or the processing server's capacity.\nOption C is incorrect because SNS (Simple Notification Service) is not the appropriate service for this architecture. SNS is a publish/subscribe messaging service used for pushing notifications to subscribers. It is not designed to store messages like SQS.\nOption D is incorrect because the Autoscaling Group should not scale based on the size of the queue messages. The size of the messages in the queue does not necessarily reflect the processing workload or the processing server's capacity.\nIn summary, option A is the correct answer as it utilizes SQS and Autoscaling Group to ensure high availability, scalability, and fault tolerance for the video processing application.\n\n"
}, {
  "id" : 451,
  "question" : "An organization has branches across the world, and there are many applications deployed across AWS regions.\nAs per organization compliance requirements, data in an S3 bucket should be copied to another S3 bucket in a different region in 24 hrs period. How can this be achieved with minimal cost?\n",
  "answers" : [ {
    "id" : "88f4ccd815484a5ab4df339b5b229da8",
    "option" : "Use the S3 cp command to copy the data from one AWS region to another AWS region.",
    "isCorrect" : "false"
  }, {
    "id" : "8febf31b84264c068ece3a69399880ef",
    "option" : "Use Amazon S3 select to copy the data from one AWS region to another AWS region.",
    "isCorrect" : "false"
  }, {
    "id" : "3e9c06fe1de442999c78724b5cf8032c",
    "option" : "Enable S3 versioning on source and destination buckets and copy the data using cross-region replication.",
    "isCorrect" : "true"
  }, {
    "id" : "add1f9f8b750414d89e046ea784ef805",
    "option" : "Create a Lambda function in each AWS region to run AWS CLI \"s3 sync\" periodically to replicate data from one region to another region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect as S3 cp is an AWS CLI command where a custom program is required.\nS3 CP can copy data from one bucket to another, but this won't be a cost-effective solution.\nOption B incorrect because S3 Select uses SQL-like statements where objects are filtered at the S3 end (service side) for an end-user application and can not be used to copy data to another bucket in another region.\nOption C is CORRECT because versioning on source and destination buckers should be enabled.\nUsing Cross-region replication rules, data from bucket in one region to bucket in another region can be copied in 24 hrs.\nThis replication is not sensitive to latency.\nOption D is incorrect because this option needs a Lambda function in each region to execute the AWS CLI command.\nThis method takes more efforts and is not cost-efficient.\nReference:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/move-objects-s3-bucket/\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\n\n\nOption C: Enable S3 versioning on source and destination buckets and copy the data using cross-region replication is the most cost-effective solution for copying data between S3 buckets in different regions.\nWith cross-region replication, objects in the source bucket are automatically replicated to the destination bucket in a different region. You can configure cross-region replication using the S3 console or programmatically using the AWS SDK or CLI.\nTo enable cross-region replication, you need to enable versioning on both the source and destination buckets. Versioning keeps multiple versions of an object in the same bucket, allowing you to recover objects that are accidentally deleted or modified.\nOnce versioning is enabled, you can create a replication configuration that specifies the source and destination buckets, as well as any replication rules. For example, you can specify that only objects with a certain prefix or tag should be replicated.\nWith cross-region replication, you can also replicate object metadata, object ACLs, and object tags. This allows you to maintain the same access controls and metadata across multiple regions.\nCross-region replication is cost-effective because you only pay for the data transfer and storage in the destination region. There are no additional charges for replication or monitoring.\nOption A: Using the S3 cp command to copy the data from one AWS region to another AWS region can be done, but it is not the most cost-effective solution. You would need to write a script that runs periodically to copy the data, which would require additional compute resources. Additionally, you would incur data transfer costs for each copy operation.\nOption B: Amazon S3 select is a data retrieval feature that allows you to retrieve a subset of data from an S3 object using SQL queries. It is not designed for copying data between S3 buckets in different regions.\nOption D: Creating a Lambda function in each AWS region to run AWS CLI \"s3 sync\" periodically to replicate data from one region to another region would also work, but it is not the most cost-effective solution. You would need to create and manage multiple Lambda functions, which would require additional compute resources. Additionally, you would incur data transfer costs for each copy operation.\n\n"
}, {
  "id" : 452,
  "question" : "A company has an Aurora MySQL DB cluster setup, and it needs to invoke a Lambda function.\nWhich of the following need to be in place for this setup to work.\n(Select TWO)\n",
  "answers" : [ {
    "id" : "987a558ff015452e8eb13ed77b70b313",
    "option" : "Ensure that the Lambda function has an IAM Role assigned to it which can be used to invoke functions on Amazon Aurora.",
    "isCorrect" : "false"
  }, {
    "id" : "5e14f9aba48443b79fc348bb08a2dd87",
    "option" : "Ensure that the Aurora MySQL DB cluster has an IAM Role which allows it to invoke Lambda functions.",
    "isCorrect" : "true"
  }, {
    "id" : "9e7fb80112584bdf988cec775d556f36",
    "option" : "Allow the Lambda function to allow outbound communication to Amazon Aurora.",
    "isCorrect" : "false"
  }, {
    "id" : "64f7d62de8cc4ad6ba63228188c67cae",
    "option" : "Configure the Aurora MySQL DB cluster to allow outbound connections to the Lambda function.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe below snapshot from the AWS Documentation shows the different steps required to ensure that the Lambda function can access Amazon Aurora.\nOptions A and C are incorrect since the configurations need to be the other way around.\nFor more information on invoking AWS Lambda using Aurora, please refer to the below URLs-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html#AuroraMySQL.Integrating.LambdaAccess\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Lambda.html\n\n\nTo invoke a Lambda function from an Aurora MySQL DB cluster, the following two things need to be in place:\nA. Ensure that the Lambda function has an IAM Role assigned to it which can be used to invoke functions on Amazon Aurora. The Lambda function must have an IAM role assigned to it that allows it to access the necessary resources. This IAM role should include the AWSLambdaVPCAccessExecutionRole policy to allow it to access resources in a VPC, and the AWSLambdaRole policy to allow it to invoke other AWS services.\nB. Ensure that the Aurora MySQL DB cluster has an IAM Role which allows it to invoke Lambda functions. The Aurora MySQL DB cluster needs to have an IAM role assigned to it that allows it to invoke the Lambda function. This IAM role should include the AWSLambdaRole policy to allow it to invoke Lambda functions.\nOption C and D are not applicable in this scenario as they are related to network connectivity, and the communication between Aurora MySQL DB cluster and Lambda function is handled by the AWS service infrastructure.\nTherefore, options A and B are the correct answers to this question.\n\n"
}, {
  "id" : 453,
  "question" : "Your application consists of a set of EC2 Instances spun up as part of an Auto Scaling group.\nThese Instances need to access objects in an S3 bucket.\nWhich of the following is the ideal approach to ensure this access is set in place?\n",
  "answers" : [ {
    "id" : "0bac7d36b3c6482da1ebec590c9082af",
    "option" : "Ensure that the Access Keys are picked up from another S3 bucket. Access Keys can be embedded in the User data during Instance Launch.",
    "isCorrect" : "false"
  }, {
    "id" : "f6a6d752f0124e1d8d7fe3f167f083ea",
    "option" : "Ensure that the launch configurations in Auto scaling group have an instance profile associated with the IAM role to access S3 Objects.",
    "isCorrect" : "true"
  }, {
    "id" : "e7fade01a87149b694fe7dc21af27939",
    "option" : "Ensure that an IAM policy is attached to the S3 bucket which allows access to the S3 buckets.",
    "isCorrect" : "false"
  }, {
    "id" : "838d4c141f4e4200a8001f72d07e63ac",
    "option" : "Ensure that the launch configurations in Auto scaling group have an IAM user to access S3 Objects.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nApplications must sign their API requests with AWS credentials.\nTherefore, if you are an application developer, you need a strategy for managing credentials for your applications that run on EC2 instances.\nFor example, you can securely distribute your AWS credentials to the instances, enabling the applications on those instances to use your credentials to sign requests while protecting your credentials from other users.\nHowever, it's challenging to securely distribute credentials to each instance, especially those that AWS creates on your behalfs, such as Spot Instances or instances in Auto Scaling groups.\nYou must also be able to update the credentials on each instance when you rotate your AWS credentials.\nWe designed IAM roles so that your applications can securely make API requests from your instances without requiring you to manage the security credentials that the applications use.\nFor details about launch configurations, please refer to the below URL-\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/create-launch-config.html\nOption A is incorrect since using Access keys is the least secure option.\nOption C is incorrect since the IAM policy is not the right option.\nYou have to use IAM Roles instead.\nAlso, attaching the IAM role should be a part of Launch Configurations.\nOption D is incorrect since you need to use IAM Roles and not IAM Users.\nTo understand the basic difference between IAM Roles and Users:\nIAM controls: Who can do What in your AWS account.\nWho (Authentication) in IAM is defined using users/groups and roles means what (Authorization) is defined by policies.\nUser - End-user think about people.\nGroups- a set of users under one set of permission(policies)\nRoles - are used for granting specific permission to specific users for a specific time.\nFor more information on IAM Roles for EC2, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html\nhttps://docs.aws.amazon.com/cli/latest/reference/autoscaling/create-launch-configuration.html\n\n\nThe ideal approach to grant EC2 instances in an Auto Scaling group access to objects in an S3 bucket is to use an instance profile associated with an IAM role. Therefore, option B is the correct answer.\nExplanation: An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. By using an instance profile associated with an IAM role, EC2 instances can securely make API requests to AWS services such as S3 without having to store long-term security credentials.\nTo configure the instance profile associated with the IAM role, you can follow these steps:\nCreate an IAM role with permissions to access the S3 bucket containing the objects. Create an instance profile and attach the IAM role to it. When creating or updating the Auto Scaling group launch configuration, specify the instance profile.\nWith this configuration, EC2 instances launched as part of the Auto Scaling group will automatically receive the necessary permissions to access the S3 bucket using the associated IAM role.\nOption A is incorrect because embedding access keys in user data is not a secure way to provide access to AWS services. User data can be viewed by anyone with access to the EC2 instance, including attackers.\nOption C is incorrect because attaching an IAM policy to an S3 bucket does not grant access to EC2 instances. Instead, it controls access to the S3 bucket itself.\nOption D is incorrect because IAM users are meant for interactive use and do not provide a secure way to access AWS services from EC2 instances.\n\n"
}, {
  "id" : 454,
  "question" : "You are an architect for a company that is going to host an application in AWS.\nThey want to load balance the traffic based on which route the user chooses.\nThe 2 possible routes for the application are /customer and /orders.\nWhich of the following would you include in the design?\n",
  "answers" : [ {
    "id" : "38791195deff4d12bf7b28bab9066443",
    "option" : "Application Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "f24e6295ca12427aa11bb608e8ede6e3",
    "option" : "EC2 Container service",
    "isCorrect" : "false"
  }, {
    "id" : "f6aa16b2b60940ff883991ebe514635d",
    "option" : "Classic Load Balancer",
    "isCorrect" : "false"
  }, {
    "id" : "fe882fae9d924fdb91e4e3f171565dfc",
    "option" : "Docker containers on EC2 Instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe below snapshot from the AWS Documentation shows the benefits of using the Application Load balancer.\nOptions B and D are incorrect since we don't have enough information on the question to decide on whether to use Docker containers or not.\nOption C is invalid since Classic Load balancers will not fit the requirement for route-based load balancing.\nFor more information on the Application Load Balancer, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\n\n\nFor load balancing the traffic based on the route the user chooses, you would need to use an Application Load Balancer (ALB). Therefore, the correct answer is A.\nHere's a detailed explanation of why:\nA. Application Load Balancer (ALB): ALB is a Layer 7 load balancer that can route traffic based on the content of the request. ALB can route traffic to different target groups based on the path (such as /customer and /orders), host, HTTP header, and HTTP method. ALB is the best choice for routing traffic to containers, microservices, and serverless applications. B. EC2 Container Service: EC2 Container Service is a container management service that allows you to run Docker containers on a cluster of EC2 instances. While this service can help with managing containers, it is not directly related to load balancing traffic based on the path of the request. C. Classic Load Balancer (CLB): CLB is a Layer 4 load balancer that can route traffic based on the IP address and port of the request. It does not have the ability to route traffic based on the path of the request. D. Docker containers on EC2 instances: While it is possible to run Docker containers on EC2 instances, this option does not provide load balancing capabilities based on the path of the request. You would need to use an additional tool, such as NGINX, to implement this kind of routing.\nIn summary, an Application Load Balancer (ALB) is the best choice for load balancing traffic based on the route the user chooses.\n\n"
}, {
  "id" : 455,
  "question" : "Your company is planning on the following architecture for their application. Â· A set of EC2 Instances hosting the web part of the application. Â· A relational database for the backend using the AWS RDS MySQL service Â· A Load balancer for distribution of traffic There is a requirement to ensure that all data hosted in the database service is encrypted at rest.\nHow can you achieve this requirement in the easiest manner? (Select 2)\n",
  "answers" : [ {
    "id" : "51e2389b73654efa93e5aa27043f7f4f",
    "option" : "Encrypt the underlying EBS volumes for the database",
    "isCorrect" : "false"
  }, {
    "id" : "c2a2f619f8fd422b9e29693d4137a853",
    "option" : "Use the Encryption feature for RDS",
    "isCorrect" : "true"
  }, {
    "id" : "480ee9f50aac4f36855348232e99dabd",
    "option" : "Use S3 server-side encryption",
    "isCorrect" : "false"
  }, {
    "id" : "05d1f79d0f454cb0a7ec7e4f9751538d",
    "option" : "Use AWS Key Management Service.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe AWS Documentation mentions the following.\nOption B is correct because, With RDS-encrypted resources, data is encrypted at rest, including the underlying storage for a database (DB) instance, its automated backups, read replicas, and snapshots.\nThis capability uses the open standard AES-256 encryption algorithm to encrypt your data, transparent to your database engine.\nThis encryption option protects against physical exfiltration or access to your data bypassing the DB instances.\nTherefore, it is critical to complement encrypted resources with an effective encryption key management and database credential management practice to mitigate any unauthorized access.\nOtherwise, compromised credentials or insufficiently protected keys might allow unauthorized users to access the plaintext data directly through the database engine.\nEncryption key management is provided using the AWS KMS.\nOption D is correct because Amazon RDS encrypts your databases using keys you manage with the AWS Key Management Service (KMS)\nOn a database instance running with Amazon RDS encryption, data stored at rest in the underlying storage is encrypted, as are its automated backups, read replicas, and snapshots.\nRDS encryption uses the industry-standard AES-256 encryption algorithm to encrypt your data on the server that hosts your RDS instance.\nOptions C is incorrect because this is used for the encryption of objects in S3.\nOption A is incorrect since this can be easily achieved using the encryption at rest feature for AWS RDS.\nThe term 'rest' means when data is resting (not in transition-while data is traveling to the database.\nFor more information on Encryption for AWS RDS, please refer to the below URLs-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html\nhttps://aws.amazon.com/blogs/database/selecting-the-right-encryption-options-for-amazon-rds-and-amazon-aurora-database-engines/\nhttps://aws.amazon.com/rds/features/security/\n\nTo ensure that all data hosted in the database service is encrypted at rest in the easiest manner, two options can be used - encrypt the underlying EBS volumes for the database or use the encryption feature for RDS.\nA. Encrypt the underlying EBS volumes for the database: This option involves encrypting the Elastic Block Store (EBS) volumes that are used to store the data for the RDS MySQL service. EBS encryption uses AWS-managed keys, and it's a simple process to enable encryption for an EBS volume. Once enabled, all data written to the volume is automatically encrypted. However, this option has some limitations. For instance, if you're using Multi-AZ deployment for your RDS instance, you'll need to ensure that the EBS volumes are encrypted for both the primary and secondary instances.\nB. Use the Encryption feature for RDS: RDS offers the ability to encrypt data at rest using the Encryption feature. When enabled, RDS uses AES-256 encryption to encrypt the data before it's stored on disk, and a unique key is used to encrypt each database instance. This key is then encrypted using a master key that's managed by AWS Key Management Service (KMS). Enabling encryption for RDS is a simple process, and it can be done either when you're creating a new database instance or by modifying an existing one. This option is the recommended method to encrypt RDS data as it's easy to enable, and it provides strong security.\nC. Use S3 server-side encryption: S3 server-side encryption is not a suitable option for encrypting data at rest in the RDS MySQL service. S3 is an object storage service that's used to store and retrieve files, and it's not designed for storing data in a relational database.\nD. Use AWS Key Management Service: AWS Key Management Service (KMS) is a managed service that makes it easy to create and control the encryption keys used to encrypt data. KMS is used in conjunction with other AWS services, such as EBS and RDS, to provide encryption at rest. However, using KMS alone is not enough to encrypt RDS data. You'll also need to use one of the other options mentioned above, such as enabling encryption for RDS or encrypting the EBS volumes.\nIn conclusion, to achieve the requirement of encrypting all data hosted in the database service at rest, the easiest way is to use the encryption feature for RDS and encrypt the underlying EBS volumes for the database.\n\n"
}, {
  "id" : 456,
  "question" : "A global IT Firm is working on a project to process Genomics data for a scientific organization.\nResearchers are looking for a quick analysis of large amounts of data they need to retrieve from any part of the world seamlessly.\nIT Firm has AWS Direct Connect link from on-premise Datacenter to AWS VPC.\nAs an AWS consultant, they are looking for your guidance to provide a long-term cost-effective solution that can handle any amount of data reliably. Which of the following solutions can be deployed to meet IT firm requirements?\n",
  "answers" : [ {
    "id" : "5f919f19d424449bb32833546a0239f4",
    "option" : "Use Amazon EMR with Apache Spark &amp; data stored in Amazon S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "89d037a3b2f0437e9cd7f1c614aaf8fd",
    "option" : "Use Apache Spark deployed at on-premise servers with data stored in Amazon S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "c95feb83d7f84022ae3ff8e50cfdc672",
    "option" : "Use Amazon EMR with Apache Spark &amp; data stored in Amazon EC2 instance store.",
    "isCorrect" : "false"
  }, {
    "id" : "5d9bd7beae9e40e79ae1e13f17829346",
    "option" : "Use Apache Spark deployed at on-premise servers with data stored in Amazon EC2 instance store.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nAmazon EMR is a big data platform used for processing large amounts of data.\nFor analyzing large amounts of data, Amazon EMR can be used to provide reliable cost-effective solutions.\nIt supports open-source tools like Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto.\nData can be stored in an Amazon S3 bucket to have scalable storage options &amp; can be accessed globally.\nWith EMR Cluster, compute &amp; storage are decoupled.\nFor storage, Amazon S3 can be used.\nFor compute, clusters can be launched as per processing required &amp; stopped when there is no requirement, saving additional cost.\nWhen Apache Spark is deployed on-premise, customers must manage physical infrastructure &amp; large processing requirements.\nThere is an additional cost of building new infrastructure.\nOption B &amp; D are incorrect as deploying Apache Spark at on-premise servers would be a non-scalable solution &amp; will incur additional cost for managing server farms.\nOption C is incorrect as storing analytical data in Amazon EC2 instance store would be ideal only for temporary solution &amp; not for long term reliable storage.\nFor more information on Amazon EMR, refer to the following URLs-\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-benefits.html\nhttps://aws.amazon.com/emr/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc\n\n\nThe best solution for this scenario would be A. Use Amazon EMR with Apache Spark & data stored in Amazon S3 bucket.\nExplanation: The primary requirement of the project is to process Genomics data quickly and efficiently. The use of AWS Direct Connect link from on-premise Datacenter to AWS VPC suggests that the organization wants to use the cloud infrastructure to handle the processing of the data. Moreover, the requirement for a solution that can handle any amount of data reliably suggests the use of big data processing tools and technologies.\nOption A suggests using Amazon EMR (Elastic MapReduce) with Apache Spark for big data processing. Amazon EMR is a managed Hadoop framework that allows for easy deployment and scaling of big data processing jobs. Apache Spark is an open-source distributed computing system that provides a high-level API for big data processing. Amazon EMR also supports other popular big data processing frameworks such as Apache Hadoop, Apache Hive, and Apache Pig.\nThe data can be stored in an Amazon S3 (Simple Storage Service) bucket, which is a highly scalable and durable object storage service. S3 provides 99.999999999% durability and 99.99% availability for objects stored in it, which ensures the reliability of the data.\nOption B suggests using Apache Spark deployed at on-premise servers with data stored in Amazon S3 bucket. This option has some disadvantages as it requires maintaining the on-premise infrastructure, which can be costly and time-consuming. Additionally, it may introduce network latency issues as the data needs to be transferred from on-premise servers to the cloud.\nOption C suggests using Amazon EMR with Apache Spark and data stored in Amazon EC2 instance store. The instance store is a temporary storage disk that is physically attached to the Amazon EC2 instance. While the instance store is ideal for temporary data storage, it is not suitable for long-term storage as the data stored in it is lost when the EC2 instance is stopped or terminated. Therefore, it is not a suitable option for this scenario.\nOption D suggests using Apache Spark deployed at on-premise servers with data stored in Amazon EC2 instance store. This option has similar disadvantages as option B, including the need to maintain on-premise infrastructure and the risk of network latency issues.\nIn conclusion, option A is the most suitable solution for this scenario as it leverages the benefits of cloud infrastructure, big data processing frameworks, and highly scalable and durable object storage service.\n\n"
}, {
  "id" : 457,
  "question" : "Your team has an application hosted on AWS.\nThis application currently interacts with a DynamoDB table which has the Read capacity set to 10\nBased on recent cloudwatch alarms which indicated that throttling was occurring in the requests to the DynamoDB table.\nWhich of the following would help ensure the issue was resolved now and help ensure the issue does not occur in the future?\n",
  "answers" : [ {
    "id" : "72d96f5438794bca94fe6c5cc4741f01",
    "option" : "Add an Elastic Load Balancer in front of the DynamoDB table.",
    "isCorrect" : "false"
  }, {
    "id" : "8837c682b5334921ab9a911ac343e9ab",
    "option" : "Change the Read Capacity for the table to 20.",
    "isCorrect" : "false"
  }, {
    "id" : "15aa6e87c88942ce86eece64880f39e1",
    "option" : "Change the Write capacity for the table to offset the Read capacity.",
    "isCorrect" : "false"
  }, {
    "id" : "848c5f26f774457bb151a6315676b02d",
    "option" : "Enable Autoscaling for the underlying DynamoDB table.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nDynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns.\nThis enables a table or a global secondary index to increase its provisioned read and write capacity to handle sudden increases in traffic without throttling.\nWhen the workload decreases, Application Auto Scaling decreases the throughput so that you don't pay for unused provisioned capacity.\nYou can optionally allow DynamoDB Auto-scaling to manage your table's throughput capacity.\nHowever, you still must provide initial settings for read and write capacity when you create the table.\nDynamoDB auto scaling uses these initial settings as a starting point and then adjusts them dynamically in response to your application's requirements.\nAs your application data and access requirements change, you might need to adjust your table's throughput settings.\nIf you're using DynamoDB Auto-scaling, the throughput settings are automatically adjusted in response to actual workloads.\nYou can also use the\nUpdateTable.\noperation to adjust your table's throughput capacity manually.\nYou might decide to do this if you need to bulk-load data from an existing data store into your new DynamoDB table.\nFor more details, please refer to the below URLs-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\nhttps://aws.amazon.com/blogs/database/amazon-dynamodb-auto-scaling-performance-and-cost-optimization-at-any-scale/\nOption A is incorrect since the Elastic Load balancer in front of the DynamoDB table won't help increase the capacity of DynamoDB.\nHere, We need to scale up and down the capacity automatically based on the requirement.\nOption B is incorrect since this would only help in temporarily resolving the situation.\nOption C is incorrect since provisioning Write capacity would not help in this case.\nFor more information on DynamoDB Autoscaling, please refer to the below URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html\n\nThe correct answer to this question is B - Change the Read Capacity for the table to 20.\nHere's why:\nDynamoDB is a fully managed NoSQL database service provided by AWS. When you create a DynamoDB table, you must specify its capacity settings, which determine the amount of read and write activity that the table can handle. These capacity settings are measured in units, with one read capacity unit equal to one strongly consistent read request per second or two eventually consistent read requests per second, and one write capacity unit equal to one write request per second.\nIn this scenario, the DynamoDB table has a Read capacity set to 10, which means that it can handle up to 10 strongly consistent read requests per second or up to 20 eventually consistent read requests per second. However, based on recent CloudWatch alarms, it appears that the application is experiencing throttling when trying to read from the table, indicating that the table's current Read capacity is not sufficient to handle the workload.\nTo resolve the issue now and prevent it from occurring in the future, the best course of action is to increase the table's Read capacity. Option B - Change the Read Capacity for the table to 20 - is the correct choice. By doubling the Read capacity to 20, the table will be able to handle up to 20 strongly consistent read requests per second or up to 40 eventually consistent read requests per second, which should be sufficient to handle the current workload without throttling.\nOption A - Add an Elastic Load Balancer in front of the DynamoDB table - is not a viable solution in this scenario. An Elastic Load Balancer is a service that automatically distributes incoming application traffic across multiple targets, such as EC2 instances or containers, to improve availability and fault tolerance. However, an Elastic Load Balancer is not designed to improve the performance of a single DynamoDB table.\nOption C - Change the Write capacity for the table to offset the Read capacity - is not a recommended solution in this scenario. While it's true that increasing the Write capacity of a DynamoDB table can help to offset increased read activity, it's generally not recommended to make changes to the Write capacity unless you actually need more write throughput. Changing the Write capacity could also cause additional costs for the customer.\nOption D - Enable Autoscaling for the underlying DynamoDB table - is a good solution for ensuring that a DynamoDB table can automatically scale its capacity up or down in response to changing workloads. However, it's not the best choice in this scenario because the question is asking for a solution that can be implemented immediately to resolve the issue. Autoscaling can take some time to set up and configure properly, so it may not be the most effective solution for addressing an immediate performance issue.\n\n"
}, {
  "id" : 458,
  "question" : "Your team is developing a Lambda function.\nThe function would need to interact with a database.\nThe Lambda function and the database will be deployed in different environments respectively.\nWhich of the following is the most secure approach for the Lambda function to get the database credentials for multiple environments?\n",
  "answers" : [ {
    "id" : "dcb4649c85534104af23012df6cb9f1b",
    "option" : "Hardcode the database credentials in GitHub for different environments of the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "34d460678b0a44f7990294f70256381d",
    "option" : "Create a lambda function for each environment and ensure each has a different programming language.",
    "isCorrect" : "false"
  }, {
    "id" : "3e64bb21b2a74ad3a3824eec3c2ee200",
    "option" : "Store the database credentials in AWS Secrets Manager.",
    "isCorrect" : "true"
  }, {
    "id" : "0602094641ac4525b5ec88dac698771e",
    "option" : "Store the database credentials in a Lambda function tag.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nSecrets Manager supports many types of secrets.\nHowever, Secrets Manager can natively rotate credentials for supported AWS databases without any additional programming.\nHowever, rotating the secrets for other databases or services requires creating a custom Lambda function to define how Secrets Manager interacts with the database or service.\nOption A is incorrect since this has a security issue since the database credentials are hardcoded in GitHub.\nOption B is incorrect since using different programming languages makes no sense.\nOption C is correct because AWS Secrets Manager can store the database credentials in a secure way for the Lambda function to get the credentials.\nAWS Secrets Manager helps users to protect and manage credentials.\nOption D is incorrect since, you can tag Lambda functions to organize them by owner, project, or department.\nTag keys and tag values are supported across AWS services for use in filtering resources and adding detail to billing reports.\nIt is not used to store such connection strings.\nPlease refer to the below references:\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\nhttps://aws.amazon.com/blogs/security/how-to-securely-provide-database-credentials-to-lambda-functions-by-using-aws-secrets-manager/\n\nThe most secure approach for a Lambda function to get database credentials for multiple environments is to store the credentials in AWS Secrets Manager. This approach provides a centralized location for storing and managing secrets, such as database credentials, and allows for secure access to these secrets by other AWS services, such as Lambda functions.\nOption A, to hardcode the database credentials in GitHub for different environments of the Lambda function, is not a secure approach as the credentials would be publicly visible and could be easily compromised.\nOption B, to create a Lambda function for each environment and ensure each has a different programming language, is not an efficient approach as it would result in multiple functions to manage and deploy, which would increase complexity and maintenance overhead.\nOption D, to store the database credentials in a Lambda function tag, is not a recommended approach as function tags are meant to be used for metadata information, such as version number, and are not secure storage locations for sensitive information.\nIn summary, the recommended and secure approach is to store the database credentials in AWS Secrets Manager and retrieve them securely from within the Lambda function.\n\n"
}, {
  "id" : 459,
  "question" : "A telecom company has installed radio devices across the country.\nOn a daily basis, they are looking to collect logs from these thousands of devices &amp; analyze them further to monitor faults &amp; uptime trends.\nFor analysis of logs, they are planning to use Amazon Redshift.\nNo additional processing needs to be done on these logs.\nThe company is concerned about collecting data &amp; sending compressed data to Amazon Redshift &amp; is looking for a scalable solution without any ad-hoc administration. Which of the following services can be used to meet this requirement?\n",
  "answers" : [ {
    "id" : "09ea45bb517945ebaf726396ff9f031c",
    "option" : "Create an Amazon Kinesis Streams, save compressed data in Amazon EC2 and then copy data to Amazon Redshift.",
    "isCorrect" : "false"
  }, {
    "id" : "0ef0362c755c43c7bfe147c9e4aa1b5a",
    "option" : "Create an Amazon Kinesis Data Firehose Delivery Stream, save compressed data in the Amazon S3 bucket and then copy data to Amazon Redshift.",
    "isCorrect" : "true"
  }, {
    "id" : "62d45f64a1704902af0ee580882596bc",
    "option" : "Create an Amazon Kinesis Data Firehose Delivery Stream, save compressed data in Amazon EC2 and then copy data to Amazon Redshift.",
    "isCorrect" : "false"
  }, {
    "id" : "2a1bec0b8c57452eb5ddd5f83578cd96",
    "option" : "Create an Amazon Kinesis Streams, save compressed data in the Amazon S3 bucket and then copy data to Amazon Redshift.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nAmazon Kinesis Data Firehose can be used to deliver data from thousands of sources to AWS service for further analysis.\nIn the above case, the telecom company will use Amazon Redshift as an analytical service, so Amazon Kinesis Data Firehose Delivery Stream can be created to collect logs from thousands of radio devices.\nAmazon Kinesis Data Firehose cannot directly send data logs to Amazon Redshift but needs to first store in the Amazon S3 bucket &amp; then it copies data to Amazon Redshift.\nAmazon Kinesis Data Firehose is used when real-time streaming data is required to be delivered to destinations like Amazon S3 bucket, Amazon Redshift, Amazon OpenSearch (Amazon ES), and AWS Splunk.\nWhile delivering data to destinations, it supports the transformation of data like compression, batch, or encryption.\nAmazon Kinesis Data Firehose stores data only up to 24 hours if the destination is unavailable.\nOptions A and D are incorrect as the telecom company is not looking for additional processing or storing logs as all these logs would be generated daily.\nCreating Amazon Kinesis Data Firehose Delivery Stream is a better option than using Amazon Kinesis Streams.\nOption C is incorrect as Amazon Kinesis Data Firehose cannot save data to the Amazon EC2 instance.\nFor more information on Amazon Kinesis Data Firehose, refer to the following URL-\nhttps://docs.aws.amazon.com/firehose/latest/dev/what-is-this-service.html\nhttps://docs.aws.amazon.com/firehose/latest/dev/create-destination.html\n\nOption B - Create an Amazon Kinesis Data Firehose Delivery Stream, save compressed data in the Amazon S3 bucket and then copy data to Amazon Redshift is the correct answer for the given requirements.\nExplanation: Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk. It can capture, transform, and load streaming data into data stores or analytics tools, without requiring you to write any custom applications.\nIn this scenario, the telecom company has thousands of devices generating logs on a daily basis. They want to collect these logs and analyze them further to monitor faults and uptime trends. Amazon Redshift is the chosen data store for storing and analyzing the logs.\nTo send these logs to Amazon Redshift, we need a scalable, reliable, and low-maintenance solution. Amazon Kinesis Data Firehose fits the requirement. We can create a delivery stream in Amazon Kinesis Data Firehose, which acts as a pipeline that ingests and delivers the data to the destination.\nThe steps involved in using Amazon Kinesis Data Firehose for this scenario are:\nCreate a delivery stream in Amazon Kinesis Data Firehose. Choose Amazon S3 as the delivery destination, where the logs will be temporarily stored before being copied to Amazon Redshift. Enable compression in the delivery stream to reduce the size of the data being stored in Amazon S3. Configure the delivery stream to copy the compressed logs from Amazon S3 to Amazon Redshift.\nUsing Amazon Kinesis Data Firehose for this scenario has several advantages:\nScalability - Amazon Kinesis Data Firehose is highly scalable and can handle thousands of devices generating logs on a daily basis. Reliability - Amazon Kinesis Data Firehose automatically retries failed deliveries, and provides monitoring and alerting for errors. Low-maintenance - Amazon Kinesis Data Firehose is a fully managed service that eliminates the need for ad-hoc administration.\nOption A - Create an Amazon Kinesis Streams, save compressed data in Amazon EC2, and then copy data to Amazon Redshift, is not the best choice because it requires additional administration and is more complex compared to Amazon Kinesis Data Firehose.\nOption C - Create an Amazon Kinesis Data Firehose Delivery Stream, save compressed data in Amazon EC2, and then copy data to Amazon Redshift, is not the best choice because it is unnecessary to store the data in Amazon EC2 before copying it to Amazon Redshift.\nOption D - Create an Amazon Kinesis Streams, save compressed data in the Amazon S3 bucket, and then copy data to Amazon Redshift, is not the best choice because it requires additional processing using Amazon Kinesis Streams before copying the data to Amazon Redshift, which is not required in this scenario.\n\n"
}, {
  "id" : 460,
  "question" : "You are working as an AWS Architect for an enterprise customer.\nUsers access Amazon S3 buckets to save all project-related documents and use business applications like Office 365 for daily work activities.\nThese applications need to be accessible from any device for a limited number of hours in the day. They are using AWS SSO to manage and control access to AWS resources centrally.\nUsers are complaining that they are getting logout from the console &amp; need to re-login after each hour.\nYou need to ensure that the User session is optimum based on completing the activity.\nWhich of the following can be set to meet this requirement?\n",
  "answers" : [ {
    "id" : "e721f897857940848f1102b5470e4798",
    "option" : "Create a custom Permission Set with session duration as 24 hours.",
    "isCorrect" : "false"
  }, {
    "id" : "36eedbd94a0e4c518ec611e0d6e5a6d4",
    "option" : "Use an existing Job Function policy to set session duration as 24 hours.",
    "isCorrect" : "false"
  }, {
    "id" : "eaaa43649fb041249cd3f4e56ab978da",
    "option" : "Create a custom Permission Set with session duration as 6 hours.",
    "isCorrect" : "true"
  }, {
    "id" : "23aa8db9c62743fdb4b5e3e62081a1ed",
    "option" : "Use an existing Job Function policy to set session duration as 6 hours.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer -C.\nPermission sets can control the time duration for user login to the AWS Console by setting session duration.\nThe Default Session duration is 1 hour, while the maximum can be set to 12 hours.\nPost this session duration.\nThe user is automatically logout.\nAWS Single Sign-On (SSO) enables you to customize the session duration to AWS accounts ranging from 1 hour up to 12 hours.\nYou can configure session duration for each permission set so that you can optimize how long your users can access the AWS Management Console and AWS CLI for your AWS accounts.\nFor example, when your users need to run long-running operations, you can increase the session duration to complete the operation using a single session.\nOption A is incorrect as the maximum Session duration that can be set is 12 hours.\nOption B is incorrect.\nThis will use predefined AWS managed policies since the requirement is for a customized permission policy for session duration.\nAlso, the maximum Session duration that can be set is 12 hours.\nOption D is incorrect.\nThis will use predefined AWS managed policies since the requirement is for a customized permission policy for session duration.\nFor more information on Permission Set properties in AWS SSO, refer to the following URLs-\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-single-sign-on-now-enables-you-to-optimize-how-long-you-can-access-aws-accounts/\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/howtosessionduration.html\n\nThe issue reported by users suggests that the session duration for AWS SSO is not meeting their requirements, and they are being logged out of the console and need to re-login after each hour. As an AWS Architect, you need to ensure that the user session is optimized based on completing the activity.\nAWS Single Sign-On (SSO) is a cloud-based service that makes it easy to centrally manage SSO access to multiple AWS accounts and business applications. With AWS SSO, users can sign in to a user portal with their existing corporate credentials and access all their assigned accounts and applications from one place.\nWhen users sign in to the AWS SSO user portal, they obtain temporary security credentials that provide access to AWS resources for a limited period. This time period is defined as the session duration, which is configurable by using AWS SSO console.\nTo ensure that user sessions are optimized based on completing the activity, you should adjust the session duration based on the user's activity level. For example, if the users are accessing the applications for a limited number of hours in the day, you may want to set the session duration to match this time frame.\nOption A: Create a custom Permission Set with session duration as 24 hours.\nThis option would not optimize the user session based on completing the activity as the session duration is set to a fixed value of 24 hours. This could potentially lead to security risks as the user's temporary security credentials would be valid for a longer period.\nOption B: Use an existing Job Function policy to set session duration as 24 hours.\nThis option would not optimize the user session based on completing the activity as the session duration is set to a fixed value of 24 hours. This option may be useful if there is a job function policy that matches the user's activity level and provides access to the required resources.\nOption C: Create a custom Permission Set with session duration as 6 hours.\nThis option would optimize the user session based on completing the activity as the session duration is set to a value that matches the user's activity level. A custom Permission Set can be created in the AWS SSO console and assigned to the users who require access to the applications. The session duration can be set to 6 hours, which would provide the users with enough time to complete their work without being logged out of the console.\nOption D: Use an existing Job Function policy to set session duration as 6 hours.\nThis option would optimize the user session based on completing the activity as the session duration is set to a value that matches the user's activity level. An existing Job Function policy can be used to provide access to the required resources and set the session duration to 6 hours, which would provide the users with enough time to complete their work without being logged out of the console.\nIn conclusion, options C and D are the most suitable solutions to ensure that the user session is optimized based on completing the activity. These options provide the users with enough time to complete their work without being logged out of the console while still maintaining a high level of security.\n\n"
}, {
  "id" : 461,
  "question" : "Your company has a set of applications hosted on AWS.\nCurrently, the IT Admin is manually checking the database storage to see if it is getting full.\nWhich of the following can be used to automate these checks? (Select 2)\n",
  "answers" : [ {
    "id" : "b6ae0eaedfdf4c7ea66ae73d11bb774b",
    "option" : "CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "069d28bf8e854dffb731667bda7d9949",
    "option" : "Cloudwatch",
    "isCorrect" : "true"
  }, {
    "id" : "e992fe61716b4041981e4bdacc1dd447",
    "option" : "VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "82db9cc2653845b7a7287fbd253cc3f4",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nThe AWS Documentation mentions the following.\nCloudWatch - You can watch a single Amazon RDS metric over a specific time period and perform one or more actions based on the value of the metric relative to a threshold you set.\nOption A is incorrect since this is only used for API monitoring.\nOption C is incorrect since this is used for monitoring network traffic to your EC2 Instances.\nOption D is correct since AWS Trusted Advisor is an online tool that provides you real-time service limit checks.\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/best-practice-checklist/\nFor more information on monitoring for databases, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/MonitoringOverview.html\n\n\nThe two services that can be used to automate the checks for database storage are CloudWatch and AWS Trusted Advisor.\nCloudWatch is a monitoring service provided by AWS that can be used to monitor AWS resources like EC2 instances, RDS databases, ELB, and more. It provides detailed insights into resource utilization and performance metrics. You can use CloudWatch to monitor the storage of your databases and set up alarms to alert you when the storage limit is reached or exceeded. You can also create custom metrics and dashboards to track and analyze the database storage.\nAWS Trusted Advisor is a service that provides guidance to optimize AWS resources. It provides recommendations in five categories: Cost Optimization, Performance, Security, Fault Tolerance, and Service Limits. AWS Trusted Advisor checks your AWS account and compares it to best practices, identifying areas where you can improve. In this case, it can provide recommendations to optimize the storage of your databases and prevent them from getting full.\nTherefore, to automate checks for database storage, you can set up CloudWatch alarms to monitor the storage of your databases and configure AWS Trusted Advisor to provide recommendations to optimize the storage of your databases.\n\n"
}, {
  "id" : 462,
  "question" : "Your company has an application hosted in AWS.\nThis application consists of a web tier and database tier.\nThe web tier is hosted on EC2 Instances.\nThe database is hosted in the AWS RDS service, and data keeps changing every few hours.\nRecently performance issues have been encountered in the application, which is due to the high number of read requests.\nWhich of the following can be used to help resolve the issue?\n",
  "answers" : [ {
    "id" : "a2a26fa56cd94f4b9242ff080c99b93d",
    "option" : "Enable Multi-AZ for the database",
    "isCorrect" : "false"
  }, {
    "id" : "612cc89782e64b68bb42347aa3d93820",
    "option" : "Use Read Replica",
    "isCorrect" : "true"
  }, {
    "id" : "01255b675a9249ef8e18cced1653a8ad",
    "option" : "Use Amazon DynamoDB Accelerator (DAX)",
    "isCorrect" : "false"
  }, {
    "id" : "5495af5514f849368ae41aff3093cdb0",
    "option" : "Place an Elastic Cache service in front of the database service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer -B.\nIn terms of load, they have the same goal, but they differ in some areas:\nUp-to-dateness of data:\nA read replica will continuously sync from the master.\nSo your results will probably lag 0 - 3s (depending on the load) behind the master.\nA cache takes the query result at a specific point in time and stores it for a certain amount of time.\nPerformance:\nA cache can only return results for queries it has already seen.\nSo if you run the same queries over and over again, it's a good match.\nIf you have many different, frequently changing, or dynamic queries, a read replica will be a better match.\nElastiCache should be much faster since it's returning values directly from RAM.\nHowever, this also limits the number of results you can store.\nOption B is correct since the question specifies data keeps frequently changing, as it keeps data up-to-date and read performance is also improved.\nElastiCache can be used to reduce the latency of requests.\nStill, it is a caching service.\nAccording to the question, data keeps changing every few hours, so Elasticache is not recommended choice.\nOption A is incorrect since this is used for high availability for the databases.\nOption C is incorrect since Amazon DynamoDB Accelerator (DAX) is a Fully managed, in-memory cache for DynamoDB only.\nFor more information on Read Replica and Elasticache, please refer to the below URLs-\nhttps://aws.amazon.com/rds/features/read-replicas/\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/elasticache-use-cases.htm\n\nTo resolve the performance issue in the application caused by a high number of read requests, there are several options to consider.\nA. Enable Multi-AZ for the database: Enabling Multi-AZ (Availability Zone) for the database creates a standby replica of the primary database in a different Availability Zone. This feature provides high availability and automatic failover capabilities for the database. Multi-AZ does not directly address the issue of high read requests but can help to ensure availability and durability of the database.\nB. Use Read Replica: A Read Replica is a copy of the database that is used to serve read requests, offloading traffic from the primary database. Read replicas are available for several AWS database services, including Amazon RDS, Amazon Aurora, and Amazon DocumentDB. The primary database continues to serve write requests, while the read replicas serve read requests. This approach can be effective in reducing the load on the primary database, improving read performance and reducing latency.\nC. Use Amazon DynamoDB Accelerator (DAX): DynamoDB Accelerator (DAX) is an in-memory cache that can improve the performance of read-heavy workloads on DynamoDB tables. It is a fully managed service that is API-compatible with DynamoDB, and can be used to speed up read requests by caching frequently accessed data. DAX can improve the performance of applications that require microsecond response times and can significantly reduce the load on the database.\nD. Place an Elastic Cache service in front of the database service: Amazon ElastiCache is a managed in-memory cache service that can be used to improve the performance of read-heavy workloads. It supports popular caching engines such as Memcached and Redis, and can be used to store frequently accessed data in memory, reducing the load on the database. Placing an Elastic Cache service in front of the database can improve the performance of read requests and reduce latency.\nIn summary, options B, C, and D can help to address the performance issue caused by high read requests. Option A can help to ensure availability and durability but does not directly address the issue of high read requests. The best option to choose depends on the specific requirements and constraints of the application.\n\n"
}, {
  "id" : 463,
  "question" : "A Startup company is launching a three-tier application with the Multicontainer Docker platform.\nThis application needs to be integrated with the Amazon RDS database instance.\nThe application will be launched using AWS Elastic Beanstalk.\nAs an AWS consultant for this company, you need to design the environment for blue/green deployment and decoupled architecture in the production environment.\nWhat would you recommend for integrating the Amazon RDS database with AWS Elastic Beanstalk?\n",
  "answers" : [ {
    "id" : "3e1b2bdba4054bb5ae980ae47390b949",
    "option" : "Launch Amazon RDS instance within the same AWS Elastic Beanstalk environment, setting connection string to the database in environment properties.",
    "isCorrect" : "false"
  }, {
    "id" : "3c7b2b33ccc7474fa8f459cb6523fa71",
    "option" : "Launch Amazon RDS instance outside the AWS Elastic Beanstalk environment storing the connection string in the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "779cd21893384f88a9ae80c04aa1f2b4",
    "option" : "Launch Amazon RDS instance within the same AWS Elastic Beanstalk environment storing the connection string in the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "0285997c9b344d52be21d10f05dfad69",
    "option" : "Launch Amazon RDS instance outside AWS Elastic Beanstalk environment, setting connection string to the database in environment properties.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Elastic Beanstalk provisions and configures all the AWS resources required to run and support your application.\nFor the Amazon RDS database instance to be launched in the production environment, the best practice is to launch it outside the AWS Elastic Beanstalk environment.\nIt helps in having multiple environments connecting to a single database, using database types not supported with the integrated database, performing blue/green deployments.\nAlso, the database instance remains up &amp; running when the AWS Elastic Beanstalk environment is terminated.\nFor a production environment, you can launch a database instance outside of your environment and configure your application to connect to it outside of the functionality provided by Elastic Beanstalk.\nUsing a database instance that is external to your environment requires additional security group and connection string configuration.\nProviding connection information to your application with environment properties is a good way to keep passwords out of your code, but it's not a perfect solution.\nEnvironment properties are discoverable in the environment management console and can be viewed by any user with permission to describe configuration settings on your environment.\nDepending on the platform, environment properties may also appear in instance logs.\nYou can lock down your connection information by storing it in an Amazon S3 bucket that you control.\nThe basic steps are as follows:\nUpload a file that contains your connection string to an Amazon S3 bucket.\nGrant the EC2 instance profile permission to read the file.\nConfigure your application to download the file during deployment.\nRead the file in your application code.\nOption A is incorrect as launching Amazon RDS in an AWS Elastic Beanstalk environment is suitable for test/development purposes &amp; not for a production environment.\nIf the AWS Elastic Beanstalk environment is terminated, the Amazon RDS database instance is also terminated.\nOption C is incorrect as launching Amazon RDS in an AWS Elastic Beanstalk environment is suitable for test/development purposes &amp; not for the production environment.\nOption D is incorrect.\nWhen the Amazon RDS instance is launched outside the AWS Elastic Beanstalk environment, the best practice is to save the connection string in the Amazon S3 bucket.\nFor more information on launching Amazon RDS instance with AWS Elastic Beanstalk and storing the connection string, refer to the following URLs-\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.db.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/AWSHowTo.RDS.html\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/rds-external-credentials.html\n\nWhen it comes to integrating an Amazon RDS database with AWS Elastic Beanstalk for a three-tier application, there are a few factors that need to be considered such as scalability, availability, and decoupling. Blue/green deployment is a deployment strategy that allows you to launch a new version of an application alongside the old version and then switch traffic to the new version once it's ready. This ensures that you can quickly roll back to the old version in case there are any issues with the new version.\nGiven these requirements, option A (Launch Amazon RDS instance within the same AWS Elastic Beanstalk environment, setting connection string to the database in environment properties) is a suitable choice for integrating Amazon RDS with Elastic Beanstalk for this three-tier application.\nExplanation:\nOption A proposes launching an Amazon RDS instance within the same AWS Elastic Beanstalk environment, which allows for better integration and scalability of the application. By having the database instance within the same environment, you can configure the environment properties to include the connection string for the database. This makes it easier to manage and maintain the connection between the application and the database.\nAdditionally, having the database within the same environment allows for easy scalability of both the application and the database. You can scale the application and database together as needed, making sure that the resources are being utilized efficiently.\nFurthermore, this option also satisfies the requirement for a decoupled architecture. With the database and application within the same environment, they can still be considered decoupled because they are running on separate containers. This means that they are isolated from each other and can be managed independently.\nOption B (Launch Amazon RDS instance outside the AWS Elastic Beanstalk environment storing the connection string in the S3 bucket) is not a suitable choice because it introduces additional complexity and can make it difficult to manage the connection between the application and the database. This option also does not provide the necessary decoupling between the application and database.\nOption C (Launch Amazon RDS instance within the same AWS Elastic Beanstalk environment storing the connection string in the S3 bucket) is not a suitable choice because storing the connection string in an S3 bucket introduces additional complexity and can lead to issues with the connection between the application and database. It also does not provide any benefits over option A.\nOption D (Launch Amazon RDS instance outside AWS Elastic Beanstalk environment, setting connection string to the database in environment properties) is not a suitable choice because it does not allow for the necessary integration and scalability of the application and database. This option also does not provide the necessary decoupling between the application and database.\nIn summary, option A is the best choice because it allows for better integration, scalability, and decoupling of the application and database in a blue/green deployment environment.\n\n"
}, {
  "id" : 464,
  "question" : "Your company has an application that has been developed and needs to be hosted on an EC2 Instance.\nThe EC2 Instance is located in a private subnet and needs to access AWS Kinesis streams without passing into the Internet.\nHow can you achieve this in the best manner possible?\n",
  "answers" : [ {
    "id" : "df59088f325745c1a46c3988dee0b9f0",
    "option" : "Attach a NAT gateway to the VPC",
    "isCorrect" : "false"
  }, {
    "id" : "72ffc158ec8443deafb3ae01bd922930",
    "option" : "Attach an Internet gateway to the VPC",
    "isCorrect" : "false"
  }, {
    "id" : "4542418f588849bfb2c5eb4258187f70",
    "option" : "Create a VPC Gateway Endpoint that would allow access to Kinesis Streams",
    "isCorrect" : "false"
  }, {
    "id" : "8329c294a5bd4b1791e933ac3b74c078",
    "option" : "Create a VPC Interface Endpoint that would allow access to Kinesis Streams.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nYou can use an interface VPC endpoint to keep traffic between your Amazon VPC and Kinesis Data Streams from leaving the Amazon network.\nInterface VPC endpoints don't require an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.\nInterface VPC endpoints are powered by AWS PrivateLink, an AWS technology that enables private communication between AWS services using an elastic network interface with private IPs in your Amazon VPC.Options A and B are incorrect since it is mentioned in the question that traffic should not go via the Internet.\nOption C is incorrect since this is mostly used for S3 and DynamoDB access from Instances in the private subnet.\nFor more information on VPC Endpoints Interfaces, please refer to the below URL-\nhttps://docs.aws.amazon.com/streams/latest/dev/vpc.html\n\nTo access AWS Kinesis streams from an EC2 instance located in a private subnet without routing through the internet, you can use a VPC endpoint. VPC endpoints allow you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.\nSo, the correct answer to this question is C. Create a VPC Gateway Endpoint that would allow access to Kinesis Streams.\nOption A, attaching a NAT gateway to the VPC, is not the best option because NAT gateway requires traffic to flow through the internet gateway, which is not the requirement here.\nOption B, attaching an Internet gateway to the VPC, is not a suitable option because this would allow the EC2 instance to access the internet, which is not required in this scenario.\nOption D, creating a VPC interface endpoint, is also not the correct option for this use case because it is used to connect to an AWS service using a private IP address within the same region, and Kinesis does not support VPC interface endpoints.\nTherefore, the best solution is to create a VPC Gateway Endpoint, which allows the EC2 instance to access Kinesis streams without going through the internet.\n\n"
}, {
  "id" : 465,
  "question" : "A company is planning to store sensitive documents in an S3 bucket.\nThey want to ensure that documents are encrypted at rest.\nThey want to ensure they manage the underlying keys used for encryption but not the encryption/decryption process.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "9b91f8033fab45b0919b7fe6f9eb226c",
    "option" : "Use S3 server-side encryption with Customer keys.",
    "isCorrect" : "true"
  }, {
    "id" : "4c1454c6974144d48474a1cde4d83cda",
    "option" : "Use S3 client-side encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "21b0666664cb4a23a231be8100463500",
    "option" : "Use S3 server-side encryption with AWS managed keys.",
    "isCorrect" : "false"
  }, {
    "id" : "b767fa8cc97c4d34850ede789a34c4d8",
    "option" : "Use S3 server-side encryption with AWS KMS keys with Key policy document of size 40kb.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nAWS Documentation mentions the following.\nServer-side encryption is about protecting data at rest.\nUsing server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your own encryption keys.\nWith the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it writes to disks, and decryption, when you access your objects.\nTherefore, you don't need to maintain any code to perform data encryption and decryption.\nThe only thing you do is manage the encryption keys you provide.\nIn short,\nSSE-S3 requires that Amazon S3 manage the data and master encryption keys.\nSSE-C requires that you manage the encryption key.\nSSE-KMS requires that AWS manage the data key, but you manage the master key in AWS KMS.\nFor more information, please refer to the following URL.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\nOption B is incorrect because when you do client-side encryption, data goes to s3 in an encrypted format.\nAgain when you download, it is the client who has to decrypt the data.\nBut question specifies customer should not manage the encryption/decryption process.\nOption C is incorrect since you will still not manage the complete lifecycle of the keys.\nOption D is incorrect because the maximum key policy document size is 32kb.\nReference:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/limits.html\nhttps://aws.amazon.com/blogs/aws/new-bring-your-own-keys-with-aws-key-management-service/\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html\n\nThe correct answer for this scenario is A. Use S3 server-side encryption with customer keys.\nExplanation:\nS3 server-side encryption with customer keys: In this method, the encryption keys are managed by the customer, not by AWS. Customers can create and manage their own encryption keys using AWS Key Management Service (KMS) or using their own encryption keys. With this option, customers have full control over their encryption keys, and AWS does not have access to them. The encryption and decryption process is handled by AWS, but the encryption keys are managed by the customer. This is the recommended method when the customer wants full control over the encryption keys. S3 client-side encryption: In this method, the encryption and decryption process is handled by the client. The data is encrypted before uploading it to S3, and decrypted after downloading it from S3. The encryption keys are managed by the customer, and AWS does not have access to them. This method is suitable when the customer wants to fully manage the encryption and decryption process. S3 server-side encryption with AWS managed keys: In this method, the encryption keys are managed by AWS. AWS provides and manages the encryption keys. The customer does not have access to the encryption keys. This method is suitable when the customer does not want to manage the encryption keys themselves. S3 server-side encryption with AWS KMS keys with Key policy document of size 40kb: This is not a valid option. AWS KMS keys are used for managing the encryption keys, and the Key policy document does not affect the encryption process.\nIn summary, S3 server-side encryption with customer keys is the best option for this scenario, as it provides full control over the encryption keys while allowing AWS to manage the encryption and decryption process.\n\n"
}, {
  "id" : 466,
  "question" : "Your company currently has the following architecture for its e-commerce application. Â· EC2 Instances hosting the application Â· An Autoscaling group for the EC2 Instances The users who use the application keep on complaining that the application is slow in the morning from 9:00 - 9:30, after which no issues occur.\nWhich of the following can be done to ensure the issue is not encountered during the morning time?\n",
  "answers" : [ {
    "id" : "80c98f0fdab3416f91a3d546626417a7",
    "option" : "Ensure that a Simple scaling policy is added to the Auto scaling Group.",
    "isCorrect" : "false"
  }, {
    "id" : "9ade351e1c2142e2a1b87e39cd92bca0",
    "option" : "Ensure that a step scaling policy is added to the Auto scaling Group.",
    "isCorrect" : "false"
  }, {
    "id" : "491f1d5bbf9b4decabce82f0f296f386",
    "option" : "Ensure that a scheduled scaling policy is added to the Auto scaling Group.",
    "isCorrect" : "true"
  }, {
    "id" : "4a3605b88e1c419280c0072fa7cd4872",
    "option" : "Ensure that a static scaling policy is added to the Auto scaling Group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nScaling based on a schedule allows you to scale your application in response to predictable load changes.\nFor example, every week, the traffic to your web application starts to increase on Wednesday, remains high on Thursday, and starts to decrease on Friday.\nYou can plan your scaling activities based on the predictable traffic patterns of your web application.\nOption A is incorrect because simple scaling increases or decreases the group's current capacity based on a single scaling adjustment.\nOption B is incorrect because Step Scaling increases or decreases the group's current capacity based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach.\nOption D is incorrect since Static scaling policy doesn't exist.\nFor more information on scaling policies for Auto scaling, please refer to the below URL's-\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html#as-scaling-types\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html\n\nThe issue of slow application performance during specific times suggests that there is a sudden increase in traffic to the application during that period. The current architecture, which uses an EC2 Autoscaling group, can help with handling increases in traffic, but it may not be configured properly to handle such sudden surges.\nA. Adding a Simple scaling policy to the Autoscaling group means that the group will automatically adjust the number of instances based on CPU utilization or any other metric that is specified. This policy may not be effective in handling sudden surges of traffic since it may take time to spin up new instances, and this could cause the application to continue to be slow during the morning period.\nB. A step scaling policy would allow the Autoscaling group to adjust the number of instances based on the metric in a predefined set of steps, which would be faster than a simple scaling policy. This policy may be helpful, but it may not be ideal if the surge in traffic is not predictable or if the number of instances required during the surge varies widely.\nC. A scheduled scaling policy allows the Autoscaling group to adjust the number of instances based on a predetermined schedule. This policy may be ideal for addressing the morning slowdown issue since it allows for preemptive scaling before the expected surge in traffic. By adding a scheduled scaling policy to the Autoscaling group, it is possible to have the group automatically spin up additional instances before the morning period, ensuring that there are enough resources available to handle the surge in traffic.\nD. A static scaling policy means that the number of instances in the Autoscaling group is fixed, and it does not change based on any metric or schedule. This policy may not be effective in handling the morning slowdown issue since it does not allow for additional resources to be added during the expected surge in traffic.\nTherefore, the best option for ensuring that the issue is not encountered during the morning period is to add a scheduled scaling policy to the Autoscaling group (Option C).\n\n"
}, {
  "id" : 467,
  "question" : "A three-tier web application is running on AWS VPC in two availability zones, as shown below.\nHere are the CIDR ranges for each subnet and the corresponding servers. Layer EU-East-1a EU-east-1b Web server 10.16.0.0/25 10.16.0.128/25 Application Server 10.16.1.0/25 10.16.1.128/25 DB Server 10.16.2.0/25 10.16.2.128/25 The DB server runs MySQL, which would run on its default port, should only allow access to the Application Server tier and access from the rest of the tiers should be denied. Which inbound rule of the Security Group on the DB server needs to be attached to meets this requirement?\n",
  "answers" : [ {
    "id" : "8eeb88ecbe144f24a990f6eb0cb78549",
    "option" : "Type : MySQL | Protocol: TCP | Port: 3306 | Source: 10.16.0.0/24",
    "isCorrect" : "false"
  }, {
    "id" : "a78b3745004744048b2a7eccecb3a2bb",
    "option" : "Type: MySQL | Protocol: TCP | Port: 3306 | Source: 10.16.2.0/24",
    "isCorrect" : "false"
  }, {
    "id" : "bd9b74c849d84c9fa5a9ae9cbd9363b6",
    "option" : "Type : MySQL | Protocol: TCP | Port : 3306 | Source: 10.16.1.0/24",
    "isCorrect" : "true"
  }, {
    "id" : "d64c3c18f526461c9a4af4cf1a2e4996",
    "option" : "Type : MySQL | Protocol: TCP | Port: 3306 | Source: 10.16.3.128/25",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nThe requirement here is to allow access from Application Tier to DB Tier.\nThe other point to note here is that MySQL would run on its default port.\nThe default port for MySQL is â€˜3306.'\nTwo /25 networks equal a /24 network.\nTwo /27 networks equal a /26 network.\nTwo /26 networks equal a /25 network.\nAnd so on, and so on.\nThe notion of combining two smaller networks into a larger one is another benefit of classless networks named supernetting.\nTo create a supernet, the smaller networks must be contiguous.\nFor example, 192.0.2.240/29 and 192.0.2.248/29 can form a supernet 192.0.2.240/28, but 192.0.2.240/29 and 192.0.2.8/29 could not as it must be 192.0.2.248/29 to form a supernet.\nApplication Server tier IPs across two subnets are - 10.16.1.0/25 and 10.16.1.128/25 that is from 10.16.1.0 to 10.16.1.255 which is the same as 10.16.1.0/24.\nSo Option C is the best answer.\nOptions A, B, and D are incorrect because the source should be 10.16.1.0/24.\nOr alternatively, two inbound rules from Port 3306 but different sources, i.e., 10.16.1.0/25 and 10.16.1.128/25.\n\nThe DB server runs MySQL and is part of a three-tier web application that is running on AWS VPC in two availability zones. The DB server must only allow access to the Application Server tier, and access from the rest of the tiers should be denied. Therefore, the Security Group on the DB server must be configured to allow access from the Application Server tier and deny access from the other tiers.\nIn AWS, Security Groups act as virtual firewalls that control the inbound and outbound traffic for instances. Security Groups are associated with instances or network interfaces, and they filter traffic based on the rules defined in them. A Security Group rule defines the type of traffic, the protocol, and the port range that are allowed or denied for the inbound or outbound traffic.\nGiven the CIDR ranges for each subnet and the corresponding servers, we can see that the Application Server tier is in the subnet 10.16.1.0/25 and 10.16.1.128/25, and the DB Server tier is in the subnet 10.16.2.0/25 and 10.16.2.128/25. Therefore, to allow access from the Application Server tier and deny access from the other tiers, we need to create a Security Group rule that allows inbound traffic to the DB server on port 3306 from the CIDR range of the Application Server tier and denies inbound traffic from the CIDR ranges of the other tiers.\nFrom the given options, the Security Group rule that meets this requirement is:\nC. Type : MySQL | Protocol: TCP | Port : 3306 | Source: 10.16.1.0/24\nThis rule allows inbound traffic to the DB server on port 3306 from the CIDR range 10.16.1.0/24, which includes the IP addresses of the Application Server tier. This rule denies inbound traffic to the DB server from the CIDR ranges of the Web Server tier and the DB Server tier.\nOption A is incorrect because it allows inbound traffic to the DB server from the CIDR range of the Web Server tier, which is 10.16.0.0/24.\nOption B is incorrect because it allows inbound traffic to the DB server from the CIDR range of the DB Server tier, which is 10.16.2.0/24.\nOption D is incorrect because it allows inbound traffic to the DB server from the CIDR range of the Web Server and Application Server tiers, which is 10.16.3.128/25. This is not a valid CIDR range as it overlaps with the subnet CIDR ranges.\n\n"
}, {
  "id" : 468,
  "question" : "You launched 9 spot instances for a specific workload in your AWS Account.\nYour bid price was $0.07 per hour, and the spot price at the time of launch was $0.06 per hour.After 1.5 hours, the spot price rises to $0.08 an hour.\nWhat is the cost incurred?\n",
  "answers" : [ {
    "id" : "6ae343ff4568471a8134ebe1d3813047",
    "option" : "$0.54",
    "isCorrect" : "true"
  }, {
    "id" : "78d765fa077c4539a431182c7e9ba7a1",
    "option" : "$0.24",
    "isCorrect" : "false"
  }, {
    "id" : "3c94e2ea5a7d48ffb0f5ad49f293913d",
    "option" : "$0.00",
    "isCorrect" : "false"
  }, {
    "id" : "c5cc727b376e4488972d11136e63dca4",
    "option" : "$0.44",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nSpot instances are those instances for which a user has to place a bid on the AWS portal.\nIf the bid price is greater than the amazon price (i.e., spot price), then the spot instances are automatically granted.\nThe user would be charged based on the â€˜spot' price instead of the â€˜bid' price.\nIf the bid price is lower than the amazon price (i.e., spot price), then the spot instances are cancelled.\nNow, if a user has got a spot instance running and if suddenly the spot price goes up, then amazon automatically cancels the instance, and the user is not charged for the extra minutes (rounded to one hour)\nThis is called as amazon's termination of the spot instance.\nIn a second case, if a user has got a spot instance running (when the bid price is greater than the spot price and the user is granted the â€˜spot instance' on the â€˜spot' price) and the user by himself voluntarily terminates the spot instance, then the user is charged till the minute he has used the spot instance.\nThis is called the user's voluntary termination of the spot instance.\nWith the above introduction, we can proceed with the below calculation.\nIn the first hour,\nBid price = $0.07\nSpot price = $0.06\nTherefore the user is granted the spot instance.\nNow the price for â€˜9' instances for the first hour would be = $0.06 * 9 = $0.54\nIn the second hour (i.e., for 0.5 hour)\nBid price = $0.07\nSpot price = $0.08\nNow the spot price is greater than the bid price which will end up in the spot instance being terminated by amazon, and the user is not charged any amount for the instances for the 0.5 hours that the instances ran.\nTherefore the total payable amount by the user for â€˜9' instances is = $0.54\nTherefore Option â€˜A' is the correct answer.\nOption â€˜B' is incorrect because the total cost is $0.24 which is lesser than $0.54\nOption â€˜C' is incorrect because the total cost is $0.00 which is lesser than $0.54\nOption â€˜D' is incorrect because the total cost is $0.44 which is lesser than $0.54\nFor details on spot instances, please refer to the following link-\nhttps://aws.amazon.com/ec2/spot/pricing/\n\nTo calculate the cost incurred, we need to determine how many hours the instances ran and at what price.\nThe instances were launched at a bid price of $0.07 per hour, and the spot price at the time of launch was $0.06 per hour. This means that the instances were launched at a bid price higher than the current spot price, so they should have been launched immediately.\nAfter 1.5 hours, the spot price rose to $0.08 an hour. We need to determine how many hours the instances ran at each price.\nFor the first 1.5 hours, the instances ran at the bid price of $0.07 per hour, which means that the cost incurred during this time is:\n9 instances x $0.07 per instance per hour x 1.5 hours = $0.945\nAfter 1.5 hours, the spot price rose to $0.08 an hour. We need to determine how many more hours the instances ran at this higher price.\nTo do this, we need to find out if the bid price of $0.07 is still higher than the current spot price of $0.08. If it is, the instances will continue to run. If not, the instances will be terminated.\nSince the bid price is lower than the current spot price, the instances will be terminated. However, we need to determine how many more hours the instances would have run if the spot price had remained at $0.06 per hour.\nTo do this, we need to find out the difference between the bid price and the current spot price, and then divide the bid price by this difference:\n$0.07 - $0.06 = $0.01\n$0.07 / $0.01 = 7\nThis means that the instances would have run for 7 more hours if the spot price had remained at $0.06 per hour.\nTherefore, the total cost incurred is:\n$0.945 for the first 1.5 hours + (9 instances x $0.06 per instance per hour x 7 hours) = $0.945 + $3.78 = $4.725\nHowever, since these are spot instances, the actual cost incurred may be lower if the instances were terminated before the full 7 hours at the higher spot price.\n\n"
}, {
  "id" : 469,
  "question" : "A start-up firm is developing a microservices-based application using open-source container orchestration.\nThis application will be integrated with other Public Cloud.\nThe firm does not have any expertise to provision &amp; manage back-end infrastructure to set up this container.\nYou have been assigned to provide consultation for deploying containers. Which of the following will you suggest meeting the requirement?\n",
  "answers" : [ {
    "id" : "53dfb67cf308486caa0b5262d9867f3c",
    "option" : "Use Amazon Elastic Kubernetes Service with Amazon EC2 launch type.",
    "isCorrect" : "false"
  }, {
    "id" : "ab8b9c1e662147259ebf0b153c740971",
    "option" : "Use Amazon Elastic Container Service with AWS Fargate launch type.",
    "isCorrect" : "false"
  }, {
    "id" : "a6eb4d4c338642b5a590fc1df64ca678",
    "option" : "Use Amazon Elastic Container Service with Amazon EC2 launch type.",
    "isCorrect" : "false"
  }, {
    "id" : "f8311bfa1a6c49869ddad0b915f09f08",
    "option" : "Use Amazon Elastic Kubernetes Service with AWS Fargate launch type.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nAmazon Elastic Kubernetes Service can be used to set up open-source Container orchestration like Kubernetes.\nAmazon Elastic Kubernetes Service can be integrated with other public or private clouds.\nAWS Fargate can be used as a serverless compute for deploying containers.\nWith AWS Fargate, there is no need to set back-end infrastructure for containers.\nWith AWS Fargate, based upon task definitions, containers are launched.\nThe following diagram depicts the deployment of AWS Kubernetes, with AWS Fargate, serverless containers can be deployed while using Amazon EC2, compute nodes are deployed manually as per requirement.\nUsing Amazon EC2 provides more control on back-end infrastructure for containers.\nBut customers have to perform management of these instances.\nOption A is incorrect as with the Amazon EC2 launch type.\nThe firm will need to set up backend infrastructure &amp; manage instance provisioning.\nOption B &amp; C are incorrect as the customer is looking for integration with Public Cloud &amp; an open-source solution.\nAmazon Elastic Container Service supports integration with AWS services &amp; is an AWS Proprietary service.\nFor more information on AWS Fargate with Amazon EKS, refer to the following URLs-\nhttps://aws.amazon.com/eks\nhttps://aws.amazon.com/fargate\n\n\nThe requirement is to deploy containers for a microservices-based application that will be integrated with other public cloud, without the need for expertise to provision and manage backend infrastructure. In this scenario, the best option is to use a managed container service that provides ease of deployment, scaling, and management of containers.\nOption A: Use Amazon Elastic Kubernetes Service with Amazon EC2 launch type. Amazon Elastic Kubernetes Service (EKS) is a managed Kubernetes service that makes it easy to deploy, scale, and manage containerized applications using Kubernetes on AWS. With the Amazon EC2 launch type, you can use your own EC2 instances to run your EKS cluster's Kubernetes control plane instances and worker nodes. This option requires expertise in managing the EC2 instances, including monitoring, scaling, and patching.\nOption B: Use Amazon Elastic Container Service with AWS Fargate launch type. Amazon Elastic Container Service (ECS) is a fully-managed container orchestration service that makes it easy to run, stop, and manage Docker containers on a cluster. With the AWS Fargate launch type, you don't need to provision or manage any servers. AWS Fargate is a serverless compute engine for containers that eliminates the need to manage servers or clusters. This option provides the easiest way to deploy and manage containers without any backend infrastructure knowledge.\nOption C: Use Amazon Elastic Container Service with Amazon EC2 launch type. This option is similar to Option B, except that you are responsible for provisioning, scaling, and managing the EC2 instances. This option requires expertise in managing the EC2 instances, including monitoring, scaling, and patching.\nOption D: Use Amazon Elastic Kubernetes Service with AWS Fargate launch type. This option combines the benefits of both Option A and Option B. You get the managed Kubernetes service with the ease of deployment provided by the AWS Fargate launch type. This option requires less expertise in managing the backend infrastructure compared to Option A.\nIn conclusion, the best option to meet the requirements of the scenario is Option B: Use Amazon Elastic Container Service with AWS Fargate launch type. This option provides a fully-managed container orchestration service with a serverless compute engine for containers, eliminating the need to provision or manage any servers or clusters.\n\n"
}, {
  "id" : 470,
  "question" : "A company has a requirement to monitor API activity for audit purposes for their AWS account.\nThis audit would be conducted in the future as well and should apply to all regions.\nHow would you design your solution to meet the present and future needs?\n",
  "answers" : [ {
    "id" : "098e2e8ea56942aa9e5ad7ca8a5547e3",
    "option" : "Ensure CloudTrail logs is enabled for each region and then enable for each future region.",
    "isCorrect" : "false"
  }, {
    "id" : "c9dc20e1c0754dd9b313a3d8b8b2529c",
    "option" : "Ensure one CloudTrail log is enabled for all regions",
    "isCorrect" : "true"
  }, {
    "id" : "8c8e7b3eebb64333838d3021a2eb717c",
    "option" : "Enable AWS Config to record the events in all regions.",
    "isCorrect" : "false"
  }, {
    "id" : "60e1a6f8a2d94a6795136880cf598ba1",
    "option" : "Enable AWS Inspector to record the events in all regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nWhen you create a trail that applies to all regions, CloudTrail records events in each region and delivers the CloudTrail event log files to an S3 bucket that you specify.\nIf a region is added after you create a trail that applies to all regions, the new region is automatically included, and events in that region are logged.\nAWS CloudTrail increases visibility into your user and resource activity by recording AWS Management Console actions and API calls.\nYou can identify which users and accounts are called AWS, the source IP address from which the calls were made, and when the calls occurred.\nOption A is incorrect since this is an overhead to enable it each time for every new region.\nOptions C is incorrect since AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources.\nConfig continuously monitors and records your AWS resource configurations.\nOption D is incorrect since Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.\nAmazon Inspector automatically assesses applications for exposure, vulnerabilities, and deviations from best practices.\nFor more information on AWS CloudTrail, please refer to the below URL-\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html\n\nThe best approach to meet the requirement of monitoring API activity for audit purposes for all regions in the AWS account is to enable CloudTrail logs for each region and also for future regions. Therefore, option A is the correct answer.\nCloudTrail is a web service that records AWS API calls made in an AWS account and delivers the log files to an Amazon S3 bucket. By enabling CloudTrail logs, one can track every event that occurs in the account, including user activities, resource access, and API calls. In addition, CloudTrail logs provide an audit trail that can be used to ensure compliance with internal policies and regulatory requirements.\nTo enable CloudTrail logs for each region, one should follow the steps below:\nNavigate to the AWS Management Console and choose the CloudTrail service. Click on the Trails link and select the Create Trail button. Provide a name for the trail, choose the S3 bucket where the log files will be stored, and enable log file validation. Under the Management events section, select the Read/Write events option to capture all API activity. Choose the regions that need to be monitored. Click on Create to create the trail.\nBy following the above steps, CloudTrail logs will be enabled for each region selected during the trail creation process. When a new region is added in the future, the process should be repeated to enable CloudTrail logs for the new region.\nOption B is not recommended since having one CloudTrail log for all regions can cause performance issues and create a single point of failure.\nOption C is not the best solution since AWS Config is primarily used to assess, audit, and evaluate the configurations of AWS resources. It can be used to monitor changes made to AWS resources and configurations but does not track API activity.\nOption D is not the best solution since AWS Inspector is a security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not intended for tracking API activity.\n\n"
}, {
  "id" : 471,
  "question" : "Your team is planning to develop and deploy an application onto AWS with the following architecture. Â· A set of EC2 Instances in a VPC hosting the web tier Â· A database hosted using the AWS RDS MySQL instance Which of the following should ideally be set so that only HTTPS users can access the web application and for the web application to access the database? (Choose 2)\n",
  "answers" : [ {
    "id" : "0baf568c97d841a4a0f6558f2933c384",
    "option" : "An Inbound Security group rule for the web EC2 Instances allowing traffic from the source of 0.0.0.0/0 and port 443",
    "isCorrect" : "true"
  }, {
    "id" : "2e0d7b07c5f545239c0733a2fe70bce6",
    "option" : "An Inbound Security group rule for the database layer allowing traffic from the source of 0.0.0.0/0 and port 443",
    "isCorrect" : "false"
  }, {
    "id" : "ffd1edef02d746f8a8e8b5d9024cd815",
    "option" : "An Inbound Security group rule for the web EC2 Instances allowing traffic from the source of the database on port 3306",
    "isCorrect" : "false"
  }, {
    "id" : "437440e61a4a44138b1a847c308a62b9",
    "option" : "An Inbound Security group rule for the database layer allowing traffic from the source of the web layer on port 3306",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nOption A is correct because port 443 will allow only HTTPS traffic from all sources.\nOption D is correct because the Database server Security Group must allow traffic from the source Web server on port 3306.\nOption B is invalid since the database should not be exposed to the Internet.\nOption C is invalid since the database security group should allow incoming traffic on port 3306.\nPlease refer to the below URL and go to the Security section for more information on this scenario.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\n\n\nTo ensure that only HTTPS users can access the web application and for the web application to access the database, the following steps should be taken:\nCreate a security group for the web EC2 instances: A security group acts as a virtual firewall for the instances in the VPC, controlling inbound and outbound traffic. To restrict access to the web tier to only HTTPS users, an inbound security group rule should be created for the web EC2 instances, allowing traffic from the source of 0.0.0.0/0 (which means any IP address) and port 443 (which is the default port for HTTPS traffic).\nTherefore, answer A is correct.\nCreate a security group for the database RDS instance: Similar to the web tier, a security group should also be created for the database RDS instance. However, instead of allowing traffic from any IP address on port 443, traffic should only be allowed from the web tier. An inbound security group rule should be created for the database layer allowing traffic from the source of the web layer on port 3306 (which is the default port for MySQL traffic).\nTherefore, answer D is correct.\nOption B is not correct because allowing traffic from any IP address to the database layer on port 443 would open up a security vulnerability, as it would allow anyone to connect to the database.\nOption C is also not correct because allowing traffic from the web tier to the database on port 3306 would expose the database to potential security threats from the internet.\nIn summary, the correct answers are A and D.\n\n"
}, {
  "id" : 472,
  "question" : "In your AWS VPC, you need to add a new subnet that will allow you to host a total of 20 EC2 instances.\nWhich IPv4 CIDR block would you use to achieve the same?\n",
  "answers" : [ {
    "id" : "0e774ef43119413dbad186801accb680",
    "option" : "151.0.0.0/27",
    "isCorrect" : "true"
  }, {
    "id" : "12b96e4f32a847c0975a5a4a2e7a1b6c",
    "option" : "151.0.0.0/28",
    "isCorrect" : "false"
  }, {
    "id" : "aef3b9b8a83a4fed9ac4b142860d9312",
    "option" : "151.0.0.0/29",
    "isCorrect" : "false"
  }, {
    "id" : "ac0264da49164676bcd8622ece47ce3e",
    "option" : "151.0.0.0/30",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nAWS reserves 5 IP addresses.\nThe first four IP addresses and the last IP address in each subnet CIDR block are not available for you to use, and cannot be assigned to an instance.\nFor example, in a subnet with CIDR block 10.0.0.0/24, the following five IP addresses are reserved:\n10.0.0.0: Network address.\n10.0.0.1: Reserved by AWS for the VPC router.\n10.0.0.2: Reserved by AWS.\nThe IP address of the DNS server is the base of the VPC network range plus two.\nFor VPCs with multiple CIDR blocks, the IP address of the DNS server is located in the primary CIDR.\nWe also reserve the base of each subnet range plus two for all CIDR blocks in the VPC.\nFor more information, see the Amazon DNS server.\n10.0.0.3: Reserved by AWS for future use.\n10.0.0.255: Network broadcast address.\nWe do not support broadcast in a VPC, therefore we reserve this address.\nThe formula to calculate the number of assignable IP addresses to CIDR networks is similar to classful networking.\nSubtract the number of network bits from 32\nFor example, a /24 network has 2^(32-24) - 2 addresses available for host assignment.\nA.Prefix Length is â€˜27'\nTherefore 32-27 = 5 and 2 ^ 5 (i.e 2 * 2 * 2 * 2 * 2) - 5 = 27\nB.Prefix Length is â€˜28'\nTherefore 32-28 = 4 and 2 ^ 4 (i.e 2 * 2 * 2 * 2) - 5= 11\nC.Prefix Length is â€˜29'\nTherefore 32-29 = 3 and 2 ^ 3 (i.e 2 * 2 * 2) - 5 = 3\nD.Prefix Length is â€˜30'\nTherefore 32-30 = 2 and 2 ^ 2 (i.e 2 * 2) - 5 = -1\nFor option â€˜A', we get â€˜27' IP addresses (or indirectly the number of instances to be provisioned) as shown above.\nSince the user has to provision â€˜20' EC2 instances, we need to go with option â€˜A', which is the only correct IPv4 CIDR block.\nOption B is incorrect because we get only â€˜11' IP addresses (or indirectly the number of instances to be provisioned ).\nOption C is incorrect because we get only â€˜3' IP addresses.\nOption D is incorrect because we get only â€˜-1' IP address.\nFor more information, please refer to the below URL-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html#vpc-sizing-ipv4\n\nTo determine the appropriate IPv4 CIDR block for a new subnet in AWS VPC that will host 20 EC2 instances, we need to first calculate the number of IP addresses required for the instances and other network resources.\nEach EC2 instance needs at least one private IP address assigned to it. Additionally, we may require IP addresses for other network resources such as a NAT gateway, a VPN gateway, and DNS servers.\nAssuming that we need 1 IP address for each instance and 4 IP addresses for other network resources, the total number of IP addresses required would be:\nTotal IP addresses = Number of instances + Number of network resources = 20 + 4 = 24\nNext, we need to choose an IPv4 CIDR block that can accommodate at least 24 IP addresses. In AWS VPC, we can choose from the following predefined CIDR blocks:\n/16 (65,536 IP addresses) /20 (4,096 IP addresses) /24 (256 IP addresses) /28 (16 IP addresses) /29 (8 IP addresses) /30 (4 IP addresses)\nOut of these, the smallest CIDR block that can accommodate at least 24 IP addresses is /27. This CIDR block provides 32 IP addresses, out of which 5 are reserved by AWS, leaving us with 27 usable IP addresses.\nTherefore, the correct answer is option A: 151.0.0.0/27. This CIDR block can accommodate up to 32 IP addresses, out of which we can use 27 for our EC2 instances and other network resources.\n\n"
}, {
  "id" : 473,
  "question" : "A Company is currently hosting an application that connects to a MySQL AWS RDS Instance.\nOf late, there have been many performance issues being encountered.\nAfter careful analysis, it has been determined that the issue is occurring due to the static (same) queries being fired against the database.\nWhich of the following can be added to the architecture to enhance the performance?\n",
  "answers" : [ {
    "id" : "b9755215543f4ef897e067bbb4cc1d1e",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  }, {
    "id" : "8b9ea47296174c8eb55c14181f0febe3",
    "option" : "Use the ElastiCache Service.",
    "isCorrect" : "true"
  }, {
    "id" : "ac248b88a3f445c3aa8551d0d0b041e1",
    "option" : "Use Read Replica.",
    "isCorrect" : "false"
  }, {
    "id" : "fd2d63cc92cb4f37b52f1c1a15caafcd",
    "option" : "Use CloudFront in front of the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nElastiCache can be used to reduce the latency of requests as it is returning values directly from RAM.\nA read replica is more suitable for frequently changing, or dynamic queries.\nUp-to-dateness of data:\nA read replica will continuously sync from the master.\nSo your results will probably lag 0 - 3s (depending on the load) behind the master.\nA cache takes the query result at a specific point in time and stores it for a certain amount of time.\nPerformance:\nA cache can only return results for queries it has already seen.\nSo if you run the same queries over and over again, it's a good match.\nIf you have many different, frequently changing, or dynamic queries, a read replica will be a better match.\nElastiCache should be much faster since it's returning values directly from RAM.\nHowever, this also limits the number of results you can store.\nOption A is incorrect since this is used for the high availability of the database.\nOption C is incorrect since using read replica is not suitable as mentioned above.\nOption D is incorrect since CloudFront can be used forWeb distributions, not for caching.\nFor more information on ElastiCache, please refer to the below URL-\nhttps://aws.amazon.com/elasticache\n\nBased on the problem statement, the performance issue is occurring due to the static queries being fired against the database. In such a scenario, there are a few options that can be explored to enhance the performance. Let's discuss each answer option and their relevance in this context.\nA. Enable Multi-AZ for the database: Multi-AZ deployment provides enhanced availability and data durability for an RDS instance. It involves automatic replication of the primary database instance to a standby instance in a different Availability Zone. While it provides redundancy, it does not address the performance issue of static queries.\nB. Use the ElastiCache Service: Amazon ElastiCache is a web service that makes it easy to deploy and run an in-memory cache in the cloud. It supports widely adopted in-memory caching engines such as Memcached and Redis. Caching can be a useful technique to reduce the number of static queries that are fired against the database. However, it may not solve all performance issues related to static queries.\nC. Use Read Replica: Amazon RDS provides the capability to create one or more read replicas of the database instance. Read replicas can be used to offload read traffic from the primary database instance, thereby reducing the load on it. In this scenario, if the static queries are mainly read queries, then using a read replica can help in enhancing the performance.\nD. Use CloudFront in front of the database: Amazon CloudFront is a content delivery network (CDN) that can be used to improve the performance of web applications. However, it is not relevant in the context of a database that is being accessed by an application.\nIn conclusion, the most relevant option in this scenario would be to use a read replica. It can help in offloading read traffic from the primary database instance and enhance the performance of static queries.\n\n"
}, {
  "id" : 474,
  "question" : "You are working as an AWS Administrator for a media company.\nThey are using AWS resources in various regions for broadcasting live sports matches.\nMultiple EC2 On-Demand, Spot &amp; Reserved Instances are launched as per user traffic daily, resulting in huge monthly charges.\nTop management is looking for customized analysis for these charges based upon various factors like month-wise, instance-wise to deep dive into the incurred cost.\nThey should be able to visualize the costs analysis on a chart display.\nThe accounts team is looking for a simpler way to analyze these charges and query this report to meet this requirement.\nSuggest a cost-effective solution that can be set up with the least effort.\n",
  "answers" : [ {
    "id" : "f87d81bffb104766bbbc1da10a05e321",
    "option" : "Upload the AWS Cost and Usage Reports to S3. Integrate these reports with Amazon Athena to analyze billing data.",
    "isCorrect" : "false"
  }, {
    "id" : "69934e187dee49eaab6e6d2e7e913105",
    "option" : "Upload AWS Cost &amp; Usage Reports to Amazon QuickSight &amp; analyze billing data.",
    "isCorrect" : "true"
  }, {
    "id" : "3fabb068f4b74d6e8c49a14039aec69b",
    "option" : "Download CSV report from Amazon S3 &amp; analyze cost &amp; usage using a third-party tool.",
    "isCorrect" : "false"
  }, {
    "id" : "7153af7a29054cc6a45baf4986215dc3",
    "option" : "Upload AWS Cost &amp; Usage Reports to Amazon Redshift &amp; analyze billing data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon QuickSight is a business analytics service you can use to build visualizations, charts, perform ad hoc analysis, and get business insights from your data.\nIt can automatically discover AWS data sources and also works with your data sources.\nOption A is incorrect as AWS Athena doesn't provide visualizations.\nOption C is incorrect as using a third-party tool to analyze cost &amp; usage reports can be costly.\nOption D is incorrect as Amazon Redshift is a petabyte-scale data warehouse service in the cloud.\nYou can start with just a few hundred gigabytes of data and scale to a petabyte or more.\nIt is not used for such cost analysis.\nIt is used for complex queries and data warehousing.\nFor more information on Amazon QuickSight, refer to the following URL-\nhttps://docs.aws.amazon.com/quicksight/latest/user/welcome.html#analyses\n\nThe media company is facing a challenge in managing the costs associated with their AWS resources used for broadcasting live sports matches. They need a customized analysis of their charges based on various factors like month-wise, instance-wise to deep dive into the incurred cost, and visualize the cost analysis on a chart display. The solution should be cost-effective and set up with the least effort.\nOption A: Upload the AWS Cost and Usage Reports to S3. Integrate these reports with Amazon Athena to analyze billing data.\nThis solution involves uploading the AWS Cost and Usage Reports to Amazon S3 and integrating these reports with Amazon Athena to analyze the billing data. Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. The Cost and Usage Reports are automatically generated by AWS and provide comprehensive cost and usage data for AWS services. This option allows the company to analyze their billing data using SQL queries and visualize the results using a chart display.\nOption B: Upload AWS Cost & Usage Reports to Amazon QuickSight & analyze billing data.\nThis solution involves uploading the AWS Cost and Usage Reports to Amazon QuickSight and analyzing the billing data. Amazon QuickSight is a cloud-based business intelligence service that enables users to create and publish interactive dashboards, reports, and charts. The Cost and Usage Reports can be used to create customized dashboards and reports to visualize the billing data.\nOption C: Download CSV report from Amazon S3 & analyze cost & usage using a third-party tool.\nThis solution involves downloading the CSV report from Amazon S3 and analyzing the cost and usage data using a third-party tool. This option requires the use of a third-party tool that can analyze the CSV report, which may not be cost-effective.\nOption D: Upload AWS Cost & Usage Reports to Amazon Redshift & analyze billing data.\nThis solution involves uploading the AWS Cost and Usage Reports to Amazon Redshift and analyzing the billing data. Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse service that makes it simple and cost-effective to analyze all your data using standard SQL and existing Business Intelligence (BI) tools. This option requires setting up and managing a Redshift cluster, which may not be cost-effective.\nOut of these options, the best solution would be Option A: Upload the AWS Cost and Usage Reports to S3. Integrate these reports with Amazon Athena to analyze billing data. This option is cost-effective, requires the least effort, and allows the company to analyze their billing data using standard SQL queries and visualize the results using a chart display. Additionally, the Cost and Usage Reports are automatically generated by AWS, making it easy to generate reports and analyze the billing data.\n\n"
}, {
  "id" : 475,
  "question" : "A Company is currently hosting an application that connects to a MySQL AWS RDS Instance.\nThe application behaves fine when there are 20 lookups against the database.\nWhen the lookups start to increase, the performance of the application starts to degrade.\nWhich of the following can be added to the architecture to alleviate the performance issue?\n",
  "answers" : [ {
    "id" : "1d39a1bdd76949689935b254b161850e",
    "option" : "Create a Read Replica for the database.",
    "isCorrect" : "true"
  }, {
    "id" : "28f35b14d02c4db389b43b6f43c47996",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  }, {
    "id" : "f9014185c1ed4280a62fe921a7fce606",
    "option" : "Place the database behind a Cloudfront distribution.",
    "isCorrect" : "false"
  }, {
    "id" : "0d085487743340b6b0e0717032d31745",
    "option" : "Create a snapshot of the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nOption B is incorrect since this is used for high availability of the database.\nOption C is incorrect since Cloudfront is used for web distributions.\nOption D is incorrect since this is used for backups of databases.\nThe AWS Documentation mentions the following.\nAmazon RDS Read Replicas provide enhanced performance and durability for database (DB) instances.\nThis feature makes it easy to elastically scale out beyond a single DB instance for read-heavy database workloads beyond the capacity constraints of a single DB instance.\nYou can create one or more replicas of a given source DB Instance and serve high-volume application read traffic from multiple copies of your data, thereby increasing aggregate read throughput.\nFor more information on AWS Read Replica's, please refer to the below URL-\nhttps://aws.amazon.com/rds/details/read-replicas/\n\nThe performance of the application can be improved by scaling the underlying database to handle the increased workload. Here are the options and their respective explanations:\nA. Create a Read Replica for the database: This option involves creating one or more read replicas of the MySQL RDS instance. Read replicas can be used to offload read queries from the primary instance and improve application performance. By default, read replicas are asynchronous, which means that data changes made to the primary instance may take some time to propagate to the read replicas. However, the increased read capacity can help alleviate performance issues caused by read-heavy workloads.\nB. Enable Multi-AZ for the database: Multi-AZ stands for Multi-Availability Zone. This option involves creating a standby replica of the MySQL RDS instance in a different availability zone. If the primary instance fails, the standby instance can be promoted to become the primary instance. Enabling Multi-AZ can improve database availability and fault tolerance, but it may not necessarily improve performance.\nC. Place the database behind a Cloudfront distribution: CloudFront is a content delivery network (CDN) that can be used to cache content and reduce the load on the underlying servers. This option involves placing the MySQL RDS instance behind a CloudFront distribution, which can help reduce the number of requests hitting the database. However, this approach may not be suitable for all applications, and it may not necessarily improve performance.\nD. Create a snapshot of the database: This option involves creating a backup of the MySQL RDS instance, which can be used to restore the database in case of data loss or corruption. However, creating a snapshot does not directly improve performance.\nIn conclusion, the best option for alleviating the performance issue is A, creating a Read Replica for the database. Option B can improve availability but not necessarily performance, Option C may not be suitable for all applications, and Option D does not directly improve performance.\n\n"
}, {
  "id" : 476,
  "question" : "Your company needs to develop an application that needs to have a login module in place.\nTheir key requirement is to ensure that users can also use their current identities with various providers such as Facebook to log into the application.\nWhich of the following can help you accomplish this?\n",
  "answers" : [ {
    "id" : "e4b7b18c61a7438ebb3ed482c622b0e1",
    "option" : "Using the AWS Cognito service",
    "isCorrect" : "true"
  }, {
    "id" : "5c5f11dadd4c4fd9aecd6ce76899ab8f",
    "option" : "Using the AWS Config service",
    "isCorrect" : "false"
  }, {
    "id" : "e2526682b9854f6188d23a9f4052ddc7",
    "option" : "Using the AWS SQS service",
    "isCorrect" : "false"
  }, {
    "id" : "c95f44d39b0d4f509f69bbf9c34f3c1a",
    "option" : "Using the AWS WAF service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon Cognito provides authentication, authorization, and user management for your web and mobile apps.\nYour users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, or Google.\nThe two main components of Amazon Cognito are user pools and identity pools.\nUser pools are user directories that provide sign-up and sign-in options for your app users.\nIdentity pools enable you to grant your users access to other AWS services.\nYou can use identity pools and user pools separately or together.\nOption B is incorrect since this is a configuration service.\nOption C is incorrect since this is a messaging service.\nOption D is incorrect since this is a web application firewall service.\nFor more information on AWS Cognito, please refer to the below URL-\nhttps://docs.aws.amazon.com/cognito/latest/developerguide/what-is-amazon-cognito.html\n\nThe correct answer is A. Using the AWS Cognito service.\nExplanation:\nAWS Cognito is a user authentication and authorization service that helps you create and manage user identities in your applications. It provides secure user sign-up, sign-in, and access control to your web and mobile apps. With Cognito, you can easily add user sign-up, sign-in, and access control to your web and mobile apps using your own identity provider (such as Facebook, Google, or Amazon), or with a user pool you create.\nAWS Cognito provides a variety of options for user authentication, including username and password, social identity providers (such as Facebook, Google, Amazon, and Apple), and enterprise identity providers (such as Microsoft Active Directory). It also supports multi-factor authentication, including SMS, time-based one-time passwords, and Amazon Cognito user pools.\nUsing AWS Cognito, you can easily add user authentication and authorization to your applications without having to write any backend code or managing any infrastructure. This can save you time and effort, and also help ensure that your users' identities are secure and protected.\nThe other options in the question are not relevant to the requirement of adding user authentication with various identity providers. AWS Config is a service that helps you assess, audit, and evaluate the configurations of your AWS resources. AWS SQS is a message queue service that enables decoupling and scaling microservices, distributed systems, and serverless applications. AWS WAF is a web application firewall service that helps protect your web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.\n\n"
}, {
  "id" : 477,
  "question" : "A mid-sized Fintech company is using AWS Organization to manage multiple AWS accounts created for each department.\nA default IAM policy is configured for all accounts.\nEach of the accounts has purchased a Reserved Instance &amp; is running web applications on a mix of On-Demand &amp; Reserved Instances.\nDue to high recurring costs, Management has appointed you as an AWS consultant to reduce costs.\nAfter the analysis, you suggested purchasing more Reserved Instance as compared to using On-Demand EC2 instance.\nWhat actions would you take to justify your suggestions?\n",
  "answers" : [ {
    "id" : "61135ecca7e54c20b54b8a451cff8f95",
    "option" : "Use Organization Management account to create RI coverage budgets for all the accounts in an organization and receive an SNS alert once the threshold is below 50%.",
    "isCorrect" : "false"
  }, {
    "id" : "e67f5f2b2a104f15beb6241be674edb9",
    "option" : "Use Organization Member account owners to create RI coverage budgets for their individual accounts in an organization &amp; receive SNS alert once the threshold is below 50%.",
    "isCorrect" : "true"
  }, {
    "id" : "27b8c5d92f31430da2159ea3d436bf44",
    "option" : "Use Organization Member account owners to create RI utilization budgets for their individual accounts in an organization and receive SNS alert once the threshold is below 50%.",
    "isCorrect" : "false"
  }, {
    "id" : "4c1135b3db224c7c85e2b14b12f542a0",
    "option" : "Use Organization Management account to create RI utilization budgets for all the accounts in an organization and receive SNS alert once the threshold is below 50%.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe Reserved Instance Utilization and Coverage reports are not the same.\nEC2 RI Utilization % offers relevant data to identify and act on opportunities to increase your Reserved Instance usage efficiency.\nIt's calculated by dividing the Reserved Instance used hours by total Reserved Instance purchased hours.\nEC2 RI Coverage % shows how much of your overall instance usage is covered by Reserved Instances.\nThis lets you make informed decisions about when to purchase or modify a Reserved Instance to ensure maximum coverage.\nIt's calculated by dividing the Reserved Instance used hours by total EC2 On-Demand and Reserved Instance hours.\nRI Coverage Budget reports the number of instances that are part of the Reserved Instance.\nThis helps you get an alert when the number of instances covered by reservation falls below 50% of the number of instances launched.\nThis report can identify the instance which is consistently running using On-Demand instance &amp; can be converted to Reserved Instance for cost savings.\nAWS Organization member accounts' owners can create a budget for individual accounts.\nAWS Organization master account pays for usage incurred by all accounts in the organization.\nOption A is incorrect since the company uses a default IAM policy.\nEach member account owner needs to create a budget policy for individual accounts &amp; not by master account.\nOption C is incorrect as RI utilization Budget reports the utilization of your RI instance.\nYou need an RI Coverage report to check when the number of instances covered by reservation falls below 50% of the number of instances launched.\nOption D is incorrect since the company uses a default IAM policy.\nEach member account owner needs to create a budget policy for individual accounts &amp; not by Management account.\nAlso, you need an RI Coverage report to check when the number of instances covered by reservation falls below 50% of the total number of instances launched.\nFor more information, refer to the following URLs:\nhttps://aws.amazon.com/aws-cost-management/reserved-instance-reporting/\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/budgets-managing-costs.html\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/grantaccess.html\n\nIn this scenario, the company is using AWS Organization to manage multiple AWS accounts created for each department. A default IAM policy is configured for all accounts. Each account has purchased a Reserved Instance and is running web applications on a mix of On-Demand and Reserved Instances. Due to high recurring costs, the management has appointed an AWS consultant to reduce costs. The consultant has suggested purchasing more Reserved Instances compared to using On-Demand EC2 instances to reduce costs.\nTo justify the suggestion, the consultant should set up RI coverage or utilization budgets for all accounts in the organization and receive an SNS alert once the threshold is below 50%. The goal of setting up these budgets and alerts is to ensure that the accounts are utilizing their RIs effectively and not overspending on On-Demand instances.\nOption A: Use Organization Management account to create RI coverage budgets for all the accounts in an organization and receive an SNS alert once the threshold is below 50%. This option is a good approach because it sets up RI coverage budgets for all accounts in the organization, which will help track the usage of RIs. Receiving an SNS alert once the threshold is below 50% will ensure that the accounts purchase more RIs before they run out, avoiding additional costs associated with On-Demand instances.\nOption B: Use Organization Member account owners to create RI coverage budgets for their individual accounts in an organization and receive an SNS alert once the threshold is below 50%. This option is not as effective as Option A because it requires each account owner to create RI coverage budgets for their individual accounts. This approach may lead to inconsistencies in tracking RI usage and alerting.\nOption C: Use Organization Member account owners to create RI utilization budgets for their individual accounts in an organization and receive SNS alert once the threshold is below 50%. This option is not as effective as Option A because it sets up RI utilization budgets for individual accounts rather than all accounts in the organization. This approach may lead to inconsistencies in tracking RI usage and alerting.\nOption D: Use Organization Management account to create RI utilization budgets for all the accounts in an organization and receive SNS alert once the threshold is below 50%. This option is a good approach because it sets up RI utilization budgets for all accounts in the organization, which will help track the usage of RIs. Receiving an SNS alert once the threshold is below 50% will ensure that the accounts purchase more RIs before they run out, avoiding additional costs associated with On-Demand instances.\nIn conclusion, the best option for justifying the suggestion to purchase more Reserved Instances is to use the Organization Management account to create RI coverage or utilization budgets for all accounts in the organization and receive an SNS alert once the threshold is below 50%.\n\n"
}, {
  "id" : 478,
  "question" : "Your current architecture consists of a set of web servers spun up as part of an Autoscaling group.\nThese web servers then communicate with a set of database servers.\nYou need to ensure that the database servers' security groups are set properly to accept traffic from the web servers.\nWhich of the following is the best way to accomplish this?\n",
  "answers" : [ {
    "id" : "c5a8ff112b5b46ecaeb311c398239b6b",
    "option" : "Ensure that the Private IP addresses of the web servers are put as sources for the incoming rules in the database server security group.",
    "isCorrect" : "false"
  }, {
    "id" : "0c2a3b72163443419417bd700dd24b07",
    "option" : "Ensure that the Public IP addresses of the web servers are put as sources for the incoming rules in the database server security group.",
    "isCorrect" : "false"
  }, {
    "id" : "7c4a59e8219d4fcc8c1d14bc4cb8a6ed",
    "option" : "Ensure that the web server security group is placed as the source for the incoming rules in the database server security group.",
    "isCorrect" : "true"
  }, {
    "id" : "7eb5bcc6753f4cfa954f7a5ac86bcecf",
    "option" : "Ensure that the Instance ID of the web servers are put as sources for the incoming rules in the database server security group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe below example from the AWS Documentation also shows the Source of the database security group involving the web server security groups' ID.Options A and B are invalid or not the best practice.\nSince they are part of the Autoscaling Group, the IP addresses of the instances can change.\nOption D is incorrect since normally you don't specify the Instance ID in Security Groups.\nFor more information on the Security Groups for the VPC, please refer to the below URL-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html\n\n\nThe best way to ensure that the database servers' security groups are set properly to accept traffic from the web servers is by configuring the security group rules on the database servers to allow inbound traffic from the web servers' security group.\nOption A, which involves adding the private IP addresses of the web servers as sources for the incoming rules in the database server security group, is not the best option. This is because the IP addresses of the web servers can change over time, especially if they are part of an Autoscaling group. Additionally, adding the IP addresses of individual servers can be cumbersome to manage.\nOption B, which involves adding the public IP addresses of the web servers as sources for the incoming rules in the database server security group, is also not the best option. This is because the public IP addresses of the web servers can change if the servers are stopped and started again. Also, if the web servers are part of a private subnet, they may not have public IP addresses.\nOption D, which involves adding the instance ID of the web servers as sources for the incoming rules in the database server security group, is also not the best option. This is because the instance IDs of the web servers can change if they are terminated and replaced.\nOption C is the best option. By configuring the database server security group to allow inbound traffic from the web server security group, you ensure that any new web servers launched by the Autoscaling group will automatically have access to the database servers. This approach simplifies security group management and ensures that the appropriate security measures are in place to protect your database servers.\nIn summary, the best way to accomplish this is to ensure that the web server security group is placed as the source for the incoming rules in the database server security group.\n\n"
}, {
  "id" : 479,
  "question" : "A global sports news company has hosted its website on Amazon EC2 instance using a single Public IP address &amp; is front-ended by TLS-enabled Application Load Balancer.\nFor an upcoming mega sports event, they plan to launch a new website on the existing Amazon EC2 instance.\nThe company has registered a different domain name &amp; possesses a separate TLS certificate for this new website. As an AWS consultant to this company, which of the following recommendations will you provide to support multiple certificates with existing Public IP addresses in the most cost-effective way?\n",
  "answers" : [ {
    "id" : "d450dc89807149eaa7316d5e8a740f4e",
    "option" : "Launch an additional TLS-enabled ALB front ending Amazon EC2 instance with different certificates for each domain.",
    "isCorrect" : "false"
  }, {
    "id" : "d89cba19016441e48a6ed142c7416bcf",
    "option" : "Use Wildcard certificates on ALB matching old &amp; new domain name.",
    "isCorrect" : "false"
  }, {
    "id" : "925f968c65754c86b72189300762b2b2",
    "option" : "Use a single certificate on ALB &amp; add Subject Alternative Name (SAN) for additional domain name.",
    "isCorrect" : "false"
  }, {
    "id" : "ed7cddd32ef0408a81335c332d999656",
    "option" : "Use multiple TLS certificates on ALB using Server Name Indication (SNI).",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nALB supports Server Name Indication (SNI), enabling hosting multiple domain names with different TLS certificates behind a single ALB.\nWith SNI, multiple certificates can be associated with listeners in ALB, enabling each web application to use separate certificates.\nThe below diagrams shows the process which takes place when a client tries to access a website.\nClient Browser starts a TLS handshake by sending a ClientHello message which consists of protocol version, extensions, cipher suites, and compression techniques.\nBased on browser capabilities, ALB responds with a valid certificate for a domain name of the requested web application.\nOption A is incorrect as launching additional ALB will work.\nBut it will incur additional cost &amp; admin work for setup.\nOption B is incorrect as using wildcard certificates can be used for related sub-domains &amp; not different domains.\nOption C is incorrect as using SAN certificates, for any new addition of domain, all certificates need to revalidate with certificate authority.\nFor more information on Amazon ALB SNI support, refer to the following URL-\nhttps://aws.amazon.com/blogs/aws/new-application-load-balancer-sni/\n\n\nThe scenario involves a global sports news company that has hosted its website on an Amazon EC2 instance using a single Public IP address and is front-ended by a TLS-enabled Application Load Balancer. The company plans to launch a new website on the existing Amazon EC2 instance with a different domain name and a separate TLS certificate.\nTo support multiple certificates with existing Public IP addresses in the most cost-effective way, the recommended approach is to use multiple TLS certificates on ALB using Server Name Indication (SNI). This approach is recommended because it allows for the use of multiple certificates on a single load balancer, which is cost-effective as it eliminates the need to launch an additional load balancer or use wildcard certificates.\nSNI is an extension of the TLS protocol that enables a server to present multiple certificates on the same IP address and port number. When a client initiates an SSL/TLS handshake with the server, the client sends the requested hostname as part of the handshake. The server uses this hostname to select the appropriate certificate to present to the client.\nTherefore, by using SNI, the ALB can be configured to use multiple certificates, and the appropriate certificate will be presented to the client based on the requested hostname. In this case, the ALB can be configured to use the existing certificate for the original domain and the new certificate for the new domain.\nOption A, Launch an additional TLS-enabled ALB front-ending Amazon EC2 instance with different certificates for each domain, is not cost-effective as it requires the launch of an additional load balancer.\nOption B, Use Wildcard certificates on ALB matching old and new domain names, is not recommended as it can be costly and less secure since wildcard certificates can be used to secure any subdomain, potentially exposing the website to security risks.\nOption C, Use a single certificate on ALB and add Subject Alternative Name (SAN) for an additional domain name, is not recommended as it requires purchasing a new certificate that supports SANs, which can be costly, and can also lead to operational overhead in managing the certificate. Additionally, some clients may not support SANs, leading to compatibility issues.\n\n"
}, {
  "id" : 480,
  "question" : "Your company is planning to store sensitive documents in a bucket in the Simple Storage service.\nThey need to ensure that all objects are encrypted at rest in the bucket.\nWhich of the following can help accomplish this? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "f25810517bea4cb4b2ef7128dc9e22ff",
    "option" : "Ensure that the default encryption is enabled for the S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "5501127f080b406abfbc0fc86f7184af",
    "option" : "Ensure that the bucket policy is set to encrypt all objects that are added to the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "b66b5b3eccbe47b397839794731a93ef",
    "option" : "Ensure that the bucket ACL is set to encrypt all objects that are added to the bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "3a346089d334460696996e48c04b2e6b",
    "option" : "Ensure to change the configuration of the bucket to use a KMS key to encrypt the objects.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A and D.\nOptions B and C are incorrect since these options cannot be used to encrypt the objects.\nUsing an S3 bucket policy, you can enforce the encryption requirement when users upload objects.\nRefer below URL for more details.\nhttps://aws.amazon.com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/\nThe term 'rest' means when data is resting (not in transition - while data is traveling to s3).\nServer-side encryption - facilitates encryption at rest.\nClient-side encryption - facilitates encryption both in transition and at rest.\nYou have three options depending on how you choose to manage the encryption keys:\nSSE-S3 requires that Amazon S3 manage the data and master encryption keys.\nSSE-C requires that you manage the encryption key.\nSSE-KMS requires that AWS manage the data key, but you manage the master key in AWS KMS.\nFor more information, please refer to the following URL-\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html\nAmazon S3 default encryption provides a way to set the default encryption behavior for an S3 bucket.\nYou can set default encryption on a bucket to encrypt all new objects when they are stored in the bucket.\nThe objects are encrypted using server-side encryption with either Amazon S3-managed keys (SSE-S3) or customer master keys (CMKs) stored in AWS Key Management Service (AWS KMS).\nFor more information on Server - Side encryption, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html\nTo know about default encryption (Option A), please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html\n\nTo ensure that all objects are encrypted at rest in an S3 bucket, two approaches can be taken:\nEnable default encryption for the S3 bucket. Use a KMS key to encrypt the objects in the bucket.\nA. Enable default encryption for the S3 bucket: AWS S3 provides a feature to enable default encryption for a bucket. This can be done by enabling server-side encryption. Once enabled, any object that is added to the bucket will be automatically encrypted at rest. AWS S3 provides two types of encryption for this purpose, namely:\nSSE-S3 (Server-Side Encryption with Amazon S3-managed keys): This is the default encryption type provided by S3. It encrypts objects using AES-256 encryption and manages the encryption keys for you. SSE-KMS (Server-Side Encryption with AWS KMS-managed keys): This type of encryption allows you to use your own encryption keys to encrypt objects in S3.\nB. Use a KMS key to encrypt the objects in the bucket: AWS KMS (Key Management Service) can be used to create and manage encryption keys that can be used to encrypt objects in S3. By using a KMS key, you have more control over the encryption process, and you can audit and manage access to the keys.\nC. The bucket ACL is not used for encrypting objects at rest in an S3 bucket. The ACL is used to set permissions for the objects in the bucket.\nD. Changing the bucket configuration to use a KMS key is one of the ways to encrypt objects at rest in an S3 bucket.\nTherefore, options A and D are the correct answers to ensure that all objects are encrypted at rest in an S3 bucket.\n\n"
}, {
  "id" : 481,
  "question" : "A global software company is using Amazon EC2 Reserved and On-Demand Instance for all project-related work.\nThey have different accounts created for each vertical like Finance, Project, HR, which are managed individually by account owners in each vertical.\nManagement is looking for options to decrease these recurring charges.\nHow could the management save monthly billing charges without impacting the performance?[Choose TWO]\n",
  "answers" : [ {
    "id" : "29d13c0e3d0a4441a9f9ff9772467f65",
    "option" : "Each account should launch a Spot Instance instead of using On-Demand Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "c65b922cd864462695c790371090c520",
    "option" : "Each account should share Reserved Instance which they have purchased with other accounts.",
    "isCorrect" : "true"
  }, {
    "id" : "e8629049085b4a5fb38316c16435f8bc",
    "option" : "create AWS organization and leverage consolidated billing feature to get the discounts on Amazon EC2.",
    "isCorrect" : "true"
  }, {
    "id" : "e57bd19ddad14ed48ebf8fc6bf8bda42",
    "option" : "Use Budgets to limit the charges incurred for using Amazon EC2.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nConsolidated Billing combines usage from all the accounts &amp; billing is generated based on the total usage.\nServices like Amazon EC2, Amazon S3, etc.\nhave volume pricing tiers where the overall charges decrease with more usage volume.\nOption A is incorrect.\nHowever, this will save the cost but will impact the performance.\nSpot Instance and On-demand Instance are very similar in nature.\nThe main difference between these is commitment.\nIn Spot Instance, there is no commitment.\nAs soon as the Bid price exceeds the Spot price, a user gets the Instance.\nIn an On-demand Instance, a user has to pay the On-demand rate specified by Amazon.\nOnce they have bought the Instance, they have to use it by paying that rate.\nIn Spot Instance, once the Spot price exceeds the Bid price, Amazon will shut the instance.\nThe benefit to users is that they will not be charged for the partial hour in which the Instance was taken back from them.\nSpot instances are not always cheaper than on-demand.\nThey can and do sometimes fluctuate wildly, even to very high per hour amounts, higher than the on-demand price at times.\nOption B is correct as Reserved Instance discounts can be applied to accounts in an organization, but Reserved Instance sharing has to be turned on or off for the account.\nFor details, please refer to the below URL-\nhttps://aws.amazon.com/premiumsupport/knowledge-center/ec2-ri-consolidated-billing/\nOption D is incorrect as the Budget will limit charges but will not provide discounts on services being used by various accounts.\nFor more information on using consolidated billing, refer to the following URLs-\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/useconsolidatedbilling-discounts.html\n\nSure, I'd be happy to provide a detailed explanation!\nThe management of the global software company is looking for options to decrease their recurring charges without impacting the performance. In order to achieve this, there are a few different strategies that they can consider. Two of these strategies are:\nEach account should launch a Spot Instance instead of using On-Demand Instance. Each account should share Reserved Instance which they have purchased with other accounts.\nLet's take a closer look at each of these options.\nLaunching Spot Instances: Amazon EC2 Spot Instances are a cost-effective way to run applications in the cloud. Spot Instances are spare compute capacity that Amazon EC2 offers at discounted prices. The prices are determined by supply and demand, so they can fluctuate over time. Spot Instances can be interrupted by Amazon EC2 with two minutes of notification when the Spot price exceeds your bid price or when Amazon EC2 needs the capacity back.\nIf the software company is willing to accept the possibility of interruptions to their instances, then using Spot Instances can be a good way to save on their monthly billing charges. Each account can launch Spot Instances instead of using On-Demand Instances, which are typically more expensive. This can significantly reduce the cost of running applications in the cloud.\nSharing Reserved Instances: Reserved Instances are a pricing model that enables customers to receive a significant discount on the hourly charge for instances that are reserved for a one- or three-year term. Reserved Instances are a good option for workloads that have predictable usage patterns.\nIf the software company has already purchased Reserved Instances, then they can save on monthly billing charges by sharing those Reserved Instances with other accounts. This can be done by using AWS Resource Access Manager (RAM) to share the Reserved Instances with other accounts. By doing this, each account can take advantage of the Reserved Instance discounts without having to purchase their own Reserved Instances.\nIn addition to these two strategies, there are a few other options that the software company can consider:\nCreate an AWS organization and leverage consolidated billing feature to get the discounts on Amazon EC2. Consolidated billing allows companies to consolidate payment for multiple AWS accounts or multiple Amazon EC2 accounts within a single organization. By doing this, the company can take advantage of the volume discounts that are available for Amazon EC2. Use Budgets to limit the charges incurred for using Amazon EC2. AWS Budgets is a free service that allows customers to set custom cost and usage budgets that alert them when their AWS costs or usage exceed their budgeted amount. By using AWS Budgets, the company can monitor their usage of Amazon EC2 and ensure that they are not exceeding their budget.\nIn summary, the management of the global software company can save on monthly billing charges by using Spot Instances, sharing Reserved Instances, creating an AWS organization, and using AWS Budgets. By taking advantage of these cost-saving strategies, they can reduce their recurring charges without impacting the performance of their applications.\n\n"
}, {
  "id" : 482,
  "question" : "A company is planning to host an active-active site.\nOne site will be deployed in AWS, and the other one on their On-premise data center.\nThey need to ensure that the traffic is distributed to multiple resources, proportionately between both sites.\nWhich of the following routing policy would you use for this purpose?\n",
  "answers" : [ {
    "id" : "c4877ef568d946609b3673fca1641ba4",
    "option" : "Simple Routing",
    "isCorrect" : "false"
  }, {
    "id" : "24e65b85a7d04e22a53cd6edd64282de",
    "option" : "Failover Routing",
    "isCorrect" : "false"
  }, {
    "id" : "13ca1a1967c54e7f8bba03536773135c",
    "option" : "Latency Routing",
    "isCorrect" : "false"
  }, {
    "id" : "98ee5e6ffa7b4df38066247a3bb7254f",
    "option" : "Weighted Routing.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nWeighted routing lets you associate multiple resources with a single domain name (example.com) or subdomain name (acme.example.com) and choose how much traffic is routed to each resource.\nThis can be useful for various purposes, including load balancing and testing new versions of software.\nTo configure weighted routing, you create records with the same name and type for each of your resources.\nYou assign each record a relative weight that corresponds with how much traffic you want to send to each resource.\nAmazon Route 53 sends traffic to a resource based on the weight you assign to the record as a proportion of the total weight for all the group records.\nOption A is incorrect since this should be used when you want to configure standard DNS records.\nOption B is incorrect since this should be used when you want to route traffic to a resource when the resource is healthy or to a different resource when the first resource is unhealthy.\nOption C is incorrect since this should be used when you want to improve your users' performance by serving their requests from the AWS Region that provides the lowest latency.\nFor more information on a Routing policy, please refer to the below URL-\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nFor the scenario of hosting an active-active site with one site deployed in AWS and the other on-premise data center, the routing policy that should be used to ensure that traffic is distributed to multiple resources proportionately between both sites is Weighted Routing.\nWeighted Routing policy is a type of Amazon Route 53 routing policy that enables traffic distribution across multiple resources based on assigned weights. The assigned weights can be used to allocate the percentage of traffic that should be sent to each resource. This is achieved by creating weighted resource record sets for each resource, where the weight assigned to each record set determines the proportion of traffic that it should receive.\nIn this case, one weighted resource record set should be created for the AWS site and another for the on-premise data center. The weights assigned to each record set should be based on the capacity and performance of the resources at each site. For example, if the AWS site has more resources than the on-premise data center, a higher weight can be assigned to the AWS record set to ensure that it can handle more traffic.\nBy using Weighted Routing policy, traffic can be distributed proportionately between both sites, ensuring that both sites are active and receiving traffic at all times. In addition, the weights assigned to each resource record set can be adjusted as needed to accommodate changes in resource capacity or performance.\n\n"
}, {
  "id" : 483,
  "question" : "Your company is planning to store sensitive documents in an S3 bucket.\nThey want to keep the documents private but serve content only to selected users based on a particular time frame.\nWhich of the following can help you accomplish this?\n",
  "answers" : [ {
    "id" : "f302fdd1296b4b01aa380cd4d8710850",
    "option" : "Enable CORS for the S3 bucket",
    "isCorrect" : "false"
  }, {
    "id" : "5aad0e444762423fa756e2f57b8dfc6e",
    "option" : "Use KMS and enable encryption for the files",
    "isCorrect" : "false"
  }, {
    "id" : "980fb9097ffd4df68778e1961fe2c28e",
    "option" : "Create pre-signed URLâ€™s",
    "isCorrect" : "true"
  }, {
    "id" : "86a8fdee3b9c43fdab6efa61dbb17d8e",
    "option" : "Enable versioning for the S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nA pre-signed URL gives you access to the object identified in the URL, provided that the creator of the pre-signed URL has permissions to access that object.\nThat is, if you receive a pre-signed URL to upload an object, you can upload the object only if the creator of the pre-signed URL has the necessary permissions to upload that object.\nAll objects and buckets by default are private.\nThe pre-signed URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don't require them to have AWS security credentials or permissions.\nWhen you create a pre-signed URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time.\nThe pre-signed URLs are valid only for the specified duration.\nOption A is incorrect since this is used for Cross-origin access.\nOption B is incorrect since this is used for encryption purposes.\nOption D is incorrect since this is used for versioning.\nFor more information on pre-signed URL's, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/PresignedUrlUploadObject.html\n\nThe correct answer to this question is C. Create pre-signed URLs.\nExplanation:\nPre-signed URLs are a way to grant temporary access to private objects in an S3 bucket. With pre-signed URLs, you can provide users with a URL that grants access to a specific S3 object for a limited time, usually ranging from 15 minutes to 7 days.\nTo use pre-signed URLs, you generate a URL that contains a security signature. The signature is created using your AWS security credentials and includes information about the user, the object, and the expiration time. You can then provide the URL to the user, and they can use it to access the object within the specified time frame.\nPre-signed URLs provide a secure way to share private S3 objects with selected users. You can also use them to serve content for a specific time frame, which is useful for things like limited-time promotions or temporary access to content.\nOption A, enabling CORS, is not related to controlling access to private S3 objects. CORS (Cross-Origin Resource Sharing) is a security feature that allows web browsers to make cross-domain requests.\nOption B, using KMS and enabling encryption, provides security for the data at rest, but it does not control access to the data based on a specific time frame or user.\nOption D, enabling versioning for the S3 bucket, does not provide any functionality related to serving content to selected users based on a specific time frame. Versioning helps to keep multiple versions of the same object in the bucket.\n\n"
}, {
  "id" : 484,
  "question" : "A company currently is hosting a Redshift cluster.\nThere is a requirement for the data to be available in the Redshift cluster in another region as part of the company's disaster recovery setup.\nHow would you achieve this?\n",
  "answers" : [ {
    "id" : "277f6b8292434e3cabea5f5292ad6204",
    "option" : "Use the Elastic Beanstalk service to copy the cluster to another region.",
    "isCorrect" : "false"
  }, {
    "id" : "fc7669b4a09d4dad9595038129816c20",
    "option" : "Use Cloudformation templates to copy the cluster to another region.",
    "isCorrect" : "false"
  }, {
    "id" : "0c74b6e4b0bb44f89b33bc2756ded48d",
    "option" : "Configure cross-region snapshots for the underlying Redshift cluster.",
    "isCorrect" : "true"
  }, {
    "id" : "2aff79a9e0854ec28e210032aaec734d",
    "option" : "Use the snapshots stored in S3 to create a new Redshift cluster in another region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nYou can configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another region.\nWhen a snapshot is created in the cluster's primary region, it will be copied to a secondary region known as the source region and destination region.\nBy storing a copy of your snapshots in another region, you have the ability to restore your cluster from recent data if anything affects the primary.\nSnapshots are point-in-time backups of a cluster.\nThere are two types of snapshots: automated and manual.\nAmazon Redshift stores these snapshots internally in Amazon S3 using an encrypted Secure Sockets Layer (SSL) connection.\nAmazon Redshift automatically takes incremental snapshots that track changes to the cluster since the previous automated snapshot.\nAutomated snapshots retain all of the data required to restore a cluster from a snapshot.\nYou can create a snapshot schedule to control when automated snapshots are taken, or you can take a manual snapshot at any time.\nWhen you restore from a snapshot, Amazon Redshift creates a new cluster and makes the new cluster available before all of the data is loaded, so you can begin querying the new cluster immediately.\nThe cluster streams data on demand from the snapshot in response to active queries then loads the remaining data in the background.\nhttps://aws.amazon.com/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/\nOption A is invalid since AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and is not used to copy snapshots for disaster recovery.\nOption B is invalid since the CloudFormation template can help create clusters.\nBut it won't help in disaster recovery.\nTo check how to create Cluster using CloudFormation, refer below URL-\nhttps://aws.amazon.com/blogs/big-data/automate-amazon-redshift-cluster-creation-using-aws-cloudformation/\nOption D is invalid since it will be a manual process.\nFor more information on working with snapshots, please refer to the below URL-\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html\n\nThe correct answer to this question is D. Use the snapshots stored in S3 to create a new Redshift cluster in another region.\nExplanation: Redshift is a fully managed, petabyte-scale data warehouse service provided by AWS. It enables customers to store and analyze large amounts of structured and semi-structured data using SQL queries. In a disaster recovery scenario, it is important to have data available in another region in case the primary region becomes unavailable.\nTo achieve this requirement, we need to copy the data from the Redshift cluster in one region to another region. AWS provides different services to achieve this, but the most efficient and cost-effective way to copy the data to another region is by using the Redshift snapshot feature.\nA snapshot is a point-in-time copy of the cluster data. It captures all the data in the cluster, including the database schema and configuration settings. Redshift snapshots are stored in S3, which provides high durability and availability.\nTo copy the Redshift cluster data to another region, we need to perform the following steps:\nTake a snapshot of the Redshift cluster in the primary region. Copy the snapshot to S3 in the secondary region. Restore the snapshot in the secondary region, which will create a new Redshift cluster with the same schema and data as the primary region.\nBy using this approach, we can achieve disaster recovery for the Redshift cluster in a cost-effective way. We do not need to create a new cluster from scratch, and we only pay for the storage and compute resources used in the secondary region when the cluster is restored from the snapshot.\nTherefore, option D is the correct answer. Options A and B are incorrect because Elastic Beanstalk and CloudFormation are not used for copying Redshift clusters between regions. Option C is incorrect because cross-region snapshots are used for backup and recovery purposes, but they do not copy the Redshift cluster to another region.\n\n"
}, {
  "id" : 485,
  "question" : "A media firm uses the Amazon S3 bucket to save all videos shared by reporters across the globe.\nOperation Team has instructed all reporters to use only Multipart Uploads while uploading these large-sized videos to Amazon S3 bucket in each region.\nMost of the reporters are working from remote areas &amp; face challenges in uploading videos.\nThe Finance Team is concerned about high costs incurred by saving data in the Amazon S3 bucket &amp; seeking your guidance.\nPost verification, you observe a large number of incomplete uploads in Amazon S3 buckets in each region.\nThe uncompleted uploads can be deleted after a certain period of time. Which of the following actions can minimize charges for saving video files in the Amazon S3 bucket?\n",
  "answers" : [ {
    "id" : "1b8fd201d6f941f3a42bbfb1cbe82829",
    "option" : "Reporter`s need to compress video files locally before uploading to Amazon S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "71fc0ec64e684d619eda7ade7228222b",
    "option" : "Reporterâ€™s need to upload Videos to Amazon S3 Glacier to save additional charges.",
    "isCorrect" : "false"
  }, {
    "id" : "6f41a787e2c8407a8af3900632980507",
    "option" : "Create a Lifecycle Policy to move all incomplete Multipart uploads to Amazon S3 Glacier after weeksâ€™ time from initiation.",
    "isCorrect" : "false"
  }, {
    "id" : "43cd7f21931d4a7a91a4a57c1cc536f1",
    "option" : "Create a Lifecycle Policy to delete all incomplete Multipart uploads after weeksâ€™ time from initiation.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nIncomplete Multipart Uploads incur storage charges on the Amazon S3 bucket.\nLifecycle rules can be used to abort the uploading of multipart uploads that are incomplete since a specific time frame &amp; also deletes these parts to free up storage, reducing costs for this storage.\nOption A &amp; B are incorrect as Incomplete Multipart Uploads incur charges.\nThese charges can be stopped by stopping multipart uploads.\nOption C is incorrect as Moving all incomplete Multipart uploads to Amazon S3 Glacier would not completely reduce the cost for storing data.\nAs data in Amazon S3 Glacier would incur cost, it would be less than data storing the Amazon S3 bucket.\nAlso, incomplete Multipart uploads would not be used until fully uploaded.\nFor more information on Amazon S3 Multipart Upload, refer to the following URLs-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html\nhttps://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/\n\nThe media firm uses Amazon S3 to store videos shared by reporters across the globe. To upload these large-sized videos to the S3 bucket, the Operations Team has instructed all reporters to use only Multipart Uploads. However, due to the remote locations of most reporters, they are facing challenges in uploading videos, and as a result, a large number of incomplete uploads are observed in the S3 bucket.\nThe Finance Team is concerned about the high costs incurred in saving data in the S3 bucket and is seeking guidance on minimizing these costs. To minimize the charges for saving video files in the Amazon S3 bucket, one of the following actions can be taken:\nA. Reporter's need to compress video files locally before uploading to Amazon S3 bucket. This option may help to reduce the size of the videos being uploaded to the S3 bucket, but it will not address the issue of incomplete uploads or minimize the charges for saving data in the S3 bucket.\nB. Reporter's need to upload Videos to Amazon S3 Glacier to save additional charges. S3 Glacier is a low-cost storage option for data archiving and backup. However, it is not suitable for storing videos that need to be accessed frequently. Additionally, this option will not address the issue of incomplete uploads.\nC. Create a Lifecycle Policy to move all incomplete Multipart uploads to Amazon S3 Glacier after weeks' time from initiation. This option addresses the issue of incomplete uploads by moving them to a lower-cost storage option after a certain period of time. This can help to minimize the charges for saving data in the S3 bucket. However, it may not be an ideal solution if the incomplete uploads need to be accessed frequently.\nD. Create a Lifecycle Policy to delete all incomplete Multipart uploads after weeks' time from initiation. This option addresses the issue of incomplete uploads by deleting them after a certain period of time. This can help to minimize the charges for saving data in the S3 bucket. However, it may not be an ideal solution if the incomplete uploads need to be accessed frequently or if there is a possibility of losing important data.\nIn summary, option C is the best solution as it addresses the issue of incomplete uploads and minimizes the charges for saving data in the S3 bucket by moving the incomplete uploads to a lower-cost storage option. However, the solution should be chosen based on the specific needs of the media firm, taking into consideration the frequency of accessing incomplete uploads and the possibility of losing important data.\n\n"
}, {
  "id" : 486,
  "question" : "Your company currently has a set of web servers in a public subnet and database servers in the private subnet.\nYou need to ensure administrators from your on-premises environment can access the database servers.\nWhich of the following is a secure way to access the database servers?\n",
  "answers" : [ {
    "id" : "14832edd9328458eb5e77c6d854c475a",
    "option" : "Create a bastion host in the private subnet as the database servers. Ask the IT administrators to log into the database servers via the bastion host.",
    "isCorrect" : "false"
  }, {
    "id" : "433ddd5e50bf4970b7d80c172fc50620",
    "option" : "Create a bastion host in the public subnet. Ask the IT administrators to log into the database servers via the bastion host.",
    "isCorrect" : "true"
  }, {
    "id" : "d8e3d43eebbd4ee9b66e9dcd0e297326",
    "option" : "Create a NAT instance in the private subnet as the database servers. Ask the IT administrators to log into the database servers via the NAT Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "56771387b9114b8194a1ca7d47409d35",
    "option" : "Create a NAT instance in the public subnet as the database servers. Ask the IT administrators to log into the database servers via the NAT Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nA bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet.\nBecause of its exposure to potential attacks, a bastion host must minimize the chances of penetration.\nFor example, you can use a bastion host to mitigate the risk of allowing SSH connections from an external network to the Linux instances launched in a private subnet of your Amazon Virtual Private Cloud (VPC).\nOption A is incorrect since the bastion host needs to be in the public subnet.\nOptions C and D are incorrect since the NAT instance should not be used as a jump server to the database servers.\nFor more information on using a bastion host, please refer to the below URL-\nhttps://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/\n\nTo enable administrators from your on-premises environment to access the database servers located in a private subnet, you need to provide a secure way to establish the connection between the on-premises environment and the database servers.\nOption A: Create a bastion host in the private subnet as the database servers. Ask the IT administrators to log into the database servers via the bastion host.\nThis is a secure way to access the database servers. A bastion host is a special-purpose instance that acts as a secure gateway between the on-premises environment and the private subnet where the database servers are located. By connecting to the bastion host first, IT administrators can then securely access the database servers via a secure shell (SSH) connection. The bastion host acts as a jump server, providing a secure way to access the private subnet without exposing the database servers to the public Internet.\nOption B: Create a bastion host in the public subnet. Ask the IT administrators to log into the database servers via the bastion host.\nThis option is less secure than option A. By creating a bastion host in the public subnet, you expose the bastion host to the public Internet, which increases the risk of a security breach. Moreover, the bastion host does not provide a secure way to access the database servers located in the private subnet.\nOption C: Create a NAT instance in the private subnet as the database servers. Ask the IT administrators to log into the database servers via the NAT Instance.\nThis option is not secure because a NAT instance is used for outbound Internet traffic from the private subnet to the public Internet. It is not designed to provide a secure way to access resources in the private subnet from the on-premises environment.\nOption D: Create a NAT instance in the public subnet as the database servers. Ask the IT administrators to log into the database servers via the NAT Instance.\nThis option is also not secure because it involves exposing the database servers to the public Internet. A NAT instance is used for outbound Internet traffic from the private subnet to the public Internet. It is not designed to provide a secure way to access resources in the private subnet from the on-premises environment.\nIn summary, option A is the most secure way to access the database servers located in the private subnet from the on-premises environment. By creating a bastion host in the private subnet, you provide a secure gateway that enables IT administrators to access the database servers via a secure shell (SSH) connection, without exposing the servers to the public Internet.\n\n"
}, {
  "id" : 487,
  "question" : "A company needs to access a service provided by a consultant company.\nThe service from the consultant company and the application of the primary company exist in their respective VPCs.The VPC's are located in different regions.What steps are needed to establish communication between these VPCs such that data should not traverse via the Internet? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "41312c7d674f4f45a380bd116afb3896",
    "option" : "Create a VPC peering between the VPCâ€™s in the primary company and consultant companyâ€™s account.",
    "isCorrect" : "true"
  }, {
    "id" : "343dc928810849e1975a8f2b045b5f48",
    "option" : "Create a Network Load Balancer in the consultant VPC in front of the service. Create a VPC Endpoint. Make the application in the other VPC access this endpoint.",
    "isCorrect" : "false"
  }, {
    "id" : "436194c1e3f14e989891902563fa128c",
    "option" : "Modify the route tables for each VPC point to the VPC peering connection to access all the IPv4 CIDR blocks of the peer VPC (either way).",
    "isCorrect" : "true"
  }, {
    "id" : "f29de33b2373491d932e432795b3a48b",
    "option" : "Create an IPSec Virtual Private connection between both accounts. Access the resources accordingly.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nOption B is incorrect since, for VPC Endpoint interfaces, they have to be in the same region.\nOption D is incorrect since the traffic will traverse via the Internet.\nThe AWS Documentation mentions the following.\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them privately.\nInstances in either VPC can communicate with each other as if they are within the same network.\nYou can create a VPC peering connection between your own VPCs, with a VPC in another AWS account, or with a VPC in a different AWS Region.\nFor more information on AWS Direct Connect and VPC peering, please refer to the below URL-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html\nThe 2 answers provided for the question are correct.\nLet me explain to you the reason behind it.\n1\nBoth Primary &amp; Consultant company are existing within their own VPCs in different AWS Regions.\nIn order to have the connection among them, we create VPC Peering across the region.\nHence,\nOption A is the correct answer.\nhttps://aws.amazon.com/about-aws/whats-new/2017/11/announcing-support-for-inter-region-vpc-peering/\nhttps://aws.amazon.com/blogs/aws/new-almost-inter-region-vpc-peering/\n2\nPlease refer to the below link, section \"Two VPCs with Multiple CIDRs Peered Together,\" for further details.\nhttps://docs.aws.amazon.com/vpc/latest/peering/peering-configurations-full-access.html#many-vpcs-full-access\n\nThe scenario describes two VPCs that are located in different regions and owned by different companies. The primary company needs to access a service provided by a consultant company without traversing data over the internet. The following two answers are the correct steps to establish communication between these VPCs:\nA. Create a VPC peering between the VPCs in the primary company and consultant company's account.\nVPC peering is a networking connection between two VPCs that enables instances in either VPC to communicate with each other as if they are within the same network. VPC peering is a private and secure way of communicating between VPCs. To establish a VPC peering, the two VPCs must not have overlapping CIDR blocks. Once the VPC peering is established, the route tables for each VPC should be modified to point to the VPC peering connection to access all the IPv4 CIDR blocks of the peer VPC.\nC. Modify the route tables for each VPC point to the VPC peering connection to access all the IPv4 CIDR blocks of the peer VPC (either way).\nAfter creating the VPC peering connection, the route tables for each VPC should be updated to point to the VPC peering connection to access all the IPv4 CIDR blocks of the peer VPC. This allows traffic to flow between the VPCs using private IP addresses. The modification of the route tables is done in the VPC console.\nOption B is incorrect because creating a Network Load Balancer and a VPC Endpoint would not establish communication between the VPCs. The VPC Endpoint would allow access to the service provided by the consultant company within the same VPC, but not between VPCs.\nOption D is incorrect because an IPSec Virtual Private Network (VPN) connection is used to establish a secure, encrypted connection between two networks over the internet. However, the question specifically states that the data should not traverse over the internet. Therefore, establishing an IPSec VPN connection is not a viable solution in this case.\n\n"
}, {
  "id" : 488,
  "question" : "Your team has deployed an application that consists of a web and database tier hosted on separate EC2 Instances.\nBoth EC2 Instances are using General Purpose SSD for their underlying volume type.\nOf late, there are performance issues related to the read and writes of the database EC2 Instance.\nWhich of the following could be used to alleviate the issue?\n",
  "answers" : [ {
    "id" : "125f5ba5aa374363b2d2acca859d8fa8",
    "option" : "Change the Instance type to a higher Instance Type.",
    "isCorrect" : "false"
  }, {
    "id" : "b831fcd52e09408dbf5d3cae66dbdfea",
    "option" : "Change the EBS volume to Provisioned IOPS SS.",
    "isCorrect" : "true"
  }, {
    "id" : "b65878c3daad4dda815c524759dc6b3e",
    "option" : "Enable Enhanced Networking on the Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "337ce2c2416848e9807edf2510fd451e",
    "option" : "Enable Multi-AZ for the database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe Provisioned IOPS SSD EBS volume type is perfect for these types of workloads.\nThe below excerpt from the documentation shows the key differences between the different volume types.\nOption A is incorrect since the primary issue is that the volume type is not correct.\nOption C is incorrect since networking is not an issue here.\nOption D is incorrect since this option is applicable for the AWS RDS service.\nFor more information on EBS volume types, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html\n\n\nWhen an application experiences performance issues related to read and writes of the database, there are a few options to alleviate the issue. Let's go through each of the options provided in the question.\nA. Change the Instance type to a higher Instance Type Changing the Instance type to a higher Instance Type could potentially help alleviate the performance issues, especially if the current Instance is running at a high CPU or memory utilization. A higher Instance Type will provide more CPU and memory resources, which could help improve the database's read and write performance. However, if the performance issue is related to the underlying storage, changing the Instance Type might not help.\nB. Change the EBS volume to Provisioned IOPS SSD Changing the EBS volume to Provisioned IOPS SSD is a more targeted approach to address the underlying storage performance issues. Provisioned IOPS SSD volumes are designed to deliver predictable and consistent performance, especially for applications with high read and write workloads. By changing to a Provisioned IOPS SSD volume, it ensures that the database EC2 instance has the required storage performance to meet the application's demands.\nC. Enable Enhanced Networking on the Instance Enabling Enhanced Networking on the Instance can help to improve network performance by allowing higher packet per second (PPS) performance, lower latency, and lower jitter. However, if the performance issue is related to the underlying storage, enabling Enhanced Networking might not help.\nD. Enable Multi-AZ for the database Enabling Multi-AZ for the database creates a standby replica of the primary database in a different Availability Zone. This option provides high availability and improves disaster recovery capabilities. However, enabling Multi-AZ does not address the underlying performance issues with the database.\nIn conclusion, the most appropriate option to alleviate performance issues related to the read and writes of the database EC2 Instance is to change the EBS volume to Provisioned IOPS SSD (Option B). This option is a more targeted approach to address the underlying storage performance issues, ensuring that the database EC2 instance has the required storage performance to meet the application's demands.\n\n"
}, {
  "id" : 489,
  "question" : "Your company currently stores documents in an S3 bucket.\nThey want to transfer the files to a low-cost storage unit after a duration of 2 months to save on cost.\nWhich of the following can be used to perform this activity automatically?\n",
  "answers" : [ {
    "id" : "67d14ba0c4e84729bd9c36057cc891d6",
    "option" : "Use the events of the S3 bucket to transfer the files to Amazon Glacier.",
    "isCorrect" : "false"
  }, {
    "id" : "dc4d9145d0bc4b53b9d72f1df9e78cd7",
    "option" : "Use the events of the S3 bucket to transfer the files to EBS volumes â€“ Cold HD.",
    "isCorrect" : "false"
  }, {
    "id" : "9d0dcd1b3bd04c09b9a683513cf24859",
    "option" : "Use the lifecycle policies of the S3 bucket to transfer the files to Amazon Glacier.",
    "isCorrect" : "true"
  }, {
    "id" : "54c041c66cc446ca97afcefbf75e8a07",
    "option" : "Use the lifecycle policies of the S3 bucket to transfer the files to EBS volumes â€“ Cold HD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nTo manage your objects so that they are stored cost-effectively throughout their lifecycle, configure their lifecycle.\nA lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects.\nThere are two types of actions.\nTransition actions-Define when objects transition to another storage class.\nFor example, you might choose to transition objects to the STANDARD_IA storage class 30 days after you created them or archive objects to the GLACIER storage class one year after creating them.\nExpiration actions-Define when objects expire.\nAmazon S3 deletes expired objects on your behalf.\nOptions B and D are incorrect because ideally you don't transfer to EBS volumes - Cold HDD.Option A is incorrect because you need to use lifecycle policies.\nFor more information on lifecycle policies, please refer to the below URL-\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\n\nThe correct answer for this scenario is Option C: Use the lifecycle policies of the S3 bucket to transfer the files to Amazon Glacier.\nAmazon S3 (Simple Storage Service) is an object storage service that provides industry-leading scalability, data availability, security, and performance. It is designed for developers and IT teams to store and retrieve any amount of data from anywhere on the web. S3 provides various storage classes to help customers optimize costs based on access frequency and retrieval time requirements.\nAmazon Glacier is a low-cost storage service that provides secure, durable, and flexible long-term storage for data archiving and backup. It is designed for data that is infrequently accessed and for which retrieval time of several hours is acceptable. Glacier is optimized for data that is rarely accessed and that is retained for months, years, or even decades.\nLifecycle policies in Amazon S3 allow users to automatically manage the lifecycle of objects in S3 buckets. Using lifecycle policies, objects can be transitioned to different storage classes, expire, or be deleted. For this scenario, we can set up a lifecycle policy to transition the documents to Amazon Glacier storage after a duration of 2 months.\nOption A and Option B are incorrect because Amazon Glacier and EBS volumes are not interchangeable. Glacier is a separate storage service optimized for long-term storage, while EBS volumes are block-level storage devices that are attached to EC2 instances. Furthermore, Cold HD is not a valid EBS volume type.\nOption D is incorrect because EBS volumes are not a suitable storage service for long-term storage, and they are not designed for data archiving or backup purposes.\n\n"
}, {
  "id" : 490,
  "question" : "You are working as an AWS architect for a global financial company that offers real-time stock trading quotes to customers.\nYou are using Kinesis Data Streams to process stock market feeds from stock exchanges &amp; provide a real-time dashboard to the customers.\nDuring stock market hours, many users are accessing these dashboards, while aftermarket hours, there are very few users accessing these dashboards.The management team is looking for an optimum number of Kinesis Shards within Kinesis Data Streams.\nWhich of the following would be an automated solution to achieve this? (Choose 2)\n",
  "answers" : [ {
    "id" : "a9a3482984f54fe3b487888efbdaaa93",
    "option" : "Use Application Auto Scaling",
    "isCorrect" : "true"
  }, {
    "id" : "9ce1d4f2eff34ff19a7dc77cc892f086",
    "option" : "Use Amazon Kinesis Scaling Utility to modify the number of Shards in Kinesis Data Streams.",
    "isCorrect" : "false"
  }, {
    "id" : "97d2390ff82745078768786bbe2218b1",
    "option" : "Use Amazon Kinesis Scaling Utility along with AWS Elastic Beanstalk to automatically modify the number of Shards in Kinesis Data Streams.",
    "isCorrect" : "true"
  }, {
    "id" : "8f8953899da443bfa1c8c859d1ea1e6a",
    "option" : "Use UpdateShardCount to scale Shard count as per requirement.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A and C.\nAWS Application Auto scaling can be used to scale Kinesis Streams automatically.\nFor this, CloudWatch can be used to monitor Kinesis Data Stream shard metrics.\nBased on the changes in these metrics, CloudWatch can initiate a notification to Application Auto Scaling.\nThis will trigger an API Gateway to call Lambda Functions to increase/decrease Kinesis Data Stream Shards' number based upon metric values.\nAlternatively, you can use the Amazon Kinesis Scaling Utilities.\nTo do so, you can use each utility manually or automated with an AWS Elastic Beanstalk environment.\nOption B is incorrect because Amazon Kinesis Scaling Utility alone is a manual process as per documentation.\nOption D is incorrect as using UpdateShardCount will be a manual process.\nFor more information on Scaling Kinesis Data Streams using Application Auto Scaling and Amazon Kinesis Scaling Utilities, refer to the following URLs-\nhttps://aws.amazon.com/blogs/big-data/scaling-amazon-kinesis-data-streams-with-aws-application-auto-scaling/\nhttps://aws.amazon.com/blogs/big-data/under-the-hood-scaling-your-kinesis-data-streams/\n\nThe correct answers are A and B.\nExplanation:\nKinesis Data Streams is a managed service provided by AWS for real-time processing of streaming data at scale. It can handle data in real-time from various sources and process it using multiple consumer applications. It stores the data for 24 hours by default, and the data can be analyzed using various services like Amazon EMR, Amazon Redshift, and Amazon Elasticsearch.\nKinesis Data Streams is designed to scale up or down automatically based on the volume of incoming data. The number of shards within a Kinesis Data Stream is a critical factor in determining the overall throughput of the stream. Therefore, it is essential to choose an optimum number of shards based on the volume of data that the stream is expected to handle.\nThe management team is looking for an automated solution to achieve this. There are multiple options to automate the scaling of Kinesis Data Streams, but the two most appropriate options are:\nA. Use Application Auto Scaling: Application Auto Scaling is a service provided by AWS that can automatically adjust the number of resources used by your application to maintain performance and reduce costs. It can be used to automatically scale Kinesis Data Streams based on the volume of data being processed. With Application Auto Scaling, you can define a target utilization level for the stream, and AWS will automatically adjust the number of shards in the stream to match the desired utilization level. This helps to ensure that you are only paying for the resources that you actually need and can avoid overprovisioning.\nB. Use Amazon Kinesis Scaling Utility to modify the number of Shards in Kinesis Data Streams: Amazon Kinesis Scaling Utility is a tool provided by AWS that allows you to automate the scaling of Kinesis Data Streams based on the volume of data being processed. It works by monitoring the incoming data and adjusting the number of shards in the stream to match the volume of data. This helps to ensure that you are only paying for the resources that you actually need and can avoid overprovisioning.\nC. Use Amazon Kinesis Scaling Utility along with AWS Elastic Beanstalk to automatically modify the number of Shards in Kinesis Data Streams: AWS Elastic Beanstalk is a service provided by AWS that makes it easy to deploy and manage applications in the AWS Cloud. However, it is not directly related to Kinesis Data Streams, and therefore, it is not a suitable option for automating the scaling of Kinesis Data Streams.\nD. Use UpdateShardCount to scale Shard count as per requirement: UpdateShardCount is an API provided by AWS that allows you to manually adjust the number of shards in a Kinesis Data Stream. While it is possible to use this API to scale the shard count, it is not an automated solution and requires manual intervention, which is not ideal in a real-time stock trading scenario.\nIn summary, the most appropriate options to automate the scaling of Kinesis Data Streams based on the volume of data being processed are Application Auto Scaling and Amazon Kinesis Scaling Utility. These services allow you to dynamically adjust the number of shards in the stream to match the volume of data, which helps to ensure that you are only paying for the resources that you actually need and can avoid overprovisioning.\n\n"
}, {
  "id" : 491,
  "question" : "You have currently contacted an AWS Partner Network (APN) Partner to carry out an audit for your AWS account.\nYou need to ensure that the partner can carry out an audit on your resources.\nWhich one of the following steps would you ideally carry out?\n",
  "answers" : [ {
    "id" : "a5b15741801e4059bc80043a25c8b77a",
    "option" : "Create an IAM user for the partner account for login purposes.",
    "isCorrect" : "false"
  }, {
    "id" : "1b678219e2254eafaa0aba8e891f6e9c",
    "option" : "Create a cross account IAM Role.",
    "isCorrect" : "true"
  }, {
    "id" : "b0fabaa20ba841fa8078ada5f3d44fe4",
    "option" : "Create an IAM group for the partner account for login purposes.",
    "isCorrect" : "false"
  }, {
    "id" : "6b25e3e852cb4246b84a1f9a2f6391ce",
    "option" : "Create an IAM profile for the partner account for login purposes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nCross-account IAM roles allow customers to securely grant access to AWS resources in their account to a third party, like an APN Partner, while retaining the ability to control and audit who is accessing their AWS account.\nCross-account roles reduce the amount of sensitive information APN Partners need to store for their customers so that they can focus on their product instead of managing keys.\nUsing an IAM user to control 3rd party access involves handing over an Access Key/Secret Key - this is the simple \"access badge.\"\nUsing AssumeRole to control 3rd party access uses the same information plus a security token.\nTo assume a role, your AWS account must be trusted by the role.\nThe trust relationship is defined in the role's trust policy when the role is created.\nThis is the \"access badge with fingerprint validation.\"\nAnyone can use the IAM keys - they're just a key pair.\nAnyone can take them and use them later on, and you would not be able to be identified from the trusted party they were given to.\nTo use the AssumeRole, you must be first authenticated as the trusted entity, and in the case of temporary credentials, use them while they haven't expired.\nThese extra security features are what make it more secure.\nTypically, you use\nAssumeRole.\nfor cross-account access.\nOptions A and C are incorrect since it is not secured as IAM users and IAM group (a set of users) will be given permissions just like giving keys to them without extra security token.\nOption D is incorrect since IAM Profile doesn't exist in AWS.\nFor more information on cross-account roles, please refer to the below URLs-\nhttps://aws.amazon.com/blogs/apn/securely-accessing-customer-aws-accounts-with-cross-account-iam-roles/\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html\n\nWhen an AWS Partner Network (APN) partner is conducting an audit on your AWS account, you need to ensure that the partner can access your resources for the purposes of the audit. There are different ways to grant access to your AWS resources.\nOption A, creating an IAM user for the partner account for login purposes, is not recommended as it would require you to provide the partner with access keys, which could be a security risk. IAM users should be created only for individuals who need ongoing access to your AWS resources, such as employees or contractors.\nOption C, creating an IAM group for the partner account for login purposes, is also not recommended as it suffers from the same security risk as option A. IAM groups should be used to manage permissions for collections of IAM users.\nOption D, creating an IAM profile for the partner account for login purposes, is not a valid option. IAM profiles are not used for login purposes, but are used to grant permissions to an EC2 instance to access other AWS resources.\nThe recommended approach is option B, creating a cross-account IAM role. A cross-account IAM role allows you to grant temporary access to your AWS resources to a trusted external account, such as an APN partner, without sharing access keys. The partner will assume the role and be granted permissions based on the policies attached to the role. This approach provides a more secure and controlled way to grant temporary access to your AWS resources.\nTo create a cross-account IAM role, you need to define a trust policy that specifies the trusted external account and a permissions policy that specifies the resources and actions that the role can access. Once the role is created, you can share the ARN (Amazon Resource Name) of the role with the APN partner, who can then assume the role and access the resources needed for the audit.\n\n"
}, {
  "id" : 492,
  "question" : "You are working for a start-up company using Amazon DynamoDB for mobile applications.\nDuring performing POC for a new application using AWS SDK, most of the cases are successfully executed, but for certain cases, Amazon DynamoDB is returning the ThrottlingException error message.\nDevelopment Team is asking for your help to suggest a solution to reduce the number of these errors.\nWhich of the following method is the most suitable?\n",
  "answers" : [ {
    "id" : "d035095fd1d5413e88a79840c87d0dc4",
    "option" : "Retry the DynamoDB operations when the error happens.",
    "isCorrect" : "false"
  }, {
    "id" : "a2e1c938d57b494986db9d5e802f0707",
    "option" : "Modify the application by adding a five seconds timer before sending a DynamoDB request.",
    "isCorrect" : "false"
  }, {
    "id" : "ef8253695c6143e0877b524b5182c267",
    "option" : "Scale up the DynamoDB table by allocating more read/write capacities.",
    "isCorrect" : "true"
  }, {
    "id" : "7e54bbead0b84b87b8072b57ab30daa3",
    "option" : "Use AWS DynamoDB SDK in the application that optimizes the DynamoDB operations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect because this method cannot reduce the number of errors.\nThis method can be used to mitigate the impacts when the error happens.\nOption B is incorrect because five seconds timer may not be suitable for the application and it needs the code change as well.\nThis is not the most suitable solution.\nOption C is CORRECT because with more read/write capacities in the table, the DynamoDB table can process more read/write requests, which can reduce the number of ThrottlingException errors.\nOption D is incorrect because the application may already use the AWS DynamoDB SDK so this option will not help.\nFor more information on the DynamoDB ThrottlingException error, refer to the following URL-\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html#Programming.Errors.MessagesAndCodes\n\nAmazon DynamoDB is a fully managed NoSQL database service provided by AWS, that can be used to store and retrieve any amount of data, and to serve any level of request traffic. However, DynamoDB has certain limits on the number of read and write requests that can be made per second per table, and exceeding these limits can lead to ThrottlingException errors. These errors occur to protect the database from being overwhelmed by excessive request traffic.\nTo reduce the number of ThrottlingException errors in the mobile application using AWS SDK for DynamoDB, we have the following options:\nA. Retry the DynamoDB operations when the error happens: Retrying the DynamoDB operations when the error occurs can help in reducing the number of errors, but it is not a recommended solution as it can lead to additional load on the database, and may not always be successful in reducing the number of errors.\nB. Modify the application by adding a five seconds timer before sending a DynamoDB request: Adding a five seconds timer before sending a DynamoDB request can help in reducing the number of ThrottlingException errors as it provides a buffer time for the database to process the previous requests and make resources available for new requests. However, this solution is not optimal as it can delay the application's response time, and may not be effective if the request rate is too high.\nC. Scale up the DynamoDB table by allocating more read/write capacities: Scaling up the DynamoDB table by allocating more read/write capacities is the recommended solution to reduce the number of ThrottlingException errors. By increasing the provisioned capacity for the table, more requests can be processed in parallel, and the database can handle higher levels of request traffic. DynamoDB allows scaling up and down the capacity as needed, and offers auto-scaling options to adjust the capacity automatically based on the workload.\nD. Use AWS DynamoDB SDK in the application that optimizes the DynamoDB operations: Using the AWS DynamoDB SDK in the application that optimizes the DynamoDB operations can help in reducing the number of ThrottlingException errors by optimizing the request patterns and reducing the number of round trips to the database. The SDK offers features like batch operations, caching, and retry logic, that can help in reducing the request traffic and improving the performance.\nIn conclusion, the most suitable method to reduce the number of ThrottlingException errors in the mobile application using AWS SDK for DynamoDB is to scale up the DynamoDB table by allocating more read/write capacities.\n\n"
}, {
  "id" : 493,
  "question" : "Your company is planning to make use of the Elastic Container service for managing their container-based applications.\nThey are going to process both critical and non-critical workloads with these applications.\nWhich of the following COST effective setup would they consider?\n",
  "answers" : [ {
    "id" : "e1377c7058f34c159ea77fe1fd1f0c23",
    "option" : "Use ECS orchestration and Spot Instances for processing critical data and On-Demand for the non-critical data.",
    "isCorrect" : "false"
  }, {
    "id" : "32bad1b8158c4c66ad1e31b53da0b16f",
    "option" : "Use ECS orchestration and On-Demand Instances for processing critical data and Spot Instances for the non-critical data.",
    "isCorrect" : "true"
  }, {
    "id" : "aa552973093046f0a76b0090de6707c0",
    "option" : "Use ECS orchestration and Spot Instances for both the processing of critical data and non-critical data.",
    "isCorrect" : "false"
  }, {
    "id" : "1ac313abb18943b4ab8fc972a8a2e24b",
    "option" : "Use ECS orchestration and On-Demand Instances for both the processing of critical data and non-critical data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nSpot Instance and On-demand Instance are very similar in nature.\nThe main difference between these is a commitment.\nIn Spot Instance, there is no commitment.\nAs soon as the Bid price exceeds the Spot price, a user gets the Instance.\nIn an On-demand Instance, a user has to pay the On-demand rate specified by Amazon.\nOnce they have bought the Instance, they have to use it by paying that rate.\nIn Spot Instance, once the Spot price exceeds the Bid price, Amazon will shut the instance.\nThe benefit to the user is that they will not be charged for the partial hour in which the Instance was taken back from them.\nSpot instances are not always cheaper than on-demand, they can and do sometimes fluctuate wildly, even to very high per hour amounts, higher than the on-demand price at times.\nA Spot Instance is an unused EC2 instance that is available for less than the On-Demand price.\nBecause Spot Instances enable you to request unused EC2 instances at steep discounts, you can significantly lower your Amazon EC2 costs.\nThe hourly price for a Spot Instance is called a Spot price.\nThe Spot price of each instance type in each Availability Zone is set by Amazon EC2 and adjusted gradually based on the long-term supply of and demand for Spot Instances.\nYour Spot Instance runs whenever capacity is available, and the maximum price per hour for your request exceeds the Spot price.\nOptions A and C are incorrect since Spot Instances can be taken back or interrupted and should not be used for critical workloads.\nOption D is not a cost-effective solution.\nYou can use Spot Instances for non-critical workloads.\nFor more information on Spot Instances, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nSure, I can explain the answer to the question in detail:\nThe Elastic Container Service (ECS) is an AWS service that allows you to easily run and manage Docker containers on a cluster of EC2 instances. ECS provides two types of instance launch types: On-Demand and Spot Instances.\nOn-Demand Instances are the traditional pay-as-you-go instances where you pay for the compute capacity by the hour, with no long-term commitments or upfront payments. On-Demand Instances are best suited for applications with steady, predictable usage patterns or that require the ability to scale up and down rapidly.\nSpot Instances, on the other hand, are AWS EC2 instances that you can bid for and purchase at a much lower price than On-Demand Instances. Spot Instances are best suited for applications that can tolerate interruptions and are flexible in terms of when they can run, such as batch processing, data analysis, and testing.\nNow coming to the answer of the question, based on the information provided, the company needs to process both critical and non-critical workloads with the container-based applications. The question also mentions that the company wants to set up a cost-effective solution.\nOption A suggests using Spot Instances for processing critical data and On-Demand Instances for non-critical data. However, this is not a recommended approach, as critical workloads require high availability and cannot tolerate interruptions, which is a common characteristic of Spot Instances.\nOption B suggests using On-Demand Instances for processing critical data and Spot Instances for non-critical data. This is a better option than Option A, as On-Demand Instances provide high availability and are suitable for critical workloads, while Spot Instances are cost-effective and can be used for non-critical workloads.\nOption C suggests using Spot Instances for both critical and non-critical workloads. This is not a recommended approach, as critical workloads require high availability and cannot tolerate interruptions, which is a common characteristic of Spot Instances.\nOption D suggests using On-Demand Instances for both critical and non-critical workloads. This is a good option, as it provides high availability and is suitable for both critical and non-critical workloads. However, this option may not be the most cost-effective, as On-Demand Instances are typically more expensive than Spot Instances.\nTherefore, the best answer to the question would be Option B: Use ECS orchestration and On-Demand Instances for processing critical data and Spot Instances for non-critical data. This option provides high availability for critical workloads and cost-effectiveness for non-critical workloads.\n\n"
}, {
  "id" : 494,
  "question" : "Your company is planning to set up an application that will consist of a web layer.\nThis web layer will consist of EC2 Instances sitting behind an Application Load Balancer.\nThe company wants to protect the application against application level attacks.\nWhich of the following can be used for this purpose?\n",
  "answers" : [ {
    "id" : "d768dd3dd846457eba8a65cf750a8adb",
    "option" : "AWS Cloudfront",
    "isCorrect" : "false"
  }, {
    "id" : "0a93828414ba4228b395c67a58496ea4",
    "option" : "AWS WAF",
    "isCorrect" : "true"
  }, {
    "id" : "21ba1b15f84c4aac8c48346ce5796bcb",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "ea5a889db0f4416cb12c9470ac1f2fe3",
    "option" : "AWS VPC NACL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nYou use AWS WAF to control how Amazon CloudFront or an Application Load Balancer responds to web requests.\nYou start by creating conditions, rules, and web access control lists (web ACLs)\nYou define your conditions, combine your conditions into rules, and combine the rules into a web ACL.\nOption A is invalid because this is used for content delivery.\nOption C is invalid because this is a configuration service.\nOption D is invalid because this is used to block traffic based on simple rules.\nFor more information on how the AWS WAF works, please refer to the below URL-\nhttps://docs.aws.amazon.com/waf/latest/developerguide/how-aws-waf-works.html\n\nThe correct answer is B. AWS WAF.\nAWS WAF (Web Application Firewall) is a service that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. It enables you to define customizable web security rules that block common attack patterns like SQL injection or cross-site scripting (XSS) before they reach your application.\nIn the scenario given, the company wants to protect their application against application-level attacks. Using AWS WAF in conjunction with the Application Load Balancer, the company can inspect incoming traffic to the web layer and filter out requests that may be malicious or unwanted. This helps to protect the EC2 instances that are serving the web content from being compromised by attackers.\nLet's take a look at the other options to see why they are not the best fit for this scenario:\nA. AWS CloudFront is a content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency and high transfer speeds. While it can provide some level of protection against certain types of attacks, it is not a dedicated web application firewall like AWS WAF.\nC. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. While it can help you ensure that your AWS resources are configured correctly and compliant with industry standards, it is not designed to protect web applications from attacks.\nD. AWS VPC NACL (Network Access Control List) is a service that acts as a firewall for controlling traffic in and out of one or more subnets. While it can be used to control access to your resources and limit exposure to attacks, it is not designed to filter out malicious requests at the application level like AWS WAF.\nIn conclusion, AWS WAF is the best option for protecting a web layer consisting of EC2 instances behind an Application Load Balancer from application-level attacks. It enables you to define customized web security rules that block common attack patterns before they reach your application.\n\n"
}, {
  "id" : 495,
  "question" : "A global IT firm has a Hybrid connectivity for its applications servers deployed across on-premise Data Centre &amp; AWS Cloud infrastructure.\nDevelopers have designed a new application that will be deployed at both locations &amp; are expecting to be accessed by 4000 users globally.\nCustomer Security Team requires the same security policies to be applied to users when accessing new applications from on-premise servers or AWS Cloud.\nA customer has already deployed an Active directory on-premise with all security compliance.\nThe IT Head has informed you about budget constraints &amp; no additional cost should be incurred for a new solution. Which of the following can be used to design AD services for new applications?\n",
  "answers" : [ {
    "id" : "ab92e6e79ff444409b87ddfdd67e587c",
    "option" : "Use AWS Active Directory Service.",
    "isCorrect" : "false"
  }, {
    "id" : "8f742fc760c54b10a8e828b1e3d37015",
    "option" : "Use Simple AD",
    "isCorrect" : "false"
  }, {
    "id" : "e4dc14c704ae4a63a87c57ca9a58551d",
    "option" : "Use AD Connector.",
    "isCorrect" : "true"
  }, {
    "id" : "b873dff9dfa64c9f87a2310f38875db8",
    "option" : "Use Amazon EC2 instance with Active Directory installed.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nUsing an AD connector, customers can use existing AD deployed at on-premise servers.\nFor this, no additional deployment is required, just over existing VPN /DX connectivity.\nAll requests can be forwarded to on-premise AD.\nWith this, the same security policies can be applied for both on-premise &amp; users accessing from the cloud.\nWith AWS Managed Active Directory Service or deploying AD on Amazon EC2 instance, additional cost will incur to deploy separate AD in AWS cloud.\nSimple AD does not forward requests to on-premise AD.\nSo security policies cannot be the same.\nOption A is incorrect since the customer is looking for no additional cost for setting a new Active Directory.\nExisting AD deployed at an on-premise location can be used.\nOption B is incorrect as with Simple AD, Policies cannot be implemented same for on-premise &amp; In AWS.\nOption D is incorrect as using the Amazon EC2 instance will incur additional costs.\nFor more information on Active Directory Connector, refer to the following URL-\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/directory_ad_connector.html\n\n\nThe customer has a requirement to deploy a new application on both on-premise and AWS cloud infrastructure, and ensure that the same security policies are applied to users accessing the application from both locations. The customer already has an Active Directory deployed on-premise with all security compliance. Additionally, the customer has budget constraints and wants to avoid incurring any additional costs for a new solution.\nGiven these requirements, the most appropriate option for designing AD services for the new application is to use AD Connector (option C).\nAD Connector is a free service provided by AWS that allows customers to connect their on-premise Active Directory to AWS, without the need for a separate Active Directory instance in the AWS cloud. It allows customers to use their existing on-premise Active Directory infrastructure to manage user identities and permissions for applications deployed in AWS. AD Connector also enables secure and efficient communication between the on-premise Active Directory and AWS services.\nBy using AD Connector, the customer can ensure that the same security policies are applied to users accessing the new application from both on-premise and AWS cloud locations. This is because AD Connector enables authentication and authorization of users using the same Active Directory deployed on-premise.\nAWS Active Directory Service (option A) and Simple AD (option B) are also Active Directory solutions provided by AWS, but they require the deployment of a separate Active Directory instance in the AWS cloud. This would result in additional costs for the customer, which is not desirable given the budget constraints.\nUsing an Amazon EC2 instance with Active Directory installed (option D) is also not the best option because it would require additional infrastructure deployment and management. Additionally, it could result in additional costs for the customer due to the need for an EC2 instance and associated resources.\nIn summary, using AD Connector is the most appropriate option for the customer given the requirement for a solution that does not require additional costs and utilizes their existing on-premise Active Directory infrastructure.\n\n"
}, {
  "id" : 496,
  "question" : "A company has recently chosen to use the AWS API Gateway service for managing their API's.\nIt needs to be ensured that code hosted in other domains can access the API's behind the API gateway service.\nWhich of the below security features of the API gateway can be used to ensure that API's resources can receive requests from a domain other than the API's own domain?\n",
  "answers" : [ {
    "id" : "57b4c1ad200d476cb0b034f4aca30a15",
    "option" : "API Stages",
    "isCorrect" : "false"
  }, {
    "id" : "4e05f1af760748df952fc7280b5526a4",
    "option" : "API Deployment",
    "isCorrect" : "false"
  }, {
    "id" : "45c369f9d0234bca9f75d891346d65b9",
    "option" : "API CORS",
    "isCorrect" : "true"
  }, {
    "id" : "6114d8ddab4244ae8d17093540894366",
    "option" : "API Access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nThe AWS Documentation mentions the following.\nWhen your API's resources receive requests from a domain other than the API's own domain, you must enable cross-origin resource sharing (CORS) for selected methods on the resource.\nThis amounts to having your API respond to the OPTIONS preflight request with at least the following CORS-required response headers:\nAccess-Control-Allow-Methods.\nAccess-Control-Allow-Headers.\nAccess-Control-Allow-Origin.\nOption A and B are invalid because these are used to ensure users can call API's.\nOption D is invalid because there is no such thing as API Access.\nFor more information on enabling CORS, please refer to the below URL-\nhttp://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html\n\nThe correct answer is C. API CORS.\nAWS API Gateway is a fully managed service that makes it easy to create, deploy, and manage APIs at any scale. When creating an API in AWS API Gateway, the API owner needs to ensure that the API resources can receive requests from a domain other than the API's own domain. This is where Cross-Origin Resource Sharing (CORS) comes into play.\nCORS is a security feature implemented in web browsers that restricts web pages from making requests to a different domain than the one that served the original web page. This security feature is implemented by adding HTTP headers to API responses that instruct the browser to allow or deny cross-domain requests.\nAWS API Gateway provides a simple and configurable way to enable CORS for APIs. When configuring CORS in API Gateway, the API owner specifies the allowed domains that can access the API resources. This is done by setting the \"Access-Control-Allow-Origin\" header in the API response to the allowed domain.\nTo configure CORS in API Gateway, the following steps need to be performed:\nOpen the API Gateway console in the AWS Management Console. Select the API that needs to be configured for CORS. Click on the \"Actions\" button and select \"Enable CORS\". Configure the CORS settings as required, including the allowed domains, headers, and methods. Save the CORS configuration.\nOnce the CORS configuration is saved, the API Gateway service will automatically add the necessary headers to the API responses to enable cross-domain access.\nIn summary, API Gateway's CORS security feature allows an API owner to enable cross-domain access to the API's resources. This is achieved by adding HTTP headers to the API responses that instruct the browser to allow or deny cross-domain requests.\n\n"
}, {
  "id" : 497,
  "question" : "You have an architecture that consists of a set of web servers in the public subnets.\nAnd database servers in the private subnet along with a NAT instance.\nThe NAT instance is now becoming a bottleneck, and you are looking to replace it with a NAT gateway.\nWhich of the following would ensure a high availability setup for the NAT device?\n",
  "answers" : [ {
    "id" : "ecdf0a30e1064b8285a1439f5fe6417a",
    "option" : "Disable source/destination check on the NAT Instances",
    "isCorrect" : "false"
  }, {
    "id" : "dbf900fb8c894eaa9c9e5fe85109ef04",
    "option" : "Deploy a NAT gateway in 2 availability zones",
    "isCorrect" : "true"
  }, {
    "id" : "7644b00cc8a441bfa637984fba255e12",
    "option" : "Deploy a NAT gateway along with the NAT instance",
    "isCorrect" : "false"
  }, {
    "id" : "c6a8cb2718fd4b729fcdf0e96a927d3f",
    "option" : "Deploy the NAT Gateway in 2 regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nFirst, here we have a difference between NAT Instance and NAT Gateway.\nA NAT instance is an Amazon EC2 instance configured to forward traffic to the Internet.\nIt can be launched from an existing AMI.\nInstances in a private subnet that want to access the Internet can have their Internet-bound traffic forwarded to the NAT Instance via a Route Table configuration.\nThe NAT Instance will then request the Internet (since it is in a Public Subnet), and the response will be forwarded back to the private instance.\nAWS introduced a NAT Gateway Service that can take the place of a NAT Instance.\nThe benefits of using a NAT Gateway service are:\nIt is a fully managed service -- create it, and it works automatically, including fail-over.\nHowever, Security Groups cannot be associated with a NAT Gateway.\nYou'll need one in each AZ since they only operate in a single AZ.\nEnsure that your NAT gateways are deployed in at least two Availability Zones (AZs) to enable EC2 instances available within private subnets to connect to the Internet or other AWS services but prevent the Internet from initiating a connection with those instances.\nOption A is incorrect since this is a requirement for the NAT instance to function and will not satisfy the requirement for the question.\nOption C is incorrect since you should use one type of device.\nOption D is incorrect since you should achieve redundancy via Availability Zones.\nFor more information on the NAT gateway, please refer to the below URL-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html\n\nThe correct answer is B: Deploy a NAT gateway in 2 availability zones.\nExplanation: A NAT (Network Address Translation) instance is an EC2 instance that is used to provide internet connectivity to instances in a private subnet. However, as the traffic volume increases, the NAT instance may become a bottleneck, leading to decreased network performance. A NAT gateway is a highly available, managed NAT service provided by AWS that can help overcome this limitation.\nTo ensure high availability for the NAT device, it is recommended to deploy a NAT gateway in at least two availability zones. This provides redundancy and failover in case one availability zone becomes unavailable or experiences network issues. Each availability zone is a separate physical location with independent power, cooling, and networking infrastructure, and deploying a NAT gateway in multiple availability zones ensures that your network traffic can continue to flow even if there is an outage in one availability zone.\nOption A (Disable source/destination check on the NAT instances) is not relevant to this scenario. The source/destination check is a security feature that is enabled by default for instances launched in a VPC. Disabling this feature on the NAT instance would not help in providing high availability for the NAT device.\nOption C (Deploy a NAT gateway along with the NAT instance) is not recommended as it defeats the purpose of using a managed NAT service like NAT gateway. Deploying both a NAT gateway and a NAT instance would not provide any additional benefits, but would add unnecessary complexity to the architecture.\nOption D (Deploy the NAT Gateway in 2 regions) is not a feasible option as it would require duplicating the entire architecture in multiple regions, which can be expensive and complex to manage. Deploying a NAT gateway in multiple availability zones within the same region is a better approach to ensure high availability.\nIn summary, deploying a NAT gateway in at least two availability zones is the recommended approach to ensure high availability for the NAT device.\n\n"
}, {
  "id" : 498,
  "question" : "Your company is planning to set up an application with the following architecture. A set of EC2 Instances will host a web application. The application will sit behind an Elastic Load balancer. The users will access the application from the internet via the Elastic Load balancer. The application will connect to a backend database server. A NAT Gateway is also implemented. Which of the following is the right architecture for the network, keeping high availability and security in mind?\n",
  "answers" : [ {
    "id" : "816be263571b41a6aeccaa173daca346",
    "option" : "2 public subnets for the Elastic Load balancer and NAT Gateway, 2 public subnets for the Web server EC2 Instances, 2 private subnets for the database server",
    "isCorrect" : "false"
  }, {
    "id" : "02d2f6fe41d344cc8bc7d4b9bdedcde5",
    "option" : "2 public subnets for the Elastic Load balancer and NAT Gateway, 2 private subnets for the Web server EC2 Instances, 2 private subnets for the database server",
    "isCorrect" : "true"
  }, {
    "id" : "207b133aa9a744fd9ef8f07f367b9746",
    "option" : "2 public subnets for the Elastic Load balancer and NAT Gateway, 2 public subnets for the Web server EC2 Instances, 2 public subnets for the database server",
    "isCorrect" : "false"
  }, {
    "id" : "362ae0af100a4be380c1d89798fb082c",
    "option" : "2 public subnets for the Elastic Load balancer and NAT Gateway, 2 private subnets for the Web server EC2 Instances, 2 public subnets for the database server.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nYou need to have public subnets for the Elastic Load balancer to ensure that traffic can flow via the Internet.\nThe Web servers can be in the Private subnet since the communication between the instances and the ELB happens via the private IP and provides better security for the Web Servers.\nThe database servers should be in the private subnet since it does not need to communicate with the Internet.\nOption A is invalid since the ELB is in the Public subnet, there is no need to place the Web Server in the Public subnet because ELB and Web Server communicate via Private IP.\nOption C is invalid since the database servers don't need to be in the public subnet.\nOption D is invalid since the database servers don't need to be in the public subnet.\nFor more information on Elastic Load balancing, please refer to the below URL-\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.html\nNote: There is no requirement for ec2 instances to be in public subnet as route53 will route the request to elb whose endpoint is exposed as lb(Load Balancer) is in public subnet.\nThe communication between elb and ec2 instances happens via private IP.\nSo it's better for security purposes.\nURL: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html.\ncreate an Internet-Facing Load Balancer.\nWhen you create a load balancer in a VPC, you can make it an internal load balancer or an Internet-facing load balancer.\nYou create an Internet-facing load balancer in a public subnet.\nLoad balancers in EC2-Classic are always Internet-facing load balancers.\nWhen you create your load balancer, you configure listeners, configure health checks, and register back-end instances.\nYou configure a listener by specifying a protocol and a port for front-end (client to load balancer) connections, and a protocol and a port for back-end (load balancer to back-end instances) connections.\nYou can configure multiple listeners for your load balancer.\n\nThe correct architecture for the network, keeping high availability and security in mind, is Option B: B. 2 public subnets for the Elastic Load balancer and NAT Gateway, 2 private subnets for the Web server EC2 Instances, 2 private subnets for the database server.\nHere's the detailed explanation:\nElastic Load Balancer: The Elastic Load Balancer is the entry point for incoming traffic from the internet. It distributes incoming traffic across multiple EC2 instances. The Elastic Load Balancer should be placed in a public subnet to allow access from the internet. EC2 Instances hosting web application: The EC2 instances hosting the web application should be placed in private subnets. Private subnets have no direct access to the internet, which adds an extra layer of security to your application. Also, the EC2 instances should be spread across multiple Availability Zones to ensure high availability. Backend Database Server: The backend database server should also be placed in a private subnet. Private subnets offer an additional layer of security by not allowing direct access to the database server from the internet. Additionally, the database server should also be spread across multiple Availability Zones to ensure high availability. NAT Gateway: The NAT Gateway is used to allow EC2 instances in the private subnets to access resources on the internet. The NAT Gateway should be placed in a public subnet to allow it to access the internet.\nSo, the correct architecture for the network, keeping high availability and security in mind, is Option B: 2 public subnets for the Elastic Load balancer and NAT Gateway, 2 private subnets for the Web server EC2 Instances, 2 private subnets for the database server.\n\n"
}, {
  "id" : 499,
  "question" : "You are working as an AWS consultant for a Bio-technology company that is working on Human genome data processing.\nThey are using HDFS to process a large amount of data.\nThey plan to migrate these systems to AWS EC2, where 50 x EC2 C5 instances will be used to compute data.\nThis is a critical project where any failure will result in huge financial loss.\nThe company is seeking your recommendation for the best solution to avoid correlated failures.\nWhich ways can be used to limit the impact of any Hardware failure in this scenario? (Select TWO)\n",
  "answers" : [ {
    "id" : "0f5c5477cf854c0cbd131d05ff7c7e8e",
    "option" : "Use AWS CLI to deploy EC2 instance in the Partitioned Placement group.",
    "isCorrect" : "true"
  }, {
    "id" : "3e8e7c8eca6e46a1884dbacff267597e",
    "option" : "Use AWS CLI to deploy EC2 instance in the Cluster Placement group.",
    "isCorrect" : "false"
  }, {
    "id" : "aa47ea6fa591402d958fe2f98bb231ab",
    "option" : "Use AWS CLI to deploy EC2 instances in multiple Regions.",
    "isCorrect" : "false"
  }, {
    "id" : "0475d675b9b840fcad26235e608bb5cc",
    "option" : "Use AWS Console to deploy EC2 instance in Partitioned Placement group.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - A and D.\nLaunching EC2 in the Placement group is possible using both CLI and Console.\nFirstly, a Rack server is a computer dedicated to use as a server and designed to be installed in a framework called a rack.\nEach rack has its own network and power source.\nIn the Partition Placement group, each logical partition within the placement group has its own set of racks.\nIf a rack fails (hardware failure), it may affect multiple instances on that rack within that logical partition.\nSo, if you have replication in other partitions, then your data will be safe.\nThis will be good for Big data applications like HDFS, HBase, Cassandra, Kafka, or any other fault-tolerant system.\nThis placement group strikes a balance between High Performance and High Availability.\nA partition placement group can have partitions in multiple Availability Zones in the same Region.\nA partition placement group can have a maximum of seven partitions per Availability Zone.\nThe number of instances that can be launched into a partition placement group is limited only by the limits of your account.\nOption B is incorrect because all instances are placed within a rack in the Cluster Placement group.\nIf the rack fails (hardware failure), all instances fail at the same time.\nThis is ideal for High-Performance applications.\nOption C is incorrect because the impact can be reduced using multiple \"availability zones\" instead of \"regions\".\nFor more information on Placement Groups, refer to the following URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\nWhen working with a critical project such as the Human genome data processing, it's important to plan for hardware failure to ensure that the system is always available. To limit the impact of hardware failure, there are several solutions that can be implemented. Here are two of the best solutions to avoid correlated failures:\nUse AWS CLI to deploy EC2 instance in the Partitioned Placement group: Partitioned Placement Groups can be used to spread EC2 instances across multiple hardware platforms to avoid correlated failures. EC2 instances that are placed in the same partition will share the same underlying hardware infrastructure. Therefore, if a failure occurs, it only affects the instances in that partition, not the whole system. This solution ensures that the data processing can continue even if some instances experience hardware failure. Use AWS CLI to deploy EC2 instance in the Cluster Placement group: Cluster Placement Groups can be used to launch instances in a low-latency, high-bandwidth network. EC2 instances in a Cluster Placement Group are placed on the same rack, which ensures that network latency between instances is very low. In addition, instances in a Cluster Placement Group are placed on different racks, which reduces the impact of hardware failure. This solution ensures that the data processing can continue with low latency even if some instances experience hardware failure.\nUsing AWS CLI to deploy EC2 instances in multiple regions (C) is not a good solution because it introduces additional latency and can result in data inconsistency issues. Using AWS Console to deploy EC2 instance in Partitioned Placement group (D) is a less efficient solution because it's manual and prone to errors.\nIn summary, the best ways to limit the impact of any hardware failure in this scenario are to use AWS CLI to deploy EC2 instance in the Partitioned Placement group (A) or Cluster Placement group (B).\n\n"
}, {
  "id" : 500,
  "question" : "An application needs to be set up on AWS.\nIt consists of several components.\nTwo primary components are required to run for 3 hours every day.\nDuring the three hours, the EC2 instances should always be in service for the components to run.\nThe other components are required every day for more than 6-8 hours.\nTasks of these components are less important and can be easily restored even if the EC2 instances are stopped.\nWhich of the following would you use to ensure COSTS are minimized for the underlying EC2 Instances?\n",
  "answers" : [ {
    "id" : "662d1259de324d3a9ee47e09fa7eba94",
    "option" : "Reserved instances for the primary components and On-Demand Instances for the remaining components.",
    "isCorrect" : "false"
  }, {
    "id" : "54e57a0ff9b149ec93e0600a2bff15e9",
    "option" : "Spot instances for the primary components and On-Demand Instances for the remaining components.",
    "isCorrect" : "false"
  }, {
    "id" : "3368ea020eb24527868a22ea95bca5ad",
    "option" : "On-Demand instances for the primary components and Spot Instances for the remaining components.",
    "isCorrect" : "true"
  }, {
    "id" : "2588ee4874b54fdbaea07fe4fbbb98ab",
    "option" : "On-Demand instances for the primary components and Reserved Instances for the remaining components.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOptions A and D are incorrect because reserved instances have one-year or three-year commitments and are not suitable in this scenario.\nThe primary components only need to run 3 hours every day.\nIt is not cost-efficient if reserved instances are used.\nOption B is incorrect because primary components cannot use spot instances as they may impact the service.\nOption C is CORRECT because on-demand instance ensures the stability of primary components while spot instances help to reduce the cost for the remaining components.\n\nTo minimize costs for the EC2 instances required to run an application with several components, we need to choose a combination of instance types that balances cost with availability.\nBased on the given requirements, we can categorize the components into two types:\nPrimary components: These are required to run for exactly 3 hours every day and are critical to the application's functionality. Other components: These are required for more than 6-8 hours every day, but their tasks are less important, and they can be easily restored even if the EC2 instances are stopped.\nConsidering the above categorization, the best option would be to use reserved instances for the primary components and On-Demand instances for the remaining components. This approach ensures that the primary components are always available during the specific three-hour window while minimizing costs for the other components that can be easily restored.\nReserved instances are a good choice for primary components because they provide a lower cost per hour than On-Demand instances, and they guarantee capacity availability. With reserved instances, you pay a one-time upfront fee and then a reduced hourly rate for the instance type you reserved. This option provides a significant discount over On-Demand instances, making it cost-effective for components that are required to run for specific times.\nOn-Demand instances are a good choice for the other components because they offer the flexibility to start and stop instances as needed, and you only pay for the hours you use them. Since these components are less critical and can be easily restored, it is more cost-effective to use On-Demand instances.\nSpot instances are not a good choice for the primary components because they are not guaranteed to be available for the entire 3-hour window. Spot instances are ideal for workloads that can handle interruptions and can be interrupted at any time if the market price for the instance type rises above your bid price.\nIn summary, the best option for minimizing EC2 instance costs for an application with primary and other components is to use reserved instances for the primary components and On-Demand instances for the remaining components.\n\n"
}, {
  "id" : 501,
  "question" : "You are the Solutions Architect for a health insurance service company that wants to start building and supporting IoT devices for patients who recently signed new clauses in the contract.\nThis will open opportunities to expand its market but also introduces some restrictions.\nAll services and data involved have to guarantee HIPAA compliance and include in-transit encryption.\nDue to high sensitivity data, bypassing the internet is crucial.\nThe architecture uses already ELBs.\nThey want to avoid DNS re-configuration and IP address caching when it comes to the IoT devices.\nWhat combination of services may be the closest option to address this challenge?\n",
  "answers" : [ {
    "id" : "cdec5f6f32ce4de78e15e6be94d5eb90",
    "option" : "AWS ELBs, AWS IoT, and Amazon Route53 configured with geolocation or latency routing policies. This requires an interface endpoint (PrivateLink) for Route53 to stay inside the AWS backbone.",
    "isCorrect" : "false"
  }, {
    "id" : "3a146f500b3140af9d7c6c5263f35744",
    "option" : "AWS ELBs, AWS IoT, and AWS Global Accelerator which provisions two anycast static IP addresses.",
    "isCorrect" : "true"
  }, {
    "id" : "b76cddc9a36d4c6596466cc492168248",
    "option" : "AWS ELBs, AWS IoT, and Amazon Route53 configured with geolocation or latency routing policies. This does not require an interface endpoint (PrivateLink) because Route53 is inside the AWS backbone.",
    "isCorrect" : "false"
  }, {
    "id" : "db3b22f19ef94c359da69aeb0cf01b3e",
    "option" : "AWS ELBs, AWS IoT, and AWS Global Accelerator which provisions one anycast static IP address.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAWS Global Accelerator allocates two IPv4 anycast static IP addresses and keeps the traffic inside the AWS globally redundant network.\nOption A is incorrect because there is no interface endpoint for Amazon Route53.\nOption C is incorrect because Amazon Route53 is not inside the AWS backbone.\nOption D is incorrect because AWS Global Accelerator provides two anycast static IP addresses.\nReferences:\nhttps://go.aws/2z24ZLL\n\nSure, I'd be happy to explain each of the options and help you understand which one would be the best fit for the given scenario.\nOption A: This option includes AWS ELBs, AWS IoT, and Amazon Route53 configured with geolocation or latency routing policies. It requires an interface endpoint (PrivateLink) for Route53 to stay inside the AWS backbone. This option would allow you to avoid DNS re-configuration and IP address caching when it comes to the IoT devices. The use of an interface endpoint ensures that Route53 traffic stays within the AWS network, reducing the risk of data being intercepted while in transit. Additionally, AWS IoT is designed to be HIPAA compliant and includes in-transit encryption. This option would likely be a good fit for the given scenario.\nOption B: This option includes AWS ELBs, AWS IoT, and AWS Global Accelerator, which provisions two anycast static IP addresses. While this option also includes AWS IoT, which is HIPAA compliant and includes in-transit encryption, it may not be the best fit for the given scenario because it does not address the requirement to bypass the internet. Additionally, the use of anycast IP addresses can lead to unpredictable routing behavior, which may negatively impact the performance of the IoT devices.\nOption C: This option is similar to option A, including AWS ELBs, AWS IoT, and Amazon Route53 configured with geolocation or latency routing policies. However, it does not require an interface endpoint (PrivateLink) because Route53 is already inside the AWS backbone. This option would also allow you to avoid DNS re-configuration and IP address caching when it comes to the IoT devices. Like option A, this option is likely a good fit for the given scenario.\nOption D: This option includes AWS ELBs, AWS IoT, and AWS Global Accelerator, which provisions one anycast static IP address. Like option B, this option may not be the best fit for the given scenario because it does not address the requirement to bypass the internet. Additionally, the use of anycast IP addresses can lead to unpredictable routing behavior, which may negatively impact the performance of the IoT devices.\nIn summary, the best option for the given scenario would likely be either option A or option C, as they both include AWS ELBs, AWS IoT, and Amazon Route53 configured with geolocation or latency routing policies, which would allow you to avoid DNS re-configuration and IP address caching when it comes to the IoT devices. Additionally, both options address the requirement to bypass the internet and ensure HIPAA compliance with in-transit encryption.\n\n"
}, {
  "id" : 502,
  "question" : "You are a consultant for a company developing a complex machine learning project relying on on-premises HPC frameworks with a plan to migrate to Amazon Web Services so they can leverage elastic and scalable cloud infrastructure and fast networking while keeping direct control over their computing infrastructure.\nWhich statement is correct in this sense?\n",
  "answers" : [ {
    "id" : "16d588b0fa784118bfeef52a15874013",
    "option" : "Think of combining Amazon SageMaker and Amazon FSx file system.",
    "isCorrect" : "false"
  }, {
    "id" : "c9e2c4dbe3f843e194d4bd4552332b79",
    "option" : "Think of combining AWS ParallelCluster and Amazon FSx file system.",
    "isCorrect" : "false"
  }, {
    "id" : "87495dd49ef542c18228d0694f35c153",
    "option" : "A customised setting of an HPC cluster running SageMaker algorithms.",
    "isCorrect" : "false"
  }, {
    "id" : "7f906399f1534b7d9556593bfd19ced7",
    "option" : "All HPC solutions on AWS are offered as a managed service to run intensive workloads.",
    "isCorrect" : "false"
  }, {
    "id" : "294f88bbe0da44af8a1ab836ef7607a5",
    "option" : "SageMaker is a self-service solution to run distributed machine learning (ML) workloads.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nThe correct answer for this question is option B: Think of combining AWS ParallelCluster and Amazon FSx file system.\nExplanation:\nOption A suggests combining Amazon SageMaker and Amazon FSx file system. While Amazon SageMaker is a powerful machine learning platform that provides a managed environment for building, training, and deploying machine learning models, Amazon FSx is a fully-managed file system that is optimized for enterprise HPC workloads. While this combination may be useful for certain use cases, it may not be the most appropriate for this scenario where the company wants to migrate its on-premises HPC framework to AWS while maintaining direct control over their computing infrastructure.\nOption B suggests combining AWS ParallelCluster and Amazon FSx file system. AWS ParallelCluster is an AWS-supported open source cluster management tool that makes it easy for users to deploy and manage HPC clusters on AWS. It provides a simple interface for launching HPC clusters that are customized for specific applications, and it supports a range of popular job schedulers, including Slurm and Torque. When combined with Amazon FSx, users can run their HPC workloads on a file system that is optimized for high performance and low latency.\nOption C suggests a customized setting of an HPC cluster running SageMaker algorithms. While this may be possible, it is not the most appropriate solution for this scenario where the company wants to maintain direct control over their computing infrastructure.\nOption D is incorrect as it suggests that all HPC solutions on AWS are offered as a managed service. While AWS does offer managed HPC services, such as AWS ParallelCluster and Amazon FSx, users can also deploy and manage their own HPC clusters on EC2 instances using open source tools like Slurm and Torque.\nOption E is incorrect as it suggests that SageMaker is a self-service solution to run distributed machine learning (ML) workloads. While SageMaker does provide a managed environment for building, training, and deploying machine learning models, it is not specifically designed for running distributed ML workloads. AWS provides a range of other services, such as Amazon EMR and AWS Batch, that are designed specifically for running distributed workloads on AWS.\n\n"
}, {
  "id" : 503,
  "question" : "The fraud detection department in a financial analytics company using Amazon Web Services with recommended configurations needs to transfer data from their POSIX-compliant file system (Lustre) to an Amazon S3 bucket.\nIn this context, which statement is correct?\n",
  "answers" : [ {
    "id" : "51b7eb70efe74e07b8aa0c8bf22c1f74",
    "option" : "AWS DataSync is natively integrated with Lustre file system.",
    "isCorrect" : "false"
  }, {
    "id" : "38e99952900a4636b8fe6b84e289a8c4",
    "option" : "Create a DataSync task with the EFS file system as the source location and the S3 bucket as the destination location.",
    "isCorrect" : "false"
  }, {
    "id" : "83c991fb1c844c80996a3c7608aab0c9",
    "option" : "Amazon FSx for Lustre integrates natively with Amazon S3.",
    "isCorrect" : "true"
  }, {
    "id" : "35e3336f4eec49eab90a2ff68874f80b",
    "option" : "Amazon FSx for Windows File Server takes highly durable backups stored in S3.",
    "isCorrect" : "false"
  }, {
    "id" : "7132b9c3062d430b97c2c9cf5dba51a8",
    "option" : "If you link your Amazon FSx for Lustre to an Amazon S3 data lake, your content will appear as objects as soon as the attached block-storage is available.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nThere is native integration of Amazon FSx for Lustre with Amazon S3.\nOption A is incorrect because AWS DataSync does not support Amazon FSx for Lustre file system.\nOption B is incorrect because AWS DataSync only supports Amazon FSx for Windows File Server file system, which is not a POSIX-compliant file system as required.\nOption D is incorrect because although the statement is correct, it does not apply to this scenario.\nOption E is incorrect because when linking Amazon FSx file systems to Amazon S3, the content will appear as files and directories instead of objects.\nReferences:\nhttps://go.aws/3bnBEtx\nhttps://go.aws/34L8Etn\n\nThe correct answer in this case is B. Create a DataSync task with the EFS file system as the source location and the S3 bucket as the destination location.\nExplanation:\nThe requirement is to transfer data from a POSIX-compliant file system (Lustre) to an Amazon S3 bucket. Lustre is a high-performance, parallel file system used in high-performance computing environments. Amazon S3 is a highly scalable and durable object storage service offered by AWS.\nOption A: AWS DataSync is natively integrated with Lustre file system. This statement is incorrect because there is no native integration between AWS DataSync and Lustre file system.\nOption B: Create a DataSync task with the EFS file system as the source location and the S3 bucket as the destination location. This statement is correct because AWS DataSync is a service that simplifies and automates data transfer between on-premises storage and AWS storage services. It can be used to transfer data from an NFS or EFS (Elastic File System) file system to an S3 bucket. In this case, we can create a DataSync task with EFS file system as the source location and S3 bucket as the destination location to transfer data from the Lustre file system to the S3 bucket.\nOption C: Amazon FSx for Lustre integrates natively with Amazon S3. This statement is incorrect because Amazon FSx for Lustre is a fully managed file system that is optimized for compute-intensive workloads, while Amazon S3 is an object storage service. Although Amazon FSx for Lustre can integrate with Amazon S3, it is not a native integration.\nOption D: Amazon FSx for Windows File Server takes highly durable backups stored in S3. This statement is incorrect because Amazon FSx for Windows File Server is a fully managed file storage service that is optimized for Windows-based workloads, while the requirement is to transfer data from a POSIX-compliant file system (Lustre) to an Amazon S3 bucket.\nOption E: If you link your Amazon FSx for Lustre to an Amazon S3 data lake, your content will appear as objects as soon as the attached block-storage is available. This statement is incorrect because it is not possible to link Amazon FSx for Lustre to an Amazon S3 data lake. Although Amazon FSx for Lustre can integrate with Amazon S3, it is not a native integration. Additionally, the statement does not address the requirement to transfer data from a Lustre file system to an S3 bucket.\n\n"
}, {
  "id" : 504,
  "question" : "As a Solutions Architect, you are working with system and networking teams.\nYou are assigned to provide disaster recovery (DR) and high-availability elements for a security planning document involving FSx for Windows.\nWhat assertions are true about Amazon FSx for Windows File Server? (Select TWO)\n",
  "answers" : [ {
    "id" : "c184c049903c4e808f012b22d2c5901e",
    "option" : "Amazon FSx for Windows File Server offers instant regional failover, fault-isolating design, and automatic traffic routing across multiple applications, multiple VPCs, accounts, or Regions.",
    "isCorrect" : "false"
  }, {
    "id" : "ad271c43fafb42a3ada1a0b980d71033",
    "option" : "Amazon FSx for Windows File Server allows to access file systems from multiple Amazon Virtual Private Clouds (VPCs), AWS accounts, and AWS Regions via VPC Peering or AWS Transit Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "83c975baab0d4d288ca9b57e424f11a8",
    "option" : "Amazon FSx for Windows File Server offers single-AZ and multi-AZ deployment options with SSD and HDD storage options.",
    "isCorrect" : "true"
  }, {
    "id" : "2d104ebec95145d8b35f562a980447be",
    "option" : "Direct Connect, VPN, VPC Peering, and AWS Transit Gateway are not supported.",
    "isCorrect" : "false"
  }, {
    "id" : "a5aa7e5554f344f69dd6b54deff801cc",
    "option" : "Amazon FSx for Windows File Server is a fully POSIX-compliant filesystem.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: B and C.\nAs of 2019, Amazon FSx for Windows File Server supports access across VPCs, accounts, and Regions via Direct Connect or VPN (on-premises) and VPC Peering or AWS Transit Gateway.\nYou can share your file data sets across multiple applications, internal organizations, or environments spanning multiple VPCs, accounts, or Regions using inter-VPC, inter-account, and inter-Region access.\nSingle-AZ files ensure high availability within a single Availability Zone (AZ) by automatically detecting and addressing component failures.\nIn addition, Multi-AZ file systems provide high availability and failover support across multiple Availability Zones by provisioning and maintaining a standby file server in a separate Availability Zone within an AWS Region.\nOption A is incorrect because those benefits are part of the AWS Global Accelerator networking service.\nOption D is incorrect because Direct Connect or VPN (on-premises) and VPC Peering or AWS Transit Gateway are supported.\nOption E is incorrect because Amazon FSx for Windows File Server is not POSIX-compliant -Amazon FSx for Luster is POSIX-compliant.\nReferences:\nhttps://aws.amazon.com/about-aws/whats-new/2019/02/amazon-fsx-for-windows-file-server-now-supports-on-premises-access/\nhttps://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html\n\nAmazon FSx for Windows File Server is a fully managed native Microsoft Windows file system that can be accessed over the Server Message Block (SMB) protocol. It provides scalable and highly available storage capacity that is compatible with Windows file services. In addition, it is integrated with other AWS services and can be used in hybrid environments.\nRegarding disaster recovery and high availability, the following assertions are true:\nA. Amazon FSx for Windows File Server offers instant regional failover, fault-isolating design, and automatic traffic routing across multiple applications, multiple VPCs, accounts, or Regions.\nThis assertion is true. Amazon FSx for Windows File Server provides built-in disaster recovery capabilities, allowing for instant regional failover in the event of an outage. It uses a fault-isolating design that allows for automatic traffic routing across multiple applications, multiple VPCs, accounts, or Regions. This ensures that the system is highly available and can continue to operate in the event of a failure.\nB. Amazon FSx for Windows File Server allows to access file systems from multiple Amazon Virtual Private Clouds (VPCs), AWS accounts, and AWS Regions via VPC Peering or AWS Transit Gateway.\nThis assertion is also true. Amazon FSx for Windows File Server can be accessed from multiple Amazon Virtual Private Clouds (VPCs), AWS accounts, and AWS Regions via VPC Peering or AWS Transit Gateway. This allows for greater flexibility in designing a disaster recovery plan and increases the availability of the file system.\nC. Amazon FSx for Windows File Server offers single-AZ and multi-AZ deployment options with SSD and HDD storage options.\nThis assertion is false. While Amazon FSx for Windows File Server offers single-AZ and multi-AZ deployment options, it does not offer HDD storage options. It only supports Solid State Drive (SSD) storage.\nD. Direct Connect, VPN, VPC Peering, and AWS Transit Gateway are not supported.\nThis assertion is false. Amazon FSx for Windows File Server supports Direct Connect, VPN, VPC Peering, and AWS Transit Gateway. These options allow for greater flexibility in designing a disaster recovery plan and accessing the file system from multiple locations.\nE. Amazon FSx for Windows File Server is a fully POSIX-compliant filesystem.\nThis assertion is false. Amazon FSx for Windows File Server is not a fully POSIX-compliant filesystem. It is a native Microsoft Windows file system that supports SMB protocol for file access. However, it is fully compatible with Windows file services and applications.\n\n"
}, {
  "id" : 505,
  "question" : "While analyzing billions of web pages in a company, you have noticed some munging processes are exceeding SLAs even after using X1e instances suitable for HPC applications.\nAfter monitoring logs, trailing, and tracing data closely, you noticed that write operations involving S3 content pre-processing causing 80% of the bottleneck.\nIn comparison, read operations for post-processing and LIST operations are together leading to the remaining 20% of congestion.\nWhich two options are recommended to increase performance in this scenario? (Select TWO)\n",
  "answers" : [ {
    "id" : "b9d787f2ea4e4d70aa1182de359f59e2",
    "option" : "Migrate S3 files to an RDS database with write-optimized IOPS.",
    "isCorrect" : "false"
  }, {
    "id" : "349c55367a424030bd1e7dd0aad9059a",
    "option" : "Using Amazon S3 Transfer Acceleration, Multipart upload, parallelized reading via byte-range fetches, and partitioned prefix for distributing key names as part of naming patterns.",
    "isCorrect" : "true"
  }, {
    "id" : "a27a9302f4b647c995ba6d82a009d035",
    "option" : "Instead of LIST operations, you can scale storage connections horizontally since Amazon S3 is a very large distributed system similar to a decoupled parallel, single network endpoint. You can achieve the best performance by issuing multiple concurrent requests to Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "3af8555607104981b3910139aa8e6dec",
    "option" : "Instead of LIST operations, you can build a search catalog to keep track of S3 metadata by using other AWS services like Amazon DynamoDB or Amazon OpenSearch Service.",
    "isCorrect" : "true"
  }, {
    "id" : "9b315317434645bc9725ab4819f604f0",
    "option" : "Combining Amazon S3 and Amazon EC2 instances by migrating your current buckets to the region where the instances are available. This will help to reduce network latency and data transfer costs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: B and D.\nThe problem description highlights write operations having 80% of the impact when compared to read operations.\nAmazon S3 Transfer Acceleration helps especially in this scenario along with multipart upload.\nX1 and X1e instances are memory-optimized instances designed for running large-scale and in-memory applications in the AWS Cloud.\nGenerally, it is a recommended practice to benefit from byte-range fetches and appropriate distributing key names.\nAnother complex practice to avoid expensive LIST operations relies on a search framework to keep track of all objects in an S3 bucket.\nLambda triggers are used to populate DynamoDB tables with object names and metadata when those objects are put into Amazon S3 then OpenSearch Service is used to search for specific assets, related metadata, and data classifications.\nOption A is incorrect because this option is not applicable as you would need to migrate the data to a relational database in RDS.\nOption C is incorrect because the phrase does not make sense, especially considering that the Amazon S3 service is exactly the opposite of a single network endpoint.\nOption E is incorrect because you cannot migrate an existing S3 bucket into another AWS Region, rather create a new S3 bucket in the region and copy the data to it.\nHowever, the statement does not solve the problem description.\nReferences:\nhttps://aws.amazon.com/s3/transfer-acceleration/\nhttps://docs.aws.amazon.com/whitepapers/latest/s3-optimizing-performance-best-practices/use-byte-range-fetches.html\nhttps://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/data-cataloging.html\n\n\nThe scenario described in this question involves analyzing billions of web pages in a company, and the munging processes are exceeding SLAs even after using X1e instances suitable for HPC applications. Upon further analysis, it was found that the write operations involving S3 content pre-processing are causing 80% of the bottleneck, and the remaining 20% of congestion is caused by read operations for post-processing and LIST operations.\nTo increase performance in this scenario, the two recommended options are:\nOption B: Using Amazon S3 Transfer Acceleration, Multipart upload, parallelized reading via byte-range fetches, and partitioned prefix for distributing key names as part of naming patterns.\nAmazon S3 Transfer Acceleration is a feature that enables faster transfer of files over long distances between the client and an S3 bucket. It utilizes Amazon CloudFront's globally distributed edge locations to accelerate the transfer of files over the internet. By using this feature, you can accelerate both upload and download speeds, which will help to reduce the overall processing time.\nMultipart upload allows you to upload large files in parts and upload them concurrently, thereby increasing the speed of uploads. This is especially useful when you are dealing with very large files, and the network bandwidth is a bottleneck.\nParallelized reading via byte-range fetches involves requesting multiple parts of a file simultaneously, thereby improving the speed of downloads. This technique is useful when you are dealing with large files that need to be processed quickly.\nPartitioned prefix for distributing key names as part of naming patterns involves creating a naming convention for S3 objects that distributes the keys across multiple prefixes, which helps to distribute the load across multiple instances of the application. This technique is useful when you have a large number of objects in an S3 bucket, and you want to improve the performance of read operations.\nOption D: Instead of LIST operations, you can build a search catalog to keep track of S3 metadata by using other AWS services like Amazon DynamoDB or Amazon OpenSearch Service.\nLIST operations can be slow and resource-intensive, especially when you have a large number of objects in an S3 bucket. Instead of using LIST operations, you can build a search catalog to keep track of S3 metadata by using other AWS services like Amazon DynamoDB or Amazon OpenSearch Service. These services can help you to search and retrieve S3 metadata quickly and efficiently, without having to perform a full LIST operation. This technique is useful when you have a large number of objects in an S3 bucket and want to improve the performance of read operations.\nOptions A, C, and E are incorrect:\nOption A: Migrate S3 files to an RDS database with write-optimized IOPS.\nMigrating S3 files to an RDS database with write-optimized IOPS is not a recommended solution in this scenario. RDS is a database service, and it is not designed to handle unstructured data like S3 objects. Additionally, RDS may not be able to handle the volume of data involved in this scenario. This technique is not useful when you are dealing with unstructured data like web pages.\nOption C: Instead of LIST operations, you can scale storage connections horizontally since Amazon S3 is a very large distributed system similar to a decoupled parallel, single network endpoint. You can achieve the best performance by issuing multiple concurrent requests to Amazon S3.\nScaling storage connections horizontally by issuing multiple concurrent requests to Amazon S3 is not a recommended solution in this scenario. While it is true that Amazon S3 is a large distributed system, issuing multiple concurrent requests may not be an efficient way to handle a large number of objects in an S3 bucket. Additionally, this technique may not help to address the bottleneck caused by write operations involving S3 content pre-processing.\nOption E: Combining Amazon S3 and Amazon\n\n"
}, {
  "id" : 506,
  "question" : "As a Solutions Architect, you are dealing with an architecture that supports Amazon EC2 Auto Scaling to handle an application load.\nWhen considering current policies in the design, you have noticed a variety of choices in place.\nWhich statement is NOT true in these cases?\n",
  "answers" : [ {
    "id" : "208c4127af7e43848021303fb7bd9388",
    "option" : "In most cases, step scaling policies are a better choice than simple scaling policies, even if you have only a single scaling adjustment.",
    "isCorrect" : "false"
  }, {
    "id" : "dc1b396618154f209a58734aa40d56f9",
    "option" : "You cannot have multiple target tracking scaling policies for an Auto Scaling group, but you can have multiple scaling policies in force simultaneously.",
    "isCorrect" : "true"
  }, {
    "id" : "de0c7bca25c9480caae0407b15d7b97d",
    "option" : "CloudWatch alarms associated with your target tracking scaling policies are deleted automatically when you delete the scaling policies.",
    "isCorrect" : "false"
  }, {
    "id" : "caf07d8f9ae4427cb37532cbf5b138c7",
    "option" : "The gaps between the target value and the actual metric data points prevents you from adding an insufficient number of instances or removing too many instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nYou can have multiple target tracking scaling policies for an Auto Scaling group, provided that each of them uses a different metric.\nYou can also have multiple scaling policies in force at the same time.\nOption A is incorrect because the statement is true.\nAmazon recommends step scaling policies as a better choice than simple scaling policies.\nOption C is incorrect because the statement is true.\nAmazon suggests not to edit or delete the CloudWatch alarms that are configured for the target tracking scaling policy.\nOption D is incorrect because the statement is true.\nAmazon acts conservatively by rounding up or down when determining how many instances to add or remove.\nReferences:\nhttps://amzn.to/2KbpIz6\nhttps://amzn.to/2xG9Emd\n\nSure, I'd be happy to help! Let me explain each statement in turn:\nA. In most cases, step scaling policies are a better choice than simple scaling policies, even if you have only a single scaling adjustment.\nThis statement is generally false. While step scaling policies can be useful in certain situations, such as when you want to add or remove a specific number of instances based on a threshold, they are not always the best choice. Simple scaling policies, which allow you to increase or decrease the desired capacity of your Auto Scaling group by a certain percentage, are often simpler and more flexible. They can also help you maintain a consistent number of instances regardless of the size of the change in demand. So, this statement is not true in all cases.\nB. You cannot have multiple target tracking scaling policies for an Auto Scaling group, but you can have multiple scaling policies in force simultaneously.\nThis statement is false. In fact, you can have multiple target tracking scaling policies for an Auto Scaling group, each tracking a different metric. For example, you might have one policy that scales based on CPU utilization and another that scales based on network throughput. You can also have multiple scaling policies in force simultaneously, including both target tracking and step scaling policies.\nC. CloudWatch alarms associated with your target tracking scaling policies are deleted automatically when you delete the scaling policies.\nThis statement is true. When you delete a target tracking scaling policy, any associated CloudWatch alarms will also be deleted automatically. This can be useful to keep your CloudWatch Alarms organized and avoid clutter. However, it's important to note that this does not apply to other types of scaling policies, such as step scaling policies.\nD. The gaps between the target value and the actual metric data points prevent you from adding an insufficient number of instances or removing too many instances.\nThis statement is generally true. When using target tracking scaling policies, you set a target value for the metric you want to track, such as CPU utilization or network throughput. The Auto Scaling group then adjusts the number of instances up or down based on the difference between the target value and the actual metric data points. However, there may be cases where the actual metric data points do not align perfectly with the target value, which can result in the Auto Scaling group adding too many or too few instances. To mitigate this, you can set a cooldown period to prevent rapid scaling events and allow the metric data to stabilize before making any further scaling decisions.\nI hope that helps! Let me know if you have any further questions.\n\n"
}, {
  "id" : 507,
  "question" : "You are analyzing dynamic scaling activity defined to process messages from an Amazon SQS queue.\nYou are using a target tracking policy.\nCloudWatch alarms are meant to invoke the scaling policy.\nHowever, you have noticed that the EC2 Auto Scaling group does not seem to be responding to a CloudWatch alarm.\nWhich option may cause it?\n",
  "answers" : [ {
    "id" : "5545bc8f7a1047fe84815aecdb7a3180",
    "option" : "Wrong CloudWatch metric is configured in the CloudWatch alarm.",
    "isCorrect" : "false"
  }, {
    "id" : "29878587423b4e9dbf5d8bc3810f1689",
    "option" : "The Auto Scaling group is under the cooldown period.",
    "isCorrect" : "true"
  }, {
    "id" : "355f3866220e4a7f90e2e812f6d51aae",
    "option" : "The minimum number of instances in the ASG is 0.",
    "isCorrect" : "false"
  }, {
    "id" : "39a7800c36a149e0be0c505526e86680",
    "option" : "The desired number of instances in the ASG is 0.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nThe cooldown period is the amount of time to wait for a previous scaling activity to take effect is called the cooldown period.\nOption A is incorrect because the CloudWatch alarm is already triggered so wrong CloudWatch metric does not cause the problem.\nOption B is CORRECT because under the cooldown period, scaling activities will not be triggered.\nOption C is incorrect because this option will not stop the ASG from adding/deleting instances.\nOption D is incorrect because with the option, the ASG can still add/delete instances by adjusting the desired number of instances.\nReferences:\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/application-auto-scaling-target-tracking.html\n\nSure, I can provide you with a detailed explanation of the answer options for this question.\nThe scenario given in the question involves an Auto Scaling group (ASG) that is not responding to a CloudWatch alarm when using a target tracking policy to process messages from an Amazon SQS queue. In this scenario, there are several possible reasons why the ASG may not be responding to the CloudWatch alarm.\nOption A: Wrong CloudWatch metric is configured in the CloudWatch alarm This option suggests that the CloudWatch alarm may be using an incorrect metric, which may cause the ASG not to respond to the alarm. If the metric is not correctly configured, then the alarm will not trigger the scaling policy to add or remove instances from the ASG. Therefore, this option could be a possible reason why the ASG is not responding to the CloudWatch alarm.\nOption B: The Auto Scaling group is under the cooldown period The cooldown period is a configurable setting that can be applied to an ASG. During this period, the ASG is temporarily prevented from scaling in or out in response to a scaling event. The cooldown period is used to prevent the ASG from overreacting to a temporary burst in traffic, and it ensures that the ASG has enough time to stabilize after scaling activities. If the ASG is currently under a cooldown period, it may not respond to the CloudWatch alarm. Therefore, this option could also be a possible reason why the ASG is not responding to the CloudWatch alarm.\nOption C: The minimum number of instances in the ASG is 0 The minimum number of instances in an ASG is a configurable setting that defines the minimum number of instances that the ASG should always maintain. If the minimum number of instances is set to 0, the ASG will not maintain any instances, even if there is an increase in traffic. Therefore, if the minimum number of instances in the ASG is set to 0, this could be a possible reason why the ASG is not responding to the CloudWatch alarm.\nOption D: The desired number of instances in the ASG is 0 The desired number of instances in an ASG is a configurable setting that defines the desired number of instances that the ASG should maintain. If the desired number of instances is set to 0, the ASG will not maintain any instances, even if there is an increase in traffic. Therefore, if the desired number of instances in the ASG is set to 0, this could be a possible reason why the ASG is not responding to the CloudWatch alarm.\nIn summary, all of the given answer options could potentially be the reason why the ASG is not responding to the CloudWatch alarm. However, without further information about the specific configuration of the ASG and CloudWatch alarm, it is difficult to determine which option is the correct answer.\n\n"
}, {
  "id" : 508,
  "question" : "A project you are part of as a Solutions Architect has a latency-sensitive workload requirement despite availability.\nYou have to consider building high-performance networking for their VPCs to operate based on SLAs already signed as part of the contract.\nWhich two statements in this context are correct? (Select TWO)\n",
  "answers" : [ {
    "id" : "7b54e2ff859148f59ee6e12b72acd050",
    "option" : "A cluster placement group is appropriate for this use case as long as the grouping of instances stays within a single Availability Zone unless using peered VPCs to span multiple regions.",
    "isCorrect" : "false"
  }, {
    "id" : "3b7adbde75bb46cfa5cb2190a22fec01",
    "option" : "A cluster placement group is appropriate for this use case because of a higher per-flow throughput limit of up to 10 Gbps except for the traffic over an AWS Direct Connect connection to on-premises resources.",
    "isCorrect" : "true"
  }, {
    "id" : "489e159a39ce408896f6ff52c1553ac8",
    "option" : "You can migrate an instance from one placement group to another but cannot merge placement groups.",
    "isCorrect" : "true"
  }, {
    "id" : "4746967f2c8a4e479990dc34808e4221",
    "option" : "You can migrate an instance from one placement group to another and merge placement groups.",
    "isCorrect" : "false"
  }, {
    "id" : "dda5c3660b104e24a774d4d038000083",
    "option" : "A spread placement group is appropriate for this use case but if you start or launch an instance in the group and there is insufficient unique hardware, the request fails and you have to try again.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: B, C.\nA cluster placement group is the only one providing high-performance networking.\nIt is also possible to migrate instances between placement groups, but merging placement groups is not supported.\nOption A is incorrect because cluster placement groups cannot span multiple regions.\nOption D is incorrect because you cannot merge placement groups.\nOption E is incorrect because a spread placement group is more appropriate for availability scenarios.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html\n\n\nThe correct statements in this context are A and B.\nA. A cluster placement group is appropriate for this use case as long as the grouping of instances stays within a single Availability Zone unless using peered VPCs to span multiple regions.\nExplanation: A cluster placement group is a logical grouping of instances within a single Availability Zone that provides low-latency network performance by ensuring that the instances are located on the same underlying hardware. It is appropriate for this use case as it ensures that the instances are in close proximity to each other, which can help reduce network latency. However, if using peered VPCs to span multiple regions, then the instances in the placement group must be in different Availability Zones.\nB. A cluster placement group is appropriate for this use case because of a higher per-flow throughput limit of up to 10 Gbps except for the traffic over an AWS Direct Connect connection to on-premises resources.\nExplanation: A cluster placement group has a higher per-flow throughput limit of up to 10 Gbps compared to other placement group types. This is because the instances in a cluster placement group are located on the same hardware and can communicate with each other at very low latencies. This makes it appropriate for a latency-sensitive workload requirement. However, this limit does not apply to traffic over an AWS Direct Connect connection to on-premises resources.\nC. You can migrate an instance from one placement group to another but cannot merge placement groups.\nExplanation: Instances can be migrated from one placement group to another using the AWS Management Console, AWS CLI, or AWS SDKs. However, it is not possible to merge placement groups. Therefore, if you need to combine instances from multiple placement groups, you would need to launch new instances in a new placement group and migrate your application to those instances.\nD. You can migrate an instance from one placement group to another and merge placement groups.\nExplanation: This statement is incorrect. As explained in the previous explanation, it is not possible to merge placement groups. Instances can be migrated from one placement group to another using the AWS Management Console, AWS CLI, or AWS SDKs.\nE. A spread placement group is appropriate for this use case but if you start or launch an instance in the group and there is insufficient unique hardware, the request fails and you have to try again.\nExplanation: A spread placement group is a logical grouping of instances that are spread across distinct underlying hardware to reduce the risk of simultaneous failure. While it can be appropriate for certain use cases, it is not the best choice for a latency-sensitive workload requirement as it cannot guarantee low latency between instances. Additionally, if you start or launch an instance in a spread placement group and there is insufficient unique hardware, the request fails and you have to try again.\n\n"
}, {
  "id" : 509,
  "question" : "You are working as a Solutions Architect in a university where you take part in a migration to the cloud.\nTheir HPC Clusters are meant to leverage research processes relying on MPI applications in Biology and Physics departments.\nThere are some decisions to make to manage the cluster correctly.\nWhat option is the most appropriate to set up the environment?\n",
  "answers" : [ {
    "id" : "27e45b85106d496a9efe64feb078552a",
    "option" : "AWS ParallelCluster is a fully managed cluster. EFA support is enabled by default but can be disabled dynamically, on-demand on any EC2 instance.",
    "isCorrect" : "false"
  }, {
    "id" : "3f87ee00e6e24faf8c5225bca3127518",
    "option" : "To use EFA, the primary ENI has to be detached, EFA enabled, and reattach the ENI either at the launch of the instance or added to a stopped instance. This will get OS-bypass capabilities for low-latency network communications with other instances on the same subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "8fc97777ffeb4800992bff34358beada",
    "option" : "You are responsible for operating ParallelCluster, including required maintenance on EC2 instances and batch schedulers, security patching, user management, and MPI troubleshooting. EFA support is enabled by default but can be disabled dynamically, on-demand on any EC2 instance.",
    "isCorrect" : "false"
  }, {
    "id" : "ae728b673dc04ae68bdb2a8f70f06352",
    "option" : "To use EFA, it has to be enabled with AWS ParallelCluster to get OS-bypass capabilities for low-latency network communications with other instances on the same subnet. This support can be enabled in specific instance types either at the launch of the instance or added to a stopped instance.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nThis option guarantees one of the fastest and scalable settings in HPC Clusters.\nAWS ParallelCluster supports EFA, which can get OS-bypass capabilities (kernel-bypass networking), which is possible only in specific instance types and limited to a single subnet.\nAlso, you cannot attach an EFA to an instance that is in the running state.\nOption A is incorrect because AWS ParallelCluster is a self-service solution.\nOption B is incorrect because you cannot detach the primary ENI.\nOption C is incorrect because EFA support is not enabled by default and is not supported in any EC2 instance type.\nReferences:\nhttps://go.aws/2ys7erk\nhttps://amzn.to/3exVVic\n\nIn this scenario, the university is planning to migrate their HPC clusters to the cloud, which will be used for research purposes in Biology and Physics departments. MPI applications will be heavily used for research workloads. Therefore, it is crucial to have an environment that can manage HPC clusters, MPI applications, and low-latency network communications.\nOption A states that AWS ParallelCluster is a fully managed cluster with Elastic Fabric Adapter (EFA) support enabled by default. This option means that the AWS ParallelCluster service will handle the management of the cluster, including necessary maintenance on EC2 instances and batch schedulers, security patching, user management, and MPI troubleshooting. Additionally, EFA support is enabled by default but can be disabled dynamically, on-demand, on any EC2 instance. EFA is an Amazon network interface that provides low-latency and high-bandwidth network communication among instances, which is essential for MPI applications. Hence, this option is the most appropriate for setting up the HPC cluster environment since it provides a fully managed service with EFA support by default.\nOption B mentions that to use EFA, the primary Elastic Network Interface (ENI) has to be detached, EFA enabled, and reattach the ENI either at the launch of the instance or added to a stopped instance. This will get OS-bypass capabilities for low-latency network communications with other instances on the same subnet. While this option might work for small scale MPI applications, it is not practical for large scale applications, and manually detaching and reattaching the ENI could be time-consuming and prone to human error.\nOption C states that the responsibility of operating ParallelCluster, including necessary maintenance on EC2 instances and batch schedulers, security patching, user management, and MPI troubleshooting, falls on the user. Although EFA support is enabled by default, the user has to disable it dynamically, on-demand, on any EC2 instance. This option can be suitable for users who are familiar with managing HPC clusters and MPI applications. Still, it is not a viable option for users who want a fully managed HPC cluster environment.\nOption D suggests that EFA support has to be enabled with AWS ParallelCluster to get OS-bypass capabilities for low-latency network communications with other instances on the same subnet. This support can be enabled in specific instance types either at the launch of the instance or added to a stopped instance. While this option provides EFA support, it does not offer a fully managed cluster service like AWS ParallelCluster does, and users have to take responsibility for managing the environment.\nIn conclusion, option A is the most appropriate option for setting up the environment for the university's HPC clusters since it provides a fully managed service with EFA support enabled by default.\n\n"
}, {
  "id" : 510,
  "question" : "During troubleshooting processes evaluating how to benefit from enhanced networking in your HPC instances, you find out that the setting does not seem functioning.\nYou have VPC peering connection configurations in place.\nSome of your instances support EFA.\nYou are using AWS CLI, and you have checked that all instances have internet connectivity.\nWhich relevant statement is correct in this scenario?\n",
  "answers" : [ {
    "id" : "3aadbbd900d94a37b3497e07ee831c81",
    "option" : "All instance types are supported for using the EN.",
    "isCorrect" : "false"
  }, {
    "id" : "f4a3dde490b843338123bc833625ea74",
    "option" : "Make sure you are using a supported version of the Linux kernel and a supported distribution, you run ethtool -i eth0 and you have to get ENA as the driver.",
    "isCorrect" : "true"
  }, {
    "id" : "86b06d8dd1714ddba2038865b6e5a8e9",
    "option" : "Some instance types are supported for using the EN.",
    "isCorrect" : "false"
  }, {
    "id" : "604bed8e0662432b908754e6c6fb838e",
    "option" : "Make sure you are using a supported version of the Linux kernel and a supported distribution, you run ethtool -i eth0 and you have to get ENA as the driver.",
    "isCorrect" : "true"
  }, {
    "id" : "79e2f0765f4543d8a50303129406b49c",
    "option" : "Neither Enhanced networking uses SR-IOV nor transitive peering is supported.",
    "isCorrect" : "false"
  }, {
    "id" : "501015667ac745b4918b215947498142",
    "option" : "Enhanced networking uses SR-IOV and transitive peering is supported.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nENA is a custom network interface optimized to deliver high throughput on specific supported EC2 instances.\nThis statement defines the correct steps to enable an ENA interface and validate the driver is in place.\nOption A is incorrect because not all instance types are supported for using the ENA interface.\nOption C is incorrect because enhanced networking uses SR-IOV, and transitive VPC peering is not supported.\nOption D is incorrect because although enhanced networking does use SR-IOV, transitive VPC peering is not relevant and not supported either.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking.html\n\nEnhanced networking (EN) is a feature that allows for higher packet per second (PPS) performance, lower network jitter, and lower latency for Amazon Elastic Compute Cloud (EC2) instances. EN can be achieved through two different technologies: Single Root I/O Virtualization (SR-IOV) and Elastic Fabric Adapter (EFA).\nIn this scenario, the user is troubleshooting why EN is not functioning on their EC2 instances that support EFA, while VPC peering connection configurations are in place, and all instances have internet connectivity.\nOption A is incorrect because not all instance types are supported for using EN. For example, t2, t3, and micro instances do not support EN. Therefore, some instance types may support EN, while others may not.\nOption B is the correct answer in this scenario. To use EN, the instances must have a supported version of the Linux kernel and a supported distribution. The user can check the driver version by running ethtool -i eth0, and the driver should be ENA (Elastic Network Adapter). If the driver is not ENA, then the user needs to install the latest version of the driver.\nOption E is incorrect because EN does use SR-IOV, and option F is incorrect because transitive peering is not supported for VPC peering connections.\nIn summary, to troubleshoot EN not functioning on EC2 instances that support EFA, the user should ensure that the instances have a supported version of the Linux kernel and a supported distribution, and that the driver version is ENA.\n\n"
}, {
  "id" : 511,
  "question" : "A web application uses Amazon CloudFront to deliver its static and dynamic web content.\nYou want to customize the error page returned to the viewer when there is a problem with the origin server.\nFor example, if the origin server returns a 500 Internal Server Error, CloudFront should present a page that you have prepared.\nHow would you achieve this requirement in CloudFront?\n",
  "answers" : [ {
    "id" : "9446dde69dfe4a3189a1520a27d23a91",
    "option" : "Modify the application to store custom error pages. CloudFront can cache these error pages automatically.",
    "isCorrect" : "false"
  }, {
    "id" : "0a40485db550408a85bf9e98a3beede2",
    "option" : "Create a new CloudFront distribution to fetch error pages. Configure the original CloudFront to use the new one as its custom error responses.",
    "isCorrect" : "false"
  }, {
    "id" : "4d1f2b587b7a4e18b7f86ae38a93f51f",
    "option" : "Put the static error pages in an S3 bucket. Create custom error responses for the HTTP 5xx status code in the CloudFront distribution.",
    "isCorrect" : "true"
  }, {
    "id" : "ae258566d2b442399d256d9bd25ef0ac",
    "option" : "Upload custom error pages to the CloudFront distribution. Return the error pages when there is a server error.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect because the custom error pages should be configured through CloudFront instead of the application.\nOption B is incorrect because there is no need to create a new CloudFront distribution.\nOption C is CORRECT because the error pages can be customized through CloudFront.\nThe following is an example.\nOption D is incorrect because this is not how custom error responses are configured in CloudFront.\nYou cannot upload custom error pages to CloudFront directly.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/custom-error-pages.html.\n\n\nThe correct answer is option C.\nTo customize error pages in Amazon CloudFront, you need to perform the following steps:\nStore error pages in an S3 bucket: You need to upload your custom error pages to an S3 bucket. These error pages can be HTML, images, or any other static content. Create a CloudFront distribution: After you have uploaded the error pages, you need to create a CloudFront distribution. In the distribution, you can specify the S3 bucket where your custom error pages are stored. Configure custom error responses: In the CloudFront distribution, you can configure custom error responses for HTTP status codes 4xx and 5xx. For example, if there is a 404 Not Found error, CloudFront will present the custom error page you have specified for this error. Verify your configuration: After you have configured custom error responses, you should verify your configuration by testing it. You can simulate an error by requesting a non-existent object from your CloudFront distribution. If everything is configured correctly, CloudFront should present the custom error page you have specified for the HTTP status code.\nOption A is incorrect because modifying the application to store custom error pages will not work. The reason is that the application is responsible for serving content to CloudFront, and CloudFront cannot modify or replace the content that the origin server returns.\nOption B is incorrect because creating a new CloudFront distribution is unnecessary. Instead, you can configure the existing CloudFront distribution to use the S3 bucket as the origin for custom error pages.\nOption D is incorrect because uploading custom error pages to the CloudFront distribution will not work. The reason is that CloudFront is a content delivery network and does not allow you to modify or replace the content that the origin server returns.\n\n"
}, {
  "id" : 512,
  "question" : "A CloudFront distribution delivers the web content of an application.\nSome static files are cached in the CloudFront distribution, and the TTL is set to 1 day.\nYou upgrade the origin server, and a configuration JSON file is updated.\nHowever, when users access the website, the old file cached in CloudFront is returned, and some services are impacted.\nHow would you resolve this problem?\n",
  "answers" : [ {
    "id" : "e7db94d394ae4556b5d8e1ba476c4194",
    "option" : "Wait for a day and the file will be updated automatically.",
    "isCorrect" : "false"
  }, {
    "id" : "bdbbd367981046fd989a498cd7339a70",
    "option" : "Invalidate the object in CloudFront so that the object is removed from the CloudFront edge cache.",
    "isCorrect" : "true"
  }, {
    "id" : "6f76477c0ee1429ba9cc11f322afb124",
    "option" : "Modify the default TTL to be 0 in the CloudFront cache setting.",
    "isCorrect" : "false"
  }, {
    "id" : "1851dd95996b47ecaf08985b0efc0e60",
    "option" : "Upgrade the origin application again and add a cache-control header to inform CloudFront to remove the JSON file from its cache.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect because, with the method, the service will be impacted for a day.\nA better approach is to invalidate the object from the cache.\nOption B is CORRECT because invalidating the object removes it from the CloudFront edge cache to return the correct file to the user.\nThe issue can be resolved immediately with this method.\nOption C is incorrect because the default TTL impacts all objects served by the CloudFront distribution.\nIn this scenario, there is only 1 file that needs to be cleared from the cache.\nOption D is incorrect because the application has to be upgraded again, which is unnecessary.\nOption B is faster and simpler.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html.\n\nThe correct answer to this question is B. Invalidate the object in CloudFront so that the object is removed from the CloudFront edge cache.\nWhen a CloudFront distribution delivers content, it caches the content at edge locations around the world. This means that users accessing the content from these edge locations will receive the cached content instead of making a request to the origin server each time. The time-to-live (TTL) value determines how long the content will remain cached in the edge location before it is considered stale and a new request is made to the origin server.\nIn this scenario, the origin server has been updated, but the old JSON file is still cached in CloudFront, causing issues for users. To resolve this, we need to remove the old cached file from CloudFront so that the new version can be fetched from the origin server.\nThe process of removing an object from CloudFront's cache is called invalidation. An invalidation request is sent to CloudFront to remove the specified object from all edge locations, which ensures that the next time the object is requested, it will be fetched from the origin server and cached again with the updated version.\nOption A, waiting for a day, is not a practical solution in this scenario because the TTL for the JSON file is set to 1 day, so users will continue to experience issues until the cache expires.\nOption C, modifying the default TTL to be 0, would force CloudFront to fetch content from the origin server every time it is requested, which can cause unnecessary load on the origin server and increase latency for users.\nOption D, upgrading the origin application again and adding a cache-control header to inform CloudFront to remove the JSON file from its cache, is not the most efficient solution. It requires an additional upgrade to the origin server, and the cache-control header would only work for subsequent requests, not the current requests that are causing issues.\nIn conclusion, the best solution to resolve the issue of old cached files in CloudFront is to use the invalidation feature to remove the specific object from all edge locations.\n\n"
}, {
  "id" : 513,
  "question" : "You plan to migrate a Classic Load Balancer to an Application Load Balancer or a Network Load Balancer.\nFor the new load balancer listener, you need to define two rules that route requests based on the host name in the host header.\nThen the traffic will be routed to two different target groups accordingly.\nHow would you configure the new load balancer?\n",
  "answers" : [ {
    "id" : "86401ec103574163907a4b145834e332",
    "option" : "Create a Network Load Balancer and select the HTTP protocol in the listener. Configure the host-based routing in the listener.",
    "isCorrect" : "false"
  }, {
    "id" : "df6f14a3b119487fadbdb525fbee1878",
    "option" : "Launch a new Network Load Balancer and choose the TCP protocol in its listener. Route the traffic based on the path.",
    "isCorrect" : "false"
  }, {
    "id" : "f215a5bcb60f4103a8a39420e64644b2",
    "option" : "Set up an Application Load Balancer and configure several rules in the listener to perform path-based routing.",
    "isCorrect" : "false"
  }, {
    "id" : "de25e70f4af446f099641c49a1a2a834",
    "option" : "Use an Application Load Balancer and configure host-based routing in the listener rule.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect because, in this scenario, Application Load Balancer should be chosen as Network Load Balancer that runs at the network layer and does not support host-based routing.\nOption B is incorrect because, with Network Load Balancer, you cannot route the traffic based on the host header.\nOption C is incorrect because path-based routing routes requests based on the URL in the requests.\nHowever, the requests need to be routed based on the host header.\nOption D is CORRECT because you can define rules in the Application Load Balancer listener by adding host conditions.\nThen the requests are forwarded based on the host field of the HTTP header.\nReference:\nhttps://aws.amazon.com/elasticloadbalancing/features/.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html.\n\nTo route requests based on the hostname in the host header to two different target groups, you would need to use an Application Load Balancer or a Network Load Balancer, as the Classic Load Balancer does not support this functionality.\nOption A is not a correct solution as Network Load Balancer does not support host-based routing. It only supports IP-based routing.\nOption B is also incorrect, as it routes traffic based on the path and not on the hostname in the host header.\nOption C is partially correct as an Application Load Balancer can perform path-based routing, but for the given requirement, you need to use host-based routing.\nTherefore, the correct answer is option D, which suggests using an Application Load Balancer and configuring host-based routing in the listener rule. Here are the steps to configure this:\nCreate a new Application Load Balancer. Configure the listener to use the HTTP or HTTPS protocol. Create two target groups, one for each set of targets that you want to route traffic to. Configure the listener rule to route traffic based on the hostname in the host header. To do this, you need to: Choose the \"Host header\" option in the routing configuration. Add the hostnames that you want to route traffic to. Specify the target group that each hostname should route traffic to.\nBy following these steps, the Application Load Balancer will be able to route requests based on the hostname in the host header to the appropriate target group.\n\n"
}, {
  "id" : 514,
  "question" : "Your team uses Amazon ECS to manage containers for several micro-services.\nTo save cost, multiple ECS tasks should run at a single container instance.\nWhen a task is launched, the host port should be dynamically chosen from the container instance's ephemeral port range.\nThe ECS service should select a load balancer that supports dynamic mapping.\nWhich types of load balancers are appropriate?\n",
  "answers" : [ {
    "id" : "966adb709a2b452b9a9d433caa3707d1",
    "option" : "Application Load Balancer or Network Load Balancer.",
    "isCorrect" : "true"
  }, {
    "id" : "b78860bfed2d4347bc06b12ba85351e0",
    "option" : "Application Load Balancer only.",
    "isCorrect" : "false"
  }, {
    "id" : "3b6dbd3a384f4eb2bd84d00f70518559",
    "option" : "Network Load Balancer only.",
    "isCorrect" : "false"
  }, {
    "id" : "77a114f1e31d43d48d523e6c5d0df76e",
    "option" : "Application Load Balancer or Classic Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT because both Application Load Balancer and Network Load Balancer support dynamic mapping.\nYou can configure the ECS service to use the load balancer, and a dynamic port will be selected for each ECS task automatically.\nWith Dynamic mapping, multiple copies of a task can run on the same instance.\nOption B and C are incorrect: Please check the below references.\nOption D is incorrect because Classic Load Balancer does not support dynamic mapping.\nWith Classic Load Balancer, you have to define the port mappings on a container instance statically.\nReference:\nhttps://docs.aws.amazon.com/AmazonECS/latest/userguide/load-balancer-types.html.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/dynamic-port-mapping-ecs/.\n\nThe appropriate load balancers for the given requirements are either Application Load Balancer (ALB) or Network Load Balancer (NLB).\nExplanation:\nALB is a Layer 7 load balancer that can distribute traffic based on application-level content such as HTTP/HTTPS requests, and can dynamically route traffic to container instances based on the content of the request. This makes ALB suitable for containerized applications running in ECS that require high levels of traffic routing flexibility and content-based routing. Additionally, ALB supports dynamic port mapping which allows the ECS service to automatically assign a port to the task from the container instance's ephemeral port range, which meets the requirement of the question.\nNLB, on the other hand, is a Layer 4 load balancer that can distribute traffic based on IP protocol data, such as TCP and UDP, making it suitable for high-volume, low-latency applications. NLB also supports dynamic port mapping, which allows ECS service to automatically assign a port from the ephemeral port range of the container instance. NLB does not support content-based routing like ALB, so it may not be suitable for applications that require advanced routing based on application-level content.\nClassic Load Balancer (CLB) is an older load balancer that does not support dynamic port mapping and content-based routing like ALB. Hence, CLB is not suitable for this scenario.\nTherefore, the appropriate load balancers for the given requirements are either Application Load Balancer or Network Load Balancer, making options A and B the correct answers, while options C and D are incorrect.\n\n"
}, {
  "id" : 515,
  "question" : "You plan to manage API keys in AWS Secrets Manager.\nThe keys need to be automatically rotated to be compliant with the company policy.\nFrom Secrets Manager, applications can get the latest version of the API credentials.\nHow would you implement the rotation of keys?\n",
  "answers" : [ {
    "id" : "d684fd41dc764a69be35ba9708786503",
    "option" : "Use AWS Parameter Store to store and rotate the keys as Secrets Manager does not support it.",
    "isCorrect" : "false"
  }, {
    "id" : "bc90ad5c70bc4d63a08f94718e0598fc",
    "option" : "Directly add multiple keys in Secrets Manager for rotation and the keys will be rotated every year automatically.",
    "isCorrect" : "false"
  }, {
    "id" : "0884e4be03664522b3e94fdbcd0135ff",
    "option" : "Customize the Lambda function that performs the rotation of secrets in Secrets Manager.",
    "isCorrect" : "true"
  }, {
    "id" : "ba8916d05bf94ac7b1c203fbd546afa5",
    "option" : "Create two secrets in Secrets Manager to store two versions of the API credentials. Modify the application to get one of them.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect because Secrets Manager supports the key rotation for database credentials, third-party services, etc.\nOption B is incorrect because Secrets Manager natively knows how to rotate secrets for supported databases such as RDS.\nFor other secret types, such as API keys, users need to customize the Lambda rotation function.\nOption C is CORRECT because, for API keys, users must provide the code for the Lambda function that rotates the secrets.\nOption D is incorrect because only one secret in Secrets Manager is required, and the application should always get the latest version of the secret.\nReference:\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html.\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets-lambda-function-customizing.html.\n\nTo implement the rotation of API keys stored in AWS Secrets Manager, you need to use the Secrets Manager built-in functionality called \"Rotation.\" Rotation automates the process of creating new versions of secrets and updating applications to use the latest version.\nOption A is incorrect because AWS Parameter Store is not designed for secret management, and it does not support automatic rotation of secrets.\nOption B is incorrect because automatic rotation of secrets is not performed on a fixed schedule. Instead, it is triggered by an event, such as the expiration of a secret or a user-defined rotation schedule.\nOption C is partially correct because customizing the Lambda function that performs the rotation of secrets is required to implement the key rotation in Secrets Manager. You need to create a Lambda function that can access the old and new versions of the secret, update the application to use the new version, and then delete the old version. Secrets Manager provides pre-built Lambda functions that can be customized to meet specific requirements.\nOption D is the correct answer because it describes the recommended way to implement key rotation in Secrets Manager. You create two secrets in Secrets Manager to store two versions of the API credentials, and then modify the application to get the latest version of the secret. You also configure Secrets Manager to rotate the secrets automatically. When a secret is rotated, a new version is created, and the old version is marked as inactive. The application will automatically start using the new version of the secret, and the old version will be deleted after a configurable number of days.\n\n"
}, {
  "id" : 516,
  "question" : "Your organization starts to store RDS credentials in AWS Secrets Manager.\nTo be compliant with security regulations, all secrets stored in the Secrets Manager should automatically rotate.\nIf rotation is not enabled for a secret, your team should get an email notification.\nWhich method is the most appropriate?\n",
  "answers" : [ {
    "id" : "f5aba276fb5241f4a590025ad2227d1c",
    "option" : "Configure AWS Secrets Manager to enable the rotation for all existing and new secrets.",
    "isCorrect" : "false"
  }, {
    "id" : "75342808f4b74c47a95732e51468504d",
    "option" : "Create a CloudWatch Event rule that matches all events in Secrets Manager. Register an SNS topic as its target to provide notifications.",
    "isCorrect" : "false"
  }, {
    "id" : "fb60d2eff3da49519931226e9a4b94d0",
    "option" : "Enable Amazon GuardDuty that monitors services including Secrets Manager.",
    "isCorrect" : "false"
  }, {
    "id" : "584a53d06ba541ccbf0f872fd8502675",
    "option" : "Add the rule â€œsecretsmanager-rotation-enabled-checkâ€ in AWS Config to check whether AWS Secrets Manager has enabled the secret rotation.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect because there is no such configuration to enable rotation for all secrets.\nThe rotation is managed in each secret.\nOption B is incorrect because the CloudWatch Event rule needs a Lambda function as its target to check if rotation is enabled.\nThe description of the option is incomplete.\nOption C is incorrect because Amazon GuardDuty, as a continuous security monitoring service, does not check the secret rotation for Secrets Manager.\nOption D is CORRECT because the AWS Config rule â€œsecretsmanager-rotation-enabled-checkâ€ checks whether AWS Secrets Manager secret has rotation enabled.\nUsers need to add the rule in AWS Config and set up a notification.\nReference:\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/aws-config-rules.html.\nhttps://docs.aws.amazon.com/config/latest/developerguide/secretsmanager-rotation-enabled-check.html.\n\nThe most appropriate method to ensure that all secrets stored in AWS Secrets Manager are compliant with security regulations is to enable automatic secret rotation or notify the team via email if rotation is not enabled. Here's an explanation of each option provided in the answers:\nA. Configure AWS Secrets Manager to enable the rotation for all existing and new secrets.\nThis option is the most straightforward and directly addresses the problem. By enabling rotation for all existing and new secrets, the compliance requirement is met, and there is no need for email notifications or monitoring tools. AWS Secrets Manager has built-in functionality for automatic secret rotation, which can be enabled for supported database engines, such as MySQL, PostgreSQL, and Oracle. This option is suitable if the compliance requirement mandates automatic rotation of all secrets.\nB. Create a CloudWatch Event rule that matches all events in Secrets Manager. Register an SNS topic as its target to provide notifications.\nThis option involves creating a CloudWatch Event rule that matches all events in Secrets Manager and registering an SNS topic as its target to provide email notifications. The rule would need to be configured to trigger a notification when a secret is created or modified but not rotated. While this option provides email notifications, it does not address the compliance requirement directly, and manual intervention is still required to rotate the secrets. This option is suitable if email notifications are sufficient to comply with the regulation, but there is no need to automatically rotate the secrets.\nC. Enable Amazon GuardDuty that monitors services including Secrets Manager.\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior. It is not designed to address the compliance requirement of automatic secret rotation or email notifications. Enabling GuardDuty for monitoring secrets manager would not address the compliance requirement.\nD. Add the rule â€œsecretsmanager-rotation-enabled-checkâ€ in AWS Config to check whether AWS Secrets Manager has enabled the secret rotation.\nAWS Config is a service that provides a detailed inventory of AWS resources and configurations, and it can be used to check compliance against desired configurations or best practices. The â€œsecretsmanager-rotation-enabled-checkâ€ rule can be added to check whether Secrets Manager has enabled secret rotation. This option provides an automated check of the rotation status but does not address the compliance requirement directly. Manual intervention is still required to rotate the secrets. This option is suitable if compliance requirements include automated checks to ensure that rotation is enabled.\nIn summary, option A, Configure AWS Secrets Manager to enable the rotation for all existing and new secrets, is the most appropriate method to ensure compliance with the security regulations. Option B, Create a CloudWatch Event rule that matches all events in Secrets Manager and register an SNS topic as its target, provides email notifications but does not automate the rotation process. Option C, Enable Amazon GuardDuty, is not designed to address the compliance requirement. Option D, Add the rule â€œsecretsmanager-rotation-enabled-checkâ€ in AWS Config, provides an automated check but does not automate the rotation process.\n\n"
}, {
  "id" : 517,
  "question" : "The customer data of an application is stored in an S3 bucket.\nYour team would like to use Amazon Athena to analyze the data using standard SQL.\nHowever, the data in the S3 bucket is encrypted via SSE-KMS.\nHow would you create the table in Athena for the encrypted data in S3?\n",
  "answers" : [ {
    "id" : "ccb3aa7764924225a2e6f6b283ccc330",
    "option" : "You need to provide the private KMS key to Athena.",
    "isCorrect" : "false"
  }, {
    "id" : "8c2df4ff0c094f31b7e1ea298fb534d3",
    "option" : "Athena decrypts the data automatically, and you do not need to provide key information.",
    "isCorrect" : "true"
  }, {
    "id" : "d525136ef13d4a6984bc6ae6e55eb318",
    "option" : "You need to convert SSE-KMS to SSE-S3 before creating the table in Athena.",
    "isCorrect" : "false"
  }, {
    "id" : "2956c2d0b97347ea870ceccf6163dcbe",
    "option" : "You need to disable the server-side encryption in S3 before creating the Athena table.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - B.\nOption A is incorrect because, for SS3-KMS, Athena can determine the proper materials to decrypt the dataset when creating the table.\nYou do not need to provide the key information to Athena.\nOption B is CORRECT because Athena can create the table for the S3 data encrypted by SSE-KMS.\nOptions C and D are incorrect because these steps are not required.\nAthena can create tables for the dataset encrypted by SSE-KMS.\nReference:\nhttps://docs.aws.amazon.com/athena/latest/ug/creating-tables-based-on-encrypted-datasets-in-s3.html.\n\nIf you want to create a table in Athena for encrypted data stored in an S3 bucket using SSE-KMS, you need to provide the KMS key to Athena. The correct answer is A.\nAmazon Athena is a serverless query service that allows users to analyze data stored in S3 using standard SQL queries. To use Athena with encrypted data stored in S3, you need to ensure that Athena can access the encryption key used to encrypt the data. SSE-KMS is a server-side encryption mechanism that uses AWS KMS to encrypt the S3 data, and the KMS key is used to encrypt and decrypt the data.\nTo create a table in Athena for encrypted data stored in an S3 bucket using SSE-KMS, you need to do the following:\nCreate a KMS key: If you haven't already created a KMS key for encrypting the S3 data, you need to create one using the AWS KMS console. Make sure that you have the necessary permissions to create and manage KMS keys. Grant permissions to Athena: You need to grant permissions to Athena to use the KMS key. To do this, you can create an IAM policy that allows Athena to access the KMS key, and then attach the policy to the Athena service role. Specify the KMS key in the CREATE TABLE statement: When you create the table in Athena, you need to specify the KMS key ID in the TBLPROPERTIES clause of the CREATE TABLE statement. For example:\nsql Copy code CREATE EXTERNAL TABLE mytable (   col1 string,   col2 int ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LOCATION 's3://mybucket/mydata' TBLPROPERTIES ('has_encrypted_data'='true', 'encryption_type'='SSE-KMS', 'kms_key_id'='my-kms-key-id');\nIn the above example, 'my-kms-key-id' is the ID of the KMS key that was used to encrypt the S3 data. The TBLPROPERTIES clause specifies that the data is encrypted and the encryption type is SSE-KMS.\nIn summary, to create a table in Athena for encrypted data stored in an S3 bucket using SSE-KMS, you need to provide the KMS key ID in the TBLPROPERTIES clause of the CREATE TABLE statement. This ensures that Athena can access the encryption key and decrypt the data during query execution.\n\n"
}, {
  "id" : 518,
  "question" : "A large amount of structured data is stored in Amazon S3 using the JSON format.\nYou need to use a service to analyze the S3 data directly with standard SQL.\nIn the meantime, the data should be easily visualized through data dashboards.\nWhich of the following services is the most appropriate?\n",
  "answers" : [ {
    "id" : "391801886f7d4227951dd5e0b4fed68e",
    "option" : "Amazon Athena and Amazon QuickSight.",
    "isCorrect" : "true"
  }, {
    "id" : "909809dbd97e4ee284c390f34b8cbbe5",
    "option" : "AWS Glue and Amazon Athena.",
    "isCorrect" : "false"
  }, {
    "id" : "4bcbf55adb644a14a27c7903ac1e189f",
    "option" : "AWS Glue and Amazon QuickSight.",
    "isCorrect" : "false"
  }, {
    "id" : "488f8ca35c5b46f1b62337aa2f52cddd",
    "option" : "Amazon Kinesis Data Stream and Amazon QuickSight.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT because Amazon Athena is the most suitable to run ad-hoc queries to analyze data in S3\nAmazon Athena is serverless, and you are charged for the amount of scanned data.\nBesides, Athena can integrate with Amazon QuickSight that visualizes the data via dashboards.\nOption B is incorrect because AWS Glue is an ETL (extract, transform, and load) service that organizes, cleanses, validates, and formats data in a data warehouse.\nThis service is not required in this scenario.\nOption C is incorrect because it is the same as Option.\nB.\nAWS Glue is not required.\nOption D is incorrect because, with Amazon Kinesis Data Stream, users cannot perform queries for the S3 data through standard SQL.\nReference:\nhttps://aws.amazon.com/athena/pricing/.\nhttps://docs.aws.amazon.com/quicksight/latest/user/create-a-data-set-athena.html.\n\nThe most appropriate service to analyze the S3 data directly with standard SQL and visualize it through data dashboards is Amazon Athena and Amazon QuickSight. The correct answer is A.\nAmazon Athena is a serverless query service that makes it easy to analyze data directly in Amazon S3 using standard SQL. It allows users to easily run ad-hoc queries against structured and unstructured data stored in S3, without the need for infrastructure provisioning or data loading. Athena can handle JSON format data natively, and it supports other common formats like CSV, ORC, and Parquet as well. Therefore, Athena is a great fit for analyzing the structured data stored in S3 using SQL.\nAmazon QuickSight is a cloud-native business intelligence service that allows users to easily create interactive visualizations, dashboards, and reports from various data sources including Athena. It supports multiple data formats including CSV, Excel, and JSON. QuickSight provides an easy-to-use interface for data visualization and exploration, which is perfect for creating dashboards to display the analyzed data.\nAWS Glue is a fully managed extract, transform, and load (ETL) service that helps users to prepare and transform data for analytics. It can be used to create and run ETL jobs that extract data from various sources, transform it, and then load it into a data warehouse or data lake. While AWS Glue can handle JSON format data, it is primarily used for ETL jobs and not for direct data analysis.\nAmazon Kinesis Data Streams is a service for real-time data streaming and processing. It allows users to collect and process large amounts of streaming data in real-time. While Kinesis can handle JSON format data, it is primarily used for real-time data processing and not for direct data analysis.\nTherefore, the correct answer is A. Amazon Athena and Amazon QuickSight are the most appropriate services for analyzing structured data in S3 with SQL and creating data visualizations through dashboards.\n\n"
}, {
  "id" : 519,
  "question" : "Your organization stores customer data in an Amazon DynamoDB table.\nYou need to use AWS Glue to create the ETL (extract, transform, and load) jobs to build the data warehouse.\nIn AWS Glue, you need a component to determine the schema from DynamoDB, and populate the AWS Glue Data Catalog with metadata.\nWhich of the following components should be used to implement it?\n",
  "answers" : [ {
    "id" : "4c0d7a83b971441eb367b409bc0dbaf0",
    "option" : "Table in AWS Glue.",
    "isCorrect" : "false"
  }, {
    "id" : "2ba7c07da3204006910b6aebc2321976",
    "option" : "Table in Amazon Athena.",
    "isCorrect" : "false"
  }, {
    "id" : "6c369440999c4f6d9dc864b3a2a77ab3",
    "option" : "Crawler in AWS Glue.",
    "isCorrect" : "true"
  }, {
    "id" : "5644119f0ed542f6bd5ec4ef6c72adf7",
    "option" : "Classifier in AWS Glue.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nYou can use a crawler to populate the AWS Glue Data Catalog with tables.\nThis is the primary method used by most AWS Glue users.\nA crawler can crawl multiple data stores in a single run.\nUpon completion, the crawler creates or updates one or more tables in your Data Catalog.\nThe AWS Glue Data Catalog can then be used to guide ETL operations.\nOption A is incorrect because the table in AWS Glue is used to define the data schema.\nIt is not the correct component to perform the ETL jobs.\nOption B is incorrect because the table in Athena is not the correct component to populate the AWS Glue Data Catalog.\nOption C is CORRECT because, in AWS Glue, you can use a Crawler that connects to the DynamoBD table and populates the Data Catalog.\nOption D is incorrect because Classifier is a component in Crawler that generates a schema.\nYou should use Crawler to perform the ETL jobs instead of Classifier.\nThe classifier is not a separate component.\nReference:\nhttps://docs.aws.amazon.com/glue/latest/dg/components-overview.html\nhttps://aws.amazon.com/blogs/database/simplify-amazon-dynamodb-data-extraction-and-analysis-by-using-aws-glue-and-amazon-athena/\n\n\nThe correct answer is C. Crawler in AWS Glue.\nAWS Glue is a fully-managed, serverless data integration service that makes it easy to move data between data stores. AWS Glue ETL jobs allow you to transform data and store it in a variety of data sources.\nIn this scenario, the organization is using Amazon DynamoDB to store customer data, and AWS Glue to create ETL jobs to build a data warehouse. To populate the AWS Glue Data Catalog with metadata, you need to use a component that can determine the schema from DynamoDB.\nAWS Glue Crawler is the component that can automatically discover the schema of the data stored in DynamoDB tables and populate the AWS Glue Data Catalog with metadata. It can infer the schema of your data and create a table definition for you, and it can also automatically update the table definition if the schema changes.\nA. Table in AWS Glue is not the correct answer because it is used to define tables manually and not to infer the schema from DynamoDB.\nB. Table in Amazon Athena is also not the correct answer because it is used to query data stored in Amazon S3 or other data sources, and not to populate the AWS Glue Data Catalog with metadata from DynamoDB.\nD. Classifier in AWS Glue is used to classify the data based on its format and schema, but it cannot infer the schema from DynamoDB.\n\n"
}, {
  "id" : 520,
  "question" : "You have an Amazon Route 53 alias record that routes the traffic to an Application Load Balancer.\nLater on, the availability zones enabled for the load balancer are changed by a team member.\nWhen you check the load balancer using the dig command, you find that the IPs of the ELB have changed.\nWhat kind of change do you need to do for the alias record in Route 53?\n",
  "answers" : [ {
    "id" : "0536540f23ab40e08c936e8b823df92a",
    "option" : "Change the record type from A to CNAM.",
    "isCorrect" : "false"
  }, {
    "id" : "ad6512869e454816a55a5944132cf149",
    "option" : "Modify the destination to the DNS name of the Application Load Balancer.",
    "isCorrect" : "false"
  }, {
    "id" : "9fae1cc3e7b6495aa889e13b363cc1a5",
    "option" : "Add the new IP addresses in the destination of the alias record.",
    "isCorrect" : "false"
  }, {
    "id" : "cd3c3087899943dcabcf35ea52f44b0b",
    "option" : "Nothing as Route 53 automatically recognizes changes in the resource for the alias record.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect because there is no need to change the type to CNAME as the alias record continues to work, although the IP addresses are changed for the ELB.Option B is incorrect because Route 53 can find out the new IPs of the ELB.\nThis change is not required.\nOption C is incorrect because you cannot add any extra IPs to this record after creating the alias record.\nRoute 53 is responsible for routing the traffic to the correct IP addresses.\nOption D is CORRECT because Route 53 automatically routes the traffic to the new ELB IP addresses.\nWith alias records, users do not need to change the record sets even if they have some configuration changes.\nReference:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html.\n\nWhen an Amazon Route 53 alias record is created for an Application Load Balancer, it points to the DNS name of the load balancer. This DNS name is resolved to one or more IP addresses, which are associated with the instances that are registered with the load balancer.\nIf the availability zones enabled for the load balancer are changed by a team member, Route 53 automatically updates the DNS name associated with the alias record to reflect the new configuration. As a result, the IP addresses associated with the DNS name also change.\nIn this scenario, you can use the dig command to verify the new IP addresses associated with the Application Load Balancer. Once you have confirmed that the IP addresses have changed, you do not need to make any changes to the Route 53 alias record. Route 53 automatically recognizes changes in the resource for the alias record and updates the DNS name accordingly.\nTherefore, the correct answer is D. Nothing as Route 53 automatically recognizes changes in the resource for the alias record.\n\n"
}, {
  "id" : 521,
  "question" : "You have a PHP application deployed in an Auto Scaling group.\nIn production, you want to use AWS WAF to block requests associated with exploiting vulnerabilities specific to the PHP use, including injection of unsafe PHP functions.\nWhich method is appropriate?\n",
  "answers" : [ {
    "id" : "8bde033493684092af23703f97c36e08",
    "option" : "Add the AWS managed PHP application rule to AWS Shield.",
    "isCorrect" : "false"
  }, {
    "id" : "87f3139632134c4986fd9bf65b5aaa7f",
    "option" : "Add the AWS managed PHP application rule in the web ACL of AWS WA.",
    "isCorrect" : "true"
  }, {
    "id" : "13deb76dd406403a98aeab0bbe366ccd",
    "option" : "Add a PHP protection rule from AWS Marketplace to the WAF web ACL.",
    "isCorrect" : "false"
  }, {
    "id" : "a0221a8c79194d1db419021e5dccddc5",
    "option" : "In WAF, create a new PHP application web ACL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect because AWS Shield is a service to protect web applications against DDoS attacks.\nAWS WAF should be used in this scenario.\nOption B is CORRECT because AWS provides the PHP protection rule in WAF.\nUsers can add the rule in web ACL.\nOption C is incorrect because, in this scenario, you need to add the AWS managed PHP rule in web ACL.\nThere is no need to use a rule offered by AWS Marketplace.\nOption D is incorrect because this is not how WAF works.\nIn WAF, users create a web ACL and add rules in the ACL to take corresponding actions.\nReference:\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-managed-rule-groups.html.\n\n\nThe appropriate method to use AWS WAF to block requests associated with exploiting vulnerabilities specific to the PHP use, including injection of unsafe PHP functions in a PHP application deployed in an Auto Scaling group in production, is to add the AWS managed PHP application rule in the web ACL of AWS WAF.\nAWS WAF (Web Application Firewall) is a managed service that helps protect web applications from common web exploits. It allows you to create rules that block or allow incoming traffic based on specific conditions. To use AWS WAF with your PHP application, you need to create a web ACL (Web Access Control List) and add rules to it.\nThe AWS managed PHP application rule is a pre-configured rule that helps protect against common PHP attacks, such as command injection, SQL injection, and cross-site scripting (XSS) attacks. This rule is specifically designed for PHP applications and can be added to the web ACL of AWS WAF.\nOption A, adding the AWS managed PHP application rule to AWS Shield, is incorrect because AWS Shield is a managed DDoS (Distributed Denial of Service) protection service that protects your applications from DDoS attacks. It does not offer protection against common web exploits like those specific to PHP.\nOption C, adding a PHP protection rule from AWS Marketplace to the WAF web ACL, is incorrect because it is not specific to PHP applications and may not offer the same level of protection as the AWS managed PHP application rule.\nOption D, creating a new PHP application web ACL in WAF, is also incorrect because it would require creating a new web ACL from scratch and manually adding all the rules necessary to protect against common PHP attacks. This would be time-consuming and may not provide the same level of protection as the pre-configured AWS managed PHP application rule.\nTherefore, option B, adding the AWS managed PHP application rule in the web ACL of AWS WAF, is the appropriate method to use AWS WAF to block requests associated with exploiting vulnerabilities specific to the PHP use, including injection of unsafe PHP functions in a PHP application deployed in an Auto Scaling group in production.\n\n"
}, {
  "id" : 522,
  "question" : "You work in a start-up company as an AWS solutions architect.\nYou create a new AWS Organization that includes a large amount of AWS accounts.\nYou want to use a tool to trigger a notification whenever the administrator performs an action in the Organization.\nWhich of the following AWS services would you use?\n",
  "answers" : [ {
    "id" : "93ad240fd94348b5be3e755be51ccf14",
    "option" : "AWS CloudWatch Events.",
    "isCorrect" : "true"
  }, {
    "id" : "3660aa4d817b4c8cb7dd6369c9e455c4",
    "option" : "AWS Config Resources.",
    "isCorrect" : "false"
  }, {
    "id" : "41506abdd9fe4c0088187719a591d639",
    "option" : "AWS CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "0cbf090ce1b34db29bbb27e369db33ea",
    "option" : "AWS CloudWatch Logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT because CloudWatch Events can raise events for actions in the AWS Organization.\nYou can register an SNS topic to trigger a notification in a CloudWatch Event rule.\nOption B is incorrect because AWS Config Resources do not log events for AWS Organizations.\nOption C is incorrect because AWS CloudTrail is used to create to trail to monitor the AWS API calls.\nOption D is incorrect because CloudWatch Logs cannot provide a real-time notification like CloudWatch Events when an administrator-specified action in the Organization occurs.\nReference:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_security_incident-response.html.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_tutorials_cwe.html.\nhttps://docs.aws.amazon.com/config/latest/developerguide/resource-config-reference.html.\n\nThe correct answer is C. AWS CloudTrail.\nAWS CloudTrail is a service that provides a record of actions taken by a user, role, or an AWS service in an account. It logs all API calls made to AWS services, including AWS Management Console, AWS CLI, and SDKs. CloudTrail enables governance, compliance, operational auditing, and risk auditing of AWS account activities.\nWhen you create a new AWS Organization, you can enable CloudTrail in the master account to monitor and log all activities in the Organization. This includes any activity performed by the administrator on any of the linked accounts within the Organization. You can then create a CloudWatch alarm to send a notification whenever a specific event occurs, such as the creation of a new user, modification of a policy, or deletion of a resource.\nAWS CloudWatch Events (option A) is a service that enables you to respond to state changes in your AWS resources. You can use it to trigger actions in response to specific events, such as the creation or deletion of an EC2 instance. It's not designed for tracking administrative actions in an AWS Organization.\nAWS Config Resources (option B) is a service that enables you to track changes to resources over time and maintain a history of resource configuration. It's useful for compliance auditing, change management, and troubleshooting. However, it doesn't provide real-time notifications for administrative actions.\nAWS CloudWatch Logs (option D) is a service that enables you to monitor, store, and access log files from AWS resources and applications. You can use it to troubleshoot issues, monitor performance, and detect security threats. However, it doesn't provide a comprehensive audit trail of administrative actions in an AWS Organization.\n\n"
}, {
  "id" : 523,
  "question" : "Your company requires that S3 objects should be replicated in different AWS regions.\nYou have an S3 bucket in the ap-southeast-1 region, and its objects are encrypted with AWS Key Management Service (AWS KMS)\nHow would you configure the Cross-Region Replication (CRR) for the encrypted objects in the S3 bucket?\n",
  "answers" : [ {
    "id" : "5c135b51fb0e458195757e40281eceb1",
    "option" : "Encrypted objects cannot be replicated with Cross-Region replication (CRR).",
    "isCorrect" : "false"
  }, {
    "id" : "067b28dfeacb4cb19ddc45e7dbfcb6f0",
    "option" : "Modify the S3 property to encrypt the objects with AES-256 and then replicate them with a Cross-Region replication rule.",
    "isCorrect" : "false"
  }, {
    "id" : "fba3b7099a3d4c608de7062fdba78e0a",
    "option" : "In the replication rule, provide the KMS key name for decrypting source objects.",
    "isCorrect" : "true"
  }, {
    "id" : "1f2643b004e145fea2feb2dea704b440",
    "option" : "Copy a KMS key from the target region to the source region. Re-encrypt the objects with the new KMS key in the source S3 bucket for the Cross-Region replication to work.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect because Cross-Region replication can copy encrypted objects across buckets in different AWS regions.\nOption B is incorrect because this is not required as CRR supports AWS-KMS.\nOption C is CORRECT because users can choose one or more KMS keys in the replication rule as follows.\nObjects encrypted by AWS KMS CMKs that are not selected will not be replicated.\nOption D is incorrect because the re-encryption is not required for the Cross-Region replication.\nFor details, please check the following references.\nReference:\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-replication.html.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html#crr-scenario.\n\n\nThe correct answer is C. In the replication rule, provide the KMS key name for decrypting source objects.\nCross-Region Replication (CRR) is a feature that replicates objects from a source S3 bucket to a destination S3 bucket in a different region. With CRR, you can have a copy of your objects in a different region, which can help you achieve data durability, reduce latency, and meet compliance requirements.\nWhen an S3 object is encrypted with AWS Key Management Service (AWS KMS), the encryption key is managed separately from the S3 bucket. Therefore, when replicating an encrypted object with CRR, the KMS key that was used to encrypt the object must be accessible from the destination region.\nTo configure CRR for encrypted objects, follow these steps:\nCreate a destination S3 bucket in the desired region. Enable versioning for both the source and destination S3 buckets. Create an AWS KMS key in the destination region if you don't already have one. Add a replication rule to the source S3 bucket and configure it as follows: Specify the destination S3 bucket and the destination region. Select the replication rule status (enabled or disabled). Specify which objects to replicate using object filters. Provide the KMS key name or KMS key ARN for decrypting source objects in the destination region. You can find the KMS key name or ARN in the KMS console or through the AWS CLI. Select an IAM role that grants permissions for replication. Choose a replication time control. Save the replication rule.\nBy providing the KMS key name or ARN in the replication rule, you allow the destination region to decrypt the encrypted objects using the same KMS key that was used to encrypt them. This ensures that the replicated objects are also encrypted and can only be decrypted by authorized users in the destination region.\nOption A is incorrect because encrypted objects can be replicated with CRR as long as the KMS key used for encryption is accessible from the destination region.\nOption B is incorrect because modifying the S3 bucket properties to use AES-256 encryption would not help with replicating encrypted objects with CRR. The replication rule still needs the KMS key name or ARN to decrypt the objects in the destination region.\nOption D is incorrect because copying a KMS key from the target region to the source region and re-encrypting objects with the new key in the source S3 bucket is not necessary for CRR. The replication rule only needs the KMS key name or ARN to decrypt the objects in the destination region.\n\n"
}, {
  "id" : 524,
  "question" : "You have an S3 bucket that is used to store important data for a web application.\nYou want to receive an email notification whenever an object removal event happens in the S3 bucket.\nHow would you configure the S3 bucket to achieve this requirement?\n",
  "answers" : [ {
    "id" : "9f6532775ba14a9899cd6164992510fe",
    "option" : "Configure the object-level logging for the S3 bucket and register an SNS topic to provide notifications.",
    "isCorrect" : "false"
  }, {
    "id" : "0d2df6faa1ff490693b4b937929f2737",
    "option" : "Configure the server access logging for the object removal events. Add an SNS topic to notify the team via emails.",
    "isCorrect" : "false"
  }, {
    "id" : "c49a8b1a542a43f4b6a117d86ca402d6",
    "option" : "Set up an AWS Config rule to check the object deletion events. Register a Lambda function to send notifications.",
    "isCorrect" : "false"
  }, {
    "id" : "9394f172d79941bd93a2568f99a4d92d",
    "option" : "Configure an S3 event notification for the object removal events. Send the events to an SNS topic.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect because object-level logging is used to record object-level API activities in CloudTrail.\nUsers cannot register an SNS topic for object-level logging.\nOption B is incorrect because server access logging does not trigger an SNS topic for the object removal events.\nOption C is incorrect because you would need to write a custom Lambda function in the AWS Config rule to check the object removal events.\nThis method is more complicated than option.\nD.Option D is CORRECT because with an S3 event notification, you can select which events are enabled for the notification.\nThe events can be sent to an SNS topic, an SQS queue, or a Lambda function.\nReference:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html.\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-event-notifications.html.\n\n\nThe correct answer to this question is D. Configure an S3 event notification for the object removal events. Send the events to an SNS topic.\nExplanation: Amazon S3 (Simple Storage Service) is an object storage service that allows storing and retrieving files from anywhere on the web. S3 provides a range of features and configurations to suit different use cases. One such feature is the ability to send notifications when certain events happen in the S3 bucket.\nTo configure the S3 bucket to send notifications for object removal events, we need to follow these steps:\nCreate an SNS (Simple Notification Service) topic: SNS is a web service that enables applications, individuals, and devices to send or receive push messages from the cloud. We can create an SNS topic from the AWS Management Console. Configure an S3 event notification for the object removal events: We need to configure the S3 bucket to send a notification to the SNS topic when an object removal event occurs. To do this, we need to create an event notification configuration in the S3 bucket. We can do this from the AWS Management Console or using the AWS Command Line Interface (CLI). Specify the SNS topic to receive the S3 event notifications: In the S3 event notification configuration, we need to specify the SNS topic we created in step 1 as the recipient of the event notifications.\nOnce the above steps are complete, whenever an object is removed from the S3 bucket, an event notification is triggered, and the notification is sent to the specified SNS topic. The SNS topic can be configured to send an email notification to a specified email address, alerting the user of the event.\nOption A is incorrect because object-level logging is used to capture detailed information about the operations performed on the objects in the S3 bucket. It does not provide notifications for object removal events.\nOption B is incorrect because server access logging is used to capture information about the requests made to the S3 bucket. It does not provide notifications for object removal events.\nOption C is incorrect because AWS Config is a service that enables us to assess, audit, and evaluate the configurations of our AWS resources. It does not provide notifications for object removal events.\n\n"
}, {
  "id" : 525,
  "question" : "You are developing an application that uses the Amazon Kinesis Producer Library (KPL) to put data records to an encrypted Kinesis data stream.\nHowever, when your application runs, there is an unauthorized KMS master key permission error.\nHow would you resolve the problem?\n",
  "answers" : [ {
    "id" : "b4dab4daa5d54473ad51ac207d220457",
    "option" : "Configure the applicationâ€™s IAM role as the key administrator of the KMS key.",
    "isCorrect" : "false"
  }, {
    "id" : "31d1c24e7c86494099d91e4e14e2f7dc",
    "option" : "In the KMS key policy, assign the permission to the application to access the key.",
    "isCorrect" : "true"
  }, {
    "id" : "cdb7dcf9bb21491d85467a9ed8262c1f",
    "option" : "Re-encrypt the Kinesis data stream with AWS/kinesis.",
    "isCorrect" : "false"
  }, {
    "id" : "20e9fefcd52848eb9284694c70c5cf16",
    "option" : "Configure the KPL not to encrypt the data records for the Kinesis data stream.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\n\nCorrect Answer - B.\nOption A is incorrect because the IAM role should be configured as a key user instead of an administrator.\nOption B is CORRECT because the application would need to have permission to use the KMS key when putting records in the stream.\nOption C is incorrect because the encryption of the Kinesis data stream should not be changed.\nThis is a key permission issue, and the key policy should be modified to address the problem.\nOption D is incorrect because KPL does not have such a configuration.\nReference:\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-producers.html.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policies.html.\n\nThe correct answer is B. In the KMS key policy, assign the permission to the application to access the key.\nExplanation:\nAmazon Kinesis Producer Library (KPL) is a library that helps users to send data to Amazon Kinesis data streams efficiently. Kinesis data streams are automatically encrypted using a default AWS-managed key or a customer-managed KMS key.\nIf you receive an \"unauthorized KMS master key permission\" error, it means that the application doesn't have permission to access the KMS key that's used to encrypt the Kinesis data stream.\nTo resolve this issue, you can modify the key policy of the KMS key to include the IAM role of your application. The key policy should allow the IAM role to use the key to encrypt and decrypt data. Here are the steps:\nOpen the AWS KMS console at https://console.aws.amazon.com/kms/. In the navigation pane, choose \"Customer managed keys.\" Choose the KMS key that's used to encrypt the Kinesis data stream. Choose the \"Key policy\" tab. Choose \"Edit.\" Modify the policy document to include the IAM role of your application. Here's an example policy:\njson Copy code {   \"Id\": \"key-consolepolicy-2\",   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Sid\": \"Enable IAM User Permissions\",       \"Effect\": \"Allow\",       \"Principal\": {         \"AWS\": \"arn:aws:iam::123456789012:role/application-iam-role\"       },       \"Action\": [         \"kms:Encrypt\",         \"kms:Decrypt\",         \"kms:ReEncrypt*\",         \"kms:GenerateDataKey*\",         \"kms:DescribeKey\"       ],       \"Resource\": \"*\"     }   ] }\nChoose \"Save changes.\"\nAfter you modify the key policy, your application should be able to access the KMS key and put data records to the encrypted Kinesis data stream.\nThe other answer choices are incorrect:\nA. Configuring the application's IAM role as the key administrator of the KMS key is not recommended because it gives the application too much permission. It's better to use the principle of least privilege and assign only the necessary permission to the IAM role.\nC. Re-encrypting the Kinesis data stream with AWS/kinesis is not necessary because the stream is already encrypted. The issue is that the application doesn't have permission to access the KMS key.\nD. Configuring the KPL not to encrypt the data records for the Kinesis data stream is not recommended because it would compromise the security of the data. It's better to fix the permission issue and continue encrypting the data.\n\n"
}, {
  "id" : 526,
  "question" : "You are building up a Lambda function that runs periodically and processes the data stored in a DynamoDB table.\nAs the data in the DynamoDB table grows fast, you need to estimate the cost of the Lambda function.\nWhich of the following factors directly determine the monthly cost of Lambda? (Select TWO.)\n",
  "answers" : [ {
    "id" : "d8b6014cef3e4651934beea279cb5e32",
    "option" : "The programming language used by the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "337a7ea9bbf947439aaf6e8d62ad864a",
    "option" : "The Lambda function handler.",
    "isCorrect" : "false"
  }, {
    "id" : "51f7d978f6ee4c90b995bee3ca2bbf77",
    "option" : "The memory allocated to the Lambda function.",
    "isCorrect" : "true"
  }, {
    "id" : "3c15d06a5a6b47d5aa88d2e3c89b16b3",
    "option" : "The timeout value of the Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "3e0ed6f292944d05af5fb6c5b8397976",
    "option" : "The total number of requests for the Lambda function.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - C, E.\nOption A is incorrect: Because users are charged based on the number of requests and the time it takes for the code to execute.\nThe programming language does not directly determine the cost.\nOption B is incorrect: Because the Lambda function handler does not impact the monthly cost at all.\nOption C is CORRECT: Because the duration price depends on the amount of memory allocated to the function.\nOption D is incorrect: Because the timeout value is not a direct factor in calculating the price.\nPlease check the below reference for details.\nOption E is CORRECT: Because the Lambda function charges you based on the number of requests, such as $0.20 per 1M requests.\nReference:\nhttps://aws.amazon.com/lambda/pricing/\n\nSure, I'd be happy to provide a detailed explanation!\nLambda is a serverless compute service provided by AWS, which enables developers to build and run applications without having to manage servers. The cost of Lambda is calculated based on the number of requests and the duration of the function execution. Let's explore the factors that directly determine the monthly cost of Lambda:\nThe memory allocated to the Lambda function:\nLambda functions are priced based on the amount of memory allocated to them. Memory allocation determines the amount of CPU power and network bandwidth that the function can access. The minimum amount of memory that can be allocated to a Lambda function is 128 MB, and the maximum is 3,008 MB (or 3 GB). When you allocate more memory to a function, you also get more CPU power and network bandwidth. The price per 100ms of execution time is calculated based on the amount of memory allocated to the function.\nThe total number of requests for the Lambda function:\nThe number of requests is the number of times your function is invoked during its execution. AWS charges for every 1 million requests. If your function is invoked more than 1 million times, the cost is calculated based on the total number of requests, divided by 1 million.\nThe other options mentioned in the question, like the programming language used, the handler, and the timeout value of the Lambda function, do not directly impact the cost of the function. The programming language and the handler only affect the development and performance of the function, and the timeout value determines the maximum duration that a function can run before it times out.\nIn summary, the two factors that directly determine the monthly cost of Lambda are the memory allocated to the function and the total number of requests.\n\n"
}, {
  "id" : 527,
  "question" : "You use a Lambda function to connect to an Oracle RDS instance and run some long SQL queries.\nThe timeout of the Lambda function is set to 15 minutes, and the function sometimes reaches a timeout.\nYou propose to increase the allocated memory of the Lambda function.\nHowever, your manager is worried about the cost.\nHow would you answer this question?\n",
  "answers" : [ {
    "id" : "f68519a529f84f4f8ca0683a7d34e7e2",
    "option" : "The cost will not change as the allocated memory does not impact the price.",
    "isCorrect" : "false"
  }, {
    "id" : "8ca9b8f6415247a8b6bb5029d10c1975",
    "option" : "The cost will decrease for sure as the total duration of code execution will be less.",
    "isCorrect" : "false"
  }, {
    "id" : "9a798b3b899f44c3998a1d285f45cfa9",
    "option" : "The cost may not increase as the execution time for the Lambda may be less although you will be charged more for the memory.",
    "isCorrect" : "true"
  }, {
    "id" : "f96cdf4cfc934047b50a57d290ec6f7a",
    "option" : "The cost will not change because the number of requests for the function does not change.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption A is incorrect: Because the amount of allocated memory impacts the cost.\nOption B is incorrect: Because the price also depends on the amount of memory that you allocate to the function.\nOption C is CORRECT: Because the price of duration is calculated based on both the amount of memory and execution time.\nWhen the function has more memory, the execution time may drop.\nOption D is incorrect: Because the memory also impacts the cost of the function.\nReference:\nhttps://aws.amazon.com/lambda/pricing/\n\nLambda functions in AWS are billed based on the duration of execution and the amount of memory allocated to the function. The more memory allocated, the higher the price. However, the cost-effectiveness of increasing the allocated memory depends on the specific use case.\nIn this scenario, increasing the allocated memory of the Lambda function could potentially reduce the overall execution time of the function, which may lead to a lower cost overall. This is because the Lambda function is charged based on the actual execution time, which includes the time spent waiting for the RDS query to complete. If the allocated memory is increased, the Lambda function will have more CPU and network resources available to it, which can help speed up the query execution time and reduce the overall duration of the function.\nHowever, it's important to note that increasing the allocated memory does not guarantee a decrease in cost. This is because the price of the function is dependent on both the duration of execution and the amount of memory allocated, so while the execution time may decrease, the cost may increase if the amount of memory allocated is also increased. The optimal balance between memory allocation and execution time for cost-effectiveness would need to be determined through testing and optimization.\nRegarding the answer options provided, option A is correct in stating that allocated memory does not impact the price of the Lambda function. However, it's important to note that the duration of execution does impact the price, and increasing the allocated memory can potentially reduce the duration of execution.\nOption B is not necessarily true, as the total cost may still increase depending on the amount of memory allocated.\nOption C is a more accurate response as it acknowledges the potential for the cost to increase due to increased memory allocation, but also acknowledges the potential for decreased execution time.\nOption D is not relevant to the question, as the number of requests for the function is not being considered in the scenario.\n\n"
}, {
  "id" : 528,
  "question" : "In your AWS account, you have configured three Application Load Balancers to route the traffic, and each ALB has its own target group.\nAs the traffic keeps growing, the cost of the Elastic Load Balancers increases as well.\nWhich method would you take to reduce the load balancer cost?\n",
  "answers" : [ {
    "id" : "c5b7f5b6b7cf43218949e62364daecb9",
    "option" : "Configure the Application Load Balancers as the spot ones.",
    "isCorrect" : "false"
  }, {
    "id" : "350ed45e22534c5e84f37ad817330b78",
    "option" : "Use one ALB instead of three. Attach all three target groups in the AL.",
    "isCorrect" : "true"
  }, {
    "id" : "e0791d9f802146a6a3b5df0cd43f134b",
    "option" : "Move the Application Load Balancers to the US East (N.Virginia) region.",
    "isCorrect" : "false"
  }, {
    "id" : "28896d92c25841679cd7ffcae43c92a1",
    "option" : "Install an ACM certificate in each Application Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect: Because Application Load Balancers do not have the spot type.\nOption B is CORRECT: Because users are charged for the time, the Application Load Balancers are running.\nThe cost will be reduced if only one ALB is used.\nOption C is incorrect: Because this method is not applicable if the application cannot be moved to another region.\nALB may not be cheaper in the US East (N.Virginia) region too.\nOption D is incorrect: Because installing a certificate does not help to reduce the cost.\nReference:\nhttps://aws.amazon.com/elasticloadbalancing/pricing/\n\nThe most cost-effective option to reduce the load balancer cost would be to use one Application Load Balancer (ALB) instead of three and attach all three target groups to the ALB. This would significantly reduce the cost of the Elastic Load Balancers since you would only be paying for one ALB instead of three.\nOption A, configuring the Application Load Balancers as spot ones, would not be a recommended solution in this scenario. Spot instances are a cost-effective option for running workloads that are flexible and can handle interruptions. However, this approach might not be suitable for load balancers that are expected to be always available and provide high availability.\nOption C, moving the Application Load Balancers to the US East (N.Virginia) region, would not have any significant impact on the cost of the load balancers. The cost of the Elastic Load Balancers is determined by the usage and configuration of the load balancers, not the region where they are located.\nOption D, installing an ACM certificate in each Application Load Balancer, is not directly related to reducing the cost of the Elastic Load Balancers. ACM certificates are used to secure the communication between the clients and the load balancers, but they do not affect the cost of the load balancers.\nIn summary, the best option to reduce the cost of the Elastic Load Balancers in this scenario would be to use one Application Load Balancer and attach all three target groups to it.\n\n"
}, {
  "id" : 529,
  "question" : "Your team creates an Application Load Balancer for a new application and registers a Lambda function as its target.\nThree availability zones are enabled for the ALB.\nYou want to take some actions to reduce the cost.\nWhich of the below options is the most appropriate?\n",
  "answers" : [ {
    "id" : "97c512f78d914cb8828c919479e8daec",
    "option" : "Replace the Lambda function with an EC2 Auto Scaling Group as the ELB target.",
    "isCorrect" : "false"
  }, {
    "id" : "aac3a3d9833843bcb9b9173b3e1536a7",
    "option" : "Enable only one availability zone in the Application Load Balancer.",
    "isCorrect" : "false"
  }, {
    "id" : "b9942726c3114b038ee2d37a735bf22d",
    "option" : "Modify the ELB listener to be TCP with a customized port.",
    "isCorrect" : "false"
  }, {
    "id" : "25d2baf4254747e49b8689de442b9e4b",
    "option" : "Optimize the Lambda function so that the function executes the code faster.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOption A is incorrect: Because the Lambda function is serverless and costs less than an Auto Scaling Group in most cases.\nOption B is incorrect: Because ALB does not charge users based on the number of enabled availability zones.\nThis method does not help.\nOption C is incorrect: Because Application Load Balancer listeners can only be HTTP or HTTPS.\nTCP cannot be configured.\nOption D is CORRECT: Because the Lambda function's cost will be reduced if the execution duration decreases.\nReference:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambda-functions.html.\nhttps://aws.amazon.com/elasticloadbalancing/pricing/.\nhttps://aws.amazon.com/lambda/pricing/\n\nThe most appropriate option to reduce the cost in this scenario would be to enable only one availability zone in the Application Load Balancer. This will reduce the cost because the ALB pricing is based on the number of Availability Zones (AZs) enabled for the load balancer. By reducing the number of AZs, the cost of the ALB will be reduced.\nOption A, which suggests replacing the Lambda function with an EC2 Auto Scaling Group, is not an appropriate option because it does not address the cost issue. In fact, using EC2 instances may result in higher costs due to the need for managing and maintaining these instances.\nOption C, which suggests modifying the ELB listener to be TCP with a customized port, is not an appropriate option because it does not address the cost issue. Modifying the listener type or port does not affect the cost of the ALB.\nOption D, which suggests optimizing the Lambda function so that the function executes the code faster, is not an appropriate option because it does not address the cost issue. While optimizing the function may reduce the execution time and therefore the cost, it does not address the cost associated with the ALB.\nTherefore, the most appropriate option to reduce the cost would be to enable only one availability zone in the Application Load Balancer.\n\n"
}, {
  "id" : 530,
  "question" : "An application is hosted in an AWS EC2 instance in a VPC private subnet.\nThe application frequently downloads a large amount of files stored in an S3 bucket.\nA NAT Gateway is configured in the public subnet for the EC2 instance to connect with the S3 bucket.\nNow you need to take some actions to reduce the cost of AWS infrastructure.\nWhich method is the most appropriate?\n",
  "answers" : [ {
    "id" : "ddcc11084f1c438b8c2f73654815f058",
    "option" : "Replace the NAT Gateway with a t2.micro NAT instance.",
    "isCorrect" : "false"
  }, {
    "id" : "99f41f076a6c487a9188317288392381",
    "option" : "Configure a gateway endpoint for Amazon S3. Delete the NAT Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "88620ae95a914c5eb37fd9165ba20d03",
    "option" : "Place the EC2 instance in the public subnet. Delete the NAT Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "894e2712b5a54b2bbcb81781bc6f7fb6",
    "option" : "Replace the NAT Gateway with an Internet Gateway. Configure an AWS PrivateLink for Amazon S3.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect: Because the NAT instance is not recommended because of its performance issue.\nIn this scenario, NAT Gateway can be deleted if the VPC gateway endpoint is configured for S3.\nOption B is CORRECT: Because with this method, you do not need to pay for the NAT Gateway or the internet data transfer for S3\nBesides, the VPC gateway endpoint is a free service.\nOption C is incorrect: Because you still need to pay for the internet data transfer for S3\nIt may also cause security issues as the EC2 instance is put in the public subnet.\nOption D is incorrect: Because the Internet Gateway cannot replace the NAT Gateway.\nReference:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html#gateway-endpoint-pricing\n\nThe most appropriate method to reduce the cost of AWS infrastructure in this scenario is to configure a gateway endpoint for Amazon S3 and delete the NAT Gateway. Therefore, the correct answer is B.\nExplanation:\nIn the given scenario, an application is hosted in an EC2 instance in a VPC private subnet, and the application frequently downloads a large amount of files stored in an S3 bucket. A NAT Gateway is configured in the public subnet for the EC2 instance to connect with the S3 bucket. However, NAT Gateway incurs additional cost, and thus, to reduce the cost, we need to explore other options.\nOption A: Replace the NAT Gateway with a t2.micro NAT instance.\nThis option does not help in reducing the cost as the t2.micro NAT instance also incurs additional cost, and also requires manual maintenance, which may be time-consuming.\nOption B: Configure a gateway endpoint for Amazon S3. Delete the NAT Gateway.\nThis option is the most appropriate in the given scenario to reduce the cost of AWS infrastructure. A gateway endpoint for Amazon S3 enables EC2 instances in a VPC to access S3 buckets without requiring an internet gateway or NAT Gateway in the VPC. This approach can help save on data transfer costs and NAT Gateway charges. Additionally, the gateway endpoint provides better security as it allows private communication between the VPC and S3 bucket. Therefore, deleting the NAT Gateway and configuring a gateway endpoint for Amazon S3 is the most cost-effective option.\nOption C: Place the EC2 instance in the public subnet. Delete the NAT Gateway.\nThis option is not suitable in the given scenario as placing the EC2 instance in a public subnet exposes it to the internet, which may not be desired. It also increases the attack surface of the application, and it incurs additional data transfer cost.\nOption D: Replace the NAT Gateway with an Internet Gateway. Configure an AWS PrivateLink for Amazon S3.\nThis option is not appropriate in the given scenario as replacing the NAT Gateway with an Internet Gateway again incurs additional cost. Additionally, configuring AWS PrivateLink for S3 requires additional setup and may not be cost-effective in this scenario.\nTherefore, option B is the most appropriate method to reduce the cost of AWS infrastructure in the given scenario.\n\n"
}, {
  "id" : 531,
  "question" : "You are working in a start-up company, and you need to save the cost of Amazon EC2, AWS Fargate, and AWS Lambda.\nYou are looking for a flexible pricing model.\nFor example, you will be charged at a lower rate if you commit to $10/hour of compute usage.\nWhich AWS service can achieve this requirement?\n",
  "answers" : [ {
    "id" : "0c74999de41f4b55a5543e4d66e09d67",
    "option" : "AWS Savings Plan",
    "isCorrect" : "true"
  }, {
    "id" : "71164f47b308457397b68383d97bb5f3",
    "option" : "AWS Reserved Instance.",
    "isCorrect" : "false"
  }, {
    "id" : "a881d500b9ca4c808c11f64a9e9223d4",
    "option" : "AWS Dedicated Host.",
    "isCorrect" : "false"
  }, {
    "id" : "7d0bc74d609645eaa2695a5eb666147f",
    "option" : "AWS Spot Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT: Because Savings Plans is a flexible pricing model that provides low prices in exchange for commitment.\nFor its details, please check the following reference.\nOption B is incorrect: Because AWS Reserved Instance is only for EC2 and does not help to reduce the cost of AWS Fargate or AWS Lambda.\nOption C is incorrect: Because AWS Dedicated Host provides dedicated hardware, which does not help cut the cost.\nOption D is incorrect: Because AWS Spot Instance is only for EC2 and not for AWS Fargate or AWS Lambda.\nReference:\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html\n\nThe AWS service that can achieve this requirement is the AWS Savings Plan (option A).\nAWS Savings Plan is a flexible pricing model that offers lower rates on EC2, Fargate, and Lambda usage in exchange for a commitment to use a specific amount of compute power for a set period of time, typically 1 or 3 years.\nSavings Plan offers two different types of savings plans: Compute Savings Plans and EC2 Instance Savings Plans.\nCompute Savings Plans provide savings on EC2 instances and Fargate usage, regardless of the instance family, size, OS, or tenancy. Compute Savings Plans are best suited for workloads with flexible compute requirements, such as batch processing, containerized services, microservices, and web applications.\nEC2 Instance Savings Plans offer savings on a specific instance family and size in a specific region. These plans are best suited for workloads with predictable compute requirements, such as production applications, database workloads, and enterprise applications.\nBoth Compute Savings Plans and EC2 Instance Savings Plans offer the flexibility to change instance families, sizes, OS, and tenancy as long as the overall commitment is met. Savings Plans are also automatically applied to the instances and Fargate tasks that match the attributes of the plan, without requiring any upfront payments or reservations.\nIn summary, AWS Savings Plan is a flexible pricing model that offers lower rates on EC2, Fargate, and Lambda usage in exchange for a commitment to use a specific amount of compute power for a set period of time. It's best suited for workloads with flexible or predictable compute requirements and offers the flexibility to change instance attributes while still meeting the commitment.\n\n"
}, {
  "id" : 532,
  "question" : "You create a new EC2 Instance Savings Plan in the Asia Pacific region (Sydney)\nYou have committed to 20$/hour of usage for one year, and the instance family is selected as C5\nFor which of the following EC2 instances can you get a discount?\n",
  "answers" : [ {
    "id" : "6a557d2b301c4a88ae76c21ef8da6e2b",
    "option" : "An Amazon Linux instance (c5.xlarge).",
    "isCorrect" : "false"
  }, {
    "id" : "2ae956e5389f4fc5b61421a436704fb1",
    "option" : "A Windows instance (c5.2xlarge).",
    "isCorrect" : "false"
  }, {
    "id" : "385e0c550f584e58b016b8cd14a68832",
    "option" : "A Red Hat Linux instance with Dedicated Tenancy (c5.2xlarge).",
    "isCorrect" : "false"
  }, {
    "id" : "852deb0ee00f4f748c1c768002dfadd4",
    "option" : "All of the above.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOptions A, B and C are incorrect: Because EC2 Instance Savings Plans apply to usage regardless of size, OS, or tenancy within the specified family.\nIn this Savings Plan, the C5 instance family is selected.\nAll instances using the C5 instance type can get a discount.\nOption D is CORRECT: Because the instances in Options A, B and C use the C5 instance type.\nSo all of them are covered by the new Savings Plan.\nAn example of creating a Savings Plan for the c5 instance family is as follows:\nReference:\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html\n\n\nEC2 Instance Savings Plans (ISPs) are a flexible pricing model for EC2 instances that offer significant savings in exchange for a commitment to a specific instance family, region, and term length. ISPs can provide up to 72% discount compared to On-Demand pricing.\nIn this scenario, you have created a new EC2 Instance Savings Plan in the Asia Pacific region (Sydney), and committed to $20/hour of usage for one year, and the instance family is selected as C5.\nThe C5 instance family is optimized for compute-intensive workloads and offers high performance and network throughput. Therefore, to receive a discount, the EC2 instances must belong to the C5 instance family.\nNow, let's examine each option to determine if they meet the criteria for receiving a discount:\nA. An Amazon Linux instance (c5.xlarge): This instance belongs to the C5 instance family and is eligible for the discount.\nB. A Windows instance (c5.2xlarge): This instance also belongs to the C5 instance family and is eligible for the discount.\nC. A Red Hat Linux instance with Dedicated Tenancy (c5.2xlarge): This instance also belongs to the C5 instance family and is eligible for the discount. The dedicated tenancy does not affect the eligibility for the discount.\nD. All of the above: This option is correct since all three instances belong to the C5 instance family and meet the criteria for receiving a discount.\nTherefore, the correct answer is D. All of the above.\n\n"
}, {
  "id" : 533,
  "question" : "A web application is hosted in AWS EC2 and serves global customers.\nAs the application is getting more and more popular, the data transfer cost keeps increasing.\nYou plan to use AWS CloudFront to improve the latency and reduce the cost.\nWhich of the following services is free for CloudFront?\n",
  "answers" : [ {
    "id" : "2e43efe393f34f9894f43b3e78f8f340",
    "option" : "Data transfer out to the Internet from edge locations.",
    "isCorrect" : "false"
  }, {
    "id" : "abbd2eb7c9b6407cb96062aafb25ac02",
    "option" : "Data transfer out of Amazon CloudFront to the origin server.",
    "isCorrect" : "false"
  }, {
    "id" : "031d9c01179d4b5ea2a685e3dee0464b",
    "option" : "Data transfer from origin to CloudFront edge locations (Amazon CloudFront \"origin fetches\").",
    "isCorrect" : "true"
  }, {
    "id" : "330c692f9e414ba08802e45f48950d69",
    "option" : "Custom SSL certificate associated with the CloudFront distribution using the Dedicated IP version of custom SSL certificate support.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nCloudFront is a content delivery network (CDN) service that offers a simple, pay-as-you-go pricing model.\nFor the Amazon CloudFront pricing details, please check the below reference.\nOptions A and B are incorrect: Because the data transfer out to the internet or origin is not free.\nA different rate is charged depending on the region.\nOption C is CORRECT: If the CloudFront origin is hosted in AWS like EC2, the origin fetches are free.\nOption D is incorrect: Because for each custom SSL certificate associated with one or more CloudFront distributions using the Dedicated IP version of custom SSL certificate support, you are charged at $600 per month.\nReference:\nhttps://aws.amazon.com/cloudfront/pricing/\n\nSure, I can explain the details for each option and help you understand which service is free for CloudFront.\nAWS CloudFront is a content delivery network (CDN) service that speeds up the delivery of your static and dynamic web content, such as HTML, CSS, JavaScript, images, and videos. CloudFront provides a global network of edge locations to cache and serve your content from the nearest location to your users, reducing latency and improving performance.\nNow, let's analyze each option:\nA. Data transfer out to the Internet from edge locations. This option refers to the data transfer costs incurred when users access your content through CloudFront's edge locations, which act as proxies for your origin server. CloudFront charges for data transfer out from its edge locations to the Internet. However, CloudFront offers a free tier of 50 GB per month for the first 12 months for new customers. Therefore, for most small-scale applications, the cost of data transfer out to the Internet is negligible.\nB. Data transfer out of Amazon CloudFront to the origin server. This option refers to the data transfer costs incurred when CloudFront retrieves content from your origin server, such as an Amazon EC2 instance or an Amazon S3 bucket. CloudFront charges for data transfer out to the origin server based on the AWS region where the origin is located and the amount of data transferred. This cost is not free, and you will be charged according to the AWS pricing model.\nC. Data transfer from origin to CloudFront edge locations (Amazon CloudFront \"origin fetches\"). This option refers to the data transfer costs incurred when CloudFront fetches content from your origin server for the first time, and then caches it at its edge locations for subsequent requests. CloudFront charges for data transfer from the origin server to its edge locations based on the AWS region where the origin is located and the amount of data transferred. This cost is not free, and you will be charged according to the AWS pricing model.\nD. Custom SSL certificate associated with the CloudFront distribution using the Dedicated IP version of custom SSL certificate support. This option refers to the use of custom SSL certificates for secure connections between CloudFront and your origin server or between CloudFront and your end-users. CloudFront supports two types of SSL certificates: Server Name Indication (SNI) and Dedicated IP. SNI certificates are free, while Dedicated IP certificates require an additional fee of $600/month. Therefore, if you choose to use the SNI version of custom SSL certificates, you will not incur any additional cost.\nIn summary, the only service that is free for CloudFront is the data transfer out to the Internet from its edge locations, up to 50 GB per month for the first 12 months for new customers. The other services, such as data transfer out to the origin server and data transfer from the origin to CloudFront edge locations, as well as the Dedicated IP version of custom SSL certificates, incur additional costs based on the AWS pricing model.\n\n"
}, {
  "id" : 534,
  "question" : "Your application is deployed in EC2 instances and uses CloudFront to deliver the content.\nIn order to reduce the cost of requests to the origin, you plan to increase the cache duration in CloudFront for certain dynamic contents.\nWhich of the following options is the most appropriate to achieve the requirement?\n",
  "answers" : [ {
    "id" : "d70db07c98744dd59d3628d88d32b760",
    "option" : "Modify the application to add a Cache-Control header to control how long the objects stay in the CloudFront cache.",
    "isCorrect" : "true"
  }, {
    "id" : "38745787f478488f86d9f8188ca7179a",
    "option" : "In the CloudFront distribution, enlarge the default TTL in the object caching setting",
    "isCorrect" : "false"
  }, {
    "id" : "ae830830fe6043db8143c6089c9b43fa",
    "option" : "Specify the minimum TTL to be 0 in the CloudFront object caching setting.",
    "isCorrect" : "false"
  }, {
    "id" : "b2bc8b313d20467eaff1d03e81060236",
    "option" : "Add a Cache-Control header by configuring the metadata of the S3 objects.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT: Because the Cache-Control header added by the origin server can be used to control the caching behavior in CloudFront.\nThe application is responsible for adding the header for individual objects.\nOption B is incorrect: Because the default TTL impacts all objects if the origin does not add the cache header.\nHowever, in the question, only certain dynamic contents need to be modified.\nOption C is incorrect: Because the cache duration is not increased by setting the value of Minimum TTL to be 0\nPlease check the below reference on how to determine the amount of time that CloudFront keeps an object in the cache.\nOption D is incorrect: The question mentions that the cache duration of dynamic contents needs to be increased.\nHowever, S3 objects belong to static contents.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Expiration.html.\n\nThe correct answer to this question is B. In the CloudFront distribution, enlarge the default TTL in the object caching setting.\nCloudFront is a Content Delivery Network (CDN) that accelerates the delivery of your content to users. It caches your content at edge locations around the world, so your users can access it with low latency. The cache duration for an object in CloudFront is called the Time To Live (TTL), and it specifies how long the object stays in the cache before CloudFront retrieves a new version from the origin.\nTo reduce the cost of requests to the origin for certain dynamic content, you can increase the cache duration in CloudFront. By doing this, CloudFront will serve the content from its cache for a longer time, reducing the number of requests to the origin.\nThere are several ways to control the cache duration in CloudFront. One of them is to modify the application to add a Cache-Control header to control how long the objects stay in the CloudFront cache. However, this requires modifying the application code, which may not be feasible or desirable.\nAnother way is to add a Cache-Control header by configuring the metadata of the S3 objects. This approach works if your content is stored in S3 and served by CloudFront. However, it requires setting the headers for each object individually, which can be time-consuming and error-prone.\nThe most appropriate option to achieve the requirement is to enlarge the default TTL in the object caching setting of the CloudFront distribution. This option is easy to implement and doesn't require any modification to the application or the objects. You can simply go to the CloudFront distribution settings, select the behavior that serves the dynamic content, and increase the default TTL in the object caching setting. This will increase the cache duration for all objects served by that behavior.\nSpecifying the minimum TTL to be 0 in the CloudFront object caching setting (option C) is not appropriate for dynamic content because it would disable caching altogether, defeating the purpose of using a CDN.\nTherefore, the correct answer is B. In the CloudFront distribution, enlarge the default TTL in the object caching setting.\n\n"
}, {
  "id" : 535,
  "question" : "You use CloudFormation to create an Auto Scaling group for a web application.\nThe application needs to be deployed in both non-production and production AWS accounts.\nYou want to use Spot Instances in the non-production environment to save costs.\nWhich of the following methods would you choose?\n",
  "answers" : [ {
    "id" : "9ad474aca50548588dfd9f188a7c80b9",
    "option" : "In the CloudFormation template, use a variable to set the OnDemandPercentageAboveBaseCapacity property. Set the variable to be 100 in non-production and 0 in production.",
    "isCorrect" : "false"
  }, {
    "id" : "f41f26562bda46c8a7c167b40846f9c4",
    "option" : "In the CloudFormation template, use a parameter to set the OnDemandPercentageAboveBaseCapacity property. Set the parameter to be 0 in non-production and 100 in production.",
    "isCorrect" : "true"
  }, {
    "id" : "f59bcb44d4634881b99d5443ae9f45b7",
    "option" : "In the CloudFormation template, use a parameter for the SpotMaxPrice property. Set the parameter to be 100 in non-production and 0 in production.",
    "isCorrect" : "false"
  }, {
    "id" : "9e17f583f067451fb3bf5dcfecac3bcb",
    "option" : ". In the CloudFormation template, use a parameter for the SpotMaxPrice property. Set the parameter to be 0.5 in non-production and the On-Demand price in production.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect: Because OnDemandPercentageAboveBaseCapacity determines the percentages of On-Demand Instances and Spot Instances beyond OnDemandBaseCapacity.\nThe value should be 100 in production as no Spot Instances should be used in production.\nOption B is CORRECT: As the value of OnDemandPercentageAboveBaseCapacity is 0 in non-production, Spot Instances will be used.\nFor the production environment, On-Demand Instances are launched as OnDemandPercentageAboveBaseCapacity is 100.\nOption C is incorrect: SpotMaxPrice determines the maximum price that you are willing to pay for Spot Instances.\nIt does not decide the percentage between Spot and On-Demand instances.\nOption D is incorrect: In this scenario, OnDemandPercentageAboveBaseCapacity should be configured to launch Spot Instances in non-production and On-Demand Instances in production.\nReference:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-purchase-options.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-autoscaling-autoscalinggroup-instancesdistribution.html#cfn-autoscaling-autoscalinggroup-instancesdistribution-spotmaxprice\n\nSure, I'll be happy to explain the answer choices to you.\nCloudFormation is an AWS service that allows you to create and manage AWS resources in a repeatable and automated way. It uses templates to define the infrastructure and configurations required to deploy and run your application. An Auto Scaling group is a feature of Amazon EC2 that automatically scales your EC2 instances up or down based on demand.\nSpot Instances are a cost-effective way to run your applications on the AWS cloud. They provide compute capacity at a significantly lower price than On-Demand instances, but the availability of the capacity is subject to change based on supply and demand. In this scenario, we want to use Spot Instances in the non-production environment to save costs.\nLet's take a look at each answer choice and see which one is the best fit for this scenario.\nA. In the CloudFormation template, use a variable to set the OnDemandPercentageAboveBaseCapacity property. Set the variable to be 100 in non-production and 0 in production.\nThe OnDemandPercentageAboveBaseCapacity property specifies the percentage of On-Demand instances to launch above the base capacity of the Auto Scaling group. This option would allow us to launch all instances as On-Demand instances in production and all instances as Spot instances in non-production. However, using variables in CloudFormation templates is not a recommended best practice because it can lead to errors and make the templates harder to read and understand.\nB. In the CloudFormation template, use a parameter to set the OnDemandPercentageAboveBaseCapacity property. Set the parameter to be 0 in non-production and 100 in production.\nThis option is similar to the previous one but uses parameters instead of variables. This is a better option because parameters are a recommended best practice in CloudFormation templates. However, this option would launch all instances as Spot instances in non-production and all instances as On-Demand instances in production, which is the opposite of what we want.\nC. In the CloudFormation template, use a parameter for the SpotMaxPrice property. Set the parameter to be 100 in non-production and 0 in production.\nThe SpotMaxPrice property specifies the maximum price that you are willing to pay for a Spot instance. This option would allow us to set the maximum price to be very high in non-production, which would effectively launch all instances as On-Demand instances. In production, we would set the maximum price to be 0, which would effectively launch all instances as Spot instances. This option is a good fit for this scenario.\nD. In the CloudFormation template, use a parameter for the SpotMaxPrice property. Set the parameter to be 0.5 in non-production and the On-Demand price in production.\nThis option sets the maximum price to be 0.5 in non-production, which would launch instances as Spot instances as long as the current Spot price is less than or equal to 0.5. If the current Spot price is higher than 0.5, instances would not launch. In production, the maximum price would be set to the On-Demand price, which would launch all instances as On-Demand instances. This option is also a good fit for this scenario.\nTherefore, the best answer choices for this scenario are C and D. Both options allow us to launch Spot instances in non-production and On-Demand instances in production. Option C is simpler because it only requires setting the SpotMaxPrice parameter, while option D requires setting both the SpotMaxPrice and On-Demand price parameters.\n\n"
}, {
  "id" : 536,
  "question" : "You are working as a Solutions Architect in an organization.\nYou have peered VPC A and VPC B as a requester and an acceptor where both the VPCs can communicate with each other.\nNow you want the resources in the private subnets of both the VPCs to reach out to the internet.\nBut no one on the internet should be able to reach the resources within both the VPCs.\nWhich of the below will achieve the desired outcome?\n",
  "answers" : [ {
    "id" : "1f62ce9b0158485d9dde3236084a0426",
    "option" : "Create a NAT Gateway in VPC A and route VPC B`s outbound to VPC A`s NAT Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "3ecab59f5e104b4d984b57f220468400",
    "option" : "Create an Internet Gateway in VPC A and route VPC B`s outbound to VPC A`s Internet Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "b827df7b26504337b490a47db7c8b513",
    "option" : "Create NAT Gateways in both VPCs and configure routes for each VPC to use its own NAT Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "26132c193b4e42e7b77ce765c5e38c93",
    "option" : "Simply create a NAT Instance in VPC",
    "isCorrect" : "false"
  }, {
    "id" : "ef4e67cc2fab48ea9eb90d96792741ee",
    "option" : "Nothing else is required.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C\nOption A is INCORRECT because you can't share NAT gateways across VPCs.\nOption B is INCORRECT because attaching an IGW to a VPC allows instances with public IPs to access the internet.\nIn contrast, NATs allow instances with no public IPs to access the internet.\nOption C is CORRECT because you can create NAT Gateways on both VPC's and configure routes to the NAT Gateways in the respective route tables.\nOption D is INCORRECT because a stand-alone NAT Instance in VPC A doesn't accomplish anything.\nFor more information:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-comparison.html\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html#nat-\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\n\nTo allow resources in private subnets of both VPCs to reach out to the internet but not allow anyone on the internet to reach the resources within both VPCs, we can use a combination of an Internet Gateway and NAT Gateways.\nThe correct answer is (C) Create NAT Gateways in both VPCs and configure routes for each VPC to use its own NAT Gateway.\nHere is the detailed explanation of why this is the correct answer:\nNAT Gateways: A NAT Gateway allows resources in a private subnet to access the internet while keeping them hidden behind the NAT Gateway's public IP address. This means that internet resources cannot directly initiate a connection to the resources in the private subnet. Internet Gateway: An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the internet. VPC Peering: VPC Peering allows communication between VPCs using private IP addresses. Once VPCs are peered, they can communicate with each other as if they were on the same network.\nBased on the above explanation, let's now examine each of the given options:\nA. Create a NAT Gateway in VPC A and route VPC Bs outbound to VPC As NAT Gateway. This option will only allow resources in VPC A to access the internet through its NAT Gateway. Resources in VPC B won't be able to access the internet.\nB. Create an Internet Gateway in VPC A and route VPC Bs outbound to VPC As Internet Gateway. This option will allow resources in both VPCs to access the internet through VPC A's Internet Gateway, but it will also allow internet resources to directly initiate connections to resources in both VPCs. This option doesn't meet the requirement that internet resources should not be able to reach the resources within both VPCs.\nC. Create NAT Gateways in both VPCs and configure routes for each VPC to use its own NAT Gateway. This option is the correct answer as it allows resources in both VPCs to access the internet through their respective NAT Gateways while keeping them hidden behind the NAT Gateway's public IP address. This also ensures that internet resources cannot directly initiate a connection to the resources in the private subnets of both VPCs.\nD. Simply create a NAT Instance in VPC. A NAT Instance is not the recommended option for this scenario as it is not as scalable, available, and easy to manage as a NAT Gateway.\nE. Nothing else is required. This is not the correct answer as we need to configure NAT Gateways to allow resources in both VPCs to access the internet while keeping them hidden behind the NAT Gateway's public IP address.\nTherefore, option (C) Create NAT Gateways in both VPCs and configure routes for each VPC to use its own NAT Gateway is the correct answer for this scenario.\n\n"
}, {
  "id" : 537,
  "question" : "Your organization already had a VPC(10.10.0.0/16) setup with one public(10.10.1.0/24) and two private subnets - private subnet 1 (10.10.2.0/24) and private subnet 2 (10.10.3.0/24)\nThe public subnet has the main route table, and two private subnets have two different route tables respectively.\nAWS sysops team reports a problem starting the EC2 instance in private subnet 1 cannot communicate to the RDS MySQL database on private subnet 2\nWhat are the possible reasons? (choose 2 options)\n",
  "answers" : [ {
    "id" : "306a182ae05e4a90b35ba3a7687346e9",
    "option" : "One of the private subnet route tableâ€™s local route has been changed to restrict access only within the subnet IP range.",
    "isCorrect" : "false"
  }, {
    "id" : "277ab6ac3f284b8c97c18aea5c275a9a",
    "option" : "RDS security group inbound rule is incorrectly configured with 10.10.1.0/24 instead of 10.10.2.0/24.",
    "isCorrect" : "true"
  }, {
    "id" : "6d3d2af2f66741eb8eda603c84687da4",
    "option" : "10.10.3.0/24 subnet`s NACL is modified to deny inbound on port 3306 from subnet 10.10.2.0/24",
    "isCorrect" : "true"
  }, {
    "id" : "b9a037a63d344f9a89f04e6f00399314",
    "option" : "RDS Security group outbound does not contain a rule for ALL traffic or port 3306 for 10.10.2.0/24 IP range.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B, C.\nFor.\nOption A, for any route table, the local route cannot be edited or deleted.\nAWS Docs says:\n\"Every route table contains a local route for communication within the VPC over IPv4\nIf your VPC has more than one IPv4 CIDR block, your route tables contain a local route for each IPv4 CIDR block.\nIf you've associated an IPv6 CIDR block with your VPC, your route tables contain a local route for the IPv6 CIDR block.\nYou cannot modify or delete these routes.\"\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html#RouteTa\nFor Option B, possible because the security group is configured with a public subnet IP range instead of a private subnet 1 IP range, and EC2 is in private subnet 1\nSo EC2 will not be able to communicate with RDS in private subnet 2.\nOption C is correct.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html#default-network-\nOption D is not correct because Security Groups are stateful.\nIf you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules.\nResponses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html#VPCSecurityGroups\n\n\nThe problem is that an EC2 instance in private subnet 1 cannot communicate with an RDS MySQL database in private subnet 2. To determine the possible reasons, we need to consider the network configurations and security group settings for both subnets.\nOption A: One of the private subnet route table's local route has been changed to restrict access only within the subnet IP range. Explanation: If the local route for one of the private subnets is changed to restrict access only within the subnet IP range, it will prevent communication between the two private subnets. The EC2 instance in private subnet 1 will not be able to reach the RDS MySQL database in private subnet 2. This could be a possible reason for the issue.\nOption B: RDS security group inbound rule is incorrectly configured with 10.10.1.0/24 instead of 10.10.2.0/24. Explanation: If the RDS security group inbound rule is configured with the wrong subnet IP range (10.10.1.0/24 instead of 10.10.2.0/24), the EC2 instance in private subnet 1 will not be able to communicate with the RDS MySQL database in private subnet 2. This could be a possible reason for the issue.\nOption C: 10.10.3.0/24 subnet's NACL is modified to deny inbound on port 3306 from subnet 10.10.2.0/24 Explanation: If the network access control list (NACL) for the private subnet 2 (10.10.3.0/24) is modified to deny inbound traffic on port 3306 from the private subnet 1 (10.10.2.0/24), the EC2 instance in private subnet 1 will not be able to communicate with the RDS MySQL database in private subnet 2. This could be a possible reason for the issue.\nOption D: RDS Security group outbound does not contain a rule for ALL traffic or port 3306 for 10.10.2.0/24 IP range. Explanation: If the RDS security group outbound rule does not contain a rule for all traffic or port 3306 for the 10.10.2.0/24 IP range, the EC2 instance in private subnet 1 will not be able to communicate with the RDS MySQL database in private subnet 2. However, this option is less likely to be the reason for the issue since the problem is with the inbound traffic from the EC2 instance to the RDS database.\nTherefore, options A and B are the most likely reasons for the problem. The solution would be to check the local route of the private subnets and the inbound rules of the RDS security group to ensure that they are configured correctly.\n\n"
}, {
  "id" : 538,
  "question" : "A new VPC with CIDR range 10.10.0.0/16 has been set up with a public and a private subnet.\nInternet Gateway and a custom route table have been created, and a route has been added with the 'Destination' as '0.0.0.0/0' and the 'Target' with Internet Gateway ( igw-id )\nA new Linux EC2 instance has been launched on the public subnet with the auto-assign public IP option enabled, but the connection is getting failed when trying to SSH into the machine.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "3d96b75f73594bc2b725ea2f2256c8d9",
    "option" : "Elastic IP is not assigned.",
    "isCorrect" : "false"
  }, {
    "id" : "09c4d2e13d51493e8760c3b9fda94861",
    "option" : "The NACL of the public subnet disallows the ingress SSH traffic.",
    "isCorrect" : "true"
  }, {
    "id" : "6f6d65e42c314369b3a11d28099fa627",
    "option" : "A public IP address is not assigned.",
    "isCorrect" : "false"
  }, {
    "id" : "0125f48c5c3b452ca770602490079ddb",
    "option" : "The Security group of the instance disallows the egress traffic on port 80.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect.\nAn Elastic IP address is a public IPv4 address with which you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.\nIf your instance does not have a public IPv4 address, you can associate an Elastic IP address with your instance to enable communication with the internet; for example, to connect to your instance from your local computer.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html#eip-\nFrom our problem statement, EC2 is launched with Auto-assign public IP enabled.\nSo, since public IP is available, Elastic IP is not necessary to connect from the internet.\nOption C is incorrect because the problem statement clearly states that EC2 is launched with Auto-assign Public IP enabled, so this option cannot be true.\nOption B is CORRECT as the NCL may not allow the ingress SSH traffic so that the connection failed to connect.\nOption D is incorrect because SSH uses port 22\nA security group is stateful and in this scenario, the security group may disallow the ingress SSH traffic instead of egress.\n\nThe most likely reason for the failed connection when trying to SSH into the new Linux EC2 instance launched in the public subnet is option B: The NACL of the public subnet disallows the ingress SSH traffic.\nHere's why:\nElastic IP is not assigned: This is not the reason for the failed connection as it's not required to assign an Elastic IP to the instance in order to SSH into it. The instance can be accessed using its public IP address. The NACL of the public subnet disallows the ingress SSH traffic: Network Access Control Lists (NACLs) are stateless and operate at the subnet level. By default, all inbound and outbound traffic is allowed. If there is no NACL associated with the subnet, then this cannot be the reason for the failed connection. However, if there is a NACL associated with the public subnet, then it is likely that the SSH traffic is being blocked by the NACL. In this case, a custom NACL rule would need to be created to allow SSH traffic on port 22. A public IP address is not assigned: This could be the reason for the failed connection if the instance was not launched with the \"Auto-assign Public IP\" option enabled. However, the question specifically states that this option was enabled, so this cannot be the reason for the failed connection. The Security group of the instance disallows the egress traffic on port 80: This cannot be the reason for the failed connection as egress traffic refers to outbound traffic, and SSH traffic is inbound traffic. Therefore, the security group egress rules would not affect the SSH connection to the instance.\n\n"
}, {
  "id" : 539,
  "question" : "You are an AWS architect in your organization.\nYour organization would want to upload files to the AWS S3 bucket.\nIn a VPC, you create a private subnet and VPC endpoint for S3\nYou also create one route table that routes the traffic from the private subnet to a NAT gateway for the internet access.\nIn AWS S3 server logs, you notice that the requests to the S3 bucket from an EC2 instance in the VPC do not go through the NAT Gateway.\nWhat could cause this situation?\n",
  "answers" : [ {
    "id" : "ef00d75fe5894bf1abcede7aae53cf3c",
    "option" : "When NAT Gateway and VPC endpoint exist on same route table, NAT Gateway always takes precedence.",
    "isCorrect" : "false"
  }, {
    "id" : "7bc1f5dd0150462f9fe4d2d4b33759e7",
    "option" : "EC2 instance has an elastic IP address associated with it.",
    "isCorrect" : "false"
  }, {
    "id" : "f3f57cb6bf894c18828a2f041aeb4088",
    "option" : "The requests are routed through the VPC endpoint.",
    "isCorrect" : "true"
  }, {
    "id" : "f433c602554d4653b21d056f51750f4d",
    "option" : "AWS S3 is a managed service so all requests will always go through the internal AWS network.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect.\nVPC Endpoint always takes precedence over NAT Gateway or Internet Gateway.\nIn the absence of a VPC endpoint, requests to S3 are routed to NAT Gateway or Internet Gateway based on their existence in the route table.\nOption B is incorrect.\nThe elastic IP address is an IPv4 public address assigned to the instance.\nIt does not explain the situation.\nOption C is CORRECT.\nThe traffic goes through the S3 VPC endpoint so the NAT Gateway is not used in this scenario.\nOption D is incorrect.\nIf the S3 VPC endpoint is not configured, the requests will go through the internet.\n\nThe correct answer to the given question is option A: When NAT Gateway and VPC endpoint exist on the same route table, NAT Gateway always takes precedence.\nExplanation: AWS S3 bucket is a service provided by Amazon Web Services for storing and retrieving data. Amazon VPC (Virtual Private Cloud) is a service that enables the users to launch AWS resources into a virtual network that they have defined. Amazon VPC endpoints for Amazon S3 provide secure and private connectivity between the VPC and S3. AWS NAT Gateway is a highly available AWS-managed service that makes it easy to connect instances in a private subnet to the Internet.\nWhen a user wants to access S3 from an EC2 instance in a VPC, there are two possible ways:\nThrough a NAT Gateway: In this case, the EC2 instance sends a request to the NAT Gateway, which forwards it to the S3 bucket through the internet gateway. The response then goes back to the NAT Gateway and finally to the EC2 instance. Through a VPC endpoint: In this case, the EC2 instance sends a request to the VPC endpoint for S3, which is directly connected to the S3 bucket over the Amazon network. The response then goes back to the EC2 instance through the same endpoint.\nIn the given scenario, the VPC has a private subnet and a VPC endpoint for S3. A route table is also created that routes the traffic from the private subnet to a NAT gateway for internet access. However, the logs show that the requests to the S3 bucket from an EC2 instance in the VPC do not go through the NAT Gateway.\nThis situation occurs because when a NAT Gateway and VPC endpoint exist on the same route table, the NAT Gateway always takes precedence. This means that all traffic destined for S3 will always be routed through the NAT Gateway, even if there is a VPC endpoint available. Therefore, in this scenario, the requests from the EC2 instance will always go through the NAT Gateway and not through the VPC endpoint.\nOption B is incorrect because an elastic IP address associated with the EC2 instance does not affect the routing of traffic to S3.\nOption C is incorrect because if the requests were routed through the VPC endpoint, it would be reflected in the AWS S3 server logs.\nOption D is incorrect because although AWS S3 is a managed service, it is still accessible through the internet and can be accessed through a VPC endpoint or NAT Gateway.\nIn conclusion, when both NAT Gateway and VPC endpoint exist on the same route table, the NAT Gateway takes precedence, and all traffic destined for S3 will be routed through the NAT Gateway.\n\n"
}, {
  "id" : 540,
  "question" : "Your organization has an existing VPC with an AWS S3 VPC endpoint created and serving certain S3 buckets.\nYou were asked to create a new S3 bucket and reuse the existing VPC endpoint to route requests to the new S3 bucket.\nHowever, after creating a new S3 bucket and sending requests from an EC2 instance via the VPC endpoint, you found the requests are failing with the â€œAccess Deniedâ€ error.\nWhat could be the issue? (select 2 options)\n",
  "answers" : [ {
    "id" : "d9fc4a4943d248f482b4e1c9b3116489",
    "option" : "VPC endpoint contains a policy, currently restricted to certain S3 buckets, and does not contain a new S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "dfdcc69e692f4eeeaf000111d6af706c",
    "option" : "AWS IAM role/user does not have access to the new S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "008a405027db41c4964c8e3466c7a3bf",
    "option" : "AWS default DENY policy restricts access to IAM user/role who already has access to the S3 bucket",
    "isCorrect" : "false"
  }, {
    "id" : "16447f72ca1f421c91bf5968ed3ada3e",
    "option" : "You need to add a new S3 bucket hostname as destination and VPC endpoint ID as target in route table in order to route requests to the new S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B.\nOption A is correct.\nThe VPC endpoint has a policy which by default allows all actions on all S3 buckets.\nWe can restrict access to certain S3 buckets and certain actions on this policy.\nIn such cases, for accessing any new buckets or for any new actions, the VPC endpoint policy needs to be modified accordingly.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints-access.html#vpc-\nOption B is correct.\nThe AWS IAM role/user used to access the S3 bucket needs to have access granted via IAM policy before accessing.\nSo if the IAM role/user is not an administrator or has full S3 access, a newly created S3 bucket must be added to the IAM policy.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/s3-access-control.html\nOption C is incorrect.\nBy default, there is no resource policy on the S3 bucket.\nIf we would like to make the bucket private, we can add a new resource policy with â€œDeny.â€ Please see the documentation for more information.\nhttps://aws.amazon.com/blogs/security/how-to-create-a-policy-that-whitelists-access-to-\nOption D is incorrect, You can have multiple endpoint routes to different services in a route table, and you can have multiple endpoint routes to the same service in different route tables.\nStill, you cannot have multiple endpoints to the same service in a single route table.\nFor example, if you have two endpoints to Amazon S3 in your VPC, you cannot use the same route table for both endpoints.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html#vpc-endpoints-limitations\n\nThe possible issues causing the \"Access Denied\" error when trying to access a new S3 bucket through an existing VPC endpoint are:\nA. VPC endpoint policy restriction: The VPC endpoint may have a policy attached to it that restricts access to certain S3 buckets only. This policy might not contain the new S3 bucket, causing the \"Access Denied\" error. To fix this issue, you need to modify the VPC endpoint policy and include the new S3 bucket in the allowed list.\nD. Incorrect route table configuration: To route requests to the new S3 bucket through the VPC endpoint, you need to add a new S3 bucket hostname as the destination and VPC endpoint ID as the target in the route table. If this configuration is missing or incorrect, it can cause the \"Access Denied\" error. To fix this issue, you need to check the route table configuration and update it if necessary.\nB. AWS IAM role/user access: If the AWS IAM role/user that is trying to access the new S3 bucket does not have the required permissions, it can cause the \"Access Denied\" error. To fix this issue, you need to ensure that the IAM role/user has the necessary permissions to access the new S3 bucket.\nC. Default DENY policy: If an AWS default DENY policy is restricting access to the IAM user/role who already has access to the S3 bucket, it can cause the \"Access Denied\" error. To fix this issue, you need to modify the policy and allow access to the IAM user/role. However, this option is less likely to be the cause of the \"Access Denied\" error in the given scenario, as it is not common to use default DENY policies in AWS.\nIn summary, the most likely issues causing the \"Access Denied\" error in the given scenario are either the VPC endpoint policy restriction or the incorrect route table configuration. Therefore, you should first check these configurations and update them if necessary. If these configurations are correct, you should also check the IAM user/role access and default DENY policies to rule out any other possible issues.\n\n"
}, {
  "id" : 541,
  "question" : "A company has its major applications deployed in AWS.\nThe company is building a new office and requires a high-performance network connection between the local office network and the AWS network.\nThe connection needs to have high bandwidth throughput and allow users in the office to connect with multiple AWS VPCs of multiple AWS Regions.\nHow would you establish the connection in the most appropriate way?\n",
  "answers" : [ {
    "id" : "3c7fa180897547de95741ee6cea0ccc5",
    "option" : "For each AWS Region, create an AWS Direct Connect by configuring a public VIF between the VPC Virtual Private Gateway and the Customer Router.",
    "isCorrect" : "false"
  }, {
    "id" : "c8a90bbe12e544f4926026b95107d78c",
    "option" : "Create a Direct Connect Gateway to connect the local network with multiple Amazon VPCs across different regions.",
    "isCorrect" : "true"
  }, {
    "id" : "78a3fc6ce4e043b7a858c72fceb4a13b",
    "option" : "Configure two Direct Connects with two private VIFs to provide highly-available and dedicated private connections.",
    "isCorrect" : "false"
  }, {
    "id" : "9f5a0d5bcc1944c191528c942e80129f",
    "option" : "Create an AWS Direct Connect dedicated network connection on top of Amazon VPN to establish an end-to-end secure IPSec connection.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect because in this option users need to configure an AWS Direct Connect for each AWS Region, which is not the most appropriate method.\nBesides, private VIF should be set up in Direct Connection instead of public VIF.\nPrivate virtual interface is used to access Amazon VPC using private IP addresses and public virtual interface is used to access AWS public services.\nIn this scenario, the connections need to be secured, so private VIF should be used.\nOption B is CORRECT because Direct Connect Gateway, as a globally available resource, can be used to establish high-performance network connections to different AWS Regions and reduce management loads.\nPlease check the following figure:\nOption C is incorrect because this option is suitable for scenarios in which highly available and redundant connections are needed.\nThere is no such requirement in the question.\nOption D is incorrect because the question does not require a secure IPSec connection therefore VPN is not needed.\nThis option also does not address the requirements to connect with different AWS Regions.\nReferences:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gateways-intro.html\n\n\nThe most appropriate way to establish a high-performance network connection between the local office network and the AWS network with high bandwidth throughput and allow users in the office to connect with multiple AWS VPCs of multiple AWS Regions is to use Direct Connect Gateway. Therefore, answer B is the correct answer.\nDirect Connect Gateway is a highly-available, regional service that allows you to connect multiple VPCs in different regions to a Direct Connect connection at your on-premises location or colocation environment. With Direct Connect Gateway, you can establish connectivity to all your VPCs in the region, rather than setting up multiple Direct Connect connections. This provides a more efficient and cost-effective way to access your resources on AWS.\nAnswer A is incorrect because creating an AWS Direct Connect for each region can be costly and inefficient, as it requires multiple connections to be established for each region.\nAnswer C is incorrect because configuring two Direct Connects with two private VIFs will only provide highly available and dedicated private connections for two VPCs.\nAnswer D is incorrect because creating an AWS Direct Connect dedicated network connection on top of Amazon VPN is not necessary since Direct Connect already provides a secure and private connection between your data center and AWS.\n\n"
}, {
  "id" : 542,
  "question" : "Your organization was looking to download patches onto an existing EC2 instance inside a private subnet in an existing custom VPC.\nYou created a NAT Gateway.\nHowever, when you try to download patches from the internet onto the EC2 instance, the connection gets timed out.\nWhat could be the reason? (Select TWO)\n",
  "answers" : [ {
    "id" : "75e6fdc9889a4c49b04ea386a6897b87",
    "option" : "NAT Gateway created in a private subnet without an Internet Gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "8b55d41c5e8a44d5afb818aef621b508",
    "option" : "NAT Gateway is created without an Elastic IP Address.",
    "isCorrect" : "false"
  }, {
    "id" : "a6966cb0bc4342308d6247141941f484",
    "option" : "The route table is NOT updated to direct Internet-bound traffic to the NAT gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "890875f1f0134b63b36140a1b6067be9",
    "option" : "NAT Gatewayâ€™s Security Group inbound rules do not allow traffic from EC2 instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nFor.\nOption A, when creating NAT Gateway, there is an option to select a subnet in which NAT Gateway will be created.\nThis must be a public subnet that has a route to the internet through Internet Gateway.\nIf a private subnet is selected when creating NAT Gateway, it cannot route traffic to the internet, and hence the requests would fail.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/nat-gateway-vpc-private-subnet/\nFor Option B, NAT Gateway cannot be created without an elastic IP address.\nDuring the creation of NAT Gateway, Elastic IP Allocation ID is a mandatory field without which we cannot proceed to create NAT Gateway.\nSo this option is incorrect.\nFor Option C, there might be a chance that the route table has not been updated to direct Internet-bound traffic to the NAT gateway.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html#VPCSe\nFor Option D, NAT Gateways does not have security groups.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html#nat-\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html\nAs per AWS documentation,\nInstances in Private Subnet Cannot Access internet.\nCheck that the NAT gateway is in the Available state.\nIn the Amazon VPC console, go to the NAT Gateways page and view the status information in the details pane.\nIf the NAT gateway is in a failed state, there may have been an error when created.\nCheck that you've configured your route tables correctly:\nThe NAT gateway must be in a public subnet with a routing table that routes internet traffic to an internet gateway.\nYour instance must be in a private subnet with a routing table that routes internet traffic to the NAT gateway.\nCheck that there are no other route table entries that route all or part of the internet traffic to another device instead of the NAT gateway.\nThe NAT gateway allows all outbound traffic and traffic received in response to an outbound request (it is therefore stateful).\nReference:\nhttps://aws.amazon.com/premiumsupport/knowledge-center/nat-gateway-vpc-private-subnet/\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html\n\n\nThe most likely reasons for the connection time-out issue while downloading patches from the internet onto an EC2 instance inside a private subnet in an existing custom VPC with NAT Gateway could be:\nA. NAT Gateway created in a private subnet without an Internet Gateway: NAT Gateway allows instances in a private subnet to connect to the internet and receive inbound traffic from the internet, but it requires an Internet Gateway to be attached to the VPC. Without an Internet Gateway, traffic from the NAT Gateway to the internet cannot be established, which could cause the time-out issue.\nB. NAT Gateway is created without an Elastic IP Address: Elastic IP Address is a static, public IPv4 address that can be associated with a NAT Gateway. It provides a consistent public IP address for instances in the private subnet to connect to the internet. If a NAT Gateway is not created with an Elastic IP Address, instances in the private subnet cannot connect to the internet, which could cause the time-out issue.\nC. The route table is NOT updated to direct Internet-bound traffic to the NAT gateway: To enable instances in a private subnet to access the internet, the route table for the subnet needs to be updated to route internet-bound traffic through the NAT Gateway. If the route table is not updated, the instance's outbound traffic will not be directed to the NAT Gateway, which could cause the time-out issue.\nD. NAT Gateway's Security Group inbound rules do not allow traffic from EC2 instance: Security Groups are virtual firewalls that control inbound and outbound traffic for EC2 instances. If the Security Group inbound rules are not set up to allow traffic from the EC2 instance to the NAT Gateway, the instance will not be able to connect to the internet, which could cause the time-out issue.\nTherefore, options A and C are the most likely reasons for the time-out issue. Option B is also a possible cause, but it is less likely to be the root cause. Option D is not a likely cause of the time-out issue as Security Group inbound rules are not relevant to outbound connections initiated by the EC2 instance.\n\n"
}, {
  "id" : 543,
  "question" : "Your organization is planning to develop a web application containing a Web Server and an RDS Instance.\nThis application will be accessed from the internet.\nYour organization asked you to architect the solution on AWS.\nYour existing AWS environment already has a VPC with a private subnet and public subnet which has a route to the internet through Internet Gateway.\nWhat would be the best and cost-effective solution you can provide?\n",
  "answers" : [ {
    "id" : "8f7b0db8493047dd9aac3224ae750a62",
    "option" : "A bastion host in public subnet, Web Server EC2 in private subnet, RDS instance in private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "402527dd35524e41a869e482a4e39c09",
    "option" : "A bastion host in public subnet, Web Server EC2 in public subnet with Elastic IP, RDS instance in private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "9617bea3ee814ed59b63fc586d80ee71",
    "option" : "A Bastion host in public subnet, Web Server EC2 in private subnet with NAT Gateway, RDS instance in private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "f51d57f5528f4564a233d3a95cfbae89",
    "option" : "Web Server EC2 in public Subnet with Elastic IP, RDS instance in private subnet.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nAn Elastic IP address is a static, public IPv4 address designed for dynamic cloud computing.\nWith an Elastic IP address, you can mask the failure of an instance by rapidly remapping the address to another instance in your VPC.Option A, EC2 instances in the private subnet cannot be reached from the internet.\nA bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet.\nIt does not act as a proxy to route traffic from the internet to private EC2 instances.\nAWS Document says:\nAmazon VPC enables you to launch AWS resources on a virtual private network that you have defined.\nThe bastion host runs on an Amazon EC2 instance, typically in a public subnet of your Amazon VPC.\nLinux instances are in a subnet that is not publicly accessible.\nThey are set up with a security group that allows SSH access from the security group attached to the underlying EC2 instance running the bastion host.\nBastion host users connect to the bastion host to connect to the Linux instances, as illustrated in the following diagram.\nOption B, with EC2 instance in the public subnet and Elastic IP attached, traffic from the internet can reach Web Server, and application works well.\nAlthough this option looks correct, this is not cost-effective since there is no use of Bastion host anywhere since the EC2 instance is already in the public subnet.\nOption C, Same as option A.\nAlthough we have NAT Gateway attached to the subnet where Web Server EC2 resides, the traffic from the internet cannot reach the EC2, and NAT Gateway only routes traffic from AWS resources within a VPC to the internet.\nAny traffic from the internet into VPC resources is not allowed by NAT Gateway.\nOption D, the Web Server EC2 instance is in public subnet with elastic IP address attached to it and RDS in private subnet which cannot be reached from the internet but only can allow traffic from EC2 in public subnet via security groups.\nFor more information on the Elastic IP address, please refer to the documentation.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html\n\n\nThe best and cost-effective solution would be option C.\nExplanation:\nOption A: This option proposes to place the Web Server EC2 instance in the private subnet, which provides better security, but the problem is that there is no way to access it from the internet. So, to manage the server, a Bastion host is proposed to be placed in the public subnet. However, this solution adds additional costs as a separate EC2 instance needs to be launched, and also it may have security concerns.\nOption B: This option places the Web Server EC2 instance in the public subnet, which means it can be directly accessed from the internet. However, this is not a secure way of implementing it. So, to access it securely, a Bastion host is proposed to be placed in the public subnet, which again adds additional costs as a separate EC2 instance needs to be launched.\nOption C: This option places the Web Server EC2 instance in the private subnet with a NAT Gateway, which allows outbound internet traffic from the private subnet. This option is secure because the Web Server is not directly exposed to the internet. To manage the server, a Bastion host is proposed to be placed in the public subnet, which can be accessed from the internet.\nOption D: This option places the Web Server EC2 instance in the public subnet, which means it can be directly accessed from the internet. However, this is not a secure way of implementing it. So, it is not a recommended solution.\nIn summary, option C is the best and cost-effective solution. The Web Server EC2 instance can be placed in the private subnet with a NAT Gateway, and a Bastion host can be placed in the public subnet to manage it securely. The RDS instance can also be placed in the private subnet, which provides better security.\n\n"
}, {
  "id" : 544,
  "question" : "You are building a fleet of EC2 Linux Instances in the AWS environment to manage heavy workloads and write data into AWS Redshift.\nThe developers and administrators need to login to these EC2 machines to develop, fix, deploy, and manage workloads within your organizational network ONLY.\nWhich of the following would allow only the personnel within the organization to access the resources most securely?\n",
  "answers" : [ {
    "id" : "2f87b6c83a014244a89521d24a07a8fa",
    "option" : "EC2 instances on public subnet with secure SSH keys to login, RedShift in private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "f4c7c6a6fca04da0a28bcb807baadc01",
    "option" : "A bastion host in public subnet with secure SSH key to login, EC2 instances in private subnet with secure SSH keys to login, RedShift in private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "b17dd6d0d341445aaa6d57fb0528b402",
    "option" : "AWS VPN connection from your organization to AWS VPC, a bastion host in VPN enabled subnet with secure SSH key to login, EC2 instances in private subnet with secure SSH keys to login, Redshift in private subnet.",
    "isCorrect" : "true"
  }, {
    "id" : "93765f1e153b4abab9ec5efdc1767617",
    "option" : "AWS VPN connection from your organization to AWS VPC, EC2 instances in VPN enabled subnet with secure SSH keys to login, Redshift in public subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nFor.\nOption A, this is not secure because EC2 instances are in public subnet and are open to attacks such as DDoS.\nIf you do not have a requirement to access the internet, try not to put AWS resources in the public subnet as a security best practice.\nFor more information on DDoS attacks, refer to documentation here\nhttps://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/\nFor Option B, Although EC2 instances are secured by putting them on private subnet and only enabling bastion host on public subnet looks correct, the requirement states, these instances should only be accessed via their organization network.\nSo this option is incorrect.\nA bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet.\nIt does not act as a proxy to route traffic from the internet to private EC2 instance.\nAWS Document says:\nThe solution architecture.\nIn this section, I present this solution's architecture and explain how you can configure the bastion host to record SSH sessions.\nLater in this post, I provide instructions about how to implement and test the solution.\nAmazon VPC enables you to launch AWS resources on a virtual private network that you have defined.\nThe bastion host runs on an Amazon EC2 instance that is typically in a public subnet of your Amazon VPC.\nLinux instances are in a subnet that is not publicly accessible.\nThey are set up with a security group that allows SSH access from the security group attached to the underlying EC2 instance running the bastion host.\nBastion host users connect to the bastion host to connect to the Linux instances, as illustrated in the following diagram.\nFor Option C, VPN connections are used to connect AWS VPC from your organization's network.\nBy default, instances that you launch into an Amazon VPC can't communicate with your own (remote) network.\nYou can enable access to your remote network from your VPC by attaching a virtual private gateway to the VPC, creating a custom route table, updating your security group rules, and creating an AWS managed VPN connection.\nFor more information on VPN, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_VPN.html\nSo, in this option, even from a VPN connection, only bastion host is exposed from AWS to VPN, and you only open one connection from your organization to AWS.\nFrom bastion host, you can open connections to other resources in private subnet or other resources in peering VPCs.\nhttps://aws.amazon.com/blogs/mt/replacing-a-bastion-host-with-amazon-ec2-systems-manager/\nOption D is INCORRECT because Redshift needs to be placed in the \"private\" subnet and not in the \"public\" subnet.\"\nNote:\nIn the question, they mentioned that \"Developers and Administrators need the login to the EC2 instances Only within your organization network.\" So, they should access it via their organization network.\nEstablish a VPN connection between your Organization network and your AWS.\n\n\nThe most secure option for accessing EC2 instances and Redshift in the AWS environment would be option C: AWS VPN connection from your organization to AWS VPC, a bastion host in VPN enabled subnet with secure SSH key to login, EC2 instances in private subnet with secure SSH keys to login, Redshift in private subnet.\nHere's why:\nA. EC2 instances on public subnet with secure SSH keys to login, RedShift in private subnet. This option puts the EC2 instances on a public subnet, which means they are directly accessible from the internet. Although the SSH keys provide some security, they are not enough to protect the instances from attacks. Additionally, Redshift is in a private subnet, which means it can only be accessed from the VPC. Therefore, this option is not the most secure way to access the resources.\nB. A bastion host in public subnet with secure SSH key to login, EC2 instances in private subnet with secure SSH keys to login, RedShift in private subnet. This option includes a bastion host, which is a jump server that allows access to the EC2 instances in the private subnet. The bastion host is in the public subnet, which means it is accessible from the internet. While the bastion host provides an extra layer of security, it is still vulnerable to attacks. Redshift is still in a private subnet, which is secure, but this option is not as secure as option C.\nC. AWS VPN connection from your organization to AWS VPC, a bastion host in VPN enabled subnet with secure SSH key to login, EC2 instances in private subnet with secure SSH keys to login, Redshift in private subnet. This option uses a VPN connection to connect to the VPC, which provides a secure way to access the resources. The bastion host is in a VPN-enabled subnet, which means it can only be accessed through the VPN connection. The EC2 instances are also in a private subnet and can only be accessed through the bastion host. Redshift is also in a private subnet, which provides an extra layer of security. This option is the most secure way to access the resources.\nD. AWS VPN connection from your organization to AWS VPC, EC2 instances in VPN enabled subnet with secure SSH keys to login, Redshift in public subnet. This option puts the EC2 instances in a VPN-enabled subnet, which means they can only be accessed through the VPN connection. However, Redshift is in a public subnet, which is accessible from the internet. This option is less secure than option C because Redshift is directly accessible from the internet.\nIn summary, option C is the most secure way to access the resources. It uses a VPN connection to connect to the VPC, a bastion host in a VPN-enabled subnet, EC2 instances in a private subnet, and Redshift in a private subnet. This provides multiple layers of security to protect the resources from attacks.\n\n"
}, {
  "id" : 545,
  "question" : "You have a bastion host EC2 instance on AWS VPC public subnet.\nYou would want to SSH to Bastion host EC2 instance.\nWhat would be the secure and minimal configuration you need for SSH requests to work? Assume route table is already set up with Internet Gateway.\n",
  "answers" : [ {
    "id" : "8b6a9be49d8e422aa967e8e8e54342a6",
    "option" : "Allow SSH protocol(port 22) on Security Group Inbound and Security Group Outbound. Allow Network ACL inbound and Network ACL outbound for IP range 0.0.0.0/0",
    "isCorrect" : "false"
  }, {
    "id" : "e2ee337d1e834380b90b6f16c713a1a8",
    "option" : "Allow SSH protocol(port 22) on Security Group Inbound and Security Group Outbound. Allow Network ACL inbound for your IP address.",
    "isCorrect" : "false"
  }, {
    "id" : "96c43cafea49416c8e04446724d4b4a8",
    "option" : "Allow SSH protocol(port 22) on Security Group Inbound and Network. ACL inbound for your IP address",
    "isCorrect" : "false"
  }, {
    "id" : "869b9e93a1954c9f87af5178009eda86",
    "option" : "Allow SSH protocol(port 22) on Security Group Inbound. Allow Network ACL inbound and Network ACL outbound for your IP address.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer D.\nSecurity groups are stateful - if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules.\nResponses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.\nNetwork ACLs are stateless; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Security.html#VPC_S\nIn option A, Security Group outbound is not necessary for SSH connection to work.\nAlso, opening to 0.0.0.0/0 is insecure as it allows ALL on SSH.\nAlthough this option works, this is not secure and not a minimal configuration.\nIn options B and C, Network ACL outbound is not open.\nAccording to Network ACL stateless definition, this option would fail.\nIn option D, this is a minimal and secure configuration to open only to your IP address.\nThis is the correct answer.\n\nThe best option for securing SSH access to a bastion host EC2 instance on a public subnet in an AWS VPC would be to choose option B.\nOption A allows SSH access to anyone from anywhere, which is not secure. Network ACLs are stateless and allow only inbound or outbound traffic, which can cause connectivity issues. Also, they require more maintenance overhead because it's necessary to create separate rules for inbound and outbound traffic.\nOption C only allows access to a specific IP address, but it's not scalable, and you would need to update the rule for every new IP address you want to grant access to.\nOption D only allows SSH access from a specific IP address, but it does not provide a secure outbound path, which can cause issues when trying to access resources from the bastion host.\nTherefore, option B is the most secure and minimal configuration as it allows SSH access only from your IP address, which can be specified in the Security Group Inbound rule. This rule allows traffic to come in from a specific IP address, and by default, traffic is not allowed. The Security Group Outbound rule is set to allow all traffic by default, which means that the instance can initiate connections to anywhere. It's also a good practice to limit outbound traffic to only the necessary protocols and ports.\nTo summarize, the minimal and secure configuration for SSH requests to work on a bastion host EC2 instance in a public subnet in an AWS VPC is to allow SSH protocol (port 22) on Security Group Inbound and Security Group Outbound, and to allow Network ACL inbound for your IP address. This ensures that only authorized users can access the bastion host and that the instance can initiate connections to anywhere it needs to.\n\n"
}, {
  "id" : 546,
  "question" : "You have the following Network ACL and Security Group rules.\nWhat would happen to an SSH request sent from 10.10.1.148 IP address to an EC2 instance with below security group and exists inside a subnet with below NACL rules? Network ACL Inbound Network ACL Outbound Security Group Inbound Security Group Outbound.\n\n",
  "answers" : [ {
    "id" : "0c7a7ec9780341169d47fb1f23361437",
    "option" : "SSH request succeeds due to rule # 100 in Network ACL inbound and outbound, Security Group inbound rule.",
    "isCorrect" : "true"
  }, {
    "id" : "6e8b2eb59e844e92961c23c69e535b8d",
    "option" : "SSH request succeeds due to rule # 300 in Network ACL inbound and rule # 100 in Network ACL outbound, Security Group inbound rule.",
    "isCorrect" : "false"
  }, {
    "id" : "48f409a22d864e62bbb4963d0f1b508c",
    "option" : "SSH request fails due rule # 200 in Network ACL inbound rule.",
    "isCorrect" : "false"
  }, {
    "id" : "c28637f9117e43e28aac0bbf774f39c8",
    "option" : "SSH request fails due to Security Group outbound rule does not allow 10.10.1.148 IP address.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nSecurity groups are stateful - if you send a request from your instance, the response traffic for that request is allowed to flow in regardless of inbound security group rules.\nResponses to allowed inbound traffic are allowed to flow out, regardless of outbound rules.\nNetwork ACLs are stateless; responses to allowed inbound traffic are subject to the rules for outbound traffic (and vice versa)\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Security.html#VPC_Security_\nA network ACL contains a numbered list of rules that we evaluate in order, starting with the lowest numbered rule, to determine whether traffic is allowed in or out of any subnet associated with the network ACL.\nRules are evaluated starting with the lowest numbered rule.\nAs soon as a rule matches traffic, it's applied regardless of any higher-numbered rule that may contradict it.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html#ACLRules\nFor.\nOption A, rule # 100 allows all traffic.\nSo this will allow SSH requests irrespective of other higher numbered rules.\nSecurity group rule allows SSH traffic for IP Range 10.10.1.0/24\nIP address 10.10.1.148 falls under this IP range, so it allows SSH requests.\nNetwork ACL outbound rule # 100 allows ALL traffic.\nSo the request would succeed.\nThis option is correct.\nFor Option B, when an SSH request is made, rule # 300 is never evaluated because the request succeeds during rule # 100 evaluation.\nHowever, Rule # 300 gets evaluated when a non-SSH request is made.\nBut, for this question, it is an incorrect answer.\nFor Option C, rule # 200 is never evaluated because the request succeeds during rule # 100 evaluation.\nSo this option is incorrect.\nFor option D, Security Groups are stateful.\nSo, for an SSH request inbound to EC2 instance, the security group outbound does not impact.\nSo this option is incorrect.\n"
}, {
  "id" : 547,
  "question" : "Your company uses an AWS Transit Gateway as a hub to manage the interconnections between multiple VPCs and the on-premises networks.\nThe security team asks you to implement a control that can allow or block the traffic between the EC2 network interface workload and the Transit Gateway.\nWhich of the following approaches would you select?\n",
  "answers" : [ {
    "id" : "f7e5b24a40c14b81aa4866ddb506328e",
    "option" : "Attach the EC2 instances with an IAM role that has the AWS managed policy â€œAWSNetworkManagerServiceRolePolicyâ€.",
    "isCorrect" : "false"
  }, {
    "id" : "497fd529737f46f2b47505b78de4ff83",
    "option" : "Associate security groups to the EC2 instance network interface and the Transit Gateway to control the traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "e6b1b9853c7846358d83c4604ea10420",
    "option" : "Create route tables in the AWS Transit Gateway to allow or disallow the traffic from the EC2 workload.",
    "isCorrect" : "false"
  }, {
    "id" : "e3e9d5176a76491b96fd88a609d2b1f7",
    "option" : "Apply NACL rules between EC2 instances in the subnets and Transit Gateway associations to control the traffic.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption A is incorrect because the IAM role in EC2 instances provisions permissions for EC2 applications.\nIt does not control the network traffic between the EC2 network interface workload and the Transit Gateway.\nOption B is incorrect because security groups cannot be applied to either the EC2 network interface workload or the Transit Gateway.\nFor the access control methods of Transit Gateway, please check the following references.\nOption C is incorrect because Transit Gateway route tables are used to configure routing for Transit Gateway attachments.\nThey do not act as a security layer to allow or block traffic.\nOption D is CORRECT because the inbound and outbound NACL rules applied in the subnets can act as a firewall and control traffic in and out of the Transit Gateway.\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/tgw/tgw-nacls.html\nhttps://docs.aws.amazon.com/vpc/latest/tgw/transit-gateway-authentication-access-control.html\n\nTo implement a control that allows or blocks the traffic between the EC2 network interface workload and the Transit Gateway, the most appropriate approach would be to associate security groups to the EC2 instance network interface and the Transit Gateway.\nOption A: Attaching IAM roles with the AWSNetworkManagerServiceRolePolicy policy is not the right approach because this policy is used to manage network resources and monitor the network, and it does not provide control over the traffic.\nOption B: Associate security groups to the EC2 instance network interface and the Transit Gateway to control the traffic. A security group acts as a virtual firewall that controls the traffic to an instance. When an EC2 instance is launched, it can be associated with one or more security groups. Security groups can be applied to a network interface, and rules can be added to the security group to allow or block traffic based on source and destination. In this scenario, we can create a security group for the Transit Gateway and another for the EC2 instance network interface. We can then add the necessary rules to the security groups to allow or block traffic between them.\nOption C: Creating route tables in the AWS Transit Gateway to allow or disallow traffic from the EC2 workload is not the right approach because route tables control how traffic is routed between VPCs and on-premises networks, and they do not provide control over the traffic itself.\nOption D: Applying NACL rules between EC2 instances in the subnets and Transit Gateway associations to control the traffic is not the right approach because NACL rules control traffic at the subnet level and do not provide control over traffic between an EC2 instance network interface and the Transit Gateway.\nIn conclusion, option B - associating security groups to the EC2 instance network interface and the Transit Gateway to control the traffic - is the most appropriate approach to implement the required control.\n\n"
}, {
  "id" : 548,
  "question" : "Your company owns several EC2 Windows servers in production.\nIn order to be compliant with recent company security policies, you need to create an EC2 Windows bastion host for users to connect to the instances via the Remote Desktop Protocol (RDP)\nHow would you ensure that users can perform remote administration for the Windows servers ONLY through the new bastion host?\n",
  "answers" : [ {
    "id" : "ea92398ab0544404ab09b721d5192a13",
    "option" : "Configure the security groups of the Windows server instances to only accept TCP/3389 connections from the security group of the Windows bastion host.",
    "isCorrect" : "true"
  }, {
    "id" : "33683585ede44ce6b3226b4069875048",
    "option" : "Configure the security group of the Windows bastion host to only allow RDP from the companyâ€™s IP addresses.",
    "isCorrect" : "false"
  }, {
    "id" : "d32ce5d014cf4e3c85137d86bedd96e0",
    "option" : "Add a NACL rule in the subnets of the Windows server instances to deny TCP/443 and TCP/22.",
    "isCorrect" : "false"
  }, {
    "id" : "d01aed410f14448c8eef8bd242116ee0",
    "option" : "In the NACL of the bastion host server, allow the inbound and outbound traffic for TCP/3389.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT because with this option, Windows server instances only allow the RDP traffic from the bastion host instance.\nUsers need to login to the bastion host to connect to the Windows servers.\nOption B is incorrect because this option only allows the connections to the bastion host.\nIt does not provide the RDP connections to the Windows servers through the bastion host.\nOption C is incorrect because this option only denies ports 443 and 22\nIt does not allow any rules for the inbound RDP connections.\nOption D is incorrect because similar to option B, it only controls the RDP traffic of the bastion host and there are no controls or limitations on the Windows instances.\nUsers can bypass the bastion host to connect to the Windows servers via RDP and there is no NACL rule to deny it.\nThe NACL of the bastion host server cannot block the unexpected connections.\nInstead, the control should be applied in the Windows instances to only allow the connections from the Windows bastion host.\nReference:\nhttps://aws.amazon.com/blogs/security/controlling-network-access-to-ec2-instances-using-a-bastion-server/\n\nSure, I'd be happy to explain each of the answer choices and which one is the most appropriate for the given scenario.\nA. Configure the security groups of the Windows server instances to only accept TCP/3389 connections from the security group of the Windows bastion host. This answer choice suggests configuring the security groups of the Windows server instances to only allow connections on TCP/3389 (RDP port) from the security group of the Windows bastion host. This would ensure that only connections originating from the bastion host can access the Windows servers. However, this answer choice does not address how to ensure that remote administration can only be performed through the bastion host. It only limits the source of the incoming RDP connections.\nB. Configure the security group of the Windows bastion host to only allow RDP from the company's IP addresses. This answer choice suggests configuring the security group of the Windows bastion host to only allow RDP connections from the company's IP addresses. This would ensure that only connections originating from the company's IP addresses can access the bastion host, which would limit the attack surface. However, this answer choice also does not address how to ensure that remote administration can only be performed through the bastion host.\nC. Add a NACL rule in the subnets of the Windows server instances to deny TCP/443 and TCP/22. This answer choice suggests adding a Network Access Control List (NACL) rule in the subnets of the Windows server instances to deny TCP/443 (HTTPS) and TCP/22 (SSH). This would limit the attack surface by blocking incoming traffic on ports commonly used for remote access. However, this answer choice does not address how to ensure that remote administration can only be performed through the bastion host.\nD. In the NACL of the bastion host server, allow the inbound and outbound traffic for TCP/3389. This answer choice suggests allowing inbound and outbound traffic for TCP/3389 in the NACL of the bastion host server. This would ensure that users can connect to the Windows servers only through the bastion host, as it is the only server with RDP access. This also limits the attack surface by ensuring that RDP traffic is only allowed through the bastion host. Therefore, this is the most appropriate answer choice for the given scenario.\nIn summary, the most appropriate answer choice for ensuring that users can perform remote administration for the Windows servers ONLY through the new bastion host is to allow inbound and outbound traffic for TCP/3389 in the NACL of the bastion host server.\n\n"
}, {
  "id" : 549,
  "question" : "You have an existing VPC in us-east-1\nYou have created a VPC Endpoint for S3 and added it to the main route table.\nYou have launched an EC2 instance inside a subnet that is associated with the main route table.\nFrom the new EC2 instance, when requesting the S3 bucket within us-east-1, you noticed that the connection is failing.\nWhat could be the reason? ( Choose 2 options)\n",
  "answers" : [ {
    "id" : "6e2339ddebb747339b12a8a0ae0f2628",
    "option" : "EC2 instance security group outbound rules are restricted and does not contain prefix list.",
    "isCorrect" : "true"
  }, {
    "id" : "7e7a668a0b774e9499341348f66b9c58",
    "option" : "Main route table does not have internet gateway association.",
    "isCorrect" : "false"
  }, {
    "id" : "d20769193f294c8da2e5845be81b888a",
    "option" : "Subnetâ€™s Network ACL inbound rule does not allow traffic from S3.",
    "isCorrect" : "true"
  }, {
    "id" : "84148ea6afd649dc86374c8ced475b90",
    "option" : "Main route table does not have NAT gateway association.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A and C.\nFor option A, By default, Amazon VPC security groups allow all outbound traffic unless you've specifically restricted outbound access.\nFor a gateway endpoint, if your security group's outbound rules are restricted, you must add a rule that allows outbound traffic from your VPC to the service that's specified in your endpoint.\nTo do this, you can use the service's prefix list ID as the destination in the outbound rule.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/connect-s3-vpc-endpoint/\nSo this option is correct.\nFor option B, when using the VPC endpoint for S3, an internet gateway is not required to route traffic to S3\nVPC endpoint routes traffic internally within AWS without going out to the internet.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-endpoints.html\nSo this option is incorrect.\nFor Option C, The default network ACL is configured to allow all traffic to flow in and out of the subnets with which it is associated.\nIf your network ACL rules restrict traffic, you must specify the CIDR block ( IP address range ) for Amazon S3\nSo this option is correct.\nFor option D, when using the VPC endpoint for S3, the NAT gateway is not required to route traffic to S3\nVPC endpoint routes traffic internally within AWS without going out to the internet.\nSo this option is incorrect.\n\nThe two possible reasons why the connection to the S3 bucket is failing from the EC2 instance are:\nB. Main route table does not have internet gateway association: If the main route table does not have an internet gateway association, the EC2 instance will not be able to access any services outside of the VPC, including S3. An internet gateway is required for traffic to go outside of the VPC, so if the main route table does not have an internet gateway association, the EC2 instance will not be able to communicate with the S3 bucket. To resolve this issue, you can associate an internet gateway with the VPC and update the main route table to include a route for all traffic to the internet gateway.\nD. Main route table does not have NAT gateway association: If the main route table does not have a NAT gateway association and the EC2 instance is not in a private subnet with a NAT gateway, the EC2 instance will not be able to access S3 or any services outside of the VPC. A NAT gateway is used to provide internet access to resources in private subnets, so if the main route table does not have a NAT gateway association and the EC2 instance is not in a private subnet with a NAT gateway, the connection will fail. To resolve this issue, you can create a NAT gateway in a public subnet and associate it with the main route table.\nThe other options, A and C, are not correct in this scenario.\nA. EC2 instance security group outbound rules are restricted and does not contain prefix list: This option is not related to the issue at hand. Security groups are used to control inbound and outbound traffic to instances, but prefix lists are not relevant to this issue.\nC. Subnet's Network ACL inbound rule does not allow traffic from S3: This option is not correct because the VPC Endpoint for S3 has been created and added to the main route table. Therefore, the traffic to S3 should be routed through the VPC Endpoint and not through the subnet's Network ACL.\n\n"
}, {
  "id" : 550,
  "question" : "Your organization had asked to be cost-efficient in designing AWS solutions.\nYou have created three VPCs(VPC A, VPC B, VPC C), peered VPC A to VPC B and VPC B to VPC C.\nYou have created a NAT gateway in VPC B and would like to use the same NAT Gateway for resources within VPC A and VPC C.\nHowever, the resources within VPC A and VPC C cannot communicate to the internet through NAT Gateway, but resources in VPC B can communicate.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "657d2b1b9ddd49a7aa1f9f5f84264890",
    "option" : "Route tables in VPC A and VPC C are not configured to have VPC Bâ€™s NAT gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "ba67f17a4281432d976f7d1cf7bbd5f4",
    "option" : "Using another VPC`s NAT Gateway is not supported in AWS.",
    "isCorrect" : "true"
  }, {
    "id" : "4968da8e0219427685d84bd895eb260d",
    "option" : "VPC Bâ€™s subnet which contains NAT gateway is not configured in VPC A and VPC C route tables.",
    "isCorrect" : "false"
  }, {
    "id" : "31d0aa0f781a4ec4a191b451b74f28d8",
    "option" : "NAT Gateway is not created inside VPC Bâ€™s public subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nIn a VPC peering connection, using the NAT Gateway of another VPC becomes transitive routing and is not supported in AWS.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpc-nat-gateway.html#nat-\nFor.\nOption A, in VPC's route table, only NAT Gateway of the belonging VPC can be configured.\nVPC A and VPC C cannot configure VPC B's NAT Gateway in their respective route tables.\nThis option is incorrect.\nFor Option B, as explained above, transitive routing is not supported.\nThis option is correct.\nFor Option C, even if two VPCs have peered and configured route tables with their entire IP range, as explained above, transitive routing is not supported.\nThis option is incorrect.\nFor Option D, the question says VPC B resources can communicate with the internet, for which NAT gateway should be on a public subnet.\nSo this option is not valid.\n\n\nThe reason why resources in VPC A and VPC C cannot communicate with the internet through the NAT Gateway in VPC B could be due to incorrect routing configuration in the VPC route tables.\nWhen a VPC is created, a default route table is also created for the VPC. This default route table has a local route for all subnets in the VPC, but it does not have a route for internet traffic. Therefore, if resources in VPC A and VPC C need to communicate with the internet through the NAT Gateway in VPC B, the route tables in these VPCs need to be modified.\nThe correct answer is A. Route tables in VPC A and VPC C are not configured to have VPC B's NAT gateway. In order for resources in VPC A and VPC C to communicate with the internet through the NAT Gateway in VPC B, the route tables in VPC A and VPC C need to have a route that directs internet traffic to the NAT Gateway in VPC B.\nTo configure the route tables, the following steps should be taken:\nIn the AWS Management Console, navigate to the VPC Dashboard and select the VPC A. Click on the \"Route Tables\" option on the left-hand side menu, and select the default route table for the VPC. Click on the \"Edit\" button at the bottom of the route table, and add a new route to the NAT Gateway in VPC B. The destination should be \"0.0.0.0/0\" to route all internet traffic, and the target should be the NAT Gateway in VPC B. Repeat steps 2 and 3 for VPC C's route table.\nC is also a correct answer as VPC B's subnet which contains NAT gateway is not configured in VPC A and VPC C route tables. This means that the subnet in which the NAT Gateway is created in VPC B is not added to the route tables of VPC A and VPC C. Therefore, traffic from VPC A and VPC C is not being routed to the NAT Gateway in VPC B.\nHowever, B is incorrect. Using another VPC's NAT Gateway is supported in AWS, and it is a common practice to use a single NAT Gateway for multiple VPCs to save costs.\nD is also incorrect as the NAT Gateway needs to be created in a public subnet in VPC B. Otherwise, it will not be able to communicate with the internet. Therefore, the NAT Gateway in VPC B should be created in a public subnet, and the route tables in VPC A and VPC C should be configured to direct traffic to the NAT Gateway.\n\n"
}, {
  "id" : 551,
  "question" : "A large corporation has several branch offices and applications are deployed in AWS.\nThe branch offices and the AWS VPC need to communicate with each other.\nThe connectivity should be secure, cost-effective, and based on existing internet connections.\nWhich of the following methods would you choose?\n",
  "answers" : [ {
    "id" : "855bcc991500419cafca485f79fbda62",
    "option" : "Configure an AWS Direct Connect with hardware VPNs to establish secure network connections between remote sites and AWS VP.",
    "isCorrect" : "false"
  }, {
    "id" : "ca50f633feb7472786a7ac135c50da5a",
    "option" : "Launch an EC2 instance with the VPN software installed to establish IPSec VPN connections.",
    "isCorrect" : "false"
  }, {
    "id" : "f1f46d53063d4de7962b105082c8e33b",
    "option" : "Use AWS VPN CloudHub to enable the communication between the branch offices and the VP.",
    "isCorrect" : "true"
  }, {
    "id" : "3e81e7bce6c0496292091d3bdc495050",
    "option" : "For each branch office, create a Virtual Private Gateway in AWS VPC to establish a software VPN connection.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect because although hardware VPNs can provide secure connections, Direct Connect should not be used as it is not cost-effective and not based on existing internet connections.\nOption B is incorrect because this option uses a single EC2 instance that may have availability issues.\nIt also does not provide the connectivity between branch offices.\nOption C is CORRECT because AWS VPN CloudHub provides secure connections based on existing internet.\nAWS VPN CloudHub uses an Amazon VPC virtual private gateway with multiple customer gateways to provide the connectivities.\nIt operates on a simple hub-and-spoke model as the following snapshot:\nOption D is incorrect because there is no need to create a Virtual Private Gateway for each branch office.\nWith AWS VPN CloudHub, a VPC only needs a single Virtual Private Gateway.\nOther than that, a hardware VPN is established with Virtual Private Gateway instead of a software VPN.\nReferences:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-vpn-cloudhub.html\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPN_CloudHub.html,\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-managed-vpn.html\n\n\nSure, I'd be happy to explain the different methods and help you choose the most appropriate one.\nOption A: Configure an AWS Direct Connect with hardware VPNs to establish secure network connections between remote sites and AWS VPC.\nAWS Direct Connect is a dedicated network connection between the customer's data center and AWS. With this option, you can establish a secure network connection between your remote sites and AWS VPC by using AWS Direct Connect along with hardware VPNs. However, this option can be expensive, especially if you have several remote sites, and may not be cost-effective if you have existing internet connections that can be leveraged.\nOption B: Launch an EC2 instance with the VPN software installed to establish IPSec VPN connections.\nWith this option, you can launch an EC2 instance with VPN software installed, configure it to establish IPSec VPN connections, and use it to securely connect the remote sites to AWS VPC. While this option is cost-effective, it requires manual configuration and maintenance of the VPN software on the EC2 instance, which can be time-consuming and error-prone.\nOption C: Use AWS VPN CloudHub to enable the communication between the branch offices and the VPC.\nAWS VPN CloudHub is a managed VPN service provided by AWS that allows you to easily establish secure network connections between your remote sites and AWS VPC. With this option, you can create a single VPN connection between the AWS VPC and AWS VPN CloudHub, and then use CloudHub to connect all the remote sites to the VPC. This option is cost-effective, requires minimal configuration and maintenance, and can scale easily to support a large number of remote sites.\nOption D: For each branch office, create a Virtual Private Gateway in AWS VPC to establish a software VPN connection.\nWith this option, you can create a Virtual Private Gateway in the AWS VPC for each remote site and establish a software VPN connection between the remote site and the VPC. While this option is secure and can scale easily, it can be expensive if you have several remote sites and may not be cost-effective if you have existing internet connections that can be leveraged.\nBased on the requirements of the large corporation, Option C would be the most appropriate method. It is cost-effective, requires minimal configuration and maintenance, and can scale easily to support a large number of remote sites. It also uses existing internet connections, which can save costs for the organization.\n\n"
}, {
  "id" : 552,
  "question" : "You created a new VPC with CIDR range 10.10.0.0/16 and a new subnet with CIDR range 10.10.1.0/24\nCIDR with /24 comes with 256 IP addresses.\nWhen you go to VPC console subnets and look at the newly created subnet, you can only see 251 IP addresses.\nYou have not launched any resources in the newly created VPC.\nWhat would have caused this?\n",
  "answers" : [ {
    "id" : "ba5b9e38469942d58f6cb34d5075eaf9",
    "option" : "The first four IP addresses and the last IP address in each subnet CIDR block are reserved by AWS.",
    "isCorrect" : "true"
  }, {
    "id" : "049c60e4fea94a94a0b8e84e669da3d3",
    "option" : "AWS reserves 5 IP addresses for every VPC and are reserved from first subnet you create.",
    "isCorrect" : "false"
  }, {
    "id" : "3523d6601d464278973a84b1f7937c86",
    "option" : "AWS launches monitoring resources on behalf of you in new VPC when first subnet is created which will reserve 5 IP addresses from first subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "a8962b293a1f4cad9f3f74dd61ba7dff",
    "option" : "None of the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Subnets.html#VPC_Sizing\nFrom the above statement, only.\nOption A is correct.\nFor Option C, AWS never launches any billable resources without notifying the account owner or administrator on behalf of you.\n\n\nThe correct answer is A, \"The first four IP addresses and the last IP address in each subnet CIDR block are reserved by AWS.\"\nWhen you create a subnet in a VPC, AWS reserves some IP addresses for its own use. These addresses are reserved for various purposes, such as routing, network addressing, and DNS. Specifically, AWS reserves the first four IP addresses and the last IP address in each subnet CIDR block for its own use. This means that out of the 256 IP addresses available in a /24 subnet, only 251 are available for you to use.\nThe first four IP addresses in a CIDR block are reserved as follows:\nNetwork address: This is the first IP address in the subnet and is used to identify the subnet itself. It cannot be assigned to any instance or resource within the subnet. VPC router address: This is the second IP address in the subnet and is used by AWS to route traffic between the VPC and other networks. It cannot be assigned to any instance or resource within the subnet. DNS server address: This is the third IP address in the subnet and is used by AWS to provide DNS resolution for instances within the VPC. It cannot be assigned to any instance or resource within the subnet. Reserved IP address: This is the fourth IP address in the subnet and is reserved for future use by AWS. It cannot be assigned to any instance or resource within the subnet.\nThe last IP address in a CIDR block is also reserved and cannot be assigned to any instance or resource within the subnet. This is because it is the broadcast address, which is used to send traffic to all devices on the subnet.\nTherefore, in a /24 subnet, 256 IP addresses are available, but only 251 can be used by your resources. The remaining five IP addresses are reserved by AWS for network infrastructure and cannot be assigned to any instances or resources within the subnet.\nOption B is incorrect because AWS only reserves five IP addresses for the VPC itself, not for each subnet. These IP addresses are used for the VPC router, DNS server, and network address, and are reserved from the first subnet you create.\nOption C is incorrect because AWS does not launch any monitoring resources on behalf of you when you create a new VPC or subnet. However, AWS does provide various monitoring tools and services that you can use to monitor your resources within a VPC.\n\n"
}, {
  "id" : 553,
  "question" : "You have created a custom VPC with public and private subnets.\nYou will also be setting up a VPN connection with your organization to communicate with resources within the VPC.\nYour organization would need DNS names for some of the on-premise applications to communicate with VPC resources.\nYou have launched a new EC2 instance with Auto-assign public IP as enable.\nWhen the instance is ready to use, you found that the Public DNS name is missing.\nWhat should be done to enable it?\n",
  "answers" : [ {
    "id" : "26afe3d7c1624507bbcd7cff83248900",
    "option" : "Check whether attributes for DNS Hostnames and DNS Resolution are enabled for VP.",
    "isCorrect" : "true"
  }, {
    "id" : "01cb7f2b46c543ee8de6ca49fbb60513",
    "option" : "Check DHCP options set",
    "isCorrect" : "false"
  }, {
    "id" : "9b98457fd2ea485caecb176c1fe2a8d5",
    "option" : "Set auto-assign public IP to Use Subnet Setting.",
    "isCorrect" : "false"
  }, {
    "id" : "b124f047e221468ab5197de31d3fe553",
    "option" : "You cannot have private DNS names for custom VPCs. Setup EC2 instance in default VP.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nBy default, both attributes are set to '\ntrue'\nin a default VPC or a VPC created by the VPC wizard.\nThe following rules apply:\nIf both attributes are set to\ntrue.\n, the following occurs:\nInstances with a public IP address receive corresponding public DNS hostnames.\nThe Amazon Route 53 Resolver server can resolve Amazon-provided private DNS hostnames.\nIf either or both of the attributes is set to\nfalse.\n, the following occurs:\nInstances with a public IP address do not receive corresponding public DNS hostnames.\nThe Amazon Route 53 Resolver cannot resolve Amazon-provided private DNS hostnames.\nInstances receive custom private DNS hostnames if there is a custom domain name in the DHCP options set.\nIf you are not using the Amazon Route 53 Resolver server, your custom domain name servers must resolve the hostname as appropriate.\nOption B is incorrect because the DHCP options set is used if there is a custom domain name created by the user.\nIf you are not using the Amazon Route 53 Resolver server, your custom domain name servers must resolve the hostname as appropriate.\nSo there is no need to check the DHCP options set.\nDefault DHCP options set: AmazonProvidedDNS.\nOption C is incorrect because it is mentioned that EC2 instance is enabled with Auto-assign public IP, auto-assign public IP defines whether you can have a public IP address for the EC2 you are launching.\nIf you launch EC2 in the public subnet, you can choose to have a public IP address or not.\nOption D is incorrect.\nCustom VPC provides an option to enable/disable DNS Hostnames as described above.\nWe provide a DNS server (the Amazon Route 53 Resolver) for your VPC.\nTo use your own DNS server, create a new set of DHCP options for your VPC.\nFor more information, see DHCP options sets for your VPC.Reference:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html#vpc-dns-viewing\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_DHCP_Options.html\n\n\nThe correct answer is A. Check whether attributes for DNS Hostnames and DNS Resolution are enabled for VP.\nExplanation: When you create a custom VPC with public and private subnets, DNS resolution and DNS hostnames attributes are disabled by default. These attributes need to be enabled in order to resolve public DNS names to private IP addresses within the VPC.\nEnabling DNS resolution allows Amazon-provided DNS servers to resolve public DNS names to their corresponding IP addresses within the VPC. Enabling DNS hostnames allows Amazon to provide unique DNS names for resources that are launched within the VPC.\nTo enable these attributes for the VPC:\nOpen the Amazon VPC console at https://console.aws.amazon.com/vpc/. In the navigation pane, choose Your VPCs. Select the custom VPC that you want to enable DNS resolution and DNS hostnames for. Choose Actions, Edit DNS resolution or Edit DNS hostnames. Select Yes to enable the attribute and choose Save.\nOnce DNS resolution and DNS hostnames are enabled, you can verify that the EC2 instance has a Public DNS name by navigating to the EC2 console, selecting the instance, and checking the Public DNS column.\nOption B, checking DHCP options set, is not relevant to this scenario as it pertains to assigning IP addresses and network configuration to instances launched within the VPC.\nOption C, setting auto-assign public IP to Use Subnet Setting, is also not relevant to this scenario as the EC2 instance already has auto-assign public IP enabled.\nOption D, setting up EC2 instance in the default VPC, is not necessary as custom VPCs can support private DNS names with the appropriate attributes enabled.\n\n"
}, {
  "id" : 554,
  "question" : "You are taking over the AWS platform in your organization.\nYou were asked to build a new application that would require a fleet of 20 EC2 instances inside a private VPC that should communicate with each other and no traffic going into the EC2 instances from the internet but should receive requests from all other EC2 instances inside the VPC.\nWhen you looked at the existing VPC, it was created with 10.10.0.0/24 CIDR range containing only 256 IP addresses.\nYou noticed that 8 subnets were consuming all 256 IP addresses with /27 CIDR ranges.\nHow can you change the CIDR range of the VPC?\n",
  "answers" : [ {
    "id" : "e329a930e46441ab8c6c5ac52947abba",
    "option" : "Create a new VPC, setup 20 EC2 instances in new VPC and peer with existing VP.",
    "isCorrect" : "false"
  }, {
    "id" : "55d893b306814e7895164da8cbd476c9",
    "option" : "Add secondary CIDR range for the VP.",
    "isCorrect" : "true"
  }, {
    "id" : "1f0a331b3ed54eecb694dbe9f84ffde5",
    "option" : "Edit subnet CIDR ranges to /28 and free up unused IP addresses.",
    "isCorrect" : "false"
  }, {
    "id" : "a21d3ab990b34fdab1531411fda58a9c",
    "option" : "Launch EC2 instances in different subnets and setup Network ACLs and Security Groups to allow traffic between EC2 instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nYou can associate secondary IPv4 CIDR blocks with your VPC.\nWhen you associate a CIDR block with your VPC, a route is automatically added to your VPC route tables to enable routing within the VPC (the destination is the CIDR block and the target is local).\nIn the following example, the VPC on the left has a single CIDR block (10.0.0.0/16) and two subnets.\nThe VPC on the right represents the architecture of the same VPC after you've added a second CIDR block (10.2.0.0/16) and created a new subnet from the range of the second CIDR.\nhttps://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-\nFor option A, although creating a new VPC, peering with the existing VPC would work, it creates a lot of configuration.\nThis solution is suited when you want to isolate certain resources within each VPC and communicate certain resources in VPCs, or if the VPCs belong to different accounts, or if VPCs are in different regions.\nThere is a limit of 5 VPCs per region, and creating VPCs without a definite need might hit the limit in the long run.\nFor option B, adding a secondary CIDR to the existing VPC is a simple configuration and can enable more IP addresses to the current VPC.For option C, a subnet's CIDR cannot be edited once created.\nAlthough this option works for option D, this would create a lot of complexity around setting up new Security Groups and network ACLs.\nThis setup would be difficult to maintain and troubleshoot in case of any issues.\nSo, with given options, although there are multiple working solutions, option B is the recommended solution.\n\n\nTo fulfill the requirement of building a new application that would require a fleet of 20 EC2 instances inside a private VPC that should communicate with each other and no traffic going into the EC2 instances from the internet but should receive requests from all other EC2 instances inside the VPC, we need to consider the existing VPC's CIDR range.\nAccording to the given information, the existing VPC was created with a 10.10.0.0/24 CIDR range, which contains only 256 IP addresses, and 8 subnets are already using /27 CIDR ranges, which consume all 256 IP addresses. So, we need to find a solution that accommodates the required 20 EC2 instances in the existing VPC.\nNow, let's evaluate the given options one by one:\nA. Create a new VPC, setup 20 EC2 instances in new VPC and peer with existing VP.\nThis option suggests creating a new VPC and peering it with the existing VPC to fulfill the requirement. However, this solution would not make efficient use of existing resources and increase complexity. Therefore, this option can be discarded.\nB. Add secondary CIDR range for the VP.\nThis option suggests adding a secondary CIDR range for the VPC to accommodate the required 20 EC2 instances. This solution is a viable option, but it requires modifying the existing VPC, which may affect other resources in the VPC. It is essential to consider the impact on other resources before modifying the VPC.\nC. Edit subnet CIDR ranges to /28 and free up unused IP addresses.\nThis option suggests editing the existing subnets' CIDR ranges to /28 to free up unused IP addresses. This solution is also a viable option, but it may require modifying the existing subnets, which may affect other resources in the subnets. It is essential to consider the impact on other resources before modifying the subnets.\nD. Launch EC2 instances in different subnets and setup Network ACLs and Security Groups to allow traffic between EC2 instances.\nThis option suggests launching EC2 instances in different subnets and setting up Network ACLs and Security Groups to allow traffic between EC2 instances. This solution is also a viable option, but it may require more management overhead and could affect the efficiency of the network traffic.\nConsidering all the above options, the most efficient and feasible solution would be to choose option B, adding a secondary CIDR range for the VPC. This option would not require creating a new VPC or modifying existing subnets, and it would efficiently accommodate the required 20 EC2 instances. However, it is crucial to consider the impact of modifying the VPC before proceeding.\n\n"
}, {
  "id" : 555,
  "question" : "Your organization has a VPC set up with a custom route table having 40 routes for different use cases such as \"VPC peering\", \"VPN connections\", \"NAT gateways\" etc with different IP ranges.\nThe Main route table had a local route to the internet gateway to act for the public subnet.\nThe CIDR IP range for the VPC is 10.10.0.0/16, which will lead to multiple subnets.\nThese subnets require a custom route table to be associated with them for the respective application(s)\nMany times, the teams forget to associate the custom route table to the subnets explicitly which leads to a lot of troubleshooting hours while connecting to the new subnets from the VPN.\nAs an architect, how would you resolve this issue?\n",
  "answers" : [ {
    "id" : "9dd02115e5de4d8f9200ae780b689f55",
    "option" : "Create a script to create a new subnet and associate it with the custom route table. Share this with all the teams.",
    "isCorrect" : "false"
  }, {
    "id" : "e8744dce18014ec0a76a7bc6a4b830eb",
    "option" : "Make the custom route table as the main route table. Any new subnets created will get associated with it implicitly.",
    "isCorrect" : "true"
  }, {
    "id" : "fc883d8218b54c28afa7f8ec5f9c052a",
    "option" : "Delete the Internet Gateway route from the main route table.",
    "isCorrect" : "false"
  }, {
    "id" : "c6d450e23f2c43feabd0fa0d2c40501e",
    "option" : "Replace all routes from the custom table with the main route table and vice versa.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nA custom route table can be made as to the main route table so that all implicit associations of subnets will now point to the newly set main route table.\nAll the future implicit associations of newly created subnets will point to the newly set main route table.\nOption A is incorrect, although subnet creation and association can be done programmatically, it may not be feasible to share access keys with all the teams (assuming the creation process is done on a remote network where roles cannot be used)\nIt is also difficult for the organization to set up the process to run this script for new teams as they might not be aware of it.\nAs described above, option B is setting a custom table as the main route table is a simple configuration.\nAll the associations would point to the new main route table implicitly.\nOption C is incorrect because deleting the internet gateway does not solve the problem.\nIt might create a new problem for EC2 instances using the NAT gateway to cause failures in connecting to the internet.\nOption D is incorrect because it is tedious and error-prone.\nWith all the given options, option B is the best suitable solution.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html#CustomRouteTables\n\n\nAs an architect, the best solution to the given problem would be to make use of Option B, i.e., make the custom route table the main route table, so that any new subnets created will get associated with it implicitly.\nThis approach ensures that all the subnets created within the VPC will automatically use the custom route table as their default route table without requiring any explicit association. This way, even if the teams forget to associate the custom route table with the subnets, the traffic will still be routed correctly without any issues.\nHowever, before implementing this solution, it is important to ensure that the custom route table has all the necessary routes that are required for the VPC to function properly. It should have routes for VPC peering, VPN connections, NAT gateways, etc., just like the previous main route table. Also, it should have a route for the internet gateway to allow traffic from the public subnet.\nOnce the custom route table is updated with all the required routes, the next step would be to make it the main route table. This can be done by navigating to the VPC dashboard and selecting \"Route Tables\" from the left-hand menu. Select the custom route table and then click on the \"Set as Main Route Table\" button.\nAfter making the custom route table as the main route table, any new subnet created in the VPC will be automatically associated with the custom route table. This will save a lot of troubleshooting hours and ensure that the traffic is routed correctly between the subnets and to the internet.\n\n"
}, {
  "id" : 556,
  "question" : "You are an architect in your organization.\nOne of the application team in your organization comes to you stating recently they noticed the requests sending from an EC2 instance to an RDS in the same VPC but in another subnet are getting timed out.\nThey claim that connections were working before.\nHow do you troubleshoot this issue?\n",
  "answers" : [ {
    "id" : "33cddf073a6a4c9f92bbb1ca486aa2e9",
    "option" : "Create VPC flow log for subnet where RDS instance is launched.",
    "isCorrect" : "true"
  }, {
    "id" : "56d6a389684947548475bee073648ea5",
    "option" : "Check CloudWatch metrics for RDS instance.",
    "isCorrect" : "false"
  }, {
    "id" : "623dfd04d68144aea5ff96a6d666d246",
    "option" : "Check OS level logs inside RDS instance.",
    "isCorrect" : "false"
  }, {
    "id" : "68a7faa377b54a42b9f43b852a7cec73",
    "option" : "Check OS level logs inside EC2 instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nFor option A, VPC Flow Logs captures IP traffic going to and from network interfaces in your VPC.\nFlow log data is stored using Amazon CloudWatch Logs.\nAfter you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs.\nYou can create a flow log for a VPC, a subnet, or a network interface.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#flow-logs-basics\nVPC Flow Logs capture following information and logs them to CloudWatch logs,\nversion account-id interface-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status.\nFind more information about each record here.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#flow-log-records\nSo, using VPC flow logs, we can identify if the traffic is being rejected by RDS instance when sent from the EC2 instance on a certain port.\nFrom there on, we can identify if there any overly restrictive Security Group rules or Network ACL rules.\nFor option B, CloudWatch metrics for RDS gives the details about RDS underlying database instance metrics.\nBut this does not contain details about networking requests sent to RDS instance.\nFor more information on CloudWatch metrics for RDS, refer documentation here.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/rds-\nFor option C, RDS underlying OS is managed by AWS and cannot be accessed by AWS customers.\nFor option D, enabling OS level logs at the EC2 instance where the request is being made does not provide any information on why the request is being timed out at RDS instance.\nSo, the correct answer is option A.\n\nTo troubleshoot the issue of timed-out requests from an EC2 instance to an RDS instance in the same VPC but in another subnet, the following steps can be taken:\nVerify the connectivity between the two subnets: The first step is to verify if there is connectivity between the subnets where the EC2 instance and RDS instance are launched. This can be done by checking the network ACLs and security groups attached to the subnets. Ensure that the necessary ports are open in the security groups and network ACLs. Check the VPC routing: Check if the VPC routing is configured correctly. Ensure that the subnets have the correct route tables attached to them. The EC2 instance should be able to communicate with the RDS instance through the correct route. Check the RDS instance status: Check the status of the RDS instance. Ensure that the RDS instance is running, and there are no performance issues. Create VPC flow logs: Create VPC flow logs for the subnet where the RDS instance is launched. This will help to identify the traffic flow between the EC2 instance and RDS instance. It will provide information on the source and destination IP addresses, ports, and protocol. Check CloudWatch metrics: Check the CloudWatch metrics for the RDS instance. The CloudWatch metrics provide information on the RDS instance's CPU usage, network throughput, disk usage, and other performance metrics. This will help to identify any performance issues with the RDS instance. Check OS-level logs: Check the OS-level logs inside the RDS instance and EC2 instance. The logs may provide information on any errors or issues related to the database or application.\nIn summary, to troubleshoot the issue of timed-out requests from an EC2 instance to an RDS instance in the same VPC but in another subnet, you should verify the connectivity between the subnets, check the VPC routing, check the RDS instance status, create VPC flow logs, check CloudWatch metrics, and check the OS-level logs.\n\n"
}, {
  "id" : 557,
  "question" : "You have set up two VPCs:VPC A has the address of \"10.10.0.0/16\"\nIt also has a subnet with address space \"10.10.1.0/24\".VPC B has the address of \"10.11.0.0/16\"\nIt also has a subnet with address space \"10.11.1.0/28\"\nYou also have set up VPC peering connection between the two VPCs.\nWhat should be the respective route table entries in VPC A and VPC B?\n",
  "answers" : [ {
    "id" : "f26d5475bc4a4c0295b49cdf563fd660",
    "option" : "VPC B route table contains route with Destination as 10.10.0.0/16",
    "isCorrect" : "false"
  }, {
    "id" : "6a65852b65e440ab9a50f42380298c63",
    "option" : "VPC A route table contains route with Destination as 10.11.0.0/16.",
    "isCorrect" : "false"
  }, {
    "id" : "594e747f1cf4443aa68d743ee288281c",
    "option" : "VPC B route table contains route with Destination as 10.10.1.0/24 and VPC A route table contains route with Destination as 10.11.1.0/28.",
    "isCorrect" : "true"
  }, {
    "id" : "d0729d1d025741109bb881f8f721c29a",
    "option" : "VPC A route table contains route with Destination as 10.10.1.0/24 and VPC B route table contains route with Destination as 10.11.1.0/28.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nTo send private IPv4 traffic from your instance to an instance in a peer VPC, you must add a route to the route table that's associated with your subnet in which your instance resides.\nThe route points to the CIDR block (or a portion of the CIDR block) of the peer VPC in the VPC peering connection.\nThe owner of the other VPC in the peering connection must also add a route to their subnet's route table to direct traffic back to your VPC.\nFor more information about supported route table configurations for VPC peering connections.\nYou can also peer a VPC with a specific subnet of another VPC instead of peering the entire VPC.https://docs.aws.amazon.com/vpc/latest/peering/peering-configurations-partial-access.html.\nFor options A and B, they do not have a second route added to return the connection to requester VPC.\nSo they are incorrect.\nFor option C, as discussed above, we can configure subnets for a peering connection.\nSo VPC A route table configured VPC B's subnet 10.11.1.0/28, and VPC B route table configured VPC A's subnet 10.10.1.0/24\nThis configuration is correct from the given options.\nFor Option D, VPC A ad VPC B configured their own subnets in the respective route tables.\nSo, this configuration is incorrect.\n\nIn order to establish connectivity between two VPCs, you can create a VPC peering connection. VPC peering connection allows you to route traffic between two VPCs using private IP addresses.\nIn this scenario, we have two VPCs, VPC A and VPC B. VPC A has a subnet with an address space of \"10.10.1.0/24\", and VPC B has a subnet with an address space of \"10.11.1.0/28\". To enable communication between these two subnets, we need to create a VPC peering connection between VPC A and VPC B.\nWhen you create a VPC peering connection, you need to add a route to the route tables of each VPC.\nThe route table in VPC A should have a route to the IP address range of VPC B (10.11.0.0/16) via the VPC peering connection. Similarly, the route table in VPC B should have a route to the IP address range of VPC A (10.10.0.0/16) via the VPC peering connection.\nIn addition to these entries, you also need to add routes for the specific subnets that you want to communicate with. In this case, VPC A needs a route to the subnet in VPC B (10.11.1.0/28), and VPC B needs a route to the subnet in VPC A (10.10.1.0/24).\nTherefore, the correct answer is D. VPC A route table contains route with Destination as 10.10.1.0/24 and VPC B route table contains route with Destination as 10.11.1.0/28.\nNote that option A and B are incorrect because they do not include the routes for the specific subnets. Option C is incorrect because the route to the IP address range of the other VPC should be added, not the subnet address space.\n\n"
}, {
  "id" : 558,
  "question" : "Following are Security Group inbound rules.\nWhat is the correct statement below?\n\n",
  "answers" : [ {
    "id" : "998f3ede1e1646729cfbcad852f6d722",
    "option" : "Only some rules are correct.",
    "isCorrect" : "false"
  }, {
    "id" : "99a97f5639184880a09fc5ea593ade18",
    "option" : "HTTP port 80 for source 10.10.1.148/32 is duplicated.",
    "isCorrect" : "true"
  }, {
    "id" : "2da69c9977fc4318bff3a78b1669ba10",
    "option" : "SSH port 22 for source 10.10.1.0/28 is duplicated.",
    "isCorrect" : "false"
  }, {
    "id" : "44364e33bc324594bc194b896ef55ba7",
    "option" : "Custom UDP rule port 3000 for source 10.10.1.148/32 is duplicated.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nLet's take a look at the inbound rules.\nÂ· Rule # 3 defines ALL TCP allowed for 10.10.1.148 IP address.\nÂ· Rule # 2 and # 4 define port 80 and 22 allowed for IP addresses 10.10.1.0- 10.10.1.16.\nÂ· Rule # 1 defines port 80 for 10.10.1.148 IP address.\nÂ· Rule # 5 defines custom UDP port 3000 for 10.10.1.148 IP address.\nOut of these rules, only rule # 1 is duplicated with rule # 3.\nSo option B is correct.\n"
}, {
  "id" : 559,
  "question" : "Which of the following statements is correct for the route table of the VPC created with the primary CIDR of 20.0.0.0/16?\n\n",
  "answers" : [ {
    "id" : "b6446a86441a431cbfffdcff9917454a",
    "option" : "VPC peering connection route for VPC with a 30.0.0.0/20 IP range.",
    "isCorrect" : "false"
  }, {
    "id" : "42564bd740b840bbad25015e2a3788af",
    "option" : "VPN connection route for the remote network with a 30.0.0.0/20 IP range.",
    "isCorrect" : "false"
  }, {
    "id" : "1164bdc0c9ce4052b12769023531da26",
    "option" : "Direct Connect connection route for the remote network with a 30.0.0.0/20 IP range.",
    "isCorrect" : "false"
  }, {
    "id" : "010cf562537f48cfa7f3e3f159119c83",
    "option" : "Secondary IP CIDR range 30.0.0.0/20 for VPC with the local route.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nYou can associate secondary IPv4 CIDR blocks with your VPC.\nWhen you associate a CIDR block with your VPC, a route is automatically added to your VPC route tables to enable routing within the VPC (the destination is the CIDR block and the target is local).\nIn the following example, the VPC on the left has a single CIDR block (10.0.0.0/16) and two subnets.\nThe VPC on the right represents the architecture of the same VPC after you've added a second CIDR block (10.2.0.0/16) and created a new subnet from the range of the second CIDR.\nhttps://aws.amazon.com/about-aws/whats-new/2017/08/amazon-virtual-private-cloud-vpc-\nFrom the above image, the Main route table shows the routes for primary and secondary IP ranges.\nSo the correct option is.\nD.For option A, VPC peering connection route contains Target as pcx-xxxxxx.\nFor option B, the routing table should contain an entry with 'vgw-xxxxx' for a VPN connection.\nFor option C, the Direct Connect connection route should contain the Target as vgw-xxxxxx.\n\n"
}, {
  "id" : 560,
  "question" : "Your organization had setup a VPC with CIDR range 10.10.0.0/16\nThere are total 100 subnets within the VPC and are being actively used by multiple application teams.\nAn application team who is using 50 EC2 instances in subnet 10.10.55.0/24 complains there are intermittent outgoing network connection failures for around 30 random EC2 instances in a given day.\nHow would you troubleshoot issue with minimal configuration and minimal logs written?\n",
  "answers" : [ {
    "id" : "dba53e57329348c69c5a93ef42103c1f",
    "option" : "Create a flow log for the VPC and filter the logs in CloudWatch log group.",
    "isCorrect" : "false"
  }, {
    "id" : "7136d01584d84fa3abd5da7cc7bce624",
    "option" : "Create flow log for each EC2 instance network interface one by one and troubleshoot the connection issue.",
    "isCorrect" : "false"
  }, {
    "id" : "71242c47b9174ca39fef5569992830ff",
    "option" : "Create a flow log for subnet 10.10.55.0/24.",
    "isCorrect" : "true"
  }, {
    "id" : "31c641427ef34a2a914cac84956dd36b",
    "option" : "None of the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nVPC Flow Logs captures IP traffic going to and from network interfaces in your VPC.\nFlow log data is stored using Amazon CloudWatch Logs.\nAfter you've created a flow log, you can view and retrieve its data in Amazon CloudWatch Logs.\nYou can create a flow log for a VPC, a subnet, or a network interface.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#flow-logs-basics\nVPC Flow Logs capture following information and logs them to CloudWatch logs,\nversion account-id interface-id srcaddr dstaddr srcport dstport protocol packets bytes start end action log-status.\nFind more information about each record here.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#flow-log-records\nFor option A, although creating a flow log for entire VPC would work, it captures lot of unrequired information from rest 99 subnets and finding out the affected EC2 instances from CloudWatch logs would become really troublesome.\nFor Option B, creating flow log at each EC2 network interface would work, but it takes log of configuration and time consuming trial and error troubleshooting.\nFor Option C, creating a flow log for the subnet would capture just the traffic going in and out of the subnet.\nThis would help us identify the network trace for the affected EC2 instances and find out the root cause in timely manner.\n\nThe best option to troubleshoot the network connection issue with minimal configuration and minimal logs written is option A, which is to create a flow log for the VPC and filter the logs in CloudWatch log group.\nA flow log captures information about the IP traffic going to and from network interfaces in a VPC. By creating a flow log for the VPC, we can capture the traffic flow information for all the subnets within the VPC, including the subnet 10.10.55.0/24 where the EC2 instances are located. This allows us to see the network traffic flow and detect any anomalies or issues.\nBy filtering the flow log in the CloudWatch log group, we can search for the specific traffic flow related to the EC2 instances in subnet 10.10.55.0/24 that are experiencing the intermittent outgoing network connection failures. With the flow log, we can see if there are any issues with the traffic flow, such as a high number of rejected packets or dropped connections. This information can help identify the root cause of the issue.\nOption B, which is to create flow logs for each EC2 instance network interface one by one, is not recommended as it is time-consuming and resource-intensive. It is also not practical for troubleshooting network issues on a large scale.\nOption C, which is to create a flow log for subnet 10.10.55.0/24, only captures traffic flow information for that specific subnet. This may not provide enough context to identify the root cause of the issue, as the issue may be caused by traffic flow from other subnets within the VPC.\nTherefore, the best option for troubleshooting the intermittent outgoing network connection failures is to create a flow log for the VPC and filter the logs in CloudWatch log group.\n\n"
}, {
  "id" : 561,
  "question" : "As a Cloud Architect, you have deployed an existing application from the local server to an On-demand EC2 instance.\nYou found out that there is an issue while connecting the application using the HTTPS Protocol.\nAfter troubleshooting the issue, you added port 443 to the security group of the instance.\nHow much time will it take to update changes to all of the resources related to Security groups?\n",
  "answers" : [ {
    "id" : "6b09c1b8d04c44e9b785f78beb4ac069",
    "option" : "It can take up to 10 minutes depending on the number of resources.",
    "isCorrect" : "false"
  }, {
    "id" : "0475de8610654eeb8bcd612cad1f609c",
    "option" : "You just need to restart the EC2 Server.",
    "isCorrect" : "false"
  }, {
    "id" : "c2b4bb1742d14da1be0e864fbadd3262",
    "option" : "You cannot make any change to existing security group, you have to create new Security group.",
    "isCorrect" : "false"
  }, {
    "id" : "3c5a0c8961e444f9afcb4d7604911c04",
    "option" : "Immediately without restart.",
    "isCorrect" : "true"
  }, {
    "id" : "0be15202c0c9444a9b941a45d1b74a39",
    "option" : "You have to deploy your application again.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - D.\nYou can assign a security group to an instance when you launch the instance.\nWhen you add or remove rules, those changes are automatically applied to all instances to which you've assigned the security group.\nOption A is incorrect.\nAny changes made to the Security Group are immediately effected.\nOption B is incorrect because you don't need to restart the server to check any Security Group updates.\nOption C is incorrect because you can modify rules in the security group.\nOption D is CORRECT because any changes made to the security group are taken into effect immediately.\nOption E is incorrect because this security group works at the instance level, not at the application level.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#adding-security-group-rule\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/working-with-security-groups.html\n\nWhen you add a new port or modify the rules of an existing Security Group in Amazon Web Services (AWS), the changes are propagated to all the instances associated with that Security Group. However, the time it takes for the changes to take effect can vary based on multiple factors.\nOption A is partially correct. It is true that the time it takes for the changes to take effect depends on the number of resources, but it is not the only factor. The changes can take anywhere from a few seconds to several minutes, depending on the number of instances, the size of the Security Group, and the AWS Region.\nOption B is incorrect because restarting the EC2 instance is not necessary to apply the changes to the Security Group.\nOption C is also incorrect. You can modify the rules of an existing Security Group. You don't have to create a new one.\nOption D is partially correct. The changes to the Security Group should take effect immediately. However, it may take some time for the instances to start using the updated rules.\nOption E is incorrect. You don't have to redeploy the application to apply changes to the Security Group.\nIn summary, the correct answer is Option A. The time it takes for the changes to take effect depends on multiple factors, including the number of instances and the size of the Security Group. It can take anywhere from a few seconds to several minutes.\n\n"
}, {
  "id" : 562,
  "question" : "You are working as Cloud Solutions Engineer in an IT firm, and the firm has set up multiple VPN connections.\nThey want to provide secure communication between multiple sites using the AWS VPN Cloud Hub.\nWhich statement is the most accurate in describing what you must do to set this up correctly? How do you connect multiple sites to a VPC?\n",
  "answers" : [ {
    "id" : "47bd141ba62c42a3839bf1238bca8804",
    "option" : "Create a virtual public gateway with multiple customer gateways, each with a unique Private subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "2b3b5c23ed124c1bacf08d5a893ba5b5",
    "option" : "Create a virtual private gateway with multiple customer gateways, each with unique Border Gateway Protocol (BGP) Autonomous System Numbers (ASNs).",
    "isCorrect" : "true"
  }, {
    "id" : "8a5f003668fa47cfb11e4ea088c453b3",
    "option" : "Create a virtual private gateway with multiple customer gateways, each with unique subnet Id.",
    "isCorrect" : "false"
  }, {
    "id" : "cffa6a1e895a41658b39e863b1fd7eb8",
    "option" : "Create a virtual private gateway with multiple customer gateways, each with a unique set of keys.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption B is CORRECTbecause to use AWS VPN Cloud Hub, one must create a virtual private gateway with multiple customer gateways, each with a unique Border Gateway Protocol (BGP) Autonomous System Number (ASN).\nOption A, C, D are incorrect because the condition to use AWS VPN Cloud Hub is not fulfilled.\nRefer: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-vpn-cloudhub-network-to-amazon.html.\n\nTo connect multiple sites to a VPC using the AWS VPN Cloud Hub, you need to create a virtual private gateway (VGW) and multiple customer gateways (CGWs). The CGWs are virtual appliances that you set up at your on-premises location, and they act as the VPN endpoints to your VPC.\nThe correct answer is B. You should create a virtual private gateway with multiple customer gateways, each with unique Border Gateway Protocol (BGP) Autonomous System Numbers (ASNs).\nBGP is a routing protocol that is used to exchange routing information between different autonomous systems (AS) on the internet. In this case, the VGW and CGWs are different ASs, and BGP is used to exchange routing information between them.\nTo set up the AWS VPN Cloud Hub, you need to follow these steps:\nCreate a virtual private gateway (VGW) for the VPC where you want to connect multiple sites. Create customer gateways (CGWs) for each on-premises location where you want to connect to the VPC. Each CGW should have a unique Border Gateway Protocol (BGP) Autonomous System Number (ASN). Create a VPN connection between the VGW and each CGW. Configure BGP on each CGW to advertise the on-premises network to the VGW. Configure the VGW to propagate the BGP routes to the other VPN connections. Configure the security group rules and routing tables in the VPC to allow traffic to and from the on-premises network.\nBy following these steps, you can create a secure communication between multiple sites using the AWS VPN Cloud Hub.\n\n"
}, {
  "id" : 563,
  "question" : "You are working in a College as a Cloud Technical Advisor, and your college was maintaining all its data locally where they felt security and redundancy issues.\nSo, you suggested deploying the application in AWS and use a NoSQL database for their database.\nWhile deploying the servers in AWS, the team needs your suggestion for creating newSecurity Groups.\nCan you select which of the following Option given by the team is true? (Select 2)\n",
  "answers" : [ {
    "id" : "583113f3ca044f2ab6b220c2ffc313eb",
    "option" : "Security Group supports \"allow rules\" and \"deny rules\".",
    "isCorrect" : "false"
  }, {
    "id" : "40673ed2d1e541639217872b4613cf4e",
    "option" : "The default rules in asecurity group disallows all incoming traffic.",
    "isCorrect" : "true"
  }, {
    "id" : "a3486c928d954b778935fa6dd95b5b86",
    "option" : "By default, outbound traffic is allowed",
    "isCorrect" : "true"
  }, {
    "id" : "f90853dc7ae24a568c6c5d5f0c2fe47e",
    "option" : "Security Group is the first layer of security.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nOption A is incorrect because the Security Group supports allow rules only.\nTo deny rules, Network ACLs should be used.\nOption B is CORRECT because, by default, custom SG has no Inbound rules.\nOption C is CORRECT because, by default, all Outgoing Traffic is allowed in the custom Security Group.\nOption D is incorrect because the security group works at the instance level, hence not the first security level.The first level is NACL to the subnet, and SG is to the instance.\nRefer: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\n\nSure, I'll be happy to provide a detailed explanation.\nAs a Cloud Technical Advisor, you suggested deploying the application in AWS and use a NoSQL database for their database. Now the team needs your suggestion for creating new Security Groups.\nSecurity Groups in AWS are virtual firewalls that control inbound and outbound traffic for instances. Security groups act as the first line of defense in protecting instances from malicious traffic. They act as a virtual firewall for instances, controlling inbound and outbound traffic.\nNow let's take a look at the given options:\nA. Security Group supports \"allow rules\" and \"deny rules\": This option is true. Security groups allow you to specify both \"allow rules\" and \"deny rules\". These rules control which traffic is allowed to enter or leave the instances associated with the security group. You can specify both the source and destination of the traffic as well as the protocol and port numbers.\nB. The default rules in a security group disallows all incoming traffic: This option is false. By default, when you create a new security group, all inbound traffic is blocked, but all outbound traffic is allowed. You can, however, modify the default rules to allow specific types of inbound traffic.\nC. By default, outbound traffic is allowed: This option is true. By default, all outbound traffic is allowed from instances associated with a security group. You can create rules to restrict outbound traffic, but it's not recommended.\nD. Security Group is the first layer of security: This option is true. Security groups act as the first line of defense in protecting instances from malicious traffic. They act as a virtual firewall for instances, controlling inbound and outbound traffic.\nIn summary, options A and C are true, and options B and D are also true.\n\n"
}, {
  "id" : 564,
  "question" : "An IT company hired you recently as Cloud Architect, and your Manager told you that the team is trying to host an Application on an EC2 Server with instance type as t2.micro.\nThe team has created a security group named APP_SG and sent it for User Acceptance Testing where the tester complained that they cannot access the website.\nWhat could be the issue from the scenario?\n",
  "answers" : [ {
    "id" : "f4d60dd405a04e5fbcd6e8593ccc9789",
    "option" : "Network ACL is blocking the user from accessing the application.",
    "isCorrect" : "false"
  }, {
    "id" : "72f189c5675c44e8b2fe4e1a344fd023",
    "option" : "By default, the new security group allows no inbound traffic.",
    "isCorrect" : "true"
  }, {
    "id" : "c4dddf4ee5b24734a055a62693529940",
    "option" : "You should configure Network ACLs to allow all inbound traffic and allow all outbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "6e5a3d25e1de4e878db14b122cfe837c",
    "option" : "You should configure IAM Roles before using the application.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nOption A is incorrect because, by default, both inbound and outbound traffic is allowed.\nOption B is CORRECT because the default rules for a custom security group that you create allows no inbound traffic and allows all outbound traffic.\nThere are two kinds of Security Groups:\nDefault Security Group.\nCustom Security Group.\nThe following are the default rules for a Default Security Group:\n- Allows all inbound traffic.\n- Allows all outbound traffic.\nIn the question, a team has created a security group means it is a Custom Security Group.\nThe below image signifies default rules for the custom security groups.\nOption C is incorrect because Network ACLs are used for both allow and deny rules.\nOption D is incorrect IAM Roles are used for Access management, not for traffic rules.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/default-custom-security-groups.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-rules\n\n\nBased on the scenario provided, the team is trying to host an application on an EC2 instance with the instance type of t2.micro, and they have created a security group named APP_SG. The user acceptance testing has revealed that the website cannot be accessed. This suggests that there is an issue with the security group configuration.\nThe correct answer is likely to be B, which is that by default, the new security group allows no inbound traffic. This means that the security group is not allowing any traffic to reach the EC2 instance, which is preventing users from accessing the website hosted on it.\nTo resolve this issue, the security group needs to be configured to allow inbound traffic on the relevant ports. The security group should allow traffic on port 80 for HTTP and port 443 for HTTPS if the application is a web-based one.\nIt is also important to note that Network ACLs (Answer A) may be another reason for the issue. However, Network ACLs operate at the subnet level and are stateless, so it is unlikely to be blocking user access to the application.\nAnswer C suggests that Network ACLs should be configured to allow all inbound and outbound traffic, but this is not recommended as it would compromise the security of the network. The principle of least privilege should be applied when configuring Network ACLs and Security Groups.\nAnswer D, which suggests that IAM Roles should be configured before using the application, is not relevant to the issue at hand. IAM roles are used to manage permissions and access to AWS resources and services, but they do not impact the accessibility of the application hosted on the EC2 instance.\n\n"
}, {
  "id" : 565,
  "question" : "You are appointed as Cloud Consultant in a Cloud Solutions Firm.\nThey have the following VPCs set up in the US East Region: The first VPC with CIDR block 172.10.0.0/16 having a subnet with CIDR block 172.10.10.0/24\nThe second VPC with CIDR block 192.168.0.0/16, having a subnet with CIDR block 192.168.20.0/24\nYour colleague is trying to establish a network connection between two subnets, a subnet with CIDR block 172.10.10.0/24 and another subnet with CIDR block 192.168.20.0/24\nAlso, they don't want any transitive peering relationship.\nThe connection should not have a single point of failure for communication or a bandwidth bottleneck.\nWhich of the following is the easiest solution?\n",
  "answers" : [ {
    "id" : "54be48730e4343f0b78078bb44ec8d87",
    "option" : "Use VPC Peering i.e. AWS-provided network connectivity between two VPCs.",
    "isCorrect" : "true"
  }, {
    "id" : "527f6e5d6dab4d91a3a7f2ea38fa34a3",
    "option" : "Use Software VPN i.e. Software appliance to VPN connection between VPCs.",
    "isCorrect" : "false"
  }, {
    "id" : "ee988a623c1648b288f13fda6041f438",
    "option" : "Use VPC-to-VPC routing over IPsec VPN connection.",
    "isCorrect" : "false"
  }, {
    "id" : "aa34bbb5ffaf42128fa8e5bbe878265f",
    "option" : "VPC-to-VPC routing in an AWS Direct Connect location.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nOption A is CORRECT because VPC Peering uses the existing infrastructure of a VPC and this solution is straightforward that facilitates the transfer of data between two VPCs.\nOptions B, C, and D are incorrect because they are not the easiest solution or may be a single point of failure for the connections.\nRefer: https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html.\n\nTo establish a network connection between two subnets located in different VPCs, there are several options available in AWS. The chosen solution should meet the requirements of not having a single point of failure for communication and not creating a transitive peering relationship between VPCs.\nOption A - Use VPC Peering: AWS VPC peering is a simple and easy-to-use solution for connecting two VPCs in the same or different regions. With VPC peering, the traffic between the subnets travels directly over the AWS network, and there is no need for a VPN connection or a dedicated physical connection.\nHowever, VPC peering creates a transitive peering relationship between VPCs, which means that traffic from one VPC can flow through another VPC to reach a third VPC. This is not desirable in this case, so option A is not the best solution.\nOption B - Use Software VPN: A software VPN allows for a secure connection between two VPCs over the internet using a software appliance. This option provides secure connectivity between VPCs but requires additional configuration, management, and potential costs for the software VPN appliance.\nAdditionally, using a software VPN appliance for connectivity may create a bottleneck in bandwidth or a single point of failure if the appliance goes down. Therefore, this option does not meet the requirements of the question.\nOption C - Use VPC-to-VPC routing over IPsec VPN connection: Using VPC-to-VPC routing over an IPsec VPN connection provides a secure and encrypted connection between the two VPCs while also meeting the requirements of not creating a transitive peering relationship and not having a single point of failure for communication.\nThis solution requires configuring a customer gateway, virtual private gateway, and VPN connection between the VPCs, which can be a bit more complicated than VPC peering or software VPN. However, this option provides secure connectivity between the VPCs and can be more scalable than software VPN.\nOption D - VPC-to-VPC routing in an AWS Direct Connect location: AWS Direct Connect is a dedicated physical connection between the on-premises infrastructure and AWS. This option is not necessary in this scenario as there is no on-premises infrastructure involved. Therefore, option D is not the best solution.\nBased on the requirements mentioned in the question, Option C is the easiest and most appropriate solution. However, depending on the specific requirements and constraints of the scenario, any of the other options may also be viable.\n\n"
}, {
  "id" : 566,
  "question" : "You are working as a Cloud Solutions Architect in a Series-B funding startup.\nThe Senior Director asked you to deploy a data mining server for their financial data on a Reserved EC2 instance in any region using IPv6\nAs the data is related to the financial, the company's CEO was worried about the security.\nHe suggested that the system be highly secured to avoid unauthorized access, and other security features must also be implemented.\nIn order to follow the instruction given by your CEO, which of the following VPC feature you will implement to achieve the given security?\n",
  "answers" : [ {
    "id" : "b31ed31998f9471088f78d93dae1552c",
    "option" : "VPC Peering",
    "isCorrect" : "false"
  }, {
    "id" : "e5893228394d432aa0cc41e48daa8cbf",
    "option" : "NAT Instances",
    "isCorrect" : "false"
  }, {
    "id" : "53b7de3011ea4a85be2a47907a5eb165",
    "option" : "Egress-only Internet Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "1b5903da2a9643e2a47095d1c148b3b2",
    "option" : "NAT Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nOption C is CORRECT because egress-only Internet gateway is a VPC component that allows outbound communication over IPv6 from instances in your VPC to the Internet, and prevents the Internet from initiating an IPv6 connection with your instances.\nOption A is incorrect because VPC Peering is used to connect multiple VPCs.\nOptions B and D are incorrect if communication is required using IPV6.\nRefer: https://docs.aws.amazon.com/vpc/latest/userguide/egress-only-internet-gateway.html.\n\nTo meet the requirements for a secure data mining server on an IPv6 reserved EC2 instance, we should implement a NAT Gateway.\nA NAT Gateway is a highly available, managed service that provides network address translation (NAT) for instances in private subnets to access the Internet. NAT Gateway allows outbound traffic from the private subnet to the Internet, while blocking inbound traffic from the Internet to the private subnet. It also provides a layer of security by hiding the IP addresses of the instances in the private subnet from the Internet.\nBy using NAT Gateway, we can ensure that the data mining server can securely access the Internet to fetch data required for analysis, while preventing unauthorized access from the Internet. Additionally, using IPv6 addressing is beneficial in terms of security, as it can provide a larger address space and better security features than IPv4.\nVPC peering allows communication between two VPCs, but it does not provide any security features. Egress-only Internet Gateway only allows outbound traffic from the VPC to the Internet, but does not provide NAT functionality. NAT Instances are less recommended than NAT Gateway because they require additional setup and management, and do not provide high availability by default.\nTherefore, the best option to achieve secure data mining server access from IPv6 on a Reserved EC2 instance is to use NAT Gateway.\n\n"
}, {
  "id" : 567,
  "question" : "You are working in a gaming company with four departments that make games for iOS, Android, Windows, and PlayStation.\nFor that, they just recently adopted a hybrid cloud architecture where their on-premise data center is connected to their Amazon VPC.\nYour VPC is configured with a CIDR block of 10.0.0.0/24 (256 IPs)\nYour supervisor told you that they need such security in all four departments so that information from one department should not reach other departments.\nThey don't want to have a new network that can be expensive and will create more overhead.\nAs a Solutions Architect, how will you configure your network to accomplish this requirement?\n",
  "answers" : [ {
    "id" : "29f15efad7c4424db8fdbd8d84de35bf",
    "option" : "Create four subnets where first one subnet will use CIDR block 10.0.0.0/28 (for addresses 10.0.0.0 - 10.0.0.63), the second subnet will use CIDR block 10.0.0.64/28 (for addresses 10.0.0.64 - 10.0.0.127), third subnet will use CIDR block 10.0.0.128/28 (for addresses 10.0.0.128 - 10.0.0.191), the fourth one will use CIDR block 10.0.0.192/28 (for addresses 10.0.0.192 - 10.0.0.255).",
    "isCorrect" : "false"
  }, {
    "id" : "8b1e23e782f44fde9f53a8bdf6fc3d10",
    "option" : "Create four subnets where first one subnet will use CIDR block 10.0.0.0/26 (for addresses 10.0.0.0 - 10.0.0.63), the second subnet will use CIDR block 10.0.0.64/26 (for addresses 10.0.0.64 - 10.0.0.127), third subnet will use CIDR block 10.0.0.128/26 (for addresses 10.0.0.128 - 10.0.0.191), the fourth one will use CIDR block 10.0.0.192/26 (for addresses 10.0.0.192 - 10.0.0.255).",
    "isCorrect" : "true"
  }, {
    "id" : "2b766bd596ec4bc88bc756307b8f1f1a",
    "option" : "Create four subnets where first one subnet will use CIDR block 10.0.0.0/32 (for addresses 10.0.0.0 - 10.0.0.63), the second subnet will use CIDR block 10.0.0.64/32 (for addresses 10.0.0.64 - 10.0.0.127), third subnet will use CIDR block 10.0.0.128/32 (for addresses 10.0.0.128 - 10.0.0.191), the fourth one will use CIDR block 10.0.0.192/32 (for addresses 10.0.0.192 - 10.0.0.255)",
    "isCorrect" : "false"
  }, {
    "id" : "7f79327010d24d6ba3be56d953edc36c",
    "option" : "Create four subnets where first one subnet will use CIDR block 10.0.0.0/26 (for addresses 10.0.0.0 - 10.0.0.64), the second subnet will use CIDR block 10.0.0.65/26 (for addresses 10.0.0.65 - 10.0.0.128), third subnet will use CIDR block 10.0.0.129/26 (for addresses 10.0.0.129 - 10.0.0.192), the fourth one will use CIDR block 10.0.0.193/26 (for addresses 10.0.0.193 - 10.0.0.255)",
    "isCorrect" : "false"
  }, {
    "id" : "24a52e299faa41229d76747137905222",
    "option" : "The requirement proposed by the supervisor is impossible.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B\nOption B is CORRECT because to achieve the requirement, you should create four subnets and CIDR 24\nAfter creating four Subnets, it will be CIDR 26, and the range will start from 0 to 63, 64 to 127, 128 to 191, and 192 to 255.\nOption A is incorrect because CIDR 28 will result in 16 subnets.\nOption C is incorrect because CIDR 32 represents the IP address itself.\nOption D is incorrect because CIDR block 10.0.0.0/26 represents 10.0.0.0 - 10.0.0.63 not 10.0.0.0 - 10.0.0.64, then it becomes 65 IP addresses which is incorrect.\nOption E is incorrect because the supervisor's requirement can be configured.\nRefer: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html.\n\nThe correct answer is A. Create four subnets where first one subnet will use CIDR block 10.0.0.0/28 (for addresses 10.0.0.0 - 10.0.0.63), the second subnet will use CIDR block 10.0.0.64/28 (for addresses 10.0.0.64 - 10.0.0.127), third subnet will use CIDR block 10.0.0.128/28 (for addresses 10.0.0.128 - 10.0.0.191), the fourth one will use CIDR block 10.0.0.192/28 (for addresses 10.0.0.192 - 10.0.0.255).\nExplanation:\nAmazon Virtual Private Cloud (VPC) is a logically isolated section of Amazon Web Services (AWS) that allows you to launch AWS resources in a virtual network. A VPC provides you with complete control over your virtual networking environment, including the selection of your IP address range, creation of subnets, and configuration of route tables and network gateways.\nIn this scenario, the gaming company has adopted a hybrid cloud architecture where their on-premise data center is connected to their Amazon VPC. The VPC has a CIDR block of 10.0.0.0/24, which means it has 256 IP addresses. The requirement is to create four subnets where each department will have its own subnet, and no information should be shared between the departments.\nTo achieve this requirement, we need to divide the available IP addresses into four subnets. The subnet size must be such that it can accommodate the required number of hosts while still being able to provide enough IP addresses for future growth. Additionally, it should be small enough to prevent unnecessary IP address wastage.\nOption A proposes the creation of four subnets, each with a CIDR block of /28. A /28 subnet has 16 IP addresses, out of which two IP addresses are reserved for network address and broadcast address, leaving 14 usable IP addresses per subnet. Using this approach, we can create four subnets, each with 14 IP addresses, which will accommodate the required number of hosts.\nOption B proposes the creation of four subnets, each with a CIDR block of /26. A /26 subnet has 64 IP addresses, out of which two IP addresses are reserved for network address and broadcast address, leaving 62 usable IP addresses per subnet. While this approach will provide enough IP addresses for future growth, it will also result in unnecessary IP address wastage, as each department requires only a few hosts.\nOption C proposes the creation of four subnets, each with a CIDR block of /32. A /32 subnet has only one IP address, making it unsuitable for creating multiple hosts in a subnet. Therefore, this option is not feasible.\nOption D proposes the creation of four subnets, each with a CIDR block of /26. However, the second subnet is using CIDR block 10.0.0.65/26, which overlaps with the first subnet. Overlapping subnets will result in IP address conflicts and will prevent the network from functioning correctly. Therefore, this option is also not feasible.\nOption E suggests that the requirement proposed by the supervisor is impossible, but this is not true. It is possible to create multiple subnets within a VPC to ensure that different departments are isolated from each other.\nIn conclusion, option A is the correct answer because it creates four subnets, each with a CIDR block of /28, which is the appropriate size to accommodate the required number of hosts while minimizing IP address wastage.\n\n"
}, {
  "id" : 568,
  "question" : "A 50 year old Computer Solutions company has a very big application that needs to be deployed to the AWS cloud from its existing server.\nThe application is media access control (MAC) address dependent as per the application licensing terms.\nThis application will be deployed in an on-demand EC2 instance with instance type r4.2xlarge.\nIn this scenario, how can you ensure that the MAC address of the EC2 instance will not change even if the instance is restarted or rebooted?\n",
  "answers" : [ {
    "id" : "2d16dee0c9f745e1b4df6808bf5982c1",
    "option" : "Assign static MAC Address to EC2 instance while setting up the server.",
    "isCorrect" : "false"
  }, {
    "id" : "7c409360ce7a45679f0f32469a2d75b1",
    "option" : "Use a VPC with an elastic network interface that has a fixed MAC Address.",
    "isCorrect" : "true"
  }, {
    "id" : "f012a3c9e1b243dfb061163689f16076",
    "option" : "Use a VPC with a private subnet for the EC2, by default MAC address will be fixed.",
    "isCorrect" : "false"
  }, {
    "id" : "79ea637c41fe4162a320309efcd8fbea",
    "option" : "Use a VPC with a private subnet and configure the MAC address to be tied to that subnet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B\nOption A is incorrect because you cannot assign a static MAC Address to EC2 Server.\nIf the server restarts, it will also change.\nOption B is correct because for the server to be MAC Dependent, you must use VPC with an ENI (Elastic Network Interface).\nOption C and D are incorrect because using a private subnet in VPC cannot help get MAC Address fixed.\nRefer: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html.\n\nThe correct answer is B - Use a VPC with an elastic network interface that has a fixed MAC Address.\nExplanation: When an Amazon Elastic Compute Cloud (EC2) instance is launched, a dynamic Media Access Control (MAC) address is assigned to it. However, for some applications, such as the one in this scenario, the licensing terms are MAC address-dependent, so it is essential to maintain a fixed MAC address to ensure proper licensing.\nTo achieve this, we can use an elastic network interface (ENI) in a Virtual Private Cloud (VPC) with a fixed MAC address. An ENI can be attached to an EC2 instance, and it acts as a virtual network interface card (NIC) that provides connectivity to the VPC network.\nBy attaching an ENI with a fixed MAC address to an EC2 instance, the MAC address of the instance will remain constant even if the instance is restarted or rebooted. This is because the MAC address is associated with the ENI and not the instance itself.\nTherefore, the best option for ensuring that the MAC address of the EC2 instance remains constant is to use a VPC with an ENI that has a fixed MAC address (option B). Option A is not recommended as it is not possible to assign a static MAC address to an EC2 instance. Option C is incorrect because a private subnet does not guarantee a fixed MAC address, and option D is incorrect because the MAC address is tied to the ENI and not the subnet.\nNote that there are some limitations to using an ENI with a fixed MAC address, such as the number of ENIs that can be attached to an instance, and the region-specific availability of certain instance types that support the use of ENIs.\n\n"
}, {
  "id" : 569,
  "question" : "One of your colleagues, who is new to the company where you work as a cloud Architect, has some issues with IP Addresses.\nHe has created an Amazon VPC with an IPV4 CIDR block 10.0.0.0/24, but now there is a requirement of hosting a few more resources to that VPC.\nAs per his knowledge, he is thinking of creating a new VPC with a greater range.\nCould you suggest to him a better way that should be reliable?\n",
  "answers" : [ {
    "id" : "112a9d41f6ff4532ad0d0ffb3ae2ffc1",
    "option" : "Delete the existing subnets in the VPC and create new Subnets in VP.",
    "isCorrect" : "false"
  }, {
    "id" : "5643a94bc0724b028df4bc84046bf015",
    "option" : "He is thinking of the right approach.",
    "isCorrect" : "false"
  }, {
    "id" : "d07b13f9efc84bc483d32d6c9f19b4f6",
    "option" : "You can create new VPC and connect old VPC with a new one.",
    "isCorrect" : "false"
  }, {
    "id" : "64f98b4a57534b669e09454bb5a4a9b4",
    "option" : "You can expand existing VPC by adding Secondary CIDR to your current VP.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nOptions A, B, C are incorrect because it is not reliable to go for this type of approach as VPC to VPC connection will take new resources like VPC Peering.\nCreating a new VPC or Subnet is also not suggested.\nOption D is correct because you can associate Secondary CIDR to your current VPC to accommodate more hosts.\nRefer: https://docs.aws.amazon.com/vpc/latest/userguide/working-with-vpcs.html#add-ipv4-cidr.\n\nThe best option for adding more resources to an Amazon VPC with an IPV4 CIDR block of 10.0.0.0/24 is to expand the existing VPC by adding a secondary CIDR block. This can be achieved without deleting any existing subnets or creating a new VPC.\nAdding a secondary CIDR block to an existing VPC enables you to increase the number of available IP addresses without having to create a new VPC. This is especially useful if you have already deployed resources in the VPC and do not want to delete and recreate them in a new VPC.\nTo add a secondary CIDR block to an existing VPC, follow these steps:\nOpen the Amazon VPC console at https://console.aws.amazon.com/vpc/. In the navigation pane, choose \"Your VPCs.\" Select the VPC for which you want to add a secondary CIDR block. Choose the \"Edit CIDRs\" button. In the \"IPv4 CIDR blocks\" section, choose \"Add IPv4 CIDR.\" Enter the new CIDR block that you want to add and choose \"Save.\"\nAfter adding the secondary CIDR block, you can create new subnets within the VPC that use the new CIDR block. You can also modify existing subnets to use the new CIDR block.\nIn conclusion, the best option for adding more resources to an Amazon VPC with an IPV4 CIDR block of 10.0.0.0/24 is to expand the existing VPC by adding a secondary CIDR block. This option is reliable, does not require deleting any existing subnets or creating a new VPC, and enables you to increase the number of available IP addresses within the existing VPC.\n\n"
}, {
  "id" : 570,
  "question" : "As an AWS Solutions Architect, you are helping the team to set up an AWS Site-to-Site VPN so that the AWS resources in a VPC can communicate with one remote site.\nYou plan to use IPSec VPN tunnels in the VPN connections as they provide secure connections with redundancy.\nWhen creating the Site-to-Site VPN connection in AWS console, which of the following options can you configure for the VPN tunnels?\n",
  "answers" : [ {
    "id" : "456296ddeccb4a29a264a189faf8135b",
    "option" : "The number of VPN tunnels.",
    "isCorrect" : "false"
  }, {
    "id" : "b19a7361faf648c598fff2848d9e9073",
    "option" : "The encryption algorithms used by the VPN tunnels.",
    "isCorrect" : "true"
  }, {
    "id" : "b22da341db144e2f8d592de610601ddc",
    "option" : "The memory used by the VPN tunnels.",
    "isCorrect" : "false"
  }, {
    "id" : "83d461ffb910416f9fdfa927092b97c8",
    "option" : "The TCP ports allowed by the VPN tunnels.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect because AWS Site-to-Site VPN has two tunnels for redundancy and users cannot modify this setting.\nOption B is CORRECT because the encryption algorithms of VPN IPSec tunnels can be configured by users such as AES128 and AES256\nPlease check the following screenshot when configuring the VPN tunnels:\nAWS Site-to-Site VPN provides two tunnels for redundancy shown by the following snapshot:\nWhen one tunnel becomes unavailable, network traffic is automatically routed to the other available tunnel.\nOption C is incorrect because users cannot configure memories used by VPN tunnels.\nOption D is incorrect because when creating a VPN tunnel, users cannot allow or block any TCP ports.\nReferences:\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/how_it_works.html\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/SetUpVPNConnections.html\nhttps://docs.aws.amazon.com/vpn/latest/s2svpn/VPNTunnels.html\n\n\nAs an AWS Solutions Architect, when creating a Site-to-Site VPN connection in AWS console, you can configure the following options for the VPN tunnels:\nA. The number of VPN tunnels: You can configure the number of VPN tunnels you want to create for redundancy purposes. Two tunnels are created by default for each VPN connection, but you can create up to four tunnels.\nB. The encryption algorithms used by the VPN tunnels: You can choose from a variety of encryption algorithms, such as AES-128, AES-192, AES-256, or Triple DES (3DES). The encryption algorithm determines how the data is encrypted and decrypted by the VPN tunnels.\nC. The memory used by the VPN tunnels: Memory configuration is not an option when creating Site-to-Site VPN connection in AWS console.\nD. The TCP ports allowed by the VPN tunnels: The TCP ports allowed by the VPN tunnels are not configurable during the creation of Site-to-Site VPN connection in AWS console. However, you can use security groups to control the traffic allowed over the VPN connection.\nIt's important to note that when setting up Site-to-Site VPN connections, you need to ensure that the IP address ranges used in the VPC and remote network do not overlap. Additionally, you will need to configure the remote site's VPN gateway to establish the VPN connection with AWS.\n\n"
}, {
  "id" : 571,
  "question" : "Which of the following default settings are INCORRECT for a newly created S3 bucket? (choose 2 options)\n",
  "answers" : [ {
    "id" : "5ef06786e18a4bdd8310ce5355e94e47",
    "option" : "Encryption is not enabled.",
    "isCorrect" : "false"
  }, {
    "id" : "ac2a41d6300349f992dc13ac8a879327",
    "option" : "Transfer Acceleration is enabled.",
    "isCorrect" : "true"
  }, {
    "id" : "874a7640130d4ad89cd103ea0b146412",
    "option" : "No bucket policy exists.",
    "isCorrect" : "false"
  }, {
    "id" : "9b82802cc8474aecab0ab9838de0f7b8",
    "option" : "Versioning is enabled.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: B, D.\nWhen creating an S3 bucket, you can change the default configuration according to your requirements or leave the default options and continue to create the bucket.\nYou can always change the configuration after you created the bucket.\nFor option A, Default encryption is not enabled.\nFor option B, Transfer Acceleration is suspended by default.\nFor Option C, the bucket policy does not exist by default.\nWe can restrict bucket access through the bucket policy.\nFor option D, By default, Versioning is Disabled.\nNote:\nThe question is, \"Which of the following options are incorrect in terms of default settings?\"\nA.\nEncryption is not enabled.\n-- we have to select the incorrect in terms of default settings, so it's not the answer.\nB.\nTransfer Acceleration is enabled.-- we have to select the incorrect in terms of default settings, so it's correct the answer.\nC.\nNo bucket policy exists.-- we have to select the incorrect in terms of default settings, so it's not the answer.\nD.\nVersioning is enabled.-- we have to select the incorrect in terms of default settings, so it's correct the answer.\n\n\nThe two incorrect default settings for a newly created S3 bucket are:\nB. Transfer Acceleration is enabled: Transfer Acceleration is not enabled by default on newly created S3 buckets. Transfer Acceleration is an Amazon S3 feature that enables faster data transfer to and from your S3 bucket over the internet using Amazon CloudFront's globally distributed edge locations. However, you need to explicitly enable this feature on your bucket if you want to use it.\nD. Versioning is enabled: Versioning is not enabled by default on newly created S3 buckets. S3 versioning allows you to store multiple versions of an object in the same bucket. When versioning is enabled, any object that is uploaded to the bucket will be assigned a unique version ID. This feature is optional, and you need to enable it explicitly on your bucket if you want to use it.\nThe other options are both true:\nA. Encryption is not enabled: By default, newly created S3 buckets do not have server-side encryption enabled. However, you can easily enable it by selecting the encryption option of your choice from the S3 bucket properties.\nC. No bucket policy exists: By default, a newly created S3 bucket has no bucket policy attached. You need to explicitly create and attach a bucket policy if you want to control access to the bucket or enforce specific permissions.\n\n"
}, {
  "id" : 572,
  "question" : "Which of the following can you configure in the AWS S3 console? (Choose 2 options)\n",
  "answers" : [ {
    "id" : "2941fc69a1e2421baa36769b3e71b6c6",
    "option" : "Configure Server access logging.",
    "isCorrect" : "true"
  }, {
    "id" : "a023dd86ab4e46da8b57941b56f6f6d7",
    "option" : "Configure life cycle policy.",
    "isCorrect" : "true"
  }, {
    "id" : "9bef68ae668b4234a9aa80bfcf167e55",
    "option" : "Configure client-side encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "4e7b79d25d244ecda889d761f95de2bb",
    "option" : "Upload some files to a Glacier vault.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B.\nOptions A and B are correct as these operations are supported in the S3 console.\nOption C is incorrect because you can configure server-side encryption in S3 console but not client-side encryption.\nOption D is incorrect because you cannot directly upload files to Glacier through the AWS S3 console.\n\nSure, I can help with that.\nThe AWS S3 console provides a user interface for managing S3 buckets and objects. Here are the two options that you can configure in the AWS S3 console:\nA. Configure Server access logging: Server access logging provides detailed records for the requests that are made to an S3 bucket. You can use this information for security analysis, tracking of usage trends, and for compliance purposes. The logging information is stored in an S3 bucket that you specify. To configure server access logging, you can navigate to the S3 bucket you want to enable it on, select the properties tab, and then select server access logging.\nB. Configure life cycle policy: A lifecycle policy allows you to automatically manage the lifecycle of objects in an S3 bucket. You can use lifecycle policies to transition objects to different storage classes, expire objects, or delete objects that are no longer needed. To configure a lifecycle policy, you can navigate to the S3 bucket you want to enable it on, select the management tab, and then select lifecycle.\nC. Configure client-side encryption: Client-side encryption is not a configuration option that you can perform directly in the S3 console. Instead, you need to use an encryption client library that is capable of encrypting data before uploading it to S3, and decrypting data after downloading it from S3. You can use the AWS SDK or third-party libraries to perform client-side encryption.\nD. Upload some files to a Glacier vault: You cannot upload files directly to a Glacier vault from the S3 console. However, you can create a lifecycle policy that automatically transitions objects from an S3 bucket to a Glacier vault after a specified amount of time.\nSo, the correct answers are A and B.\n\n"
}, {
  "id" : 573,
  "question" : "You have created an S3 bucket in the us-east-1 region with default configuration.\nVersioning is not enabled.\nYou are located in Asia and deleted an object in the bucket using AWS CLI.\nWhat may happen when you try to list the objects in the bucket?\n",
  "answers" : [ {
    "id" : "869594d813a842ba8479d520c195e00c",
    "option" : "The object is still there.",
    "isCorrect" : "false"
  }, {
    "id" : "a9e06a22f80e437899ea40d6df032cc4",
    "option" : "The object is deleted completely.",
    "isCorrect" : "true"
  }, {
    "id" : "85860fe3d249452b97d207043d7fb0a5",
    "option" : "The object may still be there or deleted, depending on whether the deletion is finished in AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "d5a20c55b3634d1e8ca1bb8859839ca3",
    "option" : "The object is attached with a deletion mark.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAmazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions.\nThis applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs.\nOption D is incorrect because the deletion marker is available with the versioning feature, which is disabled in the scenario.\nReference:\nhttps://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/.\n\nWhen an object is deleted from an S3 bucket, it is not immediately removed from the bucket. Instead, it is marked for deletion and moved to the \"Deleted Objects\" folder within the bucket. This is true regardless of whether versioning is enabled or not.\nIn this scenario, since versioning is not enabled, there is only one version of the object in the bucket. When the object is deleted using the AWS CLI, it will be marked for deletion and moved to the \"Deleted Objects\" folder within the bucket. However, the timing of when the deletion process completes may vary depending on various factors, such as the size of the object and the network latency between the client and the S3 bucket.\nTherefore, when you try to list the objects in the bucket, it is possible that the deleted object may still be visible, but with a deletion marker. The deletion marker indicates that the object has been marked for deletion but has not yet been permanently removed from the bucket. This is because S3 eventually removes the object in the background after a certain period of time, usually within a few hours, but it may take longer depending on the size of the object and the network conditions.\nTherefore, the correct answer is (D) The object is attached with a deletion mark.\n\n"
}, {
  "id" : 574,
  "question" : "Your organization is planning to upload a large number of files to the AWS cloud.\nThese files need to be immediately available for download across different geographical regions right after the upload is complete.\nThey consulted you to check if S3 is a suitable solution for the use case.\nWhat do you suggest?\n",
  "answers" : [ {
    "id" : "5b01688f08ea41ff81a36e1cedaa6f2f",
    "option" : "S3 is not suitable for immediate downloads because new AWS provides eventual consistency for new objects.",
    "isCorrect" : "false"
  }, {
    "id" : "0d08786968da46ffaf1fc4564ff19cba",
    "option" : "EFS is not suitable for immediate downloads because AWS provides eventual consistency for new objects.",
    "isCorrect" : "false"
  }, {
    "id" : "cda72594ab464077be49fd82196f86ce",
    "option" : "EFS is suitable for immediate downloads because AWS provides eventual consistency for new objects.",
    "isCorrect" : "false"
  }, {
    "id" : "674bd5afc4e7448ab8c39679242b9900",
    "option" : "S3 is suitable for immediate downloads because AWS provides strong read-after-write consistency for new objects.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nAmazon S3 provides strong read-after-write consistency for PUTS of new objects in your S3 bucket in all regions.\nThere is a new update mentioned in https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/.\nOption C is not true.\nEFS provides strong read-after-write consistency.\n\nThe correct answer for this question is D. S3 is suitable for immediate downloads because AWS provides strong read-after-write consistency for new objects.\nAmazon S3 (Simple Storage Service) is a highly scalable and durable object storage service offered by AWS. S3 can be used to store and retrieve any type of data, such as documents, images, videos, and software.\nOne of the key features of S3 is that it provides strong read-after-write consistency for new objects in all AWS regions. This means that as soon as an object is uploaded to S3, it can be immediately downloaded and accessed from any region without the risk of returning stale or inconsistent data. This makes S3 an ideal solution for storing and serving frequently accessed files or data that requires immediate availability.\nOn the other hand, Amazon Elastic File System (EFS) is a scalable, fully-managed network file system for use with AWS Cloud services and on-premises resources. EFS is designed to provide scalable storage for Linux-based applications that require shared access to data. However, EFS provides eventual consistency for new objects, which means that it can take some time for all clients to see the latest version of a file after it has been updated or created. Therefore, EFS may not be suitable for applications that require immediate and consistent access to data.\nIn summary, S3 is a suitable solution for the use case described in the question because it provides immediate and strong read-after-write consistency for new objects, which enables the files to be immediately available for download across different geographical regions right after the upload is complete.\n\n"
}, {
  "id" : 575,
  "question" : "You are a solutions architect.\nYour organization is building an application hosting at the on-premise location.\nThey would like to keep the storage on AWS.\nObjects/files must only be accessed via the application.\nAs an exception, Administrators should access the objects/files directly from AWS S3 console/API bypassing the application.\nWhat solution would you provide?\n",
  "answers" : [ {
    "id" : "d010573139d8438db1bffc549b01c680",
    "option" : "Cached Volume Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "22c182f654104921975b467390230770",
    "option" : "Stored Volume Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "1ce16314701a4adfbfc818c6437f4333",
    "option" : "File Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "3902bba64ce44b159be961edab80a6e7",
    "option" : "Tape Gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nThe File Gateway presents a file interface that enables you to store files as objects in Amazon S3 using the industry-standard NFS and SMB file protocols, and access those files via NFS and SMB from your datacenter or Amazon EC2, or access those files as objects with the S3 API.\nFor option A, with Cached Volumen Gateway, you store your data in Amazon Simple Storage Service (Amazon S3) and retain a copy of frequently accessed data subsets locally.\nWe can take incremental backups, called snapshots of the storage volume in S3.All gateway data and snapshot data for cached volumes are stored in Amazon S3 and encrypted at rest using server-side encryption (SSE)\nHowever, you can't access this data with the Amazon S3 API or other tools such as the Amazon S3 Management Console.\nFor option B, with stored volumes, you store the entire set of volume data on-premises and store periodic point-in-time backups (snapshots) in AWS.\nIn this model, your on-premises storage is primary, delivering low-latency access to your entire dataset.\nAWS storage is the backup that you can restore in the event of a disaster in your data center.\nOption D is incorrect because the Tape Gateway enables you to replace using physical tapes on-premises with virtual tapes in AWS without changing existing backup workflows.\nFor more information on AWS storage gateways, refer to the documentation here.\nhttps://docs.aws.amazon.com/storagegateway/latest/userguide/WhatIsStorageGateway.html\nhttps://d1.awsstatic.com/whitepapers/aws-storage-gateway-file-gateway-for-hybrid-architectures.pdf\nhttps://aws.amazon.com/storagegateway/vtl/\n\n\nThe solution that would fit the requirements stated in the question is AWS S3 with pre-signed URLs.\nAmazon S3 is a scalable and durable object storage service that provides industry-leading scalability, availability, durability, and security. It can store and retrieve any amount of data, at any time, from anywhere on the web. S3 offers multiple storage classes to optimize costs, and it can also integrate with a variety of AWS services to automate workflows, build applications, and analyze data.\nTo meet the requirement of accessing the objects/files only via the application, you can use pre-signed URLs. A pre-signed URL is a signed URL that grants temporary access to a private object in Amazon S3. It allows you to share your object without making it publicly accessible. Pre-signed URLs can be generated programmatically, and they have a configurable expiration time.\nFor the exception of allowing administrators to access the objects/files directly from AWS S3 console/API bypassing the application, you can create an IAM policy that grants them the necessary permissions to access the objects/files directly. You can then generate pre-signed URLs for them as well, or they can use their AWS credentials to access the S3 console/API.\nOption A, Cached Volume Gateway, is a type of AWS Storage Gateway that provides low-latency access to frequently accessed data stored in Amazon S3. It is used to store a cache of frequently accessed data on-premises, while keeping the primary data in Amazon S3. This option is not a good fit for the stated requirements.\nOption B, Stored Volume Gateway, is another type of AWS Storage Gateway that stores entire data volumes on-premises while asynchronously backing them up to Amazon S3. This option is not a good fit for the stated requirements either.\nOption C, File Gateway, is a type of AWS Storage Gateway that presents a file interface to applications, while storing data as objects in Amazon S3. It is used to store and retrieve files from Amazon S3, and it can be accessed via standard file protocols such as NFS, SMB, and FTP. However, it is not a good fit for the stated requirements, as it would allow direct access to the objects/files bypassing the application.\nOption D, Tape Gateway, is a type of AWS Storage Gateway that provides a virtual tape library interface for backup and archiving applications. It is used to store data as virtual tapes in Amazon S3, while providing tape-like access to the data. This option is not a good fit for the stated requirements.\n\n"
}, {
  "id" : 576,
  "question" : "You are a solutions architect working for a media company that produces stock images and videos for sale via a mobile app and website.\nYour app and website allow users to gain access only to stock content they have purchased.\nYour content is stored in S3 buckets.\nYou need to restrict access to multiple files that your users have purchased.\nAlso, due to the nature of the stock content (purchasable by multiple users), you don't want to change the URLs of each stock item. Which access control option best fits your scenario?\n",
  "answers" : [ {
    "id" : "0b29f352a3b548f4aad688392a8b0ca1",
    "option" : "Use CloudFront signed URLs",
    "isCorrect" : "false"
  }, {
    "id" : "94d3b31b2b6f458cb3c07f088701b60c",
    "option" : "Use S3 Presigned URLs",
    "isCorrect" : "false"
  }, {
    "id" : "1016b65aa2eb426d98c70db051a28213",
    "option" : "UseCloudFront Signed Cookies",
    "isCorrect" : "true"
  }, {
    "id" : "404a12d59a1244daa1b1173b948cd5dd",
    "option" : "Use S3 Signed Cookies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect.\nCloudFront signed URLs allow you to restrict access to individual files.\nSigned URLs require you to change your content URLs for each customer access.\nOption B is incorrect.\nS3 Presigned URLs require you to change your content URLs.\nThe presigned URL expires after its defined expiration date.\nOption C is correct.\nCloudFront Signed Cookies allow you to control access to multiple content files and you don't have to change your URL for each customer access.\nOption D is incorrect.\nThere is no S3 Signed Cookies feature.\nReferences:\nPlease see the Amazon CloudFront developer guide titled Using signed cookies (https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-cookies.html), the Amazon Simple Storage Service user guide titled Sharing an object with a presigned URL (https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html), the Amazon Simple Storage Service user guide titled Using presigned URLs (https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html#PresignedUrlUploadObject-LimitCapabilities), and the Amazon CloudFront developer guide titled Choosing between signed URLs and signed cookies (https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-choosing-signed-urls-cookies.html)\n\nThe best option for this scenario would be to use CloudFront signed URLs (Option A).\nHere's why:\nCloudFront is a content delivery network (CDN) service that can be used to distribute your content to users around the world. It can help improve the performance and availability of your content by caching it in multiple locations. CloudFront signed URLs provide a way to control access to your content by requiring users to have a signed URL in order to access it. The signed URL includes a signature that is generated using a private key, and is only valid for a specified period of time. This means that you can control exactly who has access to your content, and for how long. Using CloudFront signed URLs does not require changing the URLs of your content. This is important for your scenario since you don't want to change the URLs of each stock item. CloudFront signed URLs can be used with S3 buckets. This means that you can continue to store your content in S3 buckets, and use CloudFront to distribute it to users. S3 presigned URLs (Option B) are another option for controlling access to S3 content. However, they require you to generate a new URL for each user and each piece of content. This could be problematic for your scenario since you have multiple users purchasing the same stock content. CloudFront signed cookies (Option C) and S3 signed cookies (Option D) are other options for controlling access to your content. However, they are typically used for scenarios where you need to control access based on a user's identity, rather than specific pieces of content. In your scenario, you need to control access to specific pieces of content that have been purchased by multiple users.\n\n"
}, {
  "id" : 577,
  "question" : "Which of the following are valid statements about Amazon S3? (Select TWO)\n",
  "answers" : [ {
    "id" : "f3b41e7afc89461b9840a2158da15793",
    "option" : "S3 provides strong read-after-write consistency ONLY for PUTs.",
    "isCorrect" : "false"
  }, {
    "id" : "b0dad1fecdd34e87b86aaaa0ef63dcc4",
    "option" : "S3 provides eventual consistency for PUTs or DELETES.",
    "isCorrect" : "false"
  }, {
    "id" : "21cb9ad53c7c41269412172f69d2c3ca",
    "option" : "S3 provides strong read-after-write consistency for PUTs and DELETEs.",
    "isCorrect" : "true"
  }, {
    "id" : "4b77caf6eae642609b0c77a70959c034",
    "option" : "S3 does not support object locking for concurrent writers.",
    "isCorrect" : "true"
  }, {
    "id" : "26cd2d52a1ec44ad8213ab622894cd8c",
    "option" : "Atomic updates across keys are possible.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C and D.\nOptions A and B are incorrect because both of them are outdated features of Amazon S3.\nOption C is CORRECT because Amazon S3 provides strong read-after-write consistency for PUTs and DELETEs of objects in your Amazon S3 bucket in all AWS Regions.\nThis applies to both writes to new objects as well as PUTs that overwrite existing objects and DELETEs.\nOption D is CORRECT because Amazon S3 does not support object locking for concurrent writers.\nIf two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins.\nIf this is an issue, you will need to build an object-locking mechanism into your application.\nOption E is incorrect because atomic updates across keys are not possible.\nFor example, if you PUT to an existing key from one thread and perform a GET on the same key from a second thread concurrently, you will get either the old data or the new data, but never partial or corrupt data.\nReference:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html#ConsistencyModel\n\nAmazon S3 is a cloud-based object storage service offered by Amazon Web Services (AWS). It is designed to store and retrieve any amount of data from anywhere on the web. S3 is used for various purposes, such as backup and disaster recovery, content distribution, and big data analytics.\nLet's discuss each option in detail:\nA. S3 provides strong read-after-write consistency ONLY for PUTs. This statement is partially correct. When a new object is uploaded to S3, it is immediately available for read-after-write consistency, which means that any subsequent GET request to read the object will return the latest version of the object. However, this is not true for updates. When an existing object is updated, S3 provides eventual consistency, which means that it may take some time for the updates to be visible across all S3 locations. Therefore, option A is incorrect.\nB. S3 provides eventual consistency for PUTs or DELETES. This statement is correct. As mentioned earlier, when an existing object is updated or deleted, S3 provides eventual consistency. This means that it may take some time (usually a few seconds) for the updates or deletions to be visible across all S3 locations. Therefore, option B is correct.\nC. S3 provides strong read-after-write consistency for PUTs and DELETEs. This statement is incorrect. S3 provides strong read-after-write consistency for new objects (PUTs), but not for updates or deletions. Therefore, option C is incorrect.\nD. S3 does not support object locking for concurrent writers. This statement is correct. S3 does not support object locking for concurrent writers. This means that multiple users can write to the same object at the same time, which can lead to conflicts and data corruption. Therefore, option D is correct.\nE. Atomic updates across keys are possible. This statement is incorrect. S3 does not support atomic updates across keys. Therefore, option E is incorrect.\nIn conclusion, the correct statements about Amazon S3 are:\nS3 provides eventual consistency for PUTs or DELETES. S3 does not support object locking for concurrent writers.\n\n"
}, {
  "id" : 578,
  "question" : "You are designing a web application that stores static assets in an Amazon S3 bucket.\nYou expect this bucket to immediately receive over 400 requests with a mix of GET/PUT/DELETE per second.\nWhat should you do to ensure optimal performance?\n",
  "answers" : [ {
    "id" : "e2882e0f3ec446659f08020dab670617",
    "option" : "Amazon S3 will automatically manage performance at this scale.",
    "isCorrect" : "true"
  }, {
    "id" : "f573c52788c6407cbd6ece42b2632791",
    "option" : "Add a random prefix to the key names.",
    "isCorrect" : "false"
  }, {
    "id" : "c1b941fb913c4178999669591adc6487",
    "option" : "Use a predictable naming scheme, such as sequential numbers or date time sequences, in the key names.",
    "isCorrect" : "false"
  }, {
    "id" : "e8963755f8db4d95addb70ff82ead72c",
    "option" : "Use multi-part upload.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\n#####################\nRequest Rate and Performance Guidelines.\nAmazon S3 automatically scales to high request rates.\nFor example, your application can achieve at least 3,500 PUT/POST/DELETE and 5,500 GET requests per second per prefix in a bucket.\nThere are no limits to the number of prefixes in a bucket.\nIt is simple to increase your read or write performance exponentially.\nFor example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.\n###################################\nFor More Information:\nhttps://aws.amazon.com/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/\n\nWhen designing a web application that stores static assets in an Amazon S3 bucket, it is important to consider the performance of the bucket, especially when it is expected to receive a high volume of requests. In this case, the bucket is expected to receive over 400 requests with a mix of GET/PUT/DELETE per second.\nOption A: Amazon S3 will automatically manage performance at this scale.\nThis option is not entirely correct as Amazon S3 does not automatically manage performance at any scale. Although S3 is designed to handle massive scale and offers high durability, availability, and scalability, there are still some considerations to keep in mind to ensure optimal performance.\nOption B: Add a random prefix to the key names.\nAdding a random prefix to the key names can help to distribute the load across different partitions and improve performance. This is because Amazon S3 uses the key name to determine the partition in which the object will be stored. By adding a random prefix to the key names, the objects are distributed across multiple partitions, which can help to reduce the likelihood of hotspots and improve performance.\nOption C: Use a predictable naming scheme, such as sequential numbers or date time sequences, in the key names.\nUsing a predictable naming scheme such as sequential numbers or date time sequences in the key names can lead to performance issues. This is because Amazon S3 stores objects in alphabetical order based on the key name, and if the key names are predictable, they can cause hotspots on a single partition, leading to performance issues.\nOption D: Use multi-part upload.\nMulti-part upload can improve performance when uploading large files to Amazon S3 by breaking the file into smaller parts and uploading them in parallel. However, in this case, the request mix is a mix of GET/PUT/DELETE, and multi-part upload does not apply.\nTherefore, the best option for ensuring optimal performance for a web application that stores static assets in an Amazon S3 bucket that receives over 400 requests with a mix of GET/PUT/DELETE per second is option B: Add a random prefix to the key names.\n\n"
}, {
  "id" : 579,
  "question" : "You have an application running on EC2\nWhen the application trying to upload a 7 GB file to S3, the operation fails.\nWhat could be the reason for failure, and what would be the solution?\n",
  "answers" : [ {
    "id" : "e45d30a52ae04c50a2c64b28801734f5",
    "option" : "With a single PUT operation, you can upload objects up to 5 GB in size. Use multi-part upload for larger file uploads.",
    "isCorrect" : "true"
  }, {
    "id" : "ba2d198f81f44b5ba0f7f96e6c885daf",
    "option" : "EC2 is designed to work best with EBS volumes. Use EBS Provisioned IOPs and use an Amazon EBS-optimized instance.",
    "isCorrect" : "false"
  }, {
    "id" : "24d241cee0fd47ae845263937ea2dd60",
    "option" : "NAT gateway only supports data transfers going out upto 5 G.",
    "isCorrect" : "false"
  }, {
    "id" : "7e60bca002044edd991c2203c56f34d8",
    "option" : "Use EBS Provisioned IOPs and use an Amazon EBS-optimized instance.",
    "isCorrect" : "false"
  }, {
    "id" : "59b3f492888c48cf82381fb202b93a7e",
    "option" : "VPC Endpoints only supports data transfers going out upto 5 G.",
    "isCorrect" : "false"
  }, {
    "id" : "59acb6e3719a4f08920d519f1a55c912",
    "option" : "Use EBS Provisioned IOPs and use an Amazon EBS-optimized instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nAWS recommends using multi-part uploads for larger objects.\nFor more information on multi-part uploads, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html\nFor option B, Amazon EBS is a storage for the drives of your virtual machines.\nIt stores data as blocks of the same size and organizes them through a hierarchy similar to a traditional file system.\nEBS is not a standalone storage service like Amazon S3\nSo you can use it only in combination with Amazon EC2.\nObjects can be stored on EBS volumes, but not cost-effective and not highly resilient and fault-tolerant compared to S3.\nOptions C and D are incorrect.\nNAT Gateway ad VPC endpoints do not have any data transfer limitations.\n\n\nThe correct answer for this question is A: With a single PUT operation, you can upload objects up to 5 GB in size. Use multi-part upload for larger file uploads.\nExplanation: When uploading a file to Amazon S3 using the PutObject operation, the maximum size of the object that can be uploaded in a single operation is 5 GB. If the file is larger than 5 GB, the operation will fail. Therefore, the recommended solution is to use multi-part upload to upload the file in smaller parts.\nMulti-part upload allows you to upload large objects in parts, which can be uploaded independently and in parallel. This increases the speed of upload and also allows for resuming failed uploads from the last successful part.\nIn addition, it is important to ensure that the EC2 instance is correctly configured and has sufficient resources to upload large files. EBS Provisioned IOPs and Amazon EBS-optimized instances can help to optimize the performance of the instance and the storage.\nOption B, D, and F are incorrect because they do not address the issue of uploading a large file to S3. They refer to optimizing the performance of the EC2 instance, but not the S3 upload.\nOption C and E are incorrect because they refer to NAT gateway and VPC Endpoints, which are not relevant to the issue of uploading a file to S3.\n\n"
}, {
  "id" : 580,
  "question" : "You have an application on EC2 which stores the files in an S3 bucket.\nEC2 is being launched using a role that has GetObject permissions on the S3 bucket defined in its policy.\nThe users who authenticate this application will get a pre-signed URL for the files in an S3 bucket using EC2 role temporary credentials.\nHowever, users are reporting that they get an error when accessing pre-signed URLs.\nWhat could be the reason? (SELECT TWO)\n",
  "answers" : [ {
    "id" : "3c4f221058824bef824770796f572980",
    "option" : "Pre-Signed URLs expired.",
    "isCorrect" : "true"
  }, {
    "id" : "f708d1b3db194bea893544a0d26bbe4d",
    "option" : "Logged in user must be an IAM user to download the file through a pre-signed URL.",
    "isCorrect" : "false"
  }, {
    "id" : "58ae23560105470c92e8e2bc16b68c67",
    "option" : "Bucket has a \"Deny\" policy. EC2 role not whitelisted in the policy statement with Allow.",
    "isCorrect" : "true"
  }, {
    "id" : "55f9ebf19f4a47dba8f27335edbc9c69",
    "option" : "The default policy on temporary credentials does not have GetObject privileges on a S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C.\nAll objects in S3 are private by default.\nOnly the object owner has permission to access these objects.\nHowever, the object owner can optionally share objects with others by creating a pre-signed URL, using their own security credentials, to grant time-limited permission to download the objects.\nAnyone who receives the pre-signed URL can then access the object.\nFor example, if you have a video in your bucket and both the bucket and the object are private, you can share the video with others by generating a pre-signed URL.\nFor more information, refer to the documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ShareObjectPreSignedURL.html\nFor option A, while generating pre-signed URL programmatically using SDK/API, we estimate how long the URL should be valid.\nWhen the URL is accessed after the specified duration, you will get an error.\nFor option B, AWS recommends using temporary credentials whenever users need time-limited access to AWS resources instead of using IAM users for each request.\nFor more information on temporary credentials, refer to the documentation here.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\nFor option C, if a bucket policy contains Effect as Deny, you must whitelist all the IAM resources which need access on the bucket.\nOtherwise, IAM resources cannot access the S3 bucket even if they have full access.\nFor detailed information on how to restrict the bucket, refer to documentation here.\nhttps://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/\nFor option D, the policy is an optional parameter when temporary credentials are generated using AssumeRole (which is how EC2 generates temporary credentials using instance-profile)\nThere is no default policy.\n\n\nThe most likely reasons why users are reporting errors when accessing pre-signed URLs are:\nA. Pre-Signed URLs expired: Pre-signed URLs are valid for a limited duration of time, which is set when the URL is generated. If the users are accessing pre-signed URLs after their expiration time, they will receive an error.\nC. Bucket has a \"Deny\" policy. EC2 role not whitelisted in the policy statement with Allow: It is possible that the S3 bucket has a \"Deny\" policy, which prevents access to the bucket for all users or roles, including the EC2 instance role. If the EC2 instance role is not whitelisted in the policy statement with Allow, users will receive an error when accessing pre-signed URLs.\nB. Logged in user must be an IAM user to download the file through a pre-signed URL: This is not a valid reason why users are reporting errors when accessing pre-signed URLs. Pre-signed URLs can be used by any user, including anonymous users, to access S3 objects if they have the correct URL.\nD. The default policy on temporary credentials does not have GetObject privileges on an S3 bucket: This is also not a valid reason why users are reporting errors when accessing pre-signed URLs. The EC2 instance role has GetObject permissions on the S3 bucket defined in its policy. The pre-signed URLs generated using the EC2 role temporary credentials inherit the permissions of the role, so they should also have GetObject privileges on the S3 bucket.\nTherefore, the two most likely reasons for the error are A and C.\n\n"
}, {
  "id" : 581,
  "question" : "Your organization has an S3 bucket that stores confidential information.\nAccess is granted to certain programmatic IAM users and restricted these IAM users' requests to be originated from within your organization's IP address range.\nHowever, your organization suspects there might be requests from other IP addresses to S3 buckets to download certain objects.\nHow would you troubleshoot to find out the requester IP address? (choose 2 options)\n",
  "answers" : [ {
    "id" : "1ab8d29b95bf426fa4d47f4ae63f2123",
    "option" : "Enable VPC flow logs in the region where S3 bucket exists.",
    "isCorrect" : "false"
  }, {
    "id" : "8f7b1cf1fc0d48c4bd36735edda0b096",
    "option" : "Enable server access logging",
    "isCorrect" : "true"
  }, {
    "id" : "6189a266c88c4339911b8c83d8fd404f",
    "option" : "Enable CloudTrail logging using OPTIONS object",
    "isCorrect" : "true"
  }, {
    "id" : "275b8b456c6e453fa535f52ee7ac1158",
    "option" : "Enable CloudWatch metrics.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B, C.\nServer access logging provides detailed records for the requests that are made to a bucket.\nServer access logs are useful for many applications.\nFor example, access log information can be useful in security and access audits.\nFor details on how to enable logging for S3, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html#server-access-logging-overview\nFor information about the format of the log file, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/LogFormat.html\nFor option A, S3 is a managed service and not part of VPC.\nSo enabling VPC flow logs does not report traffic sent to the S3 bucket.\nOption B is correct.\nOption C is correct.\nUsing the information collected by CloudTrail, you can determine what request was made to Amazon S3, the source IP address from which the request was made, who made the request, when it was made, and so on.\nThis information helps you to track changes made to your AWS resources and to troubleshoot operational issues.\nFor detailed information about how S3 requests are tracked using CloudTrail, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cloudtrail-logging.html#cloudtrail-logging-\nFor option D, although CloudWatch has metrics for S3 requests, this does not provide detailed information about each request.\nIt generates metrics for the number of requests sent for each type.\nFor more information about S3 CloudWatch request metrics, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cloudwatch-monitoring.html#s3-request-cloudwatch-metrics\n\n\nTo troubleshoot and find out the requester IP address that might be accessing your S3 bucket from outside your organization's IP address range, the following two options can be used:\nEnable VPC flow logs in the region where S3 bucket exists: VPC flow logs capture information about the IP traffic going to and from network interfaces in your VPC. By enabling VPC flow logs, you can collect and analyze traffic data to identify the source and destination of traffic to your S3 bucket. VPC flow logs record information such as source and destination IP addresses, ports, protocol, and packets sent and received. You can then use this information to identify the IP addresses that are making requests to your S3 bucket and take appropriate actions such as blocking access to those IP addresses. Enable server access logging: Server access logging is a feature of S3 that captures detailed information about every request made to your S3 bucket. By enabling server access logging, you can track requests to your S3 bucket and identify the requester IP address. Server access logs provide information such as the requester's IP address, time of the request, request type, HTTP status code, and more. With this information, you can identify requests that are originating from outside your organization's IP address range and take appropriate actions such as blocking access to those IP addresses.\nTherefore, options A and B are the correct options to troubleshoot and find out the requester IP address that might be accessing your S3 bucket from outside your organization's IP address range.\n\n"
}, {
  "id" : 582,
  "question" : "You are a solutions architect working as a consultant where you build web applications for clients.\nOne of your clients needs a static website hosted on AWS.\nThe website will predominantly host content files owned by the AWS account used to create the S3 bucket that will host the website.\nHowever, some of the objects in the bucket are owned by a parent company's AWS account. How should you configure the S3 bucket access controls to achieve the most secure website that is accessible to the public? (Choose TWO)\n",
  "answers" : [ {
    "id" : "c99e85384c954a2390574823c950e886",
    "option" : "Create a bucket policy that grants s3:GetObject access to the objects owned by the parent company account.",
    "isCorrect" : "false"
  }, {
    "id" : "168ee99ec3ea4d0b82172293a81a2e57",
    "option" : "Create a bucket policy that grants s3:GetObject access to the objects in the bucket owned by the account used to create the S3 bucket that will host the website.",
    "isCorrect" : "true"
  }, {
    "id" : "59efbfe15ea04098b25025c1c2527df7",
    "option" : "Create an object access control list to grant read permissions on objects owned by the account used to create the S3 bucket that will host the website.",
    "isCorrect" : "false"
  }, {
    "id" : "1294fb6b3d734db383a4427025586531",
    "option" : "Create an object access control list to grant read permissions on objects owned by the parent company account.",
    "isCorrect" : "true"
  }, {
    "id" : "7ceb092781ce427a8ef133662ac267b4",
    "option" : "Create a bucket policy that grants s3:GetObject access to the objects owned by the parent company account and the objects owned by the account used to create the S3 bucket that will host the website.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: B and D.\nOption A is incorrect.\nThe objects owned by the parent company account need an access control list that grants read permission to all users.\nThis is because these objects are not controlled by the bucket policy, since they are not owned by the account used to create the bucket that will host the website.\nOption B is correct.\nIf you create a bucket policy that grants s3:GetObject access to the objects in the bucket owned by the account used to create the bucket, they will become publicly readable.\nOption C is incorrect.\nYou use a bucket policy to control access to objects in the bucket that are owned by the account used to create the bucket.\nYou don't use an ACL for this access control.\nOption D is correct.\nSince the account used to create the S3 bucket used to host the website is different from the parent company account, you need to use an ACL to control access to the objects owned by the parent company account.\nOption E is incorrect.\nThe bucket policy will control access to objects owned by the account used to create the S3 bucket that will host the website.\nYour bucket policy can't control access to objects owned by the parent company account.\nYou need to use an ACL to control access to objects owned by the parent company account.\nReferences:\nPlease see the Amazon Simple Storage Service user guide titled Setting permissions for website access (https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteAccessPermissionsReqd.html), the Service Authorization reference page titled Actions, resources, and condition keys for Amazon S3 (https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html)\n\nWhen configuring S3 bucket access controls for a static website, the goal is to provide the most secure website that is accessible to the public. In this scenario, some of the objects in the bucket are owned by a parent company's AWS account, and others are owned by the AWS account used to create the S3 bucket that will host the website. To achieve this goal, the following two options should be considered:\nA. Create a bucket policy that grants s3:GetObject access to the objects owned by the parent company account. Bucket policies are used to grant permissions for a specific S3 bucket and can be used to grant access to users or accounts outside of the AWS account used to create the S3 bucket. In this case, a bucket policy can be created that grants s3:GetObject access to the objects owned by the parent company account. This option will allow the public to access the objects owned by the parent company account, which is necessary for the website to function correctly.\nB. Create a bucket policy that grants s3:GetObject access to the objects in the bucket owned by the account used to create the S3 bucket that will host the website. In addition to granting access to the objects owned by the parent company account, a bucket policy can also be created that grants s3:GetObject access to the objects in the bucket owned by the account used to create the S3 bucket that will host the website. This option will allow the public to access the content files owned by the AWS account used to create the S3 bucket, which is necessary for the website to function correctly.\nC. Create an object access control list to grant read permissions on objects owned by the account used to create the S3 bucket that will host the website. An object access control list (ACL) is a set of permissions attached to an object in S3. ACLs can be used to grant read or write permissions to specific AWS accounts or users. In this case, an ACL can be created to grant read permissions on objects owned by the AWS account used to create the S3 bucket that will host the website. However, this option is not the most appropriate since a bucket policy can be used to grant permissions to the entire bucket, which is more efficient.\nD. Create an object access control list to grant read permissions on objects owned by the parent company account. Similar to option C, an ACL can be created to grant read permissions on objects owned by the parent company account. However, this option is not the most appropriate since a bucket policy can be used to grant permissions to the entire bucket, which is more efficient.\nE. Create a bucket policy that grants s3:GetObject access to the objects owned by the parent company account and the objects owned by the account used to create the S3 bucket that will host the website. Option E combines the first two options, creating a bucket policy that grants s3:GetObject access to both the objects owned by the parent company account and the objects owned by the AWS account used to create the S3 bucket that will host the website. This option provides the necessary permissions to both accounts, allowing the public to access all the content files required for the website to function correctly.\nIn conclusion, the two options that should be considered when configuring the S3 bucket access controls for a static website that will predominantly host content files owned by the AWS account used to create the S3 bucket, and some owned by a parent company's AWS account, are to create a bucket policy that grants s3:GetObject access to the objects owned by the parent company account and to create a bucket policy that grants s3:GetObject access to the objects in the bucket owned by the account used to create the S3 bucket that will host the website.\n\n"
}, {
  "id" : 583,
  "question" : "Which of the following are system metadata for objects in S3? (choose 3 options)\n",
  "answers" : [ {
    "id" : "307c279d3d5248eab2e0b8fff9125e31",
    "option" : "x-amz-server-side-encryption",
    "isCorrect" : "true"
  }, {
    "id" : "2520391b5884414db772e884c6e0ef00",
    "option" : "x-amz-meta-object-id",
    "isCorrect" : "false"
  }, {
    "id" : "f3a85cab1f4e4896abb757bd35be9937",
    "option" : "x-amz-version-id",
    "isCorrect" : "true"
  }, {
    "id" : "c6d00a226c1c405a9960f62f5eb890a9",
    "option" : "Content-Length",
    "isCorrect" : "true"
  }, {
    "id" : "f2de5f9b865d4141bb63f86671f6b25f",
    "option" : "x-amz-meta-location.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C, D.\nAWS S3 bucket objects contain two kinds of metadata, system metadata and user-defined metadata.\nSystem metadata:\nMetadata such as object creation date is system controlled where only Amazon S3 can modify the value.\nOther system metadata, such as the storage class configured for the object and whether the object has server-side encryption enabled, are examples of system metadata whose values you control.\nIf your bucket is configured as a website, sometimes you might want to redirect a page request to another page or an external URL.\nIn this case, a webpage is an object in your bucket.\nAmazon S3 stores the page redirect value as system metadata whose value you control.\nWhen you create objects, you can configure values of these system metadata items or update the values when you need to.\nUser-defined metadata:\nWhen uploading an object, you can also assign metadata to the object.\nYou provide this optional information as a name-value (key-value) pair when you send a PUT or POST request to create the object.\nWhen you upload objects using the REST API, the optional user-defined metadata names must begin with \"x-amz-meta-\" to distinguish them from other HTTP headers.\nFor more information on object metadata, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html#object-metadata\nFor additional information:\nhttps://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html\n\nSystem metadata for objects in Amazon S3 are a set of predefined metadata that S3 uses to store information about the object. These metadata are set by S3 itself and cannot be modified by the user.\nOut of the given options, the following are the system metadata for objects in S3:\nA. x-amz-server-side-encryption: This metadata specifies whether the object is encrypted server-side using S3-managed encryption keys (SSE-S3), customer-provided encryption keys (SSE-C), or AWS Key Management Service (KMS) encryption keys (SSE-KMS). This metadata is automatically set by S3 when the object is uploaded with server-side encryption.\nC. x-amz-version-id: This metadata is a unique identifier for a specific version of an object in a versioned bucket. When versioning is enabled for a bucket, each object version is assigned a unique version ID.\nE. x-amz-meta-location: This metadata can be set by the user to provide additional information about the object's physical location, such as the data center or region where the object is stored.\nThe following options are not system metadata for objects in S3:\nB. x-amz-meta-object-id: This metadata is not a system metadata but a user-defined metadata. It can be used to store a unique identifier for an object, such as a database ID.\nD. Content-Length: This metadata specifies the size of the object in bytes and is not a system metadata.\nTherefore, the correct answers are A, C, and E.\n\n"
}, {
  "id" : 584,
  "question" : "Your organization needs to meet audit compliance and hence need to log all the requests sent to 10 buckets that contain confidential information.\nThese will also be periodically used to determine if any requests are being made from outside the organization's IP address range.\nYour AWS application team had enabled S3 server access logging through AWS Console for all the buckets into a common logging bucket named s3-server-logging.\nBut after few hours they noticed no logs were being written into the logging bucket.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "15e19929e84746528ecb5d56e2ee8bc7",
    "option" : "Bucket user-defined deny policy is not allowing Log Delivery group to write into S3 logging bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "7d375f79e5ba482685847031650093bf",
    "option" : "Bucket public access is not enabled.",
    "isCorrect" : "false"
  }, {
    "id" : "ab9a705106644df9b9076713009b8b5e",
    "option" : "Write access is disabled for Log Delivery group.",
    "isCorrect" : "false"
  }, {
    "id" : "3e0eea0c8fdb44939ada098625456e5f",
    "option" : "Bucket name for server access logging should be â€œs3-server-access-loggingâ€ in order to write the request logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nServer access logging provides detailed records for the requests that are made to a bucket.\nServer access logs are useful for many applications.\nFor example, access log information can be useful in security and access audits.\nFor details on logging for S3, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html#server-access-logging-overview\nFor option A, S3 buckets would often be restricted using bucket policy with Effect as Deny except for whitelisted IAM resources that would require access.\nFor detailed information on how to restrict the bucket, refer to documentation here.\nhttps://aws.amazon.com/blogs/security/how-to-restrict-amazon-s3-bucket-access-to-a-specific-iam-role/\nTo provide access to the log delivery group, you need to add the following statement to your bucket policy explicitly.\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{ Delivery service\",\n\"Sid\": \"Permit access log delivery by AWS ID for Log\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"AWS\": \"arn:aws:iam::858827067514:root\"\n},\n\"Action\": \"s3:PutObject\",\n\"Resource\": \"arn:aws:s3:::examplebucket/logs/*\"\n}\n]\n}\nAlso, make sure the arn â€œarn:aws:iam::858827067514:rootâ€ is whitelisted in the Deny statement of your bucket policy.\nFor option B, public access is not required to be enabled for writing logs into the S3 bucket.\nThe only access required is PutObject for the Log Delivery group.\nFor option C, although the log delivery group permission is disabled by default, permission will be granted when the bucket is selected as the target for logging.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/enable-logging-console.html\nOption D is a false statement.\n\n\nThe reason why no logs were written to the logging bucket despite enabling S3 server access logging could be due to option A, where the Bucket user-defined deny policy is not allowing the Log Delivery group to write into the S3 logging bucket.\nThe S3 server access logging feature logs all the requests sent to an S3 bucket in a target bucket. In this case, the target bucket is named \"s3-server-logging,\" and the AWS application team had enabled this feature for all the ten buckets that contain confidential information.\nTo use this feature, the target bucket must allow write access to the Log Delivery group, which is a predefined Amazon S3 group that AWS uses to deliver server access logs. The Log Delivery group is the only entity that AWS grants permission to write to the logging bucket.\nIf the bucket user-defined deny policy is not allowing the Log Delivery group to write into the S3 logging bucket, the logs will not be written to the bucket. The bucket user-defined deny policy could be set up explicitly to deny access to the Log Delivery group.\nOption B, Bucket public access is not enabled, and option C, Write access is disabled for Log Delivery group, are less likely to be the cause of the issue as these are not the default configurations for S3 server access logging.\nOption D, Bucket name for server access logging should be â€œs3-server-access-loggingâ€ in order to write the request logs, is incorrect because the bucket name is not a limiting factor in writing logs to the logging bucket.\nIn summary, if the logs are not being written to the logging bucket despite enabling S3 server access logging, the most probable reason is that the bucket user-defined deny policy is not allowing the Log Delivery group to write into the S3 logging bucket.\n\n"
}, {
  "id" : 585,
  "question" : "You are building a web application that will allow authenticated users to upload videos to the AWS S3 bucket across multiple domains.\nHowever, while testing the application, you found that the upload requests to S3 are being blocked.\nWhat should you do to make the upload work?\n",
  "answers" : [ {
    "id" : "dfe3794a8d7441faa31f16867f93bf32",
    "option" : "Enable public access to allow uploads from web applications.",
    "isCorrect" : "false"
  }, {
    "id" : "1a606aefbc034b4ea61dfdba90779096",
    "option" : "Add configuration in S3 bucket CORS to allow PUT requests from web application URL.",
    "isCorrect" : "true"
  }, {
    "id" : "b1ef8896977a43ad819e7a46366671f1",
    "option" : "Add Content-Length and Content-MD5 headers while sending upload requests to S3",
    "isCorrect" : "false"
  }, {
    "id" : "7c5290ff5fd143e4a49a82b26f0ee716",
    "option" : "Web application URL must be added to bucket policy to allow PutObject requests.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nCross-origin resource sharing (CORS) defines a way for client web applications loaded in one domain to interact with resources in a different domain.\nWith CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.\nFor more information on CORS, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html#example-scenarios-cors\nFor option A, enabling public access will not enable the web application to send requests to the S3 bucket.\nFurthermore, AWS does not recommend enabling public access on an S3 bucket unless you are hosting static assets that all can access.\nFor more information on securing S3 buckets, refer to documentation here:https://aws.amazon.com/premiumsupport/knowledge-center/secure-s3-resources/\nFor option C, Content-Length and Content-MD5 are system metadata for objects.\nThey are set while creating/uploading an object.\nHowever, these parameters do not enable web applications to send requests to the S3 bucket.\nFor option D, the AWS S3 bucket policy does not grant permissions based on the web application.\nURLs.\nHowever, you can set up a condition in the policy to restrict access only if the request is being sent from a certain URL using the â€œaws:Refererâ€ context-key.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html#example-\n\nThe correct answer for this question is B. Add configuration in S3 bucket CORS to allow PUT requests from web application URL.\nExplanation:\nThe issue with the upload requests being blocked could be due to the Cross-Origin Resource Sharing (CORS) configuration of the S3 bucket. CORS is a security feature that restricts web applications from accessing resources from different domains. In this scenario, the web application is trying to access the S3 bucket from a different domain, which is causing the upload requests to be blocked.\nTo solve this issue, we need to configure the S3 bucket to allow cross-origin requests from the web application's domain. This can be done by adding the necessary configuration in the S3 bucket CORS. Specifically, we need to add a rule that allows PUT requests from the web application's domain.\nOption A, enabling public access to allow uploads from web applications, is not the recommended solution because it would open up the S3 bucket to the public, which could lead to security issues.\nOption C, adding Content-Length and Content-MD5 headers while sending upload requests to S3, is not relevant to the issue of the upload requests being blocked due to CORS.\nOption D, adding the web application URL to the bucket policy to allow PutObject requests, is also not the correct solution because it does not address the issue of CORS. Bucket policies are used to define permissions for different types of access to the bucket. However, in this scenario, the issue is related to cross-origin requests, which is a different type of security restriction.\n\n"
}, {
  "id" : 586,
  "question" : "You have uploaded a file to AWS S3 bucket with content â€˜foo'\nYou have overwritten the file with the content â€˜bar'\nWhen you made a GetObject request immediately after overwrite, what output can you expect?\n",
  "answers" : [ {
    "id" : "043516ffb49e4bc6beaace57bd085be1",
    "option" : "foo",
    "isCorrect" : "false"
  }, {
    "id" : "e117f7c93a42465597e20ca6129ba6bf",
    "option" : "bar",
    "isCorrect" : "true"
  }, {
    "id" : "bcbbe703179647d883abb2167f8e81fe",
    "option" : "either foo or bar",
    "isCorrect" : "false"
  }, {
    "id" : "44a20fb4ea394f67956b289c28c321a7",
    "option" : "An error stating â€œObject updating. Please try after some time.â€",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nS3 now supports strong read-after-write consistency.\nWhat you write is what you will read, and the results of a LIST will be an accurate reflection of what's in the bucket.\nDetails can be found in https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/.\n\nIf you have uploaded a file to an S3 bucket with the content 'foo' and then overwritten the file with the content 'bar', when you make a GetObject request immediately after the overwrite, the output you can expect is 'bar'.\nThis is because when you overwrite an existing object in an S3 bucket, the new content replaces the old content of the object. The object's metadata, including the ETag, is updated to reflect the new content.\nWhen you make a GetObject request for the object immediately after the overwrite, S3 will return the latest version of the object, which in this case is the one with the content 'bar'.\nSo the correct answer is B. bar.\nOption A, 'foo', is incorrect because the object has been overwritten with the new content 'bar'.\nOption C, 'either foo or bar', is incorrect because once the object is overwritten with the new content, the old content is replaced and is no longer available.\nOption D, 'An error stating \"Object updating. Please try after some time.\"', is incorrect because overwriting an object in an S3 bucket does not result in any errors that prevent you from reading the object immediately after the overwrite.\n\n"
}, {
  "id" : 587,
  "question" : "You created a bucket named â€œmyfirstwhizbucketâ€ in the US West region.\nWhat are valid URLs for accessing the bucket? (Choose 2 options)\n",
  "answers" : [ {
    "id" : "ffb0588a015a46efb7c362285980e964",
    "option" : "https://myfirstwhizbucket.s3.us-west-1.amazonaws.com",
    "isCorrect" : "true"
  }, {
    "id" : "0afc5ad54fb341c79a52fde7c842c99c",
    "option" : "https://s3.myfirstwhizbucket.us-west-1.amazonaws.com",
    "isCorrect" : "false"
  }, {
    "id" : "4c25d2455e5042d8a4d867e95dc692bb",
    "option" : "https://s3.us-west-1.amazonaws.com/myfirstwhizbucket",
    "isCorrect" : "true"
  }, {
    "id" : "7dc35464ccfc43958b34f8d25a8ed2e0",
    "option" : "https://s3-us-west-1-amazonaws.com/myfirstwhizbucket",
    "isCorrect" : "false"
  }, {
    "id" : "f2d182cc8e9a45ee8a864f5e3232b116",
    "option" : "https://s3.amazonaws.com/myfirstwhizbucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C.\nFor option A, it matches the virtual-hosted-style URL and it is correct.\nFor option B, it does not match any of the above-mentioned URL patterns.\nIt is incorrect.\nFor option C, it matches the path-style URL and it is correct.\nFor option D, it does not match any of the above-mentioned URL patterns.\nFor option E, it matches the path-style URL, but since the bucket is in the us-west-1 region, it must contain the region in the endpoint.\nSo it is incorrect.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html#access-bucket-intro\nNOTE: Option C and D are different.\n(Dot and Hyphen).\nOption C - https://s3.us-west-1.amazonaws.com/myfirstwhizbucket.\nOption D - https://s3-us-west-1-amazonaws.com/myfirstwhizbucket.\n\n\nSure!\nAmazon S3 is a scalable cloud storage service provided by AWS, which allows you to store and retrieve data from anywhere on the web. When you create a bucket in S3, you can access it using a unique URL.\nThe format of the S3 bucket URL depends on the region where the bucket is located. In this case, the bucket \"myfirstwhizbucket\" has been created in the US West (Oregon) region.\nLet's go through the options and explain which ones are valid URLs for accessing the bucket:\nA. https://myfirstwhizbucket.s3.us-west-1.amazonaws.com This is a valid URL for accessing the bucket because it follows the correct format for an S3 bucket URL in the US West (Oregon) region. The URL consists of the bucket name followed by the S3 endpoint in that region.\nB. https://s3.myfirstwhizbucket.us-west-1.amazonaws.com This URL is not valid. The format is incorrect because it includes \"s3\" before the bucket name, which is not necessary. Also, the region should be included after \"s3\" and before the bucket name.\nC. https://s3.us-west-1.amazonaws.com/myfirstwhizbucket This URL is valid because it follows the correct format for an S3 bucket URL in the US West (Oregon) region. However, this URL does not include the full regional endpoint like option A, which can cause issues with DNS resolution in some situations.\nD. https://s3-us-west-1-amazonaws.com/myfirstwhizbucket This URL is not valid because it does not include the correct format for an S3 bucket URL in the US West (Oregon) region. The format is incorrect because it is missing the \"s3\" before the region identifier.\nE. https://s3.amazonaws.com/myfirstwhizbucket This URL is not valid because it does not include the correct format for an S3 bucket URL in the US West (Oregon) region. It uses the default S3 endpoint, which can be used to access any bucket in any region, but it does not include the region identifier, which means it will likely connect to a bucket in a different region.\nTherefore, the valid URLs for accessing the \"myfirstwhizbucket\" bucket in the US West (Oregon) region are options A and C.\n\n"
}, {
  "id" : 588,
  "question" : "What are the minimum and maximum file sizes that can be stored in S3 respectively?\n",
  "answers" : [ {
    "id" : "dc80cd52c74c43aea1628b679c730058",
    "option" : "1 KB and 5 gigabytes",
    "isCorrect" : "false"
  }, {
    "id" : "3ecae3e144904764933a9f081ed4aee3",
    "option" : "1 KB and 5 terabytes",
    "isCorrect" : "false"
  }, {
    "id" : "2f8eeec114354a45886e96c1ac7ae3d6",
    "option" : "1 Byte and 5 gigabytes",
    "isCorrect" : "false"
  }, {
    "id" : "f51579cda6c7419ebf14a5fc5d7b5418",
    "option" : "0 Bytes and 5 terabytes.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\n\n\nAmazon S3 (Simple Storage Service) is an object storage service that is designed to store and retrieve any amount of data from anywhere on the web. It is a highly scalable and durable storage system that can store data from a few bytes to petabytes of data.\nIn terms of file sizes, the minimum and maximum file sizes that can be stored in S3 are:\nThe minimum file size that can be stored in S3 is 0 bytes. This means that you can store an empty object in S3. The maximum file size that can be stored in S3 is 5 terabytes (TB). This means that you can store a file that is as large as 5 terabytes in S3.\nTherefore, the correct answer to the question is D.\nIt is important to note that there are certain limitations when uploading large files to S3. For example, if you are using the Amazon S3 console, you can only upload files that are less than 160 GB in size. However, you can use other methods such as the AWS Command Line Interface (CLI) or the AWS SDKs to upload larger files to S3.\nAdditionally, it is important to consider the cost implications of storing large amounts of data in S3. S3 charges for data storage, data transfer, and other related services. Therefore, you should always consider the cost implications before storing large amounts of data in S3.\n\n"
}, {
  "id" : 589,
  "question" : "You are a solutions architect working for a media company that produces and stores image and video content that is sold as stock content to other companies that wish to use your stock content in their web and mobile apps.\nYou are storing your stock content in S3 and you need to optimize for cost.\nSome of your images are small, less than 128 KB in size.\nHowever, most of your stock content is much larger.\nThe amount of content you manage is very large, with over 1 million objects in S3\nThese objects have varying access patterns.\nSome are accessed frequently, while others are accessed very infrequently.\nAlso, the access patterns for the stock objects change over time. Which S3 storage class should you choose for your stock content to optimize your costs while also providing the best overall performance?\n",
  "answers" : [ {
    "id" : "5a7b5287f3634ad3927e94c9e350345a",
    "option" : "S3 Standard",
    "isCorrect" : "false"
  }, {
    "id" : "2cfda1ae30a6411880815dec2bc9848a",
    "option" : "S3 Standard-IA",
    "isCorrect" : "false"
  }, {
    "id" : "7a5e8543050044288872562aac0a61dc",
    "option" : "S3 Intelligent-Tiering",
    "isCorrect" : "true"
  }, {
    "id" : "0d16a4ba1848414283886d2700eb1265",
    "option" : "S3 One Zone-IA.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption A is incorrect.\nS3 Standard tiering is not a cost-optimized solution.\nYou have a lot of objects and many of them are accessed very infrequently.\nWith S3 Standard, all objects are charged at the same rate, regardless of how often they are accessed.\nOption B is incorrect.\nS3 Standard-IA, or Infrequent Access, would be a cost-optimized solution for less frequently accessed objects.\nHowever, retrieval latency will not be optimal for frequently accessed objects.\nOption C is correct.\nS3 Intelligent-Tiering gives you the ability to have S3 monitor the access patterns of your objects and move objects across the various storage tiers based on the relevant access patterns.\nThis will give you both cost and performance optimization.\nAlso, smaller files (less than 128 KB) can be stored in S3 Intelligent-Tiering but they will always remain in the S3 Standard storage class.\nOption D is incorrect.\nS3 One Zone-IA is used for files that are easily recreated if the one AZ becomes unavailable.\nAlso, an infrequent access tier such as S3 One Zone-IA would be a cost-optimized solution for less frequently accessed objects.\nHowever, retrieval latency will not be optimal for frequently accessed objects.\nReference:\nPlease see the Amazon Simple Storage Service user guide titled Using Amazon S3 storage classes (https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html)\n\nAs a solutions architect, you need to optimize the cost of storing and accessing your stock content in S3 while maintaining good performance. The storage class you choose will depend on the access patterns, size, and frequency of your objects.\nA. S3 Standard: This storage class provides high durability and availability and is suitable for frequently accessed objects. It has the highest storage cost per GB compared to other S3 storage classes, making it less cost-effective for objects that are accessed less frequently. This is not the best option for optimizing cost for your scenario.\nB. S3 Standard-IA: This storage class provides high durability and availability with lower storage cost per GB compared to S3 Standard. It is suitable for infrequently accessed objects that still require fast access times when accessed. This option is cost-effective for objects that are accessed less frequently than the frequently accessed objects, but more than the objects that are hardly ever accessed.\nC. S3 Intelligent-Tiering: This storage class automatically moves objects between two access tiers based on changing access patterns. This storage class is suitable for objects with unknown or changing access patterns, and it is cost-effective for data with unknown access patterns. This option automatically moves objects to a lower-cost storage tier when access patterns change, reducing costs while maintaining performance.\nD. S3 One Zone-IA: This storage class provides lower storage costs than S3 Standard-IA by storing data in a single Availability Zone (AZ). However, it has a lower durability level because it is stored in only one AZ, and it is only suitable for data that can be recreated easily, as it cannot be recovered in case of an AZ outage. This option is not suitable for stock content, which is critical for the media company's business.\nTherefore, based on the given requirements, the best storage class to optimize costs while maintaining performance for the media company's stock content is C. S3 Intelligent-Tiering. It provides the best overall performance, automatically moving objects between access tiers based on changing access patterns, while optimizing costs.\n\n"
}, {
  "id" : 590,
  "question" : "With S3 Versioning enabled on the bucket, how will billing be applied for the following scenario using the same key for upload? Total days bucket in use: 25 days. 1st File uploaded on 1st Day of the use - 1 GB.2nd File uploaded within the same bucket on the 15th Day of the use - 5 GB.\n",
  "answers" : [ {
    "id" : "c6109a3b03c544b8be3d139ea89377a4",
    "option" : "Charges 6 GB for 25 days.",
    "isCorrect" : "false"
  }, {
    "id" : "62f84463b9ca4c5c8bcde1baf8625e80",
    "option" : "Charges 1 GB for 25 days and 5 GB for 11 days.",
    "isCorrect" : "true"
  }, {
    "id" : "f6c7bbf226694e3499e27e8244927098",
    "option" : "Charges 1 GB for 14 days and 5 GB for 11 days.",
    "isCorrect" : "false"
  }, {
    "id" : "86504cb077774e1d9744731aa804e157",
    "option" : "Charges 5 GB for 25 days.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWhen versioning is enabled on the S3 bucket, and a new version is added to an existing object, remember that the older version still remains and AWS charges the same price for old versions and new versions.\nIn the given use case, 1 GB uploaded on day 1 remains in S3 for 25 days.\n5 GB uploaded on day 15 will be in S3 for only 11 days.\n\n\nWhen versioning is enabled on an S3 bucket, each version of an object is stored as a separate object, and each object version has a unique version ID. In this scenario, we have two versions of the object with the same key, and both versions have been uploaded within the same bucket.\nNow, let's consider each option:\nA. Charges 6 GB for 25 days. This option assumes that S3 charges for the total storage consumed by all versions of the object for the entire duration of the bucket usage. However, this is not correct. S3 charges based on the amount of storage consumed by each version of the object and the duration that each version is stored.\nB. Charges 1 GB for 25 days and 5 GB for 11 days. This option is correct. S3 charges for each version of the object based on the amount of storage consumed and the duration that each version is stored. In this case, the first version of the object is 1 GB and is stored for the entire duration of the bucket usage (25 days). The second version of the object is 5 GB and is stored for 11 days. Therefore, the total charge will be for 1 GB for 25 days and 5 GB for 11 days.\nC. Charges 1 GB for 14 days and 5 GB for 11 days. This option assumes that the first version of the object is deleted or overwritten by the second version of the object. However, with versioning enabled, both versions are stored, and S3 charges for the storage and duration of each version.\nD. Charges 5 GB for 25 days. This option assumes that only the latest version of the object is stored, and S3 charges based on the storage and duration of the latest version. However, with versioning enabled, all versions of the object are stored, and S3 charges for the storage and duration of each version.\nTherefore, the correct answer is B. S3 will charge for 1 GB for 25 days and 5 GB for 11 days.\n\n"
}, {
  "id" : 591,
  "question" : "You have a version enabled S3 bucket.\nYou have accidentally deleted an object which contains 3 versions.\nYou would want to restore the deleted object.\nWhat can be done?\n",
  "answers" : [ {
    "id" : "70d7e550f8d1488bab71db7c3ca01049",
    "option" : "Select the deleted object and choose to restore option in More menu.",
    "isCorrect" : "false"
  }, {
    "id" : "2336fe58bff344ef897502daa37a3f6e",
    "option" : "Delete the delete-marker on the object.",
    "isCorrect" : "true"
  }, {
    "id" : "e14daf5c965748e09d37832495db682c",
    "option" : "Versioning in S3 only supports keeping multiple copies. It does not support restoring deleted objects.",
    "isCorrect" : "false"
  }, {
    "id" : "896a49a1f1dd49faa40af948670fa7fa",
    "option" : "In version enabled bucket, Delete request only deletes the latest version. You can still see the older versions of the object using version Id in the GET request.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWhen you delete an object in a versioning-enabled bucket, all versions remain in the bucket, and Amazon S3 creates a delete marker for the object.\nTo undelete an object, you must delete the delete marker.\nSelect the checkbox next to the object's delete marker to recover, and then choose delete from the More menu.\nFor more information on how to undelete objects in version enabled S3 bucket, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/undelete-objects.html\n\n\nIf you have a versioned S3 bucket and accidentally delete an object that has multiple versions, you can restore the deleted object by following these steps:\nNavigate to the S3 console and select the bucket that contains the deleted object. Click on the \"Versions\" tab to see the list of all versions of the objects in the bucket. Identify the version of the object that you want to restore. You can do this by looking at the \"Version ID\" column. Select the version of the object that you want to restore by clicking the checkbox next to it. Click on the \"Actions\" drop-down menu and select \"Restore\" from the list of options. In the \"Restore object\" dialog box, choose the duration for which you want to restore the object. You can choose from a minimum of 1 day to a maximum of 365 days. Click \"Restore\" to restore the object. The restored object will be available in the S3 bucket after the restoration is complete.\nOption A is the correct answer. Option B is not correct as deleting the delete-marker on the object will only hide the object and its versions from view but will not restore the object. Option C is incorrect as versioning in S3 supports restoring deleted objects. Option D is partially correct but does not provide the complete solution to restoring a deleted object with multiple versions.\n\n"
}, {
  "id" : 592,
  "question" : "You have an application that writes application logs to version enabled S3 bucket.\nEach object has multiple versions attached to it.\nAfter 60 days, the application deletes the objects in S3 through DELETE API on the object.\nHowever, in next month's bill, you see charges for S3 usage on the bucket.\nWhat could have caused this?\n",
  "answers" : [ {
    "id" : "ff54a953262348649d4413e085be2792",
    "option" : "DELETE API call on the object only deletes latest version.",
    "isCorrect" : "false"
  }, {
    "id" : "d11f68464e7e4051bfa7b668b061e365",
    "option" : "DELETE API call on the object does not delete the actual object, but places delete marker on the object.",
    "isCorrect" : "true"
  }, {
    "id" : "3b1984f4c79c4fa68b6a71dac4f78995",
    "option" : "DELETE API call moves the object and its versions to S3 recycle bin from where the object can be restored till 30 days.",
    "isCorrect" : "false"
  }, {
    "id" : "d80d0c4a36714b1e8ea1b624f8b5c53b",
    "option" : "DELETE API for all versions of the object in version enabled bucket cannot be done through API. It can be only done by bucket owners through the console.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWhen versioning is enabled, a simple DELETE cannot permanently delete an object.\nInstead, Amazon S3 inserts a delete marker in the bucket, and that marker becomes the current version of the object with a new ID.\nWhen you try to GET an object whose current version is a.\ndelete marker, Amazon S3 behaves as though the object has been deleted (even though it has not been erased) and returns a 404 error.\nThe following figure shows that a simple DELETE does not actually remove the specified object.\nInstead, Amazon S3 inserts a delete marker.\nTo permanently delete versioned objects, you must use DELETE Object versioned.\nThe following figure shows that deleting a specified object version permanently removes that object.\nFor information on how to delete versioned objects through API, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/DeletingObjectVersions.html#delete-obj-version-enabled-bucket-rest\nOption A is not true.\nDELETE call on an object does not delete the latest version unless the DELETE call is made with the latest version id.\nOption C is not true.\nAWS S3 does not have to recycle bin.\nOption D is not true.\nDELETE call on a versioned object can be made through API by providing the version id of the object's version to be deleted.\n\n\nIn this scenario, the application writes logs to an S3 bucket, which has versioning enabled. The application deletes objects after 60 days using the DELETE API call on the object. However, there are charges for S3 usage on the bucket in the next month's bill.\nThe possible reasons for this are:\nA. DELETE API call on the object only deletes the latest version: This answer is incorrect because when versioning is enabled, every new version of the object is stored as a separate object, and the DELETE API call deletes all versions of the object.\nB. DELETE API call on the object does not delete the actual object, but places a delete marker on the object: This answer is partially correct. When versioning is enabled, the DELETE API call places a delete marker on the object to indicate that the object is deleted. However, the object and its versions still exist in S3 and can be accessed.\nC. DELETE API call moves the object and its versions to S3 recycle bin from where the object can be restored till 30 days: This answer is incorrect because when versioning is enabled, the DELETE API call places the object and its versions in the S3 versioning recycle bin, where they can be restored for an indefinite period.\nD. DELETE API for all versions of the object in version-enabled bucket cannot be done through the API. It can be only done by bucket owners through the console: This answer is incorrect. The DELETE API call can be used to delete all versions of an object in an S3 bucket with versioning enabled.\nTherefore, the correct answer is B - DELETE API call on the object does not delete the actual object, but places a delete marker on the object. When versioning is enabled, the DELETE API call only adds a delete marker on the object instead of actually deleting it. This means that the object and its versions still exist in S3 and can be accessed. As a result, there will still be charges for S3 usage on the bucket in the next month's bill. To completely delete the object and its versions, the application must use the DELETE API call to delete the object and all of its versions, including any delete markers.\n\n"
}, {
  "id" : 593,
  "question" : "You are uploading multiple files ranging 10 GB - 20 GB in size to the AWS S3 bucket by using a multi-part upload from an application on EC2\nOnce the upload is complete, you would like to notify a group of people who do not have AWS IAM accounts.\nHow can you achieve this? (Select TWO)\n",
  "answers" : [ {
    "id" : "443f90b12fc34bbba77c1ad6938faabc",
    "option" : "Use S3 event notification and configure Lambda function which sends email using AWS SES non-sandbox.",
    "isCorrect" : "true"
  }, {
    "id" : "65a55f61f1d249b4a408541fbe26cbfc",
    "option" : "Use S3 event notification and configure SNS which sends email to subsribed email addresses.",
    "isCorrect" : "true"
  }, {
    "id" : "c4f5041853db42a48f65607775fc43ef",
    "option" : "Write a custom script on your application side to poll S3 bucket for new files and send email through SES non-sandbox.",
    "isCorrect" : "false"
  }, {
    "id" : "b592d9a361f14555a339b2b53f5b7b77",
    "option" : "Write a custom script on your application side to poll S3 bucket for new files and send email through SES sandbox.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A and B.\nThe Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket.\nTo enable notifications, you must first add a notification configuration identifying the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the event notifications.\nAWS Simple Email Service (SES) is a cost-effective email service built on the reliable and scalable infrastructure that Amazon.com developed to serve its own customer base.\nWith Amazon SES, you can send transactional emails, marketing messages, or any other type of high-quality content.\nWhile your account is in the sandbox, you can use all of the features of Amazon SES.\nHowever, when your account is in the sandbox, we apply the following restrictions to your account:\nYou can only send mail to verified email addresses and domains or to the Amazon SES mailbox simulator.\nYou can only send mail from verified email addresses and domains.\nhttps://docs.aws.amazon.com/ses/latest/DeveloperGuide/request-production-access.html\nOption A triggers Lambda function which uses non-sandbox SES to send email to people who does not have an AWS IAM account nor are verified in AWS SES.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/lambda-send-email-ses/\nOption B is correct as it triggers SNS.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ways-to-add-notification-config-to-bucket.html\nOptions C and D are incorrect because they needs compute resources to monitor S3 for new files continuously.\nReferences:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html#supported-notification-event-types\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html\n\n\nSure, I can help you with that.\nThe scenario described in the question involves uploading large files to an S3 bucket using a multi-part upload from an EC2 instance. After the upload is complete, you need to notify a group of people who do not have AWS IAM accounts.\nTo achieve this, there are two options available:\nUse S3 event notification and configure a Lambda function which sends email using AWS SES non-sandbox. Use S3 event notification and configure SNS which sends email to subscribed email addresses.\nLet's discuss both options in detail:\nOption 1: Use S3 event notification and configure a Lambda function which sends email using AWS SES non-sandbox.\nThis option involves using S3 event notification to trigger a Lambda function that sends an email using AWS SES (Simple Email Service) non-sandbox.\nHere's how it works:\nStep 1: Create an S3 bucket and enable event notification.\nStep 2: Create a Lambda function that sends an email using AWS SES non-sandbox.\nStep 3: Configure the Lambda function as a target for S3 event notification.\nStep 4: Upload the files to the S3 bucket using multi-part upload. Once the upload is complete, S3 will trigger the Lambda function which will send an email to the specified recipients.\nOption 2: Use S3 event notification and configure SNS which sends email to subscribed email addresses.\nThis option involves using S3 event notification to trigger an SNS (Simple Notification Service) topic that sends an email to subscribed email addresses.\nHere's how it works:\nStep 1: Create an S3 bucket and enable event notification.\nStep 2: Create an SNS topic and subscribe email addresses to it.\nStep 3: Configure the SNS topic as a target for S3 event notification.\nStep 4: Upload the files to the S3 bucket using multi-part upload. Once the upload is complete, S3 will trigger the SNS topic which will send an email to the subscribed email addresses.\nNow, let's discuss why the other two options are not valid:\nOption C: Write a custom script on your application side to poll S3 bucket for new files and send email through SES non-sandbox.\nThis option involves writing a custom script on your application side to poll the S3 bucket for new files and send an email using AWS SES non-sandbox. While this option is technically feasible, it is not the best approach as it requires you to manage the script and the associated infrastructure.\nOption D: Write a custom script on your application side to poll S3 bucket for new files and send email through SES sandbox.\nThis option is not valid because AWS SES sandbox only allows you to send emails to verified email addresses or domains. Since the recipients in this scenario do not have AWS IAM accounts, it is likely that their email addresses are not verified, making this option impractical.\nI hope this helps! Let me know if you have any more questions.\n\n"
}, {
  "id" : 594,
  "question" : "Your organization had built a video sharing website on EC2 within the US for which the S3 bucket in us- east-1 is used to store the video files.\nThe website has been receiving excellent feedback, and your organization decided to expand the website all over the world.\nHowever, customers in Europe and Asia started to complain that website access, upload, and download of video files are slow.\nHow can you resolve the issue? (Select TWO)\n",
  "answers" : [ {
    "id" : "a81dd0290cac45c9a9c616c188df6df6",
    "option" : "Use CloudFront for improving the performance on website by caching static files.",
    "isCorrect" : "true"
  }, {
    "id" : "982e108a0eca4e328d600888a68142d8",
    "option" : "Use VPC Endpoints in Europe and Asia regions to improve S3 uploads and downloads.",
    "isCorrect" : "false"
  }, {
    "id" : "8ac28317e0f84edd9c39d902b66d9c9a",
    "option" : "Enable Transfer Acceleration feature on S3 bucket which uses AWS edge locations to improve upload and download speeds.",
    "isCorrect" : "true"
  }, {
    "id" : "deb9af695110424791bf60a41d31405e",
    "option" : "Change your application design to provision higher-memory configuration EC2 instances and process S3 requests through EC2.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C.\nOption A is correct.\nAWS CloudFront can be used to improve the performance of your website where network latency is an issue.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-cloudfront-walkthrough.html\nOption B is incorrect.\nVPC endpoints do not support cross-region requests.\nLimitations.\nEndpoints are supported within the same Region only.\nYou cannot create an endpoint between a VPC and a service in a different Region.\nEndpoints support IPv4 traffic only.\nYou cannot transfer an endpoint from one VPC to another, or from one service to another.\nOption C is correct.\nAmazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket.\nTransfer Acceleration takes advantage of Amazon CloudFront's globally distributed edge locations.\nAs the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\nWhen mobile or web applications have users that are far from the destination S3 bucket, uploads and downloads may be slow.\nS3TA (S3 Transfer Acceleration) can help speed up these long-distance transfers to provide users a better experience.\nOption D is incorrect because it increases the cost of EC2 usage and does not solve the problem with slower upload and download speeds to S3.\nFor more information on transfer acceleration, refer to the documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html#transfer-acceleration-why-use\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html\n\nTo resolve the issue of slow website access, upload, and download of video files for customers in Europe and Asia, there are two recommended solutions:\nUse CloudFront for improving the performance on website by caching static files. CloudFront is a content delivery network (CDN) service provided by AWS that improves website performance by caching static files, such as images, videos, and other assets, and delivering them from edge locations closer to end-users. By using CloudFront, your organization can distribute the static content of the website to edge locations worldwide, thus reducing the latency and improving the website's overall performance. CloudFront also provides security features such as DDoS protection, SSL/TLS encryption, and access control. Use Transfer Acceleration feature on S3 bucket which uses AWS edge locations to improve upload and download speeds. Transfer Acceleration is a feature of Amazon S3 that enables faster uploads and downloads of objects over long distances. It utilizes AWS edge locations to accelerate the data transfer between the client and the S3 bucket. With Transfer Acceleration, your organization can achieve faster upload and download speeds, regardless of the distance between the client and the S3 bucket. It's particularly useful when transferring large files or when the internet connection is unstable or congested.\nTherefore, the correct answers are A and C.\nThe other two options (B and D) may not be the best solutions to address the issue.\nB. Using VPC Endpoints in Europe and Asia regions to improve S3 uploads and downloads would only improve the data transfer between EC2 instances and S3 buckets within the same VPC, but it would not address the issue of slow website access for customers outside the VPC.\nD. Changing the application design to provision higher-memory configuration EC2 instances and process S3 requests through EC2 could improve the performance of S3 requests, but it would not address the issue of slow website access for customers outside the US. Additionally, it could result in increased operational costs due to higher EC2 instance types.\n\n"
}, {
  "id" : 595,
  "question" : "Where does cross-region replication require versioning to be enabled?\n",
  "answers" : [ {
    "id" : "496343421da94c82a6acbabd400f84f9",
    "option" : "Only on Destination bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "067b590a9632463585aa857de5a01e3b",
    "option" : "Versioning is useful to avoid accidental deletes and not a requirement for replicating across regions.",
    "isCorrect" : "false"
  }, {
    "id" : "b9e80f28310b4a708b8bb712b5b0b422",
    "option" : "Only on Source bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "35b4773035014720be875a5db7019987",
    "option" : "Both on Source and Destination buckets.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nCross-region replication is a bucket-level configuration that enables automatic, asynchronous copying of objects across buckets in different AWS Regions.\nWe refer to these buckets as source bucket and destination bucket.\nThese buckets can be owned by different AWS accounts.\nFor more information on AWS S3 cross-region replication, refer to the documentation here.\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/crr.html\n\n\nCross-region replication is a feature provided by Amazon S3 that enables automatic replication of objects across different regions. When an object is uploaded to a source bucket, the object is automatically replicated to the destination bucket located in another region.\nWhen using cross-region replication, it is important to ensure that the destination bucket has versioning enabled. This is because versioning provides an extra layer of protection against accidental deletion or modification of objects. With versioning enabled, each version of an object is stored and can be retrieved at any time. This helps to protect against accidental deletion, overwriting, or data corruption.\nOn the other hand, enabling versioning on the source bucket is not required for cross-region replication. However, it can provide additional protection in case an object is modified or deleted in the source bucket before it can be replicated to the destination bucket.\nTherefore, the correct answer to the question is D. Both the source and destination buckets must have versioning enabled to ensure that the replicated objects are protected against accidental deletion or modification.\n\n"
}, {
  "id" : 596,
  "question" : "Your organization has an existing VPC in us-east-1 with two subnets in us-east-1b.\nThey are running few EC2 instances each in both subnets and would need a low latency common File Store for all instances to share files for heavy workloads.\nThey have created an EFS, mounted on all the EC2 instances, and share files across all the EC2 instances.\nYou were tasked to increase the number of instances due to the increase in workload.\nYou created a new subnet in us-east-1c and launched a few instances.\nWhen you tried to mount the previously created EFS on new EC2 instances, the operations fail.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "bf9c632577d94409929950eb961693ca",
    "option" : "AWS EFS does not support cross availability zone mounting.",
    "isCorrect" : "false"
  }, {
    "id" : "6661f9a34831435dbdd8da0abea7f55a",
    "option" : "By default, EFS is only available in one availability zone. Create a case with AWS support to increase EFS availability zones.",
    "isCorrect" : "false"
  }, {
    "id" : "deea2163560f4d69a91c71f90913d8b4",
    "option" : "EFS created with mount targets in an us-east-1b availability zone. Instances in us-east-1c cannot use the EFS mount target in us-east-1b.",
    "isCorrect" : "false"
  }, {
    "id" : "0f627b698bae42ba8c2eeb7f1184c551",
    "option" : "EFS mount target security group inbound rules do not allow traffic from new EC2 instances.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nAmazon EFS provides scalable file storage for use with Amazon EC2\nYou can create an EFS file system and configure your instances to mount the file system.\nYou can use an EFS file system as a common data source for workloads and applications running on multiple instances.\nFor options A, B, C, EFS mount targets from one availability zone can be mounted on another availability zone, although this approach is not recommended.\nHowever, this approach will not cause operations to fail.\nCreating or Deleting Mount Targets in a VPC.\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private Internet Protocol version 4 (IPv4) or Internet Protocol version 6 (IPv6) addresses.\nFor more information on VPC peering, see What is VPC Peering? in the Amazon VPC Peering Guide.\nYou can mount Amazon EFS file systems over VPC connections by using VPC peering within a single AWS Region when using the Amazon EC2 instance types T3, C5, C5d, I3.metal, M5, M5d, R5, R5d, and z1d.\nHowever, other VPC private connectivity mechanisms such as inter-region VPC peering and VPC peering within an AWS Region using other instance types are not supported.\nNote the following restrictions:\nYou can mount an Amazon EFS file system on instances in only one VPC at a time.\nBoth the file system and VPC must be in the same AWS Region.\nFor option D, when using Amazon EFS, you specify Amazon EC2 security groups for the EFS mount targets associated with the file system.\nSecurity groups act as a firewall, and the rules you add define the traffic flow.\nYou can authorize inbound and outbound access to your EFS file system.\nTo do so, you add rules that allow your EC2 instance to connect to your Amazon EFS file system through the mount target using the Network File System (NFS) port.\n\n\nThe reason for the failure of mounting the previously created EFS on new EC2 instances in the newly created subnet (us-east-1c) could be due to option C, which states that the EFS was created with mount targets in an us-east-1b availability zone. Instances in us-east-1c cannot use the EFS mount target in us-east-1b.\nAmazon Elastic File System (EFS) is a scalable and fully managed file storage service that provides an interface to create and manage file systems in the AWS Cloud. EFS is designed to provide shared file storage for multiple Amazon Elastic Compute Cloud (EC2) instances.\nWhen creating an EFS file system, it is important to note that EFS is designed for high availability and durability within a single availability zone (AZ) by default. To provide multi-AZ support, EFS uses a feature called \"EFS Mount Targets,\" which creates a mount point for the EFS file system in each availability zone.\nIn this scenario, the EFS was created with mount targets in an us-east-1b availability zone. This means that the EFS file system is only accessible by EC2 instances in the us-east-1b subnet. When new EC2 instances are launched in a different subnet, such as us-east-1c, they cannot access the EFS file system because there are no EFS mount targets available in us-east-1c.\nTo resolve this issue, new EFS mount targets must be created in the us-east-1c availability zone, which can be done by following these steps:\nOpen the Amazon EFS console. Choose the EFS file system that you want to add mount targets to. Choose the \"Add Mount Targets\" button. Select the VPC and the new subnet in which you want to add mount targets. Choose \"Add Mount Targets.\"\nAfter creating new EFS mount targets in us-east-1c, EC2 instances in the us-east-1c subnet can now access the EFS file system.\nOption A is incorrect because EFS does support cross-AZ mounting through the use of EFS Mount Targets.\nOption B is incorrect because there is no need to create a case with AWS support to increase EFS availability zones. EFS availability zones are based on the availability zones of the VPC subnets in which EFS mount targets are created.\nOption D is incorrect because the EFS mount target security group inbound rules are not related to the ability of new EC2 instances to mount the EFS file system. If the inbound rules were not configured correctly, the EC2 instances would not be able to access the EFS mount target at all, but this is not the issue presented in this scenario.\n\n"
}, {
  "id" : 597,
  "question" : "You have an AWS setup with an existing VPC in us-east-1\nYou have a fleet of 20 EC2 instances attached to EFS with mount targets on all existing VPC's availability zones.\nYour organization had requested you to replicate the same setup in another VPC within us-east-1 keeping the same EFS volume.\nHow will you achieve this?\n",
  "answers" : [ {
    "id" : "437d1b957cb84a9b97f197c6e0014d04",
    "option" : "Attach new VPC to existing EFS, create new mount targets for new VPC and mount EFS on EC2 instances within new VP.",
    "isCorrect" : "false"
  }, {
    "id" : "2e8a05c22f4f43de9db6fc4334de82c7",
    "option" : "Create a new VP.",
    "isCorrect" : "true"
  }, {
    "id" : "52557f3d4d2948d88f5e849c516879e3",
    "option" : "Establish a VPC peering connection between the VPCs. Use the instances that are created in the new VPC to access the already existing EFS with mount targets.",
    "isCorrect" : "false"
  }, {
    "id" : "1e297d23072147aa89141d9a8e7d0287",
    "option" : "EFS is available for all VPCs within a region by default. Mount EFS on new EC2 instances and configure EFS security group to allow inbound traffic.",
    "isCorrect" : "false"
  }, {
    "id" : "af5688e665994ea1b20c6a66ec1d0ddc",
    "option" : "EFS can be used only within one VPC at a time. You need to launch EC2 instances in existing VP.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWorking with VPC Peering in Amazon EFS.\nA VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private Internet Protocol version 4 (IPv4) or Internet Protocol version 6 (IPv6) addresses.\nFor more information on VPC peering, see What is VPC Peering? in the Amazon VPC Peering Guide.\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/amazon-efs-now-supports-access-across-accounts-and-vpcs/\nhttps://docs.aws.amazon.com/efs/latest/ug/manage-fs-access-change-vpc.html#manage-fs-access-vpc-peering\nFor options A and C, you can use an Amazon EFS file system in one VPC at a time.\nThat is, you create mount targets in a VPC for your file system and use those mount targets to provide access to the file system from EC2 instances in that VPC.For option D, although the statement is correct, launching EC2 instances within the same VPC is not a solution when you were asked to do in a new VPC.\nThe correct answer from the given options would be to peer the VPC and use appropriate instance types.\n\nThe correct answer to this question is C. Establish a VPC peering connection between the VPCs. Use the instances that are created in the new VPC to access the already existing EFS with mount targets.\nExplanation: The first thing to note is that EFS is not VPC-specific, which means that EFS volumes can be shared across multiple VPCs within a region. Therefore, we can reuse the same EFS volume that is already attached to the existing EC2 instances.\nTo set up the new VPC, we can use VPC peering. VPC peering enables you to connect two VPCs together so that they can communicate with each other using private IP addresses. This is done without the need for a VPN connection or a dedicated connection. In this case, we will establish a VPC peering connection between the existing VPC and the new VPC.\nOnce the VPC peering connection is established, we can create new EC2 instances within the new VPC and use them to access the EFS volume that is already mounted on the existing EC2 instances. We can do this by configuring the security groups for the EC2 instances to allow traffic between the two VPCs.\nIn summary, the steps to achieve the required setup are:\nEstablish a VPC peering connection between the existing VPC and the new VPC. Create new EC2 instances within the new VPC. Mount the existing EFS volume on the new EC2 instances. Configure the security groups for the EC2 instances to allow traffic between the two VPCs.\nOption A is incorrect because we cannot attach a single EFS volume to multiple VPCs at the same time. Therefore, we cannot simply attach the new VPC to the existing EFS volume.\nOption B is incorrect because creating a new VPC alone does not solve the problem. We need to establish a connection between the new VPC and the existing VPC.\nOption D is incorrect because we do not need to configure the security group for the EFS volume. Instead, we need to configure the security group for the EC2 instances to allow traffic between the two VPCs.\nOption E is incorrect because EFS volumes can be shared across multiple VPCs within a region. Therefore, we can use the existing EFS volume in the new VPC.\n\n"
}, {
  "id" : 598,
  "question" : "Which of the following ports should be open on the security groups for the EC2 instance and the EFS respectively to enable Secure Shell (SSH) access between them?\n",
  "answers" : [ {
    "id" : "afa386108d4e4bbcb0c22e80b42840b1",
    "option" : "Open port 22(SSH) on EC2 security group and port 2049(NFS) on EFS security group.",
    "isCorrect" : "true"
  }, {
    "id" : "001a1088f55f4a38819d1b7e4021568b",
    "option" : "Open port 22(SSH) on EC2 security group and ports 111(NFS) &amp; 2049(NFS) on EFS security group.",
    "isCorrect" : "false"
  }, {
    "id" : "aa946449f593469f9b2ddd36320f9ec7",
    "option" : "Open port 2049(NFS) on EC2 security group and ports 111(NFS) &amp; 2049(NFS) on EFS security group.",
    "isCorrect" : "false"
  }, {
    "id" : "27a12a0b9ad74ebc807764f3bfff362b",
    "option" : "Open port 111(NFS) on EC2 security group and ports 111(NFS) &amp; 2049(NFS) on EFS security group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nhttps://docs.aws.amazon.com/efs/latest/ug/accessing-fs-create-security-groups.html#create-security-groups-console\nAWS EFS does not require any other port to be open except NFS (2049) on its security group.\n\n\nThe correct answer is A. Open port 22(SSH) on EC2 security group and port 2049(NFS) on EFS security group.\nExplanation: To enable Secure Shell (SSH) access between an EC2 instance and an EFS file system, we need to open the necessary ports on their respective security groups.\nSSH is used for remote access to the EC2 instance, so we need to open port 22 on the EC2 security group to allow incoming SSH traffic. Port 22 is the default port used for SSH.\nEFS uses the Network File System (NFS) protocol to allow EC2 instances to access its file systems. NFS uses ports 2049 and 111, so we need to open these ports on the EFS security group to allow the EC2 instance to access the EFS file system.\nHowever, in this scenario, we only need to enable SSH access between the EC2 instance and EFS, so we only need to open port 22 on the EC2 security group and port 2049 on the EFS security group.\nTherefore, the correct answer is A. Open port 22(SSH) on EC2 security group and port 2049(NFS) on EFS security group.\n\n"
}, {
  "id" : 599,
  "question" : "You have two VPCs in different regions ( VPC A and VPC B) peered with each other.\nYou have created an EFS for VPC A.\nWhen you tried to mount the EFS on EC2 instances on VPC B, you are getting a connection timed out error.\nWhat can cause this? (Select TWO)\n",
  "answers" : [ {
    "id" : "05d9d3a9e50040f6b93e06ef5db61138",
    "option" : "AWS EFS takes upto an hour after creation to make mount targets available.",
    "isCorrect" : "false"
  }, {
    "id" : "286ace6fdfc14d9f8964e594a0959853",
    "option" : "Security group is improperly configured for the EFS mount target.",
    "isCorrect" : "true"
  }, {
    "id" : "310e61ebb0df4e81875db021d20f145d",
    "option" : "Security group on mount targets does not have inbound NFS port open to VPC Bâ€™s EC2 instances.",
    "isCorrect" : "true"
  }, {
    "id" : "5f2d7c228e8042b28f0c037b9d5aaca7",
    "option" : "EFS cannot be mounted through VPC peering.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B and C.\nYou can connect to Amazon EFS file systems from EC2 instances in other AWS regions using an inter-region VPC peering connection, and from on-premises servers using an AWS VPN connection.\nOption A is incorrect.\nUsually, EFS and its mount targets get created within a few moments.\nOption B is CORRECT because both an Amazon EC2 instance and a mount target have associated security groups.\nThese security groups can act as a virtual firewall that controls the traffic between them.\nOption C is CORRECT because the security groups you associate with a mount target must allow inbound access for the TCP protocol on the NFS port from all EC2 instances on which you want to mount the file system.\nOption D is incorrect because VPC peering within a single AWS Region when using certain Amazon EC2 instance types is supported.\nInter-region VPC peering is supported for all instance types.\nReference:\nhttps://docs.aws.amazon.com/efs/latest/ug/limits.html\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/amazon-efs-now-supports-aws-vpn-and-inter-region-vpc-peering/\n\nThe correct answers are B and C:\nB. Security group is improperly configured for the EFS mount target. C. Security group on mount targets does not have inbound NFS port open to VPC B's EC2 instances.\nExplanation:\nWhen VPCs are peered, they are connected via a private network and can communicate with each other. However, there are some considerations when accessing resources across VPCs. In this case, the issue is related to the EFS mount target in VPC A not being accessible from EC2 instances in VPC B.\nThe most likely causes of the connection timeout error are:\nB. Security group is improperly configured for the EFS mount target: When you create an EFS file system, it automatically creates mount targets in each of the subnets that you specify. Each mount target has a security group associated with it. If the security group is not properly configured to allow inbound traffic from the EC2 instances in VPC B, the connection will be timed out. Make sure that the security group for the mount target allows inbound traffic on port 2049 (NFS).\nC. Security group on mount targets does not have inbound NFS port open to VPC B's EC2 instances: In addition to the security group for the mount target, the security group for the EC2 instances in VPC B must also allow inbound traffic from the mount target on port 2049 (NFS). If this port is not open, the connection will be timed out.\nA. AWS EFS takes upto an hour after creation to make mount targets available: This is not a likely cause of the connection timeout error because the error message indicates that the EC2 instances in VPC B are able to reach the mount target, but the connection is timing out. If the mount target was not yet available, the error message would be different.\nD. EFS cannot be mounted through VPC peering: This is not true. EFS can be mounted through VPC peering. In fact, this is one of the benefits of using VPC peering, as it allows you to access resources across VPCs.\n\n"
}, {
  "id" : 600,
  "question" : "You have created AWS EFS with default settings and mounted it on an EC2 instance.\nDue to regulatory policies, your organization had asked you to encrypt data stored on EFS.\nWhat would you do to enable encryption?\n",
  "answers" : [ {
    "id" : "fe918815156a4afdab2038991578609f",
    "option" : "Edit EFS volume and enable the â€œencryption at restâ€ setting. All existing data automatically gets encrypted as a background process. You will be notified once the process is completed.",
    "isCorrect" : "false"
  }, {
    "id" : "332333b6f78343409fdacf6130188652",
    "option" : "Encryption at rest is enabled by default when creating a new file system using the Amazon EFS console.",
    "isCorrect" : "true"
  }, {
    "id" : "9e63d0a117ae43c092ab7b6fe2107e24",
    "option" : "You can enable encryption at rest during the mounting of EFS on EC2. To encrypt an existing EFS mount, unmount the EFS and remount with the encryption option.",
    "isCorrect" : "false"
  }, {
    "id" : "4a4d6e34c34945bd8c9712283d162425",
    "option" : "EFS does not support encryption. Use S3 for encrypting data at rest.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAmazon EFS supports two forms of encryption for file systems, encryption of data in transit and encryption at rest.\nYou can enable encryption of data at rest when creating an Amazon EFS file system.\nYou can enable encryption of data in transit when you mount the file system.\nNote: Encryption at rest is not enabled by default when creating a new file system using the AWS CLI, API, and SDKs.\nAmazon EFS uses an industry-standard AES-256 encryption algorithm to encrypt EFS data and metadata at rest.\nOption A is incorrect.\nYou cannot enable encryption once EFS is created.\nOption C is incorrect.\nYou cannot enable encryption at rest through mounting options.\nOption D is incorrect.\nIt is a distractor.\nEFS does support encryption.\nReference:\nhttps://docs.aws.amazon.com/efs/latest/ug/encryption-at-rest.html\n\nThe correct answer is A. Edit EFS volume and enable the \"encryption at rest\" setting. All existing data automatically gets encrypted as a background process. You will be notified once the process is completed.\nExplanation:\nAmazon Elastic File System (Amazon EFS) provides a simple, scalable, fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. Amazon EFS file systems can be encrypted using AWS Key Management Service (KMS) keys. When data is at rest, it is encrypted using AWS KMS customer master keys (CMKs).\nTo enable encryption for an existing EFS volume, you can edit the volume and enable the \"encryption at rest\" setting. When you enable encryption, all data on the volume gets automatically encrypted as a background process. Once the process is completed, you will be notified.\nEncryption at rest is not enabled by default when creating a new file system using the Amazon EFS console (Option B). However, you can enable it during the creation process.\nOption C is incorrect because you cannot enable encryption at rest during the mounting of EFS on EC2. However, you can encrypt an existing EFS mount by unmounting the EFS and remounting with the encryption option.\nOption D is incorrect because EFS does support encryption at rest.\nTherefore, the correct answer is A.\n\n"
}, {
  "id" : 601,
  "question" : "You have created AWS EFS with default settings and mounted it on an EC2 instance.\nDue to regulatory policies, your organization had asked you to encrypt data during transit to EFS.\nWhat would you do to enable encryption during transit?\n",
  "answers" : [ {
    "id" : "2bfce6cbd51e4c5b82764677d61f0bb0",
    "option" : "AWS EFS uses NFS protocol which encrypts the data in transit by default.",
    "isCorrect" : "false"
  }, {
    "id" : "fd09263077da4b3683dba23b6a2079fd",
    "option" : "Edit EFS to enable â€œencryption during transitâ€ setting.",
    "isCorrect" : "false"
  }, {
    "id" : "7a44f765cd4946d3a4eaff9adcc63456",
    "option" : "Encryption during transit can only be enabled during EFS creation. You need to create encryption during transit EFS, copy data from old EFS to new EFS and delete old EFS.",
    "isCorrect" : "false"
  }, {
    "id" : "7bb9c614651e4c43827040d2fdd2162f",
    "option" : "Encryption during transit can be enabled when mounting the file system using the Amazon EFS mount helper. The mount helper uses TLS version 1.2 to communicate with the file system.",
    "isCorrect" : "true"
  } ],
  "explanations" : "\n\nAnswer: D.\nYou can encrypt data in transit using an Amazon EFS file system, without modifying your applications.\nEnabling encryption of data in transit for your Amazon EFS file system is done by enabling Transport Layer Security (TLS) when you mount your file system using the Amazon EFS mount helper.\nWhen encryption of data in transit is declared as a mount option for your Amazon EFS file system, the mount helper initializes a client stunnel process.\nStunnel is an open-source multipurpose network relay.\nThe client stunnel process listens on a local port for inbound traffic, and the mount helper redirects Network File System (NFS) client traffic to this local port.\nOption A is incorrect because NFS protocol is not provided by default.\nOptions B and C are incorrect.\nEncryption during transit is not an option on EFS during or after creation.\nReference:\nhttps://docs.aws.amazon.com/efs/latest/ug/encryption-in-transit.html\n\n\nThe correct answer is D. Encryption during transit can be enabled when mounting the file system using the Amazon EFS mount helper. The mount helper uses TLS version 1.2 to communicate with the file system.\nAWS Elastic File System (EFS) provides a simple, scalable, fully managed file storage service for use with Amazon EC2 instances. It uses the Network File System (NFS) protocol for file access, which provides file-level access to shared data.\nWhen you mount an EFS file system on an EC2 instance, all data transmitted between the EC2 instance and the EFS file system is sent over the network. By default, the data is not encrypted, but there is a way to enable encryption during transit.\nTo enable encryption during transit to EFS, you can use the Amazon EFS mount helper. The mount helper is a set of scripts and utilities that simplifies the mounting of EFS file systems on Amazon EC2 instances. It provides support for encryption of data in transit using Transport Layer Security (TLS) version 1.2.\nTo enable encryption during transit, you need to mount the EFS file system using the Amazon EFS mount helper with the appropriate options. You can do this by specifying the tls mount option in the mount command, like this:\njavascript Copy code sudo mount -t efs -o tls fs-12345678:/ /mnt/efs\nWhen you specify the tls mount option, the Amazon EFS mount helper uses TLS version 1.2 to encrypt data in transit between the EC2 instance and the EFS file system. The data is decrypted on the other side of the connection and stored in the EFS file system in its unencrypted form.\nIt is important to note that enabling encryption during transit only encrypts the data in transit between the EC2 instance and the EFS file system. It does not encrypt the data at rest in the EFS file system. To encrypt the data at rest, you can use the EFS encryption feature or encrypt the data before storing it in the EFS file system.\n\n"
}, {
  "id" : 602,
  "question" : "You are building a content-serving web application with 20 EC2 instances.\nThe EC2 servers are all load-balanced, and content storage for the instances will remain the same. You have chosen AWS EFS to act as a common storage repository.\nYour application needs to have as low of latency as possible when serving content to the web users.\nWhich of the following options is the best choice for this situation?\n",
  "answers" : [ {
    "id" : "a2c4bdadb1cb47eb857e3f8ff85c0743",
    "option" : "Max I/O Performance Mode",
    "isCorrect" : "false"
  }, {
    "id" : "be5a755f6b5b40e68d6d8889ea1c16a2",
    "option" : "General Purpose Performance Mode",
    "isCorrect" : "true"
  }, {
    "id" : "d51ccf11834943abaeab5071da824957",
    "option" : "Bursting Throughput Mode",
    "isCorrect" : "false"
  }, {
    "id" : "acb0d482e22545fa9ae1f853ae38010b",
    "option" : "Provisioned Throughput Mode.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAlthough Max I/O is recommended to be used when tens, hundreds, or thousands of EC2 instances sharing the same EFS, it can slightly increase the latency.\nIn this case, the question states the latency needs to be as low as possible.\nPerformance Modes.\nTo support a wide variety of cloud storage workloads, Amazon EFS offers two performance modes.\nYou select a file system's performance mode when you create it.\nThe two performance modes have no additional costs, so your Amazon EFS file system is billed and metered the same, regardless of your performance mode.\nFor information about file system limits, see Limits for Amazon EFS File Systems.\nNote: An Amazon EFS file system's performance mode can't be changed after the file system has been created.\nGeneral Purpose Performance Mode.\nWe recommend the General Purpose performance mode for the majority of your Amazon EFS file systems.\nGeneral Purpose is ideal for latency-sensitive use cases, like web serving environments, content management systems, home directories, and general file serving.\nIf you don't choose a performance mode when you create your file system, Amazon EFS selects the General Purpose mode for you by default.\nMax I/O Performance Mode.\nFile systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations.\nHighly parallelized applications and workloads, such as big data analysis, media processing, and genomics analysis, can benefit from this mode.\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html#performancemodes\nVia the explanations above, Option B is the only correct statement.\nFor Bursting and Provisioned Throughput modes, please refer to page 85 to 89 on the below link:\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-ug.pdf\n\n\nFor this scenario, the best performance mode for EFS is Provisioned Throughput Mode (D).\nProvisioned Throughput Mode provides consistent throughput performance for file operations, and it's suitable for applications that require high and consistent throughput to access file systems.\nIn this case, since you have 20 EC2 instances, you need a storage solution that can handle high traffic, provide low latency, and maintain consistency. Provisioned Throughput Mode will allow you to set the desired throughput for your file system, which can help you manage the traffic and performance of your web application.\nMax I/O Performance Mode (A) is optimized for applications that need maximum I/O throughput and operations per second (OPS) with the lowest possible latency. This mode is ideal for workloads that require heavy I/O, but it doesn't provide consistent performance as Provisioned Throughput Mode.\nGeneral Purpose Performance Mode (B) is ideal for workloads that have a mix of operations, and it's optimized for latency-sensitive workloads. However, in this scenario, where consistency is essential, Provisioned Throughput Mode is a better choice.\nBursting Throughput Mode (C) is optimized for workloads that have bursts of activity, and it provides a baseline throughput with the ability to burst for short periods. However, for consistent performance, Provisioned Throughput Mode is the best option.\nIn conclusion, Provisioned Throughput Mode is the best choice for this scenario since it provides consistent throughput performance for file operations, which is essential for a content-serving web application with multiple EC2 instances.\n\n"
}, {
  "id" : 603,
  "question" : "You are building a content serving web application (size not exceeding 25 GB) on 5 EC2 load-balanced instances.\nYou have chosen EFS for content storage.\nThe content is accessed frequently by a large number of users.\nWhich of the following throughput mode would you choose to ensure that application on EC2 instances transfers the data to EFS without any performance bottleneck?\n",
  "answers" : [ {
    "id" : "a516da5b3a544f11aec618f97fa6cd02",
    "option" : "Throughput mode = Bursting, provides a consistent high throughput for smaller data sizes.",
    "isCorrect" : "false"
  }, {
    "id" : "21be3ddcb22c49ddb86c54cc02a2be3d",
    "option" : "General Purpose Performance Mode",
    "isCorrect" : "false"
  }, {
    "id" : "e3c1c94e41784c1a9bd798c928472b94",
    "option" : "Throughput mode = Provisioned, you can configure specific throughput irrespective of EFS data size.",
    "isCorrect" : "true"
  }, {
    "id" : "f5e81715b0f749898e87e3da2be81d39",
    "option" : "Max I/O Performance Mode.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nSpecifying Throughput with Provisioned Mode.\n\"Provisioned Throughput mode is available for applications with high throughput to storage (MiB/s per TiB) ratios, or with requirements greater than those allowed by the Bursting Throughput mode.\nFor example, say, you're using Amazon EFS for development tools, web serving, or content management applications where the amount of data in your file system is low relative to throughput demands.\nYour file system can now get the high levels of throughput your applications require without having to pay your file system\".\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html#throughput-modes\nPlease refer to page 87, section \"Specifying Throughput with Provisioned Mode\" in the below link.\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-ug.pdf.\nFor this case, since the data is low compared to the throughput demand, provisioned mode is the right choice for throughput mode.\nSince the question asks for a \"throughout mode,\" the \"performance mode\" cannot be used here.\nTherefore Options B and D are INCORRECT here.\n\nWhen using Amazon Elastic File System (EFS) for content storage in a web application, it is important to choose the appropriate throughput mode to ensure that there are no performance bottlenecks in transferring data between the EC2 instances and the EFS.\nThe throughput mode determines how EFS manages file system performance and cost. There are four throughput modes available in Amazon EFS:\nBurstable throughput mode Provisioned throughput mode Max I/O throughput mode General purpose performance mode\nOf the four available throughput modes, the best option for a content serving web application on 5 EC2 load-balanced instances is Throughput mode = Provisioned.\nHere is a detailed explanation of each throughput mode and why Provisioned is the best option for this particular use case:\nBurstable throughput mode: Burstable throughput mode is the default mode for Amazon EFS. It is ideal for file systems with infrequent access and a low data transfer rate. In this mode, EFS can burst up to the maximum throughput of the file system for short periods of time before returning to the baseline throughput level. Bursting is useful for handling short-term spikes in traffic. However, it is not recommended for applications that require consistent high throughput over time, as the performance can degrade over time. Provisioned throughput mode: Provisioned throughput mode is designed for applications that require consistent, high throughput access to a file system. In this mode, you can specify the throughput level you need, and EFS provides the corresponding amount of I/O operations per second (IOPS) and throughput. The provisioned throughput mode is ideal for applications with high data transfer rates or those that require consistent, low-latency access to files. Max I/O throughput mode: Max I/O throughput mode is designed for applications that require high levels of parallelism and low-latency access to data. It is useful for applications with a large number of threads accessing files simultaneously or those with a high rate of small I/O operations. This mode can achieve the highest level of performance but is also the most expensive. General purpose performance mode: General purpose performance mode is designed for applications with a moderate amount of data transfer and I/O operations. It balances cost with performance and is ideal for applications with variable or unpredictable workloads.\nIn this use case, a content serving web application that is frequently accessed by a large number of users requires consistent high throughput access to the file system. Therefore, the best option is Throughput mode = Provisioned, which allows for configuring specific throughput levels regardless of the EFS data size. This mode ensures that there are no performance bottlenecks in transferring data between the EC2 instances and the EFS, which is crucial for a high-traffic web application.\n\n"
}, {
  "id" : 604,
  "question" : "Your organization is planning to use AWS for BigData analysis.\nTotal data is expected to be 400 TB.\nThey were planning to use 150 EC2 instances with EFS because of better performance needs for the analysis.\nThey have reached out to you asking for recommendation on performance mode.\nWhat would you suggest?\n",
  "answers" : [ {
    "id" : "cff93dec475040018c174163bd998e6a",
    "option" : "Performance mode = General Purpose, AWS can handle performance with general purpose mode till 10s of EC2 instances.",
    "isCorrect" : "false"
  }, {
    "id" : "cb09b943c03d48ec9d7dc993ab6d0a2a",
    "option" : "Performance mode = General Purpose, provides low-latency access to EFS.",
    "isCorrect" : "false"
  }, {
    "id" : "86f04c9c09524e8f9cfe074d21f81bbf",
    "option" : "Performance mode = General Purpose, provides higher levels of aggregate throughput and operations per second.",
    "isCorrect" : "false"
  }, {
    "id" : "ce8905aacd554a23b5886458265b22d1",
    "option" : "Performance mode = Max I/O, provides higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nMax I/O Performance Mode.\n\"File systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations.\nHighly parallelized applications and workloads, such as big data analysis, media processing, and genomics analysis, can benefit from this mode\".\nFor more information, Please check following AWS Docs:\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html\n\nAs the organization is planning to use AWS for BigData analysis and the total data size is expected to be 400TB, they are planning to use 150 EC2 instances with EFS for better performance needs. Now, they have asked for a recommendation on performance mode for EFS.\nAmazon Elastic File System (Amazon EFS) is a fully managed, scalable, and highly available cloud-native file system. It provides a simple, scalable, and fully managed elastic NFS file system for use with AWS Cloud services and on-premises resources. EFS supports two performance modes: General Purpose and Max I/O.\nGeneral Purpose mode is the default performance mode for EFS. It provides low-latency access to EFS, which is suitable for a wide range of use cases such as web serving, content management, and home directories. General Purpose mode can handle performance with general purpose mode till 10s of EC2 instances.\nMax I/O mode provides higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies. It is designed for higher levels of concurrency and higher throughput workloads such as big data, analytics, and media processing. Max I/O mode is recommended for use cases that require higher levels of parallelism and throughput, such as HPC, big data, and media processing.\nConsidering the use case of BigData analysis and the requirement of better performance with 150 EC2 instances, it is recommended to use Performance mode = Max I/O. This will provide higher levels of aggregate throughput and operations per second, which is suitable for big data analytics workloads.\nIn conclusion, the recommendation for the organization is to use Performance mode = Max I/O for EFS when analyzing BigData with 150 EC2 instances.\n\n"
}, {
  "id" : 605,
  "question" : "Which of the following are typical use cases of Amazon EFS One Zone storage classes? (Select THREE)\n",
  "answers" : [ {
    "id" : "f6b3e2f9e49b48788f91b7d1d9308fd4",
    "option" : "Data is stored redundantly in a single AZ.",
    "isCorrect" : "true"
  }, {
    "id" : "dfa39a63017a4b11aed228c2d4adacfa",
    "option" : "Up to thousands of Amazon EC2 instances from multiple AZs, can connect concurrently to a file system.",
    "isCorrect" : "true"
  }, {
    "id" : "1682f5bea8a7407a8bb604d902ccc304",
    "option" : "Boot volumes, transactional and NoSQL databases, data warehousing, and ETL.",
    "isCorrect" : "false"
  }, {
    "id" : "7ac4e747b07e478cb99faf9327cc2147",
    "option" : "Big data and analytics, media processing workflows, content management, web serving, and home directories that do not require Multi-AZ resilience.",
    "isCorrect" : "true"
  }, {
    "id" : "2ed65728a5954ef082a5e78958fc3cd6",
    "option" : "Cross region replication.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B, D.\nThe following table shows the characteristics of EFS vs.\nEBS.\nRefer to page 123 on the below link:\nhttps://docs.aws.amazon.com/efs/latest/ug/efs-ug.pdf#performance\nOption A, Option B, and Option D are characteristics of EFS.\nOption C is characteristic of EBS.\nOption E is characteristic of S3.\nFor more information on AWS EFS use cases, refer to the documentation here.\nhttps://docs.aws.amazon.com/efs/latest/ug/performance.html#performance-usecases\n\n\nAmazon EFS (Elastic File System) is a fully managed file storage service for EC2 instances that provides scalable and highly available file storage for use with AWS Cloud services and on-premises resources. EFS offers three storage classes: Standard, Infrequent Access (IA), and One Zone Infrequent Access (One Zone-IA). One Zone-IA is a lower-cost storage class compared to Standard and IA but provides a lower level of durability because data is stored in a single Availability Zone (AZ).\nThe typical use cases of Amazon EFS One Zone storage classes are:\nA. Data is stored redundantly in a single AZ: One Zone-IA is designed for use cases where data is replaceable, such as content distribution, development and test, and transient data storage. It is not recommended for data that requires high levels of durability, such as mission-critical production workloads.\nB. Up to thousands of Amazon EC2 instances from multiple AZs can connect concurrently to a file system: One Zone-IA can handle up to thousands of concurrent connections from multiple Amazon EC2 instances within the same VPC and AZ.\nD. Big data and analytics, media processing workflows, content management, web serving, and home directories that do not require Multi-AZ resilience: One Zone-IA is suitable for workloads that do not require Multi-AZ resilience, such as big data and analytics, media processing workflows, content management, web serving, and home directories.\nTherefore, the correct answers are A, B, and D. Option C is incorrect because One Zone-IA is not suitable for boot volumes, transactional and NoSQL databases, data warehousing, and ETL workloads that require high durability and Multi-AZ resilience. Option E is also incorrect because One Zone-IA does not support cross-region replication.\n\n"
}, {
  "id" : 606,
  "question" : "Which of the following are valid integration sources for API Gateway? (choose 3 options)\n",
  "answers" : [ {
    "id" : "c749d08acb6248f180e5b009c8139d0e",
    "option" : "Public facing HTTP-based endpoints outside AWS network.",
    "isCorrect" : "true"
  }, {
    "id" : "400b1b76edc0428bad0c9ed820b7eb46",
    "option" : "Lambda functions from another account.",
    "isCorrect" : "true"
  }, {
    "id" : "d2ee4b46eaae4ce9a5274acafdb322ae",
    "option" : "Database connections on internet outside AWS network.",
    "isCorrect" : "false"
  }, {
    "id" : "edd1f6a2880b4c208d84ddaf0607fa9f",
    "option" : "VPC Link",
    "isCorrect" : "true"
  }, {
    "id" : "1253c2aec90f49d3af8636776d25b734",
    "option" : "SFTP connection.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B, D.\nOption A is correct.\nAWS API Gateway can integrate with any HTTP-based endpoints available over the internet.\nQ: With what backends can Amazon API Gateway communicate?\nAmazon API Gateway can execute AWS Lambda functions in your account, start AWS Step Functions state machines, or call HTTP endpoints hosted on AWS Elastic Beanstalk, Amazon EC2, and also non-AWS hosted HTTP based operations that are accessible via the public Internet.\nAPI Gateway also allows you to specify a mapping template to generate static content to be returned, helping you mock your APIs before the backend is ready.\nYou can also integrate API Gateway with other AWS services directly.\nFor example, you could expose an API method in API Gateway that sends data directly to Amazon Kinesis.\nOption B is correct.\nAWS can use Lambda function from another account as an integration type.\nOption C is incorrect.\nAWS API gateway can connect to AWS services, making proxy calls only to their respective AWS APIs.\nThere is no integration type for database connections directly from API Gateway.\nYou can use the Lambda function to connect with the database and make Lambda as an integration type for API Gateway.\nOption D is correct.\nAWS has introduced VPC Link, a way to connect to the resources within a private VPC.Refer to the documentation here for more information on VPC Links.\nhttps://aws.amazon.com/blogs/compute/introducing-amazon-api-gateway-private-endpoints/\n\n\nAPI Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. Integration sources for API Gateway determine where the backend service resides, and how requests should be processed.\nThe valid integration sources for API Gateway are:\nA. Public facing HTTP-based endpoints outside AWS network. This option refers to integrating API Gateway with HTTP/HTTPS endpoints that are hosted outside the AWS network. It enables API Gateway to serve as a proxy between clients and the backend HTTP endpoints.\nB. Lambda functions from another account. This option enables integration of API Gateway with Lambda functions that are hosted in another AWS account. It is useful when multiple teams or organizations need to collaborate on building and deploying serverless applications.\nD. VPC Link This option enables integration of API Gateway with resources that are hosted inside a Virtual Private Cloud (VPC). It provides a secure and private connection between the API Gateway and VPC resources, without exposing them to the public internet.\nNote: Only a private endpoint can be used to connect a VPC with API Gateway.\nTherefore, options A, B, and D are valid integration sources for API Gateway.\nC. Database connections on the internet outside AWS network This option is not a valid integration source for API Gateway. Instead, the best practice is to use AWS services such as Amazon RDS, Amazon DynamoDB, or Amazon Aurora, which are highly available, scalable, and secure.\nE. SFTP connection This option is also not a valid integration source for API Gateway. Instead, AWS provides services such as AWS Transfer Family, which is a fully managed service that enables the transfer of files over Secure File Transfer Protocol (SFTP), FTPS, and FTP.\n\n"
}, {
  "id" : 607,
  "question" : "A Company ABC has 100 REST APIs exposed to the Internet from their on-premise network.\nThey have already integrated with AWS through DirectConnect.\nThey have approached you asking for a cost-effective way of making these REST APIs available through AWS API Gateway because of the resiliency and cost reductions provided by it.\nWhat solution would you provide?\n",
  "answers" : [ {
    "id" : "6a1e558bf33345abafc902cb88699f4c",
    "option" : "API Gateway cannot integrate with on-premises backend APIs which are not over the public internet. Rebuild all the backend APIs using Lambda and integrate it with API Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "186010b81deb40a4bd0f3ed89ae6cdb8",
    "option" : "Use VPC Link to integrate on-premises backend solutions through DirectConnect and private VP.",
    "isCorrect" : "true"
  }, {
    "id" : "efe318051d5e420bab28283da69e53d1",
    "option" : "Build API Gateway using the existing on-premises public facing REST APIs as HTTPS endpoints integration type.",
    "isCorrect" : "false"
  }, {
    "id" : "293339a6d4d642d497b29e78084bb4f0",
    "option" : "Build API Gateway with integration type as AWS Service and select Direct Connect service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B\nFor more information on VPC Link, refer to the documentation here.\nhttps://aws.amazon.com/blogs/compute/introducing-amazon-api-gateway-private-endpoints/\nOption A is INCORRECT because you can use API Gateway to integrate with on-premises backend APIs.\nTherefore this option is invalid.\nOption C is INCORRECT because you can choose the integration type as \"HTTPS\" if your API is integrated with an existing HTTPS endpoint.\nSince the question does not state any integration with any HTTPS endpoint, this option is invalid.\nOption D is INCORRECT because you can choose the integration type as \"AWS Service\" only if your API will be integrated with an AWS service.\nSince the question does not state any integration with any AWS service, this option is invalid.\nPlease refer to page 605 on the below link:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-dg.pdf\n\n\nOption B - Use VPC Link to integrate on-premises backend solutions through DirectConnect and private VPC - is the best solution for this scenario.\nAPI Gateway can integrate with backend APIs hosted in a variety of locations, including on-premises servers. However, it is not recommended to expose on-premises APIs to the public internet directly due to security concerns.\nIn this scenario, the company has already integrated with AWS through DirectConnect, which establishes a dedicated network connection between their on-premises network and their AWS VPC (Virtual Private Cloud). This means that their on-premises network is already connected to AWS and they can use the VPC Link feature of API Gateway to securely access their on-premises REST APIs.\nVPC Link is a feature of API Gateway that allows you to privately integrate your API Gateway with resources in your VPC. With VPC Link, you can create a private connection between your API Gateway and a VPC in your account, or a VPC in a different account or region. You can then use this VPC Link to access resources in the VPC from your API Gateway.\nIn this scenario, the company can create a VPC Link between their API Gateway and their on-premises network through their existing DirectConnect connection. They can then use the VPC Link to access their on-premises REST APIs securely from API Gateway without exposing them to the public internet. This solution is both cost-effective and provides the resiliency benefits of API Gateway.\nOption A - Rebuilding all the backend APIs using Lambda and integrating them with API Gateway - would be a costly and time-consuming solution. It would require significant development effort to rewrite all the existing REST APIs as Lambda functions and would likely require changes to the existing application architecture.\nOption C - Building API Gateway using the existing on-premises public-facing REST APIs as HTTPS endpoints integration type - would expose the on-premises APIs to the public internet, which is not recommended for security reasons.\nOption D - Building API Gateway with integration type as AWS Service and selecting Direct Connect service - is not a valid option as AWS Service integration type is used to integrate API Gateway with other AWS services such as Lambda or DynamoDB, but not with on-premises resources.\n\n"
}, {
  "id" : 608,
  "question" : "You have built a REST API using API gateway and distributed to your customers.\nHowever, your API is receiving large number of requests and overloading your backend system causing performance bottlenecks and eventually causing delays and failures in serving the requests for your important customers.\nHow would you improve the API performance? (Choose 2 options)\n",
  "answers" : [ {
    "id" : "e5db70a0af40475baad8c614ba3de5d6",
    "option" : "Enable throttling and control the number of requests per second.",
    "isCorrect" : "true"
  }, {
    "id" : "d93e05328ed34af795c6faee8dd60c64",
    "option" : "Create a resource policy to allow access for specific customers during specific time period.",
    "isCorrect" : "false"
  }, {
    "id" : "eeae15a34c0f44fbb1e2c57733d10994",
    "option" : "Enable API caching to serve frequently requested data from API cache.",
    "isCorrect" : "true"
  }, {
    "id" : "8a7c1ac2a3f8434791e726d10c978790",
    "option" : "Enable load balancer on your backend systems.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C.\nOption A is correct.\nTo prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API.\nSpecifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions against all APIs in your account.\nFor more information on throttling, refer documentation here.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html\nOption B is not correct.\nThis is not a viable solution.\nResource policies cannot have a time range based condition.\nFollowing documentation shows the conditions supported for API Gateway resource policies.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-aws-condition-keys.html\nOption C is correct.\nYou can enable API caching in Amazon API Gateway to cache your endpoint's responses.\nWith caching, you can reduce the number of calls made to your endpoint and also improve the latency of requests to your API.\nWhen you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds.\nAPI Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint.\nThe default TTL value for API caching is 300 seconds.\nThe maximum TTL value is 3600 seconds.\nTTL=0 means caching is disabled.\nFor details on enabling caching, refer documentation here.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html#enable-api-gateway-caching\nOption D is not correct.\nWe can improve performance by increasing the capacity of backend systems if above settings does not help.\nSimply adding a load balancer does not improve any performance.\n\nTo improve the performance of your REST API and prevent overloading your backend system, there are a number of strategies you can employ. Two of the most effective options are:\nA. Enable throttling and control the number of requests per second: API Gateway offers a powerful feature called throttling that enables you to limit the number of requests per second that are allowed to access your API. By setting appropriate limits, you can prevent excessive traffic from overwhelming your backend system, reducing the likelihood of performance bottlenecks and failures. With throttling, you can control the maximum number of requests per second for each API key or client IP address. You can also set up burst limits that allow short-term bursts of traffic above the regular request rate. By controlling the traffic to your backend systems, you can ensure that your API remains responsive and reliable for all your customers.\nC. Enable API caching to serve frequently requested data from the API cache: API caching can be used to store frequently requested data in memory, reducing the number of requests that need to be processed by your backend system. By caching data at the API Gateway level, you can significantly reduce the load on your backend system, as requests that are identical to previously requested data can be served directly from the cache. This can reduce latency and improve the overall performance of your API. You can configure API caching to work with specific resources, methods, or stages of your API, and you can set cache TTLs (time-to-live) to control how long data is stored in the cache.\nThe other two options - B and D - may also be useful in certain scenarios, but they may not be the most effective ways to address the specific problem described in this question. A resource policy (B) can be used to control access to your API, but it does not directly address the issue of overloading your backend system. Similarly, a load balancer (D) can distribute traffic across multiple backend servers, but it may not be sufficient to prevent overloading and performance bottlenecks. It is important to choose the right set of strategies based on the specific needs of your API and the challenges you are facing.\n\n"
}, {
  "id" : 609,
  "question" : "You are a solutions architect working for an online retailer.\nYour online website uses REST API calls via API Gateway and Lambda from your Angular SPA front-end to interact with your DynamoDB data store.\nYour DynamoDB tables are used for customer preferences, account, and product information.\nWhen your web traffic spikes, some requests return a 429 error response.\nWhat might be the reason your requests are returning a 429 response? (Select TWO)\n",
  "answers" : [ {
    "id" : "1b8c51f7ddfd4312a7bd66c4b6ff8358",
    "option" : "Your Lambda function has exceeded the concurrency limit.",
    "isCorrect" : "true"
  }, {
    "id" : "0de6903484194ffcb06d816b93d7ee90",
    "option" : "DynamoDB concurrency limit has been exceeded",
    "isCorrect" : "false"
  }, {
    "id" : "6f8d7b6aaa374311beea4f7bab810b00",
    "option" : "Your Angular service failed to connect to your API Gateway REST endpoint.",
    "isCorrect" : "false"
  }, {
    "id" : "0dad2a18aef64e529fd783520fa501c9",
    "option" : "Your Angular service cannot handle the volume spike",
    "isCorrect" : "false"
  }, {
    "id" : "6d23b0592d2a4da1a1c3a791d6b17c40",
    "option" : "Your API Gateway has exceeded the steady-state request rate and burst limits.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers: A and E.\nOption A is correct.\nWhen your traffic spikes, your Lambda function can exceed the limit set on the number of concurrent instances that can be run (burst concurrency limit in the US: 3,000).\nOption B is incorrect.\nWhen your table exceeds its provisioned throughput DynamoDB will return a 400 error to the requesting service, in this case, API Gateway.\nThis will not result in the propagation of a 429 error response (too many requests) back to the Angular SPA service.\nOption C is incorrect.\nIf your Angular service fails to connect to your API Gateway REST endpoint your code will not generate a 429 error response (too many requests).\nOption D is incorrect.\nSince your Angular SPA code runs in the individual user's web browser, this option makes no sense.\nOption E is correct.\nWhen your API Gateway request volume reaches the steady-state request rate and bursting limit, API Gateway throttles your requests to protect your back-end services.\nWhen these requests are throttled, API Gateway returns a 429 error response (too many requests).\nReferences:\nPlease see the Amazon API Gateway developer guide titled Throttle API requests for better throughput (https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html), the Towards Data Science article titled Full Stack Development Tutorial: Integrate AWS Lambda Serverless Service into Angular SPA (https://towardsdatascience.com/full-stack-development-tutorial-integrate-aws-lambda-serverless-service-into-angular-spa-abb70bcf417f), the Amazon API Gateway developer guide titled Invoking a REST API in Amazon API Gateway (https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-call-api.html), the AWS Lambda developer guide titled Lambda function scaling (https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html), and the Amazon DynamoDB developer guide titled Error Handling with DynamoDB (https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html)\n\nThe 429 Too Many Requests error response indicates that the user has sent too many requests in a given amount of time. This error can occur due to various reasons in the given scenario.\nA. Your Lambda function has exceeded the concurrency limit: Lambda function concurrency refers to the number of requests being served by a function at any given time. If the concurrency limit is exceeded, any further requests will result in a 429 error. This could happen if the Lambda function is handling a large number of requests simultaneously, and its limit is reached. You can check the concurrency limit for your Lambda function in the AWS Management Console.\nB. DynamoDB concurrency limit has been exceeded: DynamoDB also has a concurrency limit that can cause the 429 error if exceeded. This could happen if the number of requests to read or write to DynamoDB exceeds its capacity to handle them. You can check the capacity and utilization metrics for your DynamoDB tables in the AWS Management Console.\nC. Your Angular service failed to connect to your API Gateway REST endpoint: If the Angular service fails to connect to the API Gateway REST endpoint, it will not be able to make requests to DynamoDB. This could happen due to various reasons, such as network issues or incorrect configuration.\nD. Your Angular service cannot handle the volume spike: If the volume of traffic to your website spikes suddenly, your Angular service may not be able to handle the increased load. This could result in delays or errors, including 429 errors.\nE. Your API Gateway has exceeded the steady-state request rate and burst limits: API Gateway has steady-state request rate limits and burst limits, which can be exceeded if there is a sudden spike in traffic to your website. This could result in API Gateway rejecting some requests and returning a 429 error response. You can check the API Gateway usage metrics in the AWS Management Console to identify if this is the issue.\nIn summary, the two most likely reasons for 429 errors in this scenario are: A. Your Lambda function has exceeded the concurrency limit, and E. Your API Gateway has exceeded the steady-state request rate and burst limits.\n\n"
}, {
  "id" : 610,
  "question" : "You are a solutions architect working for a financial services firm.\nYour firm requires a very low latency response time for requests via API Gateway and Lambda integration to your securities master database.\nThe securities master database, housed in Aurora, contains data about all of the securities your firm trades.\nThe data consists of the security ticker, the trading exchange, trading partner firm for the security, etc.\nAs this securities data is relatively static, you can improve the performance of your API Gateway REST endpoint by using API Gateway caching.\nYour REST API calls for equity security request types and fixed income security request types to be cached separately. Which of the following options is the most efficient way to separate your cache responses via request type using API Gateway caching?\n",
  "answers" : [ {
    "id" : "7fc08aa8ddd24e5b898a91f30109febf",
    "option" : "Payload compression",
    "isCorrect" : "false"
  }, {
    "id" : "6efaf4a27da44a84a849e410fbc10181",
    "option" : "Custom domain name",
    "isCorrect" : "false"
  }, {
    "id" : "f66418e2214e42e4b53874ccb1c0cff8",
    "option" : "API Stage",
    "isCorrect" : "false"
  }, {
    "id" : "baf4e8ad41354106ab006b911f49eb1f",
    "option" : "Query string.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption A is incorrect.\nPayload compression is used to compress and decompress the payload to and from your API Gateway.\nIt is not used to separate cache responses.\nOption B is incorrect.\nCustom domain names are used to provide more readable URLs for the users of your AIPs.\nThey are not used to separate cache responses.\nOption C is incorrect.\nAn API stage is used to create a name for your API deployments.\nThey are used to deploy your API in an optimal way.\nOption D is correct.\nYou can use your query string parameters as part of your cache key.\nThis allows you to separate cache responses for equity requests from fixed income request responses.\nReferences:\nPlease see the Amazon API Gateway developer guide titled Enabling API caching to enhance responsiveness (https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html),\nthe Amazon API Gateway REST API Reference page titled Making HTTP Requests to Amazon API Gateway (https://docs.aws.amazon.com/apigateway/api-reference/making-http-requests/),\nthe Amazon API Gateway developer guide titled Enabling payload compression for an API (https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-gzip-compression-decompression.html),\nthe Amazon API Gateway developer guide titled Setting up custom domain names for REST APIs (https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html),\nand the Amazon API Gateway developer guide titled Setting up a stage for a REST API (https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-stages.html)\n\nTo separate cache responses via request type using API Gateway caching, the most efficient option is to use the query string. The query string is a part of the URL that contains data to be passed to the API Gateway.\nIn this scenario, the query string can be used to specify the type of security request being made, such as equity or fixed income. This allows the API Gateway caching to cache the responses for each request type separately.\nPayload compression is a technique used to reduce the size of the data being sent over the network, but it does not relate to separating cache responses via request type using API Gateway caching.\nCustom domain name is a feature that allows a user to map their own domain name to their API Gateway, but it does not relate to separating cache responses via request type using API Gateway caching.\nAPI stage is a way to manage different versions of your API, and it does not relate to separating cache responses via request type using API Gateway caching.\nIn summary, the most efficient way to separate cache responses via request type using API Gateway caching is to use the query string.\n\n"
}, {
  "id" : 611,
  "question" : "Which of the following are not access control mechanisms for AWS API Gateway? (Choose 2 options)\n",
  "answers" : [ {
    "id" : "ca72bbb6608c49989b162d0de07b9e20",
    "option" : "Resource policies",
    "isCorrect" : "false"
  }, {
    "id" : "3c04c1cd46db46e9aafcc20cef65efcc",
    "option" : "Lambda authorizers",
    "isCorrect" : "false"
  }, {
    "id" : "092dc4f39ae6424480fd23c03a492c2e",
    "option" : "Server-side certificates",
    "isCorrect" : "true"
  }, {
    "id" : "52e453af72ba4f9daceb2258f4bc7997",
    "option" : "VPC RouteTables",
    "isCorrect" : "true"
  }, {
    "id" : "3f6a0ea595d347e6a5de4b468c43a556",
    "option" : "Usage Plans.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C, D.\nFollowing are different ways of controlling access to your AWS API Gateway.\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-controlling-access-to-apis.html\nOption C is not an access control mechanism.\nAPI Gateway accepts the client-side certificates of your backend system.\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started-client-side-ssl-authentication.html#configure-api\nOption D is not an access control mechanism.\nRouteTables in VPCs are to control network traffic flow within a VPC.For more information on VPC route tables, refer to documentation here:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Route_Tables.html\n\n\nSure, I can provide you with a detailed explanation of the access control mechanisms for AWS API Gateway and which options are not part of those mechanisms.\nAWS API Gateway provides several ways to control access to your APIs. These include:\nResource policies: These policies are applied to individual resources in your API and can control who has access to that specific resource. Lambda authorizers: These are custom authorizers that you can use to control access to your API using a Lambda function. The Lambda function can perform custom authentication and authorization logic to allow or deny access to the API. Server-side certificates: These are used to secure connections between the client and the API Gateway using SSL/TLS encryption. They are not used for access control. VPC RouteTables: These are used to control traffic between a VPC and the internet. They are not used for access control in API Gateway. Usage plans: These are used to control access to your API by limiting the number of requests that can be made by a particular client over a given period of time.\nBased on the above explanations, the two options that are not access control mechanisms for AWS API Gateway are:\nC. Server-side certificates - These are used to secure connections between the client and the API Gateway using SSL/TLS encryption. They are not used for access control.\nD. VPC RouteTables - These are used to control traffic between a VPC and the internet. They are not used for access control in API Gateway.\nTherefore, the correct answers are C and D.\n\n"
}, {
  "id" : 612,
  "question" : "You are a solutions architect working for a healthcare provider.\nYour company uses REST APIs to expose critical patient data to internal front-end systems used by doctors and nurses.\nThe data for your patient information is stored in Aurora. How can you ensure that your patient data REST endpoint is only accessed by your authorized internal users? (Select TWO)\n",
  "answers" : [ {
    "id" : "89cca536c4ea4e1a9b883bd1611c530d",
    "option" : "Run your Aurora DB cluster on an EC2 instance in a private subnet",
    "isCorrect" : "false"
  }, {
    "id" : "a918afb6ac76453ca54a872c8e311dc9",
    "option" : "Use a Gateway VPC Endpoint to make your REST endpoint private and only accessible from within your VPC",
    "isCorrect" : "false"
  }, {
    "id" : "fa6e73908d7b4a8cb4413811e9bc167e",
    "option" : "Use IAM resource policies to restrict access to your REST APIs by adding the aws:SourceVpce condition to the API Gateway resource policy",
    "isCorrect" : "true"
  }, {
    "id" : "4f512e851a404f579b8d57bbba0473fb",
    "option" : "Use an Interface VPC Endpoint to make your REST endpoint private and only accessible from within your VPC and through your VPC endpoint.",
    "isCorrect" : "true"
  }, {
    "id" : "cff0ced2b757436c8adba1920410adee",
    "option" : "Use IAM resource policies to restrict access to your REST APIs by adding the aws:SourceArn condition to the API Gateway resource policy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: C and D.\nOption A is incorrect.\nControlling access to your back-end database running on Aurora will not restrict access to your API Gateway REST endpoint.\nAccess to your API Gateway REST endpoint must be controlled at the API Gateway and VPC level.\nOption B is incorrect.\nThe Gateway VPC Endpoint is only used for the S3 and DynamoDB services.\nOption C is correct.\nYou can make your REST APIs private by using the aws:SourceVpce condition in your API Gateway resource policy to restrict access to only your VPC Endpoint.\nOption D is correct.\nUse a VPC Interface Endpoint to restrict access to your REST APIs to traffic that arrives via the VPC Endpoint.\nOption E is incorrect.\nThe aws:SourceArn condition key is not used to restrict access to traffic that arrives via the VPC Endpoint.\nReferences:\nPlease see the Amazon API Gateway developer guide titled Creating a private API in Amazon API Gateway (https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html), the Amazon API Gateway developer guide titled Example: Allow private API traffic based on source VPC or VPC endpoint (https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-examples.html#apigateway-resource-policies-source-vpc-example), the Amazon Aurora user guide titled Amazon Aurora security (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Security.html), the Amazon Aurora user guide titled Amazon Aurora DB clusters (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html), the Amazon Aurora user guide titled Aurora DB instance classes (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.DBInstanceClass.html), the Amazon API Gateway developer guide titled AWS condition keys that can be used in API Gateway resource policies (https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies-aws-condition-keys.html), and the Amazon Virtual Private Cloud AWS PrivateLink page titled VPC endpoints (https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html)\n\nAs a healthcare provider, it is essential to secure patient data and ensure that only authorized internal users have access to it. Here are two ways you can ensure that your patient data REST endpoint is only accessed by authorized internal users:\nRun your Aurora DB cluster on an EC2 instance in a private subnet: You can launch your Aurora DB cluster within a private subnet in your Amazon Virtual Private Cloud (VPC). A private subnet is not accessible from the internet, making it more secure. You can configure your Aurora security group to only allow connections from your internal front-end systems. This way, only authorized internal users can access the patient data REST endpoint. Use a Gateway VPC Endpoint or Interface VPC Endpoint to make your REST endpoint private: You can use a Gateway VPC Endpoint or Interface VPC Endpoint to make your REST endpoint private and only accessible from within your VPC. A Gateway VPC Endpoint is used for Amazon S3 and DynamoDB, while an Interface VPC Endpoint is used for other AWS services such as Amazon EC2 and Amazon ECR. With both types of VPC endpoints, you can create a policy that allows only authorized internal users to access the REST endpoint.\nC. Use IAM resource policies to restrict access to your REST APIs by adding the aws:SourceVpce condition to the API Gateway resource policy: You can use IAM resource policies to restrict access to your REST APIs by adding the aws:SourceVpce condition to the API Gateway resource policy. This will only allow traffic from your VPC endpoints to access your REST endpoint.\nE. Use IAM resource policies to restrict access to your REST APIs by adding the aws:SourceArn condition to the API Gateway resource policy: You can also use IAM resource policies to restrict access to your REST APIs by adding the aws:SourceArn condition to the API Gateway resource policy. This allows only the specified ARN to access your REST endpoint, making it more secure.\nIn summary, to ensure that your patient data REST endpoint is only accessed by authorized internal users, you can run your Aurora DB cluster in a private subnet, use VPC endpoints to make your REST endpoint private, or use IAM resource policies to restrict access to your REST APIs by adding the appropriate conditions.\n\n"
}, {
  "id" : 613,
  "question" : "In AWS API Gateway, which of the following security measures is provided default by AWS to protect the backend systems?\n",
  "answers" : [ {
    "id" : "39068367fe194af7b816d6c440ac88d8",
    "option" : "Default Cross-Origin Resource Sharing (CORS) configuration.",
    "isCorrect" : "false"
  }, {
    "id" : "cd63be22480e4afead54b89fe4dd96a3",
    "option" : "Default Resource Policy.",
    "isCorrect" : "false"
  }, {
    "id" : "4d972100143340a7a50bab1d6c767d2a",
    "option" : "Protection from distributed denial-of-service (DDoS) attacks.",
    "isCorrect" : "true"
  }, {
    "id" : "a5a77540b23d4e37b0684e53f843b778",
    "option" : "Security of backend systems falls under customer responsibility. AWS provides different mechanisms to protect backend systems which are not configured by default.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nAPI Gateway supports throttling settings for each method or route in your APIs.\nYou can set a standard rate limit and a burst rate limit per second for each method in your REST APIs and each route in WebSocket APIs.\nFurther, API Gateway automatically protects your backend systems from distributed denial-of-service (DDoS) attacks, whether attacked with counterfeit requests (Layer 7) or SYN floods (Layer 3).\nOptions A and B are part of the above list and do not have any default configurations.\nOption C is correct.\nOption D's statement is incorrect as it is a distractor.\nThe above screenshot shows AWS automatically protects from DDoS attacks.\nReference:\nhttps://aws.amazon.com/api-gateway/faqs/\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html\n\n\nIn AWS API Gateway, the security of backend systems is the responsibility of the customer. However, AWS provides different mechanisms to protect the backend systems, which are not configured by default. Therefore, option D is the correct answer.\nLet's take a look at each of the options:\nA. Default Cross-Origin Resource Sharing (CORS) configuration: CORS is a security mechanism used by web browsers to prevent web pages from making requests to a different domain. While API Gateway does provide support for CORS, it is not a default security measure to protect backend systems.\nB. Default Resource Policy: Resource policies are used to define access control for AWS resources. While they are important for securing access to API Gateway, they are not a default security measure to protect backend systems.\nC. Protection from distributed denial-of-service (DDoS) attacks: AWS provides protection against DDoS attacks through its AWS Shield service, which is enabled by default for all AWS customers. However, this protection is not specific to API Gateway and does not protect backend systems.\nD. Security of backend systems falls under customer responsibility. AWS provides different mechanisms to protect backend systems which are not configured by default: This is the correct answer. While AWS provides a number of security mechanisms to help protect backend systems, it is ultimately the responsibility of the customer to properly configure and secure their own systems.\nIn conclusion, while AWS provides many security measures to protect API Gateway, the security of backend systems is the customer's responsibility. AWS provides different mechanisms to protect backend systems, but these mechanisms are not configured by default.\n\n"
}, {
  "id" : 614,
  "question" : "With respect to API caching for API Gateway through the console, which of the following is not a cache setting?\n",
  "answers" : [ {
    "id" : "c0768b568e224c5094199211c07fb569",
    "option" : "Cache capacity",
    "isCorrect" : "false"
  }, {
    "id" : "e13abfea63fd4068847d5ab7c438bc67",
    "option" : "Encrypt cache data",
    "isCorrect" : "false"
  }, {
    "id" : "454fe4e8dd994abe845a93285a207651",
    "option" : "Import cache",
    "isCorrect" : "true"
  }, {
    "id" : "891d528f5d9947eab35c093fc01bf1a8",
    "option" : "Flush entire cache.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nFollowing are the settings when enabling/disabling API caching for API Gateway.\nOptions A, B, D are highlighted in the above screenshots.\nThere is no 'Import cache' on API Gateway.\nReference:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html\n\n\nAPI caching is a feature of Amazon API Gateway that enables you to cache the responses from backend APIs and reduce the number of requests made to those APIs. Caching can also help to reduce latency and improve the performance of your APIs.\nWhen configuring caching for an API Gateway through the console, you can adjust several settings. Let's review each of the options listed in the question:\nA. Cache capacity: This setting allows you to specify the maximum size of the cache. This is the amount of memory allocated to store the cached responses. When the cache reaches its capacity limit, the least frequently used items are evicted from the cache to make room for new entries.\nB. Encrypt cache data: This setting enables you to encrypt the cache data at rest using an AWS KMS key. This adds an additional layer of security to your cached data.\nC. Import cache: This setting allows you to import an existing cache configuration from another API Gateway deployment. This is useful if you want to reuse a cache configuration for a similar API.\nD. Flush entire cache: This setting allows you to clear the entire cache for an API. This can be useful if you need to invalidate all cached responses for an API.\nTherefore, the correct answer to the question is C. Import cache is not a cache setting. It is a cache management feature that enables you to import an existing cache configuration from another API Gateway deployment.\nIn summary, API caching can help improve the performance of your APIs by reducing the number of requests made to the backend APIs. When configuring caching for an API Gateway through the console, you can adjust several settings, including cache capacity, encrypt cache data, and flush entire cache.\n\n"
}, {
  "id" : 615,
  "question" : "You are a solutions architect working for a data analytics company that delivers analytics data to politicians that need the data to manage their campaigns.\nPolitical campaigns use your company's analytics data to decide on where to spend their campaign money to get the best results for the efforts.\nYour political campaign users access your analytics data through an Angular SPA via API Gateway REST endpoints.\nYou need to manage the access and use of your analytics platform to ensure that the individual campaign data is separate.\nSpecifically, you need to produce logs of all user requests and responses to those requests, including request payloads, response payloads, and error traces.\nWhich type of AWS logging service should you use to achieve your goals?\n",
  "answers" : [ {
    "id" : "b0591e4326824a8f9839a6392929cb5c",
    "option" : "Use CloudWatch access logging",
    "isCorrect" : "false"
  }, {
    "id" : "f30b3e11d0f04eb6bfc155c26ee4bcbb",
    "option" : "Use CloudWatch execution logging",
    "isCorrect" : "true"
  }, {
    "id" : "540739a935c9470493f67667123f5922",
    "option" : "Use CloudTrail logging",
    "isCorrect" : "false"
  }, {
    "id" : "a3f23f74a0fe4f4591b4fd59d6613603",
    "option" : "Use CloudTrail execution logging.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect.\nCloudWatch access logging captures which resource accessed an API and the method used to access the API.\nIt is not used for execution traces, such as capturing request and response payloads.\nOption B is correct.\nCloudWatch execution logging allows you to capture user request and response payloads as well as error traces.\nOption C is incorrect.\nCloudTrail captures actions by users, roles, and AWS services.\nCloudTrail records all AWS account activity.\nCloudTrail does not capture error traces.\nOption D is incorrect.\nCloudTrail does not have a feature called execution logging.\nReferences:\nPlease see the Amazon API Gateway developer guide titled Setting up CloudWatch logging for a REST API in API Gateway (https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-logging.html), and the AWS CloudTrail user guide titled How CloudTrail works (https://docs.aws.amazon.com/awscloudtrail/latest/userguide/how-cloudtrail-works.html)\n\nThe best logging service to use in this scenario is CloudTrail logging (Option C).\nCloudTrail is a service that provides a record of actions taken by a user, role, or an AWS service in an AWS account. CloudTrail captures information about the API calls made in an account, including the identity of the caller, the time of the call, the source IP address of the caller, the request parameters, and the response elements returned by the AWS service.\nCloudTrail logs can be used for security analysis, resource change tracking, compliance auditing, and operational troubleshooting. In this case, CloudTrail logs will be used to log all user requests and responses to those requests, including request payloads, response payloads, and error traces.\nOption A, CloudWatch access logging, can also log API calls but does not capture the request and response payloads. It only logs metadata, such as the time of the request, the source IP address, and the resource that was accessed.\nOption B, CloudWatch execution logging, is a feature of AWS Lambda and only logs the output of the function, not the input.\nOption D, CloudTrail execution logging, does not exist. Therefore, it is not a valid option.\nIn conclusion, CloudTrail logging (Option C) is the best choice for logging all user requests and responses, including request payloads, response payloads, and error traces, in this scenario.\n\n"
}, {
  "id" : 616,
  "question" : "Which of the following are features of AWS ECS? (Choose 3 options)\n",
  "answers" : [ {
    "id" : "7113e90bfec640089214b3163ade91a3",
    "option" : "Task Definition",
    "isCorrect" : "true"
  }, {
    "id" : "31c8448481c34be2ad0005a53425c351",
    "option" : "Tasks",
    "isCorrect" : "true"
  }, {
    "id" : "81d7365d0a294f1faa4983782ec3a79f",
    "option" : "Container Registery",
    "isCorrect" : "false"
  }, {
    "id" : "21ba4c5cc95347d7be14b70e7e84d429",
    "option" : "Cluster",
    "isCorrect" : "true"
  }, {
    "id" : "4b127552b8fb458694287622c7c82acc",
    "option" : "Source Image Storage.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B, D.\nHere is a high-level overview of ECS service.\nFollowing are the features for AWS ECS.\nContainers and Images.\nTask Definitions.\nTasks and Scheduling.\nClusters.\nContainer Agent.\nFor more information on ECS features, refer documentation here.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html#welcome-features\nOptions A, B and D are part of above feature list.\nOption C is not part of ECS.\nAmazon Elastic Container Registry (Amazon ECR) is a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.\nFor more information on AWS ECR, refer documentation here.\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html\nOption E is not correct.\nIt is part of AWS ECR.\n\n\nSure, I'd be happy to help!\nAWS ECS (Elastic Container Service) is a fully managed container orchestration service provided by AWS that allows you to run and manage Docker containers at scale. With ECS, you can deploy, manage, and scale containerized applications with ease.\nLet's take a look at the options you provided and the features they correspond to:\nA. Task Definition: A task definition is a blueprint that defines how a Docker container should be run, including the container image to use, how much CPU and memory to allocate to the container, and any data volumes to mount. Task definitions also specify network and port information and can define multiple containers that are run together as part of a task.\nB. Tasks: Tasks are the basic unit of work in ECS and represent one or more containers that are run together as part of a service. Tasks are defined by a task definition, and each task is assigned to a specific instance in an ECS cluster.\nC. Container Registry: AWS provides a fully-managed container registry called Amazon Elastic Container Registry (ECR) that you can use to store, manage, and deploy your Docker container images. ECR integrates seamlessly with ECS, making it easy to deploy your container images to your ECS tasks and services.\nD. Cluster: An ECS cluster is a logical grouping of EC2 instances or Fargate tasks that run your containers. Clusters provide a way to manage and scale your containers across multiple hosts or availability zones, and they also provide a way to isolate resources and applications from one another.\nE. Source Image Storage: This option is not a feature of AWS ECS.\nIn summary, the three features of AWS ECS are Task Definition, Tasks, and Container Registry. These features allow you to define how your containers should run, manage and scale your containerized applications, and store your Docker images in a secure and scalable way.\n\n"
}, {
  "id" : 617,
  "question" : "Which of the following statement defines task definition?\n",
  "answers" : [ {
    "id" : "969d4dcb22c14cb280eb8f1c21ee2011",
    "option" : "JSON template that describes containers which forms your application.",
    "isCorrect" : "true"
  }, {
    "id" : "1468dbe6e2c9495cb3aea3aac42fcde0",
    "option" : "Template for a program that runs inside AWS ECS Cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "0b384aca62074e0699f8703906de811f",
    "option" : "AWS managed service that launches ECS clusters.",
    "isCorrect" : "false"
  }, {
    "id" : "6147969cfaa74b8f9206dbf59ebbac7c",
    "option" : "Template that defines actions for each IAM user on the ECS cluster and its containers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nFor more information on how to create task definitions, refer documentation here.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html\n\n\nThe correct answer is A: \"JSON template that describes containers which forms your application.\"\nExplanation: In Amazon Web Services (AWS) Elastic Container Service (ECS), a task definition is a JSON template that describes one or more containers that together form an application. It includes information such as the Docker image to use, CPU and memory requirements, networking information, and any data volumes to be used by the container. A task definition also specifies the port to use for communication with the container, the protocols to use, and any environment variables needed.\nAWS ECS uses task definitions to launch containers within an ECS cluster. You can create multiple versions of a task definition to support different requirements or to update the application. When a task is launched, ECS creates a container instance of each container defined in the task definition.\nOption B is incorrect because a task definition is not a template for a program that runs inside AWS ECS Cluster. It is a template for the containers that form your application.\nOption C is incorrect because ECS is the AWS managed service that launches ECS clusters, not a task definition.\nOption D is incorrect because an IAM (Identity and Access Management) user is not associated with an ECS cluster or its containers. Task definitions define the configuration for containers in an ECS cluster.\n\n"
}, {
  "id" : 618,
  "question" : "Your organization is planning to use AWS ECS for docker applications.\nHowever, they would like to apply 3rd party monitoring tools on the ECS container instances to monitor the instances.\nThey approached you asking for a recommendation.\nWhat do you suggest?\n",
  "answers" : [ {
    "id" : "336b497849ef49858c1fd375f088f819",
    "option" : "AWS ECS is a managed service. Customers cannot install 3rd party software. Use CloudWatch for monitoring metrics.",
    "isCorrect" : "false"
  }, {
    "id" : "ef84047e26a548cf9e5e54ee5759c183",
    "option" : "Customers will have control over AWS ECS container instances and can set up monitoring like a normal EC2 instance.",
    "isCorrect" : "true"
  }, {
    "id" : "0b4031cb0f484e7f8d869fa8b07755eb",
    "option" : "Consult AWS Support team to install 3rd party software on ECS. The support team will review the case and install if 3rd party software is in their trusted software entries.",
    "isCorrect" : "false"
  }, {
    "id" : "00c6716db0564c8499c7b008fc65dceb",
    "option" : "AWS ECS is a managed service. Customers cannot install 3rd party software. Use application-level monitoring.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOptions A and D are incorrect because AWS ECS uses EC2 instances with ECS-optimized AMI.\nYou will have root access to the instances and you can manage them.\nOption C is incorrect as it is just a distractor, there is no need for a support team.\nFor more information on ECS instances, refer to the documentation here:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html\nhttps://aws.amazon.com/ecs/faqs/\n\n\nOption A is partially correct, but it is not entirely accurate. AWS ECS is a managed service, but customers can still install 3rd party monitoring tools on the container instances running on ECS. While CloudWatch can provide basic monitoring metrics for ECS services, it may not meet all of the organization's specific monitoring requirements. Therefore, using 3rd party monitoring tools is a better approach.\nOption B is the correct answer. Customers have control over the underlying EC2 instances on which ECS runs. Therefore, customers can use the same monitoring tools on the ECS container instances as they would on any other EC2 instance. They can install agents and monitoring software to gather metrics and logs and set up alarms to notify the team when an issue occurs.\nOption C is not a good option because it involves a lengthy process that may not result in the installation of the requested software. It is also not necessary because customers have control over the container instances and can install the software themselves.\nOption D is not the best answer because it only provides application-level monitoring, which may not cover all of the monitoring requirements of the organization. Additionally, it is always best to have multiple layers of monitoring, including infrastructure-level monitoring, to detect and diagnose problems quickly.\nIn summary, Option B is the best recommendation for the organization. Customers can install 3rd party monitoring tools on the underlying EC2 instances, allowing them to monitor the ECS container instances effectively.\n\n"
}, {
  "id" : 619,
  "question" : "Which of the following is a correct statement concerning ECS instances when accessing the Amazon ECS service endpoint? Choose 2 options.\n",
  "answers" : [ {
    "id" : "c90fc3fdd4f24985b5b89f07ed9f5472",
    "option" : "Create an Interface VPC Endpoint for ECS service and attach to VPC subnetâ€™s route table in which ECS instances are running.",
    "isCorrect" : "true"
  }, {
    "id" : "1e2a24baadab494894388f2871aee30c",
    "option" : "ECS intances are launched with ECS-optimized AMI which contains an inbuilt mechanism to communicate with ECS service endpoints through AWS network.",
    "isCorrect" : "false"
  }, {
    "id" : "6818f5c0e2d1405ea87348c592049c45",
    "option" : "Container instances have public IP addresses.",
    "isCorrect" : "true"
  }, {
    "id" : "3a05a7d8060a4def8ab2402c0cdf30de",
    "option" : "AWS service endpoints are accessible internally across VPCs. You need to enable IAM role access on the service which needs to be accessed.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A and C.\nThe container agent runs on each infrastructure resource within an Amazon ECS cluster.\nIt sends information about the resource's current running tasks and resource utilization to Amazon ECS, and starts and stops tasks whenever it receives a request from Amazon ECS.\nRefer page 328 on the below link:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-dg.pdf\nOption A is correct.ECS supports interface VPC endpoints.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html\nhttps://aws.amazon.com/blogs/aws/aws-privatelink-update-vpc-endpoints-for-your-own-applications-services/\nOption B is not correct.\nAny network communication in/out of VPC must follow the rules defined on route tables, Network ACLs and Security Groups.\nAny external communication (internet facing or AWS service endpoints) must either go through Internet Gateway, NAT Gateway or VPC Endpoints (if applicable).\nOption C is correct because this container instance can communicate with the Amazon ECS service endpoint with its public IP address.\nFor more information on traffic between VPC and outside networks, refer to documentation here.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/connect-vpc/\nOption D is not a valid statement.\nRefer to the above documentation.\n\n\nSure, I'll provide a detailed explanation for each option.\nOption A: Create an Interface VPC Endpoint for ECS service and attach to VPC subnet's route table in which ECS instances are running.\nThis option is correct. When you run Amazon ECS tasks or services in your VPC, you can create an interface VPC endpoint for the Amazon ECS service. An interface endpoint is an elastic network interface with a private IP address that serves as an entry point for traffic destined to a supported service. By creating an interface VPC endpoint, you can connect to Amazon ECS without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. To enable Amazon ECS to communicate with your interface endpoint, you need to attach the endpoint to the route table of the subnet where your Amazon ECS instances are running.\nOption B: ECS instances are launched with ECS-optimized AMI which contains an inbuilt mechanism to communicate with ECS service endpoints through AWS network.\nThis option is also correct. When you launch Amazon ECS instances, you can choose an Amazon Machine Image (AMI) that's optimized for Amazon ECS. This AMI includes the Amazon ECS container agent, which is a component of the Amazon ECS container agent and is required on every Amazon ECS container instance. The agent communicates with the Amazon ECS service to register the instance, retrieve task definitions, and send container logs. The agent also sends heartbeats to the service to indicate that the instance is active and available to run tasks.\nOption C: Container instances have public IP addresses.\nThis option is not correct. Amazon ECS container instances don't have public IP addresses by default. If you need to access a container instance from outside of the VPC, you can use a NAT gateway, a VPN connection, or an AWS Direct Connect connection to provide the instance with internet connectivity.\nOption D: AWS service endpoints are accessible internally across VPCs. You need to enable IAM role access on the service which needs to be accessed.\nThis option is not correct. AWS service endpoints are accessible within a VPC or VPC peering connection. To access an AWS service endpoint from a different VPC, you need to create a VPC peering connection between the VPCs. IAM roles control access to AWS resources, but they don't affect the ability to access service endpoints across VPCs.\nTo summarize, options A and B are correct statements concerning ECS instances when accessing the Amazon ECS service endpoint. Option C is incorrect, and option D is also incorrect as it doesn't specifically address the correct way to access the Amazon ECS service endpoint.\n\n"
}, {
  "id" : 620,
  "question" : "You have launched an ECS cluster with 5 EC2 instances with its task definitions.\nHowever, ECS is not getting any status information back from the container agent in each ECS instance.\nWhat could be the reason? (choose 3 options)\n",
  "answers" : [ {
    "id" : "b1456617bc304f379873e1e28c3d4e43",
    "option" : "IAM role used to run ECS instance does not have ecs:Poll action in its policy.",
    "isCorrect" : "true"
  }, {
    "id" : "ddbac60e14a84b8b883f9da1ea30bf06",
    "option" : "Key-pair information is missing in ECS cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "27cdd441428b43eab31acd8b205a8cb1",
    "option" : "ECSInstancesecuritygroupsâ€™outboundrulesarenotallowingtraffictoECSservice endpoint",
    "isCorrect" : "true"
  }, {
    "id" : "541262126ce24331851a175569291bf0",
    "option" : "Interface VPC endpoint is not configured for ECS service.",
    "isCorrect" : "true"
  }, {
    "id" : "94906a4f20954880b521d4c7181e26e5",
    "option" : "You are running ECS on t2.micro instance type which is not supported.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C, D.\nOption A is correct.\nThe Amazon ECS container agent makes calls to the Amazon ECS API on your behalf.\nContainer instances that run the agent require an IAM policy and role for the service to know that the agent belongs to you.\nBefore you can launch container instances and register them into a cluster, you must create an IAM role for those container instances to use when they are launched.\nThis requirement applies to container instances launched with theAmazonECS-optimizedAMI provided by Amazon or with any other instances that you intend to run the agent on.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html\nOption B is not correct.\nAmazon ECS container instance has no password to use for SSH access; you use a key pair to securely log in to your instance.\nYou specify the name of the key pair when you launch your container instance, then provide the private key when you log in using SSH.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/get-set-up-for-amazon-ecs.html?shortFooter=true#create-a-key-pair\nOption C is correct.\nSecurity groups act as a firewall to ECS container instances.\nIf outbound rules are not allowing any traffic to ECS service endpoints, the container agent will not be able to report the status back to ECS.\nFor more information on Security Groups, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/get-set-up-for-amazon-ecs.html?shortFooter=true#create-a-base-security-group\nOption D is correct.ECS supports interface VPC endpoints.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html\nhttps://aws.amazon.com/blogs/aws/aws-privatelink-update-vpc-endpoints-for-your-own-applications-services/\nOption E is not correct.\nT2.micro is supported for container instance.\n\n\nThe reason why ECS is not getting any status information back from the container agent in each ECS instance can be due to multiple factors.\nA. IAM role used to run ECS instance does not have ecs:Poll action in its policy:\nThe ECS agent installed on each EC2 instance requires permission to poll the ECS service for task updates. If the IAM role used to launch the EC2 instances does not have the \"ecs:Poll\" action in its policy, the ECS agent will not be able to poll the ECS service for task updates. Therefore, this could be the reason why ECS is not getting any status information back from the container agent in each ECS instance.\nB. Key-pair information is missing in ECS cluster:\nKey-pair information is used to securely connect to EC2 instances from the ECS console. If key-pair information is missing in the ECS cluster, it can prevent the ECS console from communicating with the EC2 instances, including the ECS agent. However, this is unlikely to be the root cause of the problem as it would prevent the ECS console from connecting to the instances, but it would not affect the agent's ability to communicate with the ECS service.\nC. ECSInstance security groups' outbound rules are not allowing traffic to ECS service endpoint:\nIf the outbound rules of the ECSInstance security group do not allow traffic to the ECS service endpoint, the ECS agent running on each EC2 instance will not be able to communicate with the ECS service. This could be the reason why ECS is not getting any status information back from the container agent in each ECS instance.\nD. Interface VPC endpoint is not configured for ECS service:\nThe Interface VPC endpoint is used to provide secure and private communication between Amazon ECS and other resources in a VPC. If the Interface VPC endpoint is not configured for the ECS service, the ECS agent running on each EC2 instance will not be able to communicate with the ECS service. This could be the reason why ECS is not getting any status information back from the container agent in each ECS instance.\nE. You are running ECS on t2.micro instance type which is not supported:\nThe t2.micro instance type is supported by ECS, so this is not a valid reason for the problem.\nIn summary, the three possible reasons why ECS is not getting any status information back from the container agent in each ECS instance are:\nIAM role used to run ECS instance does not have ecs:Poll action in its policy ECSInstance security groups' outbound rules are not allowing traffic to ECS service endpoint Interface VPC endpoint is not configured for ECS service\n\n"
}, {
  "id" : 621,
  "question" : "Which of the following can be used with Amazon ECS to run containers without managing servers or clusters of Amazon EC2 instances?\n",
  "answers" : [ {
    "id" : "df0ff64e3323416e842ffe1f6d15d672",
    "option" : "AWSVPC",
    "isCorrect" : "false"
  }, {
    "id" : "4be89f4108db445da13f40e31e05e9aa",
    "option" : "FARGATE",
    "isCorrect" : "true"
  }, {
    "id" : "03035c40fb164669bab81d311462591a",
    "option" : "AWS ECR",
    "isCorrect" : "false"
  }, {
    "id" : "daaec699785e4390b0e0ec12e7265008",
    "option" : "Docker.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nFor detailed information on AWS ECS Launch types, refer to documentation here.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_types.html\nFor Option D, Docker is a container type, not a launch type.\nAmazon ECS uses Docker images in task definitions to launch containers on EC2 instances in your clusters.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html.\n\n\nThe correct answer is B. FARGATE.\nAmazon Elastic Container Service (ECS) is a fully-managed container orchestration service that supports Docker containers. ECS can be used to run containers on clusters of Amazon Elastic Compute Cloud (EC2) instances, or on AWS Fargate.\nAWS Fargate is a serverless compute engine for containers that allows you to run containers without the need to manage servers or clusters of EC2 instances. With Fargate, you can specify the amount of CPU and memory required for your containers, and AWS will take care of provisioning the necessary resources to run them.\nAWS VPC (Virtual Private Cloud) is a service that lets you launch Amazon Web Services resources into a virtual network that you define. It provides you with complete control over your virtual networking environment, including IP address ranges, subnets, and routing tables.\nAWS ECR (Elastic Container Registry) is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. It integrates with Amazon ECS and other container services, making it easy to deploy containers on AWS.\nDocker is a platform that allows developers to build, package, and deploy applications as Docker containers. Docker containers can run on any system that supports Docker, including Amazon ECS and Fargate.\nIn summary, while AWS VPC and AWS ECR are related to container management in AWS, they are not directly used to run containers without managing servers or clusters of Amazon EC2 instances. The correct answer is B. FARGATE, as it is a serverless compute engine for containers that allows you to run containers without the need to manage servers or clusters of EC2 instances.\n\n"
}, {
  "id" : 622,
  "question" : "Which of the following are the parameters specified in task definition? (choose 3 options)\n",
  "answers" : [ {
    "id" : "39e2dd35ad2a4e7cad160235cd87183d",
    "option" : "The Docker images to use with the containers in your task.",
    "isCorrect" : "true"
  }, {
    "id" : "59dadf6f77154ab1a50ee2e905481eff",
    "option" : "EC2 instance types to be used as container instances.",
    "isCorrect" : "false"
  }, {
    "id" : "7057d02e709f41f7a0206953ecc1a9e0",
    "option" : "How much CPU and memory to use with each container.",
    "isCorrect" : "true"
  }, {
    "id" : "9049d71408164afcafc46fb9766f9345",
    "option" : "AWS VPC and subnets to launch containers in.",
    "isCorrect" : "false"
  }, {
    "id" : "43f885c0782f432d9d8ff9a51abcfa3c",
    "option" : "The command the container should run when it is started.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A, C, E.\nFollowing are the parameters used in task definition.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html.\nOption B and D are paramters specified in creating an ECS cluster.\n\n\nSure, I'd be happy to help!\nIn AWS, a task definition is a blueprint that defines how to run a Docker container in a task. Each task definition consists of various parameters that define how the container should be launched and run.\nTo answer the question, the parameters specified in a task definition are as follows:\nA. The Docker images to use with the containers in your task: This parameter specifies the Docker image that will be used to launch the container. It could be an image from a public or private Docker repository, or a custom image that you have created.\nC. How much CPU and memory to use with each container: This parameter specifies the CPU and memory resources that each container in the task should be allocated. You can set limits on the maximum amount of CPU and memory that a container can use.\nE. The command the container should run when it is started: This parameter specifies the command that should be executed when the container is started. This can be a simple command or a more complex script that runs multiple commands.\nB, D are incorrect: B. EC2 instance types to be used as container instances: This parameter is not specified in a task definition. Instead, you would specify the instance types when you create a cluster of EC2 instances to use as container instances.\nD. AWS VPC and subnets to launch containers in: This parameter is not specified in a task definition either. Instead, you would specify the VPC and subnets when you create a task or a service.\nI hope this helps!\n\n"
}, {
  "id" : 623,
  "question" : "Which of the following are the parameters specified in Amazon ECS Service Definition? (Select Three)\n",
  "answers" : [ {
    "id" : "21f8ba13f9d24c52829728764d51c7ca",
    "option" : "Cluster",
    "isCorrect" : "true"
  }, {
    "id" : "0be4504e4d1448f39fdd751d6e7250fa",
    "option" : "Capacity provider strategy",
    "isCorrect" : "true"
  }, {
    "id" : "c67e4aa04ebe4bde9520d451deef74e7",
    "option" : "Environment Variables",
    "isCorrect" : "false"
  }, {
    "id" : "29818b49162045b0b18b46e5dc53db7e",
    "option" : "Data Volumes",
    "isCorrect" : "false"
  }, {
    "id" : "d0d77c874af74cb1826e3cb0b5e33456",
    "option" : "Client token.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A, B, E.\nA service definition defines which task definition to use with your service, how many instantiations of that task to run, and which load balancers (if any) associate with your tasks.\nFollowing are some parameters defined in Service Definition.\n\nReferences:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service_definition_parameters.html\n\n\nAmazon Elastic Container Service (ECS) is a highly scalable, high-performance container orchestration service that supports Docker containers and allows you to run and manage Docker containers in the AWS Cloud. When creating a service in Amazon ECS, you can specify various parameters in the service definition.\nOut of the given options, the following are the parameters specified in Amazon ECS Service Definition:\nA. Cluster: A cluster is a logical grouping of resources, such as EC2 instances, that you can run container instances on. In Amazon ECS, you can specify a cluster in which to run a service.\nB. Capacity provider strategy: Capacity provider strategy specifies how the tasks for the service are placed and where they are placed. You can choose from different strategies like the \"spread\" strategy, which places tasks across different Availability Zones, or the \"binpack\" strategy, which places tasks on the least utilized instance in a cluster.\nC. Environment Variables: Environment variables are key-value pairs that can be set for a container. They provide a way to pass configuration information to the container and to customize its behavior.\nD. Data Volumes: Data volumes provide a way for containers to persist data even after the container has been terminated. You can specify data volumes in the task definition, which is used by the service.\nE. Client token: Client token is an optional parameter that you can use to ensure idempotency of your service deployment. When you create or update a service, you can provide a unique client token, which ensures that the operation is performed only once, even if it is retried multiple times.\nTherefore, the correct answers are A (Cluster), B (Capacity provider strategy), and C (Environment Variables).\n\n"
}, {
  "id" : 624,
  "question" : "You are launching the AWS ECS instance.\nYou would like to set the ECS container agent configuration during the ECS instance launch.\nWhat should you do?\n",
  "answers" : [ {
    "id" : "c8c83ff9bd0c437a9b2a1f6fd696f0c1",
    "option" : "Set configuration in the ECS metadata parameter during cluster creation.",
    "isCorrect" : "false"
  }, {
    "id" : "64fa501251674c05ab10f1f3dece6eac",
    "option" : "Set configuration in the user data parameter of EC2 instance.",
    "isCorrect" : "true"
  }, {
    "id" : "67dc767870114a56864b9fc535e9cc5e",
    "option" : "Define configuration in the task definition.",
    "isCorrect" : "false"
  }, {
    "id" : "008e0387d59344b7ba21ae34542c6fcc",
    "option" : "Define configuration in the service definition.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWhen you launch an Amazon ECS container instance, you have the option of passing user data to the instance.\nThe data can be used to perform common automated configuration tasks and even run scripts when the instance boots.\nFor Amazon ECS, the most common use cases for user data are to pass configuration information to the Docker daemon and the Amazon ECS container agent.\nReference:\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\n\n\nWhen launching an AWS ECS instance, there are different ways to set the ECS container agent configuration. Here are the possible ways:\nA. Set configuration in the ECS metadata parameter during cluster creation. This option is incorrect because the ECS metadata parameter is used to provide metadata to the container instance, such as resource limits, IAM roles, and user data. It is not used to set the ECS container agent configuration.\nB. Set configuration in the user data parameter of EC2 instance. This option is correct. The user data parameter is used to pass data to the EC2 instance when it is launched. It can be used to install software, run scripts, and set configuration. To set the ECS container agent configuration, you can include the configuration in the user data script. When the instance starts, the user data script will be executed and the ECS container agent configuration will be set.\nC. Define configuration in the task definition. This option is incorrect because the task definition is used to define the containers that run in the task, including their image, ports, environment variables, and resource requirements. It is not used to set the ECS container agent configuration.\nD. Define configuration in the service definition. This option is also incorrect because the service definition is used to define how tasks are deployed and managed, including the number of tasks to run, the load balancer to use, and the deployment strategy. It is not used to set the ECS container agent configuration.\nIn summary, the correct way to set the ECS container agent configuration during the ECS instance launch is to include the configuration in the user data parameter of the EC2 instance.\n\n"
}, {
  "id" : 625,
  "question" : "You are working for an organization which is actively using AWS.\nThey have noticed that few AWS ECS clusters are running and they do not know who and when the clusters are created.\nThey tasked you to find out the logs regarding this.\nWhat will you do?\n",
  "answers" : [ {
    "id" : "9eb3521bfdaf4f5f98fc9d1c39e73dea",
    "option" : "Check CloudWatch event logs.",
    "isCorrect" : "false"
  }, {
    "id" : "ac5ca2b0ce55453f8aa309427d787b20",
    "option" : "Check CloudTrail logs.",
    "isCorrect" : "true"
  }, {
    "id" : "cd3251f3c62849feab6ac16f1700939d",
    "option" : "Check CloudWatch metrics dashboard.",
    "isCorrect" : "false"
  }, {
    "id" : "703d18686cf2446384aad4c0531a72d8",
    "option" : "Check Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAmazon ECS is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon ECS.\nCloudTrail captures all API calls for Amazon ECS as events, including calls from the Amazon ECS console and from code calls to the Amazon ECS APIs.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/logging-using-cloudtrail.html#understanding-service-name-entries\nOptions A and C are for monitoring the ECS resources, not for the API actions made on ECS.\nYou can monitor your Amazon ECS resources using Amazon CloudWatch, which collects and processes raw data from Amazon ECS into readable, near real-time metrics.\n\n\nIn this scenario, the organization wants to know who and when the ECS clusters were created. To achieve this, the best option is to check the CloudTrail logs.\nCloudTrail is a service that records AWS API calls and stores the resulting logs in an S3 bucket or CloudWatch Logs. It provides a history of AWS API calls for an account, including the source IP address of the API call, the user identity, the time of the API call, and the parameters passed to the API.\nUsing the CloudTrail logs, we can determine who created the ECS clusters and when they were created. The CloudTrail logs capture all the API calls that were made to the ECS service, including those for creating, modifying, and deleting clusters.\nThe other options provided in the answers are not relevant for this scenario. CloudWatch event logs and metrics dashboard are used for monitoring and analyzing the performance of AWS services. Trusted Advisor is used to provide recommendations for optimizing AWS resources and reducing costs.\nIn conclusion, the correct answer for this scenario is B, Check CloudTrail logs.\n\n"
}, {
  "id" : 626,
  "question" : "You are uploading large files to AWS S3 bucket, ranging from 1GB - 3GB.\nYour organization has a requirement to calculate the hash checksum of the file by reading entire file so the users can validate the checksum to identify any potential corruptions during downloads.\nFor this, you created a Lambda function and getting it triggered through S3 notifications.\nHowever, the request is getting timed out.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "a3c2af49ed8b4e01bfcf5a81720f10a7",
    "option" : "Lambda function is configured with minimal memory of 128 M.",
    "isCorrect" : "false"
  }, {
    "id" : "8edac811917447189c4a424e25c24019",
    "option" : "Lambda function is set to run in a private VPC without NAT Gateway or VPC Endpoint.",
    "isCorrect" : "true"
  }, {
    "id" : "8abe5aecd58e414495378a06a1f89ecc",
    "option" : "You have not setup S3 bucket name in the environment variable.",
    "isCorrect" : "false"
  }, {
    "id" : "f2ffa4820a4f4bce9981d9d8e77eb4a7",
    "option" : "Lambda function is created in a different region than S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is not correct.\nIf the function reaches the maximum configured memory, in this case.\n128 MB, the function gets terminated with an error message as below, not as request timed out.\nREPORT RequestId: xxxxxxxx Duration: xxxxx ms Billed Duration: xxxxx ms.\nMemory Size: 128 MB Max Memory Used: 129 MB RequestId: xxxxxxx Process exited before completing request.\nOption B is correct.\nAWS Lambda functions can run within a private VPC with the resources allocated inside the subnet provided during configuration.\nFor the lambda function to access S3 service endpoint from within private VPC, there should be a NAT Gateway or S3 VPC Endpoint configured in the route table associated with the subnet which was chosen during Lambda function setup.\nIf not, the request would get timed out.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/\nOption C is not correct.\nBucket need not be configured as environment variable.\nLambda function environment variables are used to configure additional parameters that can be passed to lambda function.\nhttps://docs.aws.amazon.com/lambda/latest/dg/env_variables.html\nOption D is not correct.\nAs long as Lambda function has internet access, it can access S3 service endpoints irrespective of S3 bucket region.\n\n\nThe most likely reason for the Lambda function timing out when triggered through S3 notifications could be due to the default timeout limit of the Lambda function being exceeded. The default timeout limit for a Lambda function is 3 seconds.\nSince the Lambda function is required to read an entire large file and calculate the hash checksum, it might take longer than the default timeout limit. Therefore, it is necessary to increase the timeout limit of the Lambda function to ensure that it has sufficient time to complete the task.\nOption A states that the Lambda function is configured with minimal memory of 128 M. This might be a concern if the Lambda function requires more memory to read and process large files. However, it is not the main reason for the request timing out.\nOption B states that the Lambda function is set to run in a private VPC without NAT Gateway or VPC Endpoint. This might also be a concern if the Lambda function needs to communicate with resources outside the VPC, such as S3 bucket or internet. However, it is not the main reason for the request timing out.\nOption C states that the S3 bucket name has not been set up in the environment variable. This might cause issues with the Lambda function accessing the S3 bucket. However, it is not directly related to the request timing out.\nOption D states that the Lambda function is created in a different region than the S3 bucket. This might cause latency issues when accessing the S3 bucket, but it is not the main reason for the request timing out.\nTo resolve the issue of the Lambda function timing out, the timeout limit of the Lambda function should be increased to a value that is sufficient for the function to read and process the large file. This can be done by adjusting the timeout configuration in the AWS Lambda console or by updating the function's code with a higher timeout value.\nIn summary, the most likely reason for the Lambda function timing out when triggered through S3 notifications is due to the default timeout limit of the Lambda function being exceeded. Increasing the timeout limit of the Lambda function should resolve the issue.\n\n"
}, {
  "id" : 627,
  "question" : "Which of the following services does not asynchronously invoke the AWS Lambda function? (choose 2 options)\n",
  "answers" : [ {
    "id" : "d22fe24003c042ed9dbff583425fc0f3",
    "option" : "AWS S3",
    "isCorrect" : "false"
  }, {
    "id" : "10c2e45aba4241ab99295c908441d727",
    "option" : "AWS SNS",
    "isCorrect" : "false"
  }, {
    "id" : "a0e5f3eb120049998a426c2fd0cd7a5d",
    "option" : "AWS Cognito",
    "isCorrect" : "true"
  }, {
    "id" : "121ace2084c349f4b318c64f5649bfd0",
    "option" : "AWS CodeCommit",
    "isCorrect" : "false"
  }, {
    "id" : "14ea0fe3f6864c9f9eed8cd8fb655745",
    "option" : "AWS CloudFront.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: C, E.\nThe following are the functions that invoke synchronously and asynchronously the AWS Lambda function.\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-dg.pdf\n\n\nAmong the options listed, the two services that do not asynchronously invoke AWS Lambda functions are:\nC. AWS Cognito and D. AWS CodeCommit.\nAWS Lambda can be triggered in two ways - Synchronous and Asynchronous.\nSynchronous invocation of Lambda function happens when the calling service waits for the Lambda function to return a response or the calling service receives a failure response immediately.\nAsynchronous invocation of Lambda function happens when the calling service sends an event to the Lambda function and does not wait for the response. Instead, Lambda function processes the event asynchronously and writes the response to an Amazon S3 bucket or an Amazon Kinesis stream or an Amazon DynamoDB table.\nLet's look at the options:\nA. AWS S3 can trigger a Lambda function when a new object is created in the S3 bucket. This can happen either synchronously or asynchronously, depending on how the Lambda function is configured to be triggered.\nB. AWS SNS can trigger a Lambda function asynchronously when a new message is published to an SNS topic.\nC. AWS Cognito is an identity management service that can authenticate and authorize users. However, it does not trigger a Lambda function.\nD. AWS CodeCommit is a version control service that stores code in repositories. It does not trigger a Lambda function.\nE. AWS CloudFront is a content delivery network that can trigger a Lambda function asynchronously when a viewer request is received for a resource that is associated with the Lambda function.\nSo, the correct options are C. AWS Cognito and D. AWS CodeCommit.\n\n"
}, {
  "id" : 628,
  "question" : "Your organization must perform big data analysis to transform data and store the result in the AWS S3 bucket.\nThey have implemented the solution using AWS Lambda due to its zero-administrative maintenance and cost-effective nature.\nHowever, in very few cases, the execution is getting abruptly terminated after 15 minutes.\nThey would like to get a notification in such scenarios.\nWhat would you do?\n",
  "answers" : [ {
    "id" : "d44f1a8dbe5248949a23721281c38757",
    "option" : "Setup timer in the code and send a notification when the timer reaches 900 seconds.",
    "isCorrect" : "false"
  }, {
    "id" : "42400e3561cf4f50a786c037c0b094c1",
    "option" : "Configure SES for failures under the Configuration option in the lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "85259bdbf0bc4f068ea16a2cd3571047",
    "option" : "Setup the ERROR_NOTIFY environment variable with an email address. Lambda function has an inbuilt feature to send an email during max memory and time out terminations using this environment variable.",
    "isCorrect" : "false"
  }, {
    "id" : "e1485e27395743b6a7ab2c819c4e5539",
    "option" : "Configure Dead-letter Queue and send a notification to SNS topic.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is not correct.\nAlthough you can set the timers in the code, it may not be an accurate measure to determine if the lambda function is terminated after 900 seconds or just finished executing on the 900th second.\nOption B is not correct.\nThere is no option to configure AWS SES within the Lambda setup.\nOption C is not a valid statement.\nOption D is correct.\nYou can forward non-processed payloads to Dead Letter Queue (DLQ) using AWS SQS, AWS SNS.\nRefer:\nhttps://aws.amazon.com/about-aws/whats-new/2018/10/aws-lambda-supports-functions-that-can-run-up-to-15-minutes/\nhttps://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/\n\n\nThe correct answer for this scenario is option C, which involves setting up an environment variable to notify via email when the AWS Lambda function terminates due to a maximum timeout.\nAWS Lambda is a serverless computing service that runs code in response to events and automatically manages the compute resources required for the function's execution. One of the key benefits of using Lambda is its cost-effectiveness and zero-administrative maintenance, which allows developers to focus on writing code and not worry about server management.\nHowever, AWS Lambda has a maximum timeout limit of 15 minutes for each function execution. If the function runs for more than 15 minutes, it will abruptly terminate, and any running processes will be lost. In such cases, it is important to get notified of the abrupt termination to diagnose and troubleshoot the issue.\nOption A suggests setting up a timer in the code to send a notification when the timer reaches 900 seconds (15 minutes). While this may work in some scenarios, it is not a reliable solution, as the Lambda function may terminate before the timer goes off, and the notification may not be sent.\nOption B suggests configuring SES (Simple Email Service) for failures under the Configuration option in the lambda function. While SES can be used to send emails, it is not a reliable solution, as it is not specifically designed to notify Lambda function terminations.\nOption D suggests configuring a Dead-letter Queue (DLQ) and sending a notification to SNS (Simple Notification Service) topic. While this is a good practice for handling errors and failed executions, it does not directly address the issue of abrupt termination due to the maximum timeout limit.\nOption C, on the other hand, suggests setting up the ERROR_NOTIFY environment variable with an email address. The Lambda function has an inbuilt feature to send an email during maximum memory and timeout terminations using this environment variable. This solution is specifically designed to handle the issue of abrupt termination due to maximum timeout limit and is the most appropriate and reliable solution for this scenario.\n\n"
}, {
  "id" : 629,
  "question" : "Your organization uploads relatively large compressed files ranging between 100MB - 200MB in size to AWS S3 bucket.\nOnce uploaded, they are looking to calculate the total number objects in the compressed file and add the total count as a metadata to the compressed file in AWS S3\nThey approached you for a cost-effective solution.\nYou have recommended using AWS Lambda through S3 event notifications to perform this operation.\nHowever, they were concerned about failures as S3 event notification is an asynchronous one-time trigger and Lambda can fail due to operation time outs, max memory limits, max execution time limits etc.\nWhat is the best retry approach you recommend?\n",
  "answers" : [ {
    "id" : "3feef89c1f0b453a8550db1f24039b7c",
    "option" : "All the failed events will be logged to CloudWatch. You can manually retrigger failed events.",
    "isCorrect" : "false"
  }, {
    "id" : "c7b533510d86455992b81d755c676ee0",
    "option" : "Configure Dead-letter queue with SQS. Configure SQS to trigger Lambda function again.",
    "isCorrect" : "true"
  }, {
    "id" : "d37ff51f56014456b2c42a95fedfb116",
    "option" : "All failures will be caught during exception inside Lambda function. Trigger lambda function inside lambda function code to process failed event.",
    "isCorrect" : "false"
  }, {
    "id" : "19e057f58bc84e3d9754802ba91e8d08",
    "option" : "Enable Active tracing using AWS X-Ray. It will automatically retrigger failed events.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is not recommended approach.\nAlthough you can configure logging to CloudWatch, it is difficult to find the specific failure logs.\nManual retries are not a best practice in an enterprise level solution designs.\nOption B is correct.\nYou can forward non-processed or failed payloads to Dead Letter Queue.\n(DLQ) using AWS SQS, AWS SNS.\nhttps://aws.amazon.com/blogs/compute/robust-serverless-application-design-with-aws-lambda-dlq/\nOption C is not correct.\nMax memory limit and max execution time limit gets terminated without being caught in the handler exception.\nOption D is not correct.\nActive tracing option can be used for detailed logging.\nIt will not retry failed events.\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html\n\n\nWhen uploading relatively large compressed files to AWS S3 bucket, and then looking to calculate the total number of objects in the compressed file and add the total count as metadata to the compressed file, it is recommended to use AWS Lambda through S3 event notifications. However, there are concerns about failures as S3 event notification is an asynchronous one-time trigger, and Lambda can fail due to various reasons such as operation timeouts, max memory limits, and max execution time limits.\nTo address this concern, the best retry approach is to configure a Dead-Letter Queue (DLQ) with Simple Queue Service (SQS) and configure SQS to trigger the Lambda function again. This approach allows for automatic retries of failed events, and if the maximum number of retries is exceeded, the event will be sent to the DLQ for further analysis. The DLQ can be monitored for errors, allowing for efficient troubleshooting and problem resolution.\nOption A, where failed events are logged to CloudWatch, and then manually retriggered, is not a recommended approach as it requires manual intervention and is not an automatic retry mechanism.\nOption C, where failed events are caught during exceptions inside the Lambda function, and the Lambda function is triggered inside the Lambda function code to process failed events, can result in an infinite loop of retries and is not a recommended approach.\nOption D, where Active tracing using AWS X-Ray is enabled to automatically retrigger failed events, is not a correct option as AWS X-Ray is a debugging and analysis tool that provides information on the performance of applications and services, and does not provide automatic retry functionality.\nIn summary, the best retry approach for handling Lambda function failures when processing S3 events is to configure a DLQ with SQS and configure SQS to trigger the Lambda function again.\n\n"
}, {
  "id" : 630,
  "question" : "You must create a REST API using AWS API Gateway with Lambda as backend system and Oracle RDS instance as database.\nYou have created API methods, Lambda function code and spun up Oracle RDS instance in a private VPC with no Internet Gateway.\nWhen you are trying to connect to the RDS instance from your Lambda, the connection getting failed.\nWhat could be the reason? (choose 2 options)\n",
  "answers" : [ {
    "id" : "b0389d8a6e3c4afabb4fa9e878a80cea",
    "option" : "Lambda execution role does not have policy to access RDS instance.",
    "isCorrect" : "false"
  }, {
    "id" : "2bb7e102219a4087b4770b2b7cd7d199",
    "option" : "Lambda function is running in â€œno VPCâ€ network mode.",
    "isCorrect" : "true"
  }, {
    "id" : "6b2d57f1657d41098514adfb1c8bc273",
    "option" : "RDS instance security group is not allowing connections from Lambda subnet range.",
    "isCorrect" : "true"
  }, {
    "id" : "92e684e0eb774a8794035c120ff61c51",
    "option" : "RDS instance is not configured as destination in Lambda setup.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B, C.\nOption A is not correct.\nA policy on the role can only define access to which API actions can be made on RDS instance such as rds:CreateDBInstance, rds:CreateDBSecurityGroup, rds:CreateDBSnapshot etc.\nThe policy will not define whether a resource can connect to an RDS instance or not.\nOption B is correct.\nWhen the Lambda function runs in â€œno VPCâ€ network mode, it will not have access to resources running in a private VPC.\nhttps://docs.aws.amazon.com/lambda/latest/dg/vpc.html\nOption C is correct.\nSecurity groups act as a firewall for any resources (such as RDS instance and Lambda in this case) they are connected with.\nIf there is no inbound rule defined to allow connections from the Lambda subnet IP range or the Lambda security group, connections will fail.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html\nOption D is not correct.\nThere is no such configuration for the destination in the Lambda setup.\n\n\nThe reason for the connection failure between the Lambda function and the RDS instance could be two of the following options:\nA. Lambda execution role does not have policy to access RDS instance. When a Lambda function needs to access any AWS service, including RDS, the Lambda execution role must have the necessary permissions to access those services. In this case, the Lambda execution role should have the AmazonRDSFullAccess policy or a custom policy with the necessary permissions to access the RDS instance. If the role doesn't have the correct policy attached, it will not be able to connect to the RDS instance.\nC. RDS instance security group is not allowing connections from Lambda subnet range. Another possible reason for the connection failure could be that the security group associated with the RDS instance is not configured to allow connections from the Lambda function's subnet range. By default, RDS instances are created with a security group that only allows connections from within the same VPC. Therefore, if the Lambda function is running in a different subnet from the RDS instance, the security group must be updated to allow incoming connections from the subnet where the Lambda function is running.\nB. Lambda function is running in â€œno VPCâ€ network mode. This option is not directly related to the connection failure between the Lambda function and the RDS instance. However, it's worth mentioning that if the Lambda function is running in \"no VPC\" network mode, it will not be able to access resources in a VPC, including RDS instances that are located within a VPC.\nD. RDS instance is not configured as destination in Lambda setup. This option is also not directly related to the connection failure between the Lambda function and the RDS instance. However, it's worth mentioning that the RDS instance must be configured as the destination for the Lambda function to properly connect and send/receive data to/from the RDS instance. This can be done by configuring the appropriate database endpoint, username, and password in the Lambda function's code or environment variables.\nTo summarize, the two possible reasons for the connection failure between the Lambda function and the RDS instance are a lack of permissions in the Lambda execution role and a misconfigured security group for the RDS instance.\n\n"
}, {
  "id" : 631,
  "question" : "Which of the following is the customer's responsibility concerning the AWS Lambda service? (Select TWO)\n",
  "answers" : [ {
    "id" : "7397e17f43db4c9ea26756cc7babb846",
    "option" : "Lambda function code.",
    "isCorrect" : "true"
  }, {
    "id" : "211b2f61893547b0845c06e9958ad60e",
    "option" : "Monitoring and logging lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "e5ecc6616f9f4b2d85feecb8b85d70cf",
    "option" : "Security patches.",
    "isCorrect" : "false"
  }, {
    "id" : "fc8b18b9949b4c5eb3961a9abfa6954c",
    "option" : "Installing required libraries in underlying compute instances for Lambda execution.",
    "isCorrect" : "false"
  }, {
    "id" : "7709af3deb3248cf941ecfe76d9f61d9",
    "option" : "Providing AWS Lambda with the required role and permissions to access the AWS resources.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A, E.\nOption A is correct.\nOptions B, C is incorrect.\nOption D is incorrect.\nYou cannot log in to underlying compute instances of lambda execution.\nSo, we cannot install any required libraries.\nHowever, you can package all the required dependent libraries along with your code.\nRefer to the below documentation for more information on creating a deployment package for Lambda functions.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-s3-example-deployment-pkg.html\nOption E is correct.\nAWS Lambda assumes the role assigned during setup to access any AWS resources it performs any action on.\nPolicy on the role must grant access to any such resources for Lambda to perform operations, for example, S3 gets object, Dynamodb GetItem etc.\nhttps://docs.aws.amazon.com/lambda/latest/dg/intro-permission-model.html#lambda-intro-\n\n\nAs an AWS customer, there are several responsibilities that you need to take care of when using the AWS Lambda service. Two of the most important responsibilities are:\nA. Lambda function code: As an AWS customer, you are responsible for developing, testing, and deploying the code that will be executed by the Lambda service. This means that you need to create the code that defines the behavior of your Lambda functions, and upload it to AWS Lambda. AWS Lambda supports several programming languages, including Node.js, Python, Java, C#, and Go, among others. You can write your code directly in the AWS Lambda console, or you can use an integrated development environment (IDE) such as Visual Studio or Eclipse to develop your code.\nB. Monitoring and logging lambda functions: You are also responsible for monitoring and logging the behavior of your Lambda functions. AWS Lambda provides several built-in monitoring tools, such as CloudWatch Logs and CloudWatch Metrics, which you can use to track the execution of your Lambda functions, identify errors and performance issues, and troubleshoot problems. You can also use third-party monitoring tools such as Datadog or New Relic to monitor the behavior of your Lambda functions.\nC, D, and E are incorrect options:\nC. Security patches: AWS is responsible for securing the underlying infrastructure that runs the Lambda service, including the operating system, the network, and the physical facilities. AWS also provides security features such as identity and access management (IAM), encryption, and compliance certifications that you can use to secure your Lambda functions and the data they process. However, you are not responsible for applying security patches to the underlying infrastructure; AWS takes care of that.\nD. Installing required libraries in underlying compute instances for Lambda execution: AWS Lambda is a fully managed service, which means that AWS takes care of provisioning and managing the compute resources that are required to execute your Lambda functions. You don't need to worry about installing or managing any libraries or dependencies in the underlying compute instances; AWS takes care of that.\nE. Providing AWS Lambda with the required role and permissions to access the AWS resources: When you create a Lambda function, you need to specify the AWS resources that it will interact with, such as Amazon S3 buckets, Amazon DynamoDB tables, or Amazon SQS queues. You also need to provide the Lambda function with the required role and permissions to access these resources. However, this is not a customer responsibility as it is done through the IAM role and policy, and AWS takes care of it.\nTherefore, the correct answers are A and B.\n\n"
}, {
  "id" : 632,
  "question" : "Which of the following is NOT a potential use case for using AWS Lambda?\n",
  "answers" : [ {
    "id" : "40d36bf18a0c49f8a4ab424ec79ba3f1",
    "option" : "Periodically check the log files for errors in CloudWatch or CloudTrail and send out notifications through SNS.",
    "isCorrect" : "false"
  }, {
    "id" : "5c18fdd4001445dca0e2767e13460596",
    "option" : "Download S3 bucket objects of size varying between 500 MB-2 GB to a Lambda Ephemeral disk or temp location, read and analyze them for keywords and add the keywords to the metadata of file object for search purposes.",
    "isCorrect" : "true"
  }, {
    "id" : "0d3f0d1a03c7405e9ccc18ade63e383e",
    "option" : "Schedule a job and invoke a Lambda function to generate AWS resource usage reports based on certain tags.",
    "isCorrect" : "false"
  }, {
    "id" : "e3764233978847109fb68441cdf31ee8",
    "option" : "A website with highly scalable backend layer that will persist data into RDS or DynamoD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is a potential use case for AWS Lambda.\nYou can use Lambda as a scheduled event and read log files from AWS CloudWatch or CloudTrail and report any errors through SNS notifications.\nOption C is a potential use case.\nFor more information on scheduling Lambda functions, refer to documentation here.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-scheduled-events.html?shortFooter=true\nOption D is a potential use case.\nYou can host the web frontend on S3 and accelerate content delivery with Cloudfront caching.\nThe web frontend can send requests to Lambda functions via API Gateway HTTPS endpoints.\nLambda can handle the application logic and persist data to a fully managed database service.\n(RDS for relational, or DynamoDB for non-relational database)\nYou can host your Lambda functions and databases within a VPC to isolate them from other networks.\nHere is the documentation for building a serverless website.\nhttps://aws.amazon.com/getting-started/projects/build-serverless-web-app-lambda-apigateway-s3-dynamodb-cognito/\nOption B looks like a potential use case.\nBut the scenario will fail due to the /tmp directory space limitation.\n\n\nAWS Lambda is a serverless compute service offered by Amazon Web Services (AWS) that allows you to run code without the need to provision or manage servers. It automatically scales up or down based on incoming requests or events, making it an ideal choice for a wide range of use cases.\nLet's analyze each answer option to find the one that is NOT a potential use case for using AWS Lambda:\nA. Periodically check the log files for errors in CloudWatch or CloudTrail and send out notifications through SNS.\nThis is a valid use case for AWS Lambda. You can create a Lambda function that is triggered by CloudWatch or CloudTrail logs, which then sends notifications through SNS. This helps you proactively detect and fix issues before they become critical.\nB. Download S3 bucket objects of size varying between 500 MB-2 GB to a Lambda Ephemeral disk or temp location, read and analyze them for keywords and add the keywords to the metadata of file object for search purposes.\nThis is also a valid use case for AWS Lambda. You can create a Lambda function that reads and analyzes S3 objects for specific keywords, and then adds those keywords to the metadata of the file object for search purposes.\nC. Schedule a job and invoke a Lambda function to generate AWS resource usage reports based on certain tags.\nThis is another valid use case for AWS Lambda. You can create a Lambda function that is invoked on a schedule, which generates AWS resource usage reports based on certain tags.\nD. A website with highly scalable backend layer that will persist data into RDS or DynamoDB.\nThis is the answer that is NOT a potential use case for using AWS Lambda. While Lambda can be used for backend processing of a website, it is not the best choice for persisting data into RDS or DynamoDB. For persisting data, it is better to use Amazon RDS or Amazon DynamoDB directly.\nIn conclusion, the answer that is NOT a potential use case for using AWS Lambda is D.\n\n"
}, {
  "id" : 633,
  "question" : "You have created a Lambda function for reading data from the Kinesis stream of transactions.\nIn the code, you were using a context logger to log to CloudWatch, and you can monitor them at a later point of time.\nLambda function started running along with Kinesis stream.\nHowever, you do not see any log entries for the new Lambda function.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "a8f5786e74d6426a84b78d732b531e41",
    "option" : "Lambda functions with Kinesis stream as event source do not write logs to CloudWatch.",
    "isCorrect" : "false"
  }, {
    "id" : "022e441411f7406db3e6a708f336c076",
    "option" : "Lambda execution role policy does not have access to create CloudWatch logs.",
    "isCorrect" : "true"
  }, {
    "id" : "7d06e7f21d754062952f98337b24b017",
    "option" : "Lambda function execution logs will be written to CloudTrail, not to CloudWatch.",
    "isCorrect" : "false"
  }, {
    "id" : "47da264dbe204149979e8e92a0c0a87a",
    "option" : "Active tracing is not enabled on the Lambda function setup configuration.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is not a valid statement.\nLambda function will write logs as long as the execution role has access to create and write CloudWatch logs irrespective of the source that triggered it.\nOption B is correct.\nOption C is not correct.\nAWS CloudTrail is used for logging API calls made to services such as AWS Lambda, AWS S3 etc.\nAWS CloudWatch for Lambda is used for execution logging.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-cloudtrail.html\nOption D is not correct.\nAWS X-Ray traces requests made to your serverless applications built using AWS Lambda.\nThis will not be the reason for failing to write logs to CloudWatch.\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-x-ray.html\n\n\nIn this scenario, a Lambda function has been created to read data from a Kinesis stream of transactions. The code within the function is using a context logger to log to CloudWatch for monitoring purposes. However, there are no log entries for the new Lambda function, and the possible reasons are as follows:\nA. Lambda functions with Kinesis stream as an event source do not write logs to CloudWatch: This answer is not correct. Lambda functions with Kinesis stream as an event source can write logs to CloudWatch. In fact, CloudWatch is a common logging destination for Lambda functions.\nB. Lambda execution role policy does not have access to create CloudWatch logs: This answer is a possible reason for the issue. If the Lambda execution role policy does not have access to create CloudWatch logs, then the Lambda function will not be able to write logs to CloudWatch. To fix this, the execution role policy needs to be updated with the required permissions to create CloudWatch logs.\nC. Lambda function execution logs will be written to CloudTrail, not to CloudWatch: This answer is not correct. CloudTrail is used to monitor and log AWS API calls made by users or services in the account, and it does not store Lambda function execution logs.\nD. Active tracing is not enabled on the Lambda function setup configuration: This answer is also a possible reason for the issue. If active tracing is not enabled on the Lambda function setup configuration, then the Lambda function will not write logs to CloudWatch. To enable active tracing, the function needs to be updated with a tracing configuration that specifies the tracing mode.\nIn summary, possible reasons for the issue of no log entries for the new Lambda function include the Lambda execution role policy not having access to create CloudWatch logs and active tracing not being enabled on the Lambda function setup configuration.\n\n"
}, {
  "id" : 634,
  "question" : "When configuring AWS SQS as event source for AWS Lambda function, what is the maximum batch size supported by AWS SQS for ReceiveMessage call?\n",
  "answers" : [ {
    "id" : "dccf726546e2421fbba0c2458fb0c951",
    "option" : "20",
    "isCorrect" : "false"
  }, {
    "id" : "3691448df9964c55be67b8825e3cfd83",
    "option" : "40",
    "isCorrect" : "false"
  }, {
    "id" : "a932e5eaa4d641e28aedeb6d65ce9066",
    "option" : "10",
    "isCorrect" : "true"
  }, {
    "id" : "1082ebff86f64842a917e2ef80de0793",
    "option" : "100",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nHere are other parameters apart from batch size.\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html.\n\n\nWhen configuring AWS SQS as an event source for AWS Lambda, the Lambda function can be triggered in response to messages arriving in the queue. To retrieve messages from SQS, the Lambda function needs to call the SQS API method ReceiveMessage.\nWhen the Lambda function reads messages from the SQS queue, it can specify the maximum number of messages to read in a single batch. This is known as the \"batch size\". The maximum batch size supported by AWS SQS for the ReceiveMessage call is 10.\nTherefore, the correct answer to this question is option C: 10.\nIt's important to note that when configuring a Lambda function to read messages from an SQS queue, you can configure the BatchSize parameter of the event source mapping. The value of BatchSize can be between 1 and 10, and specifies the maximum number of messages that the Lambda function can receive in a single batch.\nIf you need to process more than 10 messages at a time, you can use parallel processing in your Lambda function to process multiple batches of messages simultaneously. However, keep in mind that there are limits to the number of concurrent executions of a Lambda function, so you should design your architecture accordingly to ensure that you don't exceed those limits.\n\n"
}, {
  "id" : 635,
  "question" : "You are planning to schedule a daily job with AWS CloudWatch scheduled event and AWS Lambda function triggered by the event that will perform a daily health check on your applications running on a fleet of EC2 instances.\nTo achieve this, you need to provide the EC2 instances' name tags to identify the right resources.\nWhat is the correct way of passing the inputs in this case?\n",
  "answers" : [ {
    "id" : "bb58422249c24ab0a08775c4469536e0",
    "option" : "Configure the â€œVariablesâ€ option on the AWS CloudWatch scheduled event.",
    "isCorrect" : "false"
  }, {
    "id" : "027d3383c6da48a89b6599f7df02945a",
    "option" : "You can modify the â€œMatched Eventâ€ option while selecting AWS Lambda as the trigger for CloudWatch scheduled event.",
    "isCorrect" : "false"
  }, {
    "id" : "3f9215022c08446d8d527a5630695f67",
    "option" : "You can set the â€œConstant (JSON text)â€ option while configuring the AWS Cloudwatch rule to trigger a Lambda function from AWS CloudWatch scheduled event.",
    "isCorrect" : "true"
  }, {
    "id" : "918f19f16aa746b7897976ac417ec642",
    "option" : "â€œDetailsâ€ object of â€œMatched Eventâ€ can be configured while creating an AWS CloudWatch scheduled event.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nWhen using an AWS Cloudwatch rule to trigger a Lambda event, one of the multiple options you have to pass data onto your Lamba function is â€œConstant (JSON Text)â€\nThis handy feature allows you to send static content to your function instead of the matched event.\nYou can also create the input targets via the CLI:\naws events put-targets --rule testjson --targets file://targetfile.\nReference:\nhttps://aws.amazon.com/blogs/compute/simply-serverless-use-constant-values-in-cloudwatch-event-triggered-lambda-functions/\n\n\nIn order to schedule a daily job with AWS CloudWatch scheduled event and AWS Lambda function triggered by the event that will perform a daily health check on your applications running on a fleet of EC2 instances, you need to provide the EC2 instances' name tags to identify the right resources.\nThe correct way of passing the inputs in this case is option C: You can set the \"Constant (JSON text)\" option while configuring the AWS Cloudwatch rule to trigger a Lambda function from AWS CloudWatch scheduled event.\nHere's how to configure this option:\nNavigate to the AWS CloudWatch console. Click on \"Rules\" in the left-hand menu. Click \"Create rule\" to create a new rule. In the \"Event Source\" section, select \"Schedule\" as the Event Source Type. Configure the \"Fixed rate of\" field to define how often the rule will be triggered. In the \"Targets\" section, click \"Add target\". Select \"Lambda function\" as the target type. Select your Lambda function from the dropdown list. In the \"Configure input\" section, select \"Constant (JSON text)\". Enter the name tag of your EC2 instances in JSON format, for example:\n{ \"tagKey\": \"Name\", \"tagValue\": \"myEC2Instance\" }\nThis will pass the name tag \"myEC2Instance\" to your Lambda function, which can then use it to identify the correct EC2 instance for performing the health check.\nOption A, configuring the \"Variables\" option on the AWS CloudWatch scheduled event, is not correct because this option is not available for a scheduled event. Option B, modifying the \"Matched Event\" option while selecting AWS Lambda as the trigger for CloudWatch scheduled event, is not correct because the \"Matched Event\" option is used for event pattern matching, not for passing inputs to Lambda functions. Option D, configuring the \"Details\" object of \"Matched Event\" while creating an AWS CloudWatch scheduled event, is not correct because the \"Details\" object is used for passing event data to the Lambda function, not for passing inputs.\n\n"
}, {
  "id" : 636,
  "question" : "You have an existing AWS setup with DirectConnect.\nYou have migrated certain on premise backend applications to AWS EC2 instances which are having certain processes run based on triggers from other applications.\nThese processes are developed on JAVA programming language.\nYour organization is looking to migrate these processes to Lambda and reduce the cost incurred on EC2 instances.\nWhat would be your recommendation?\n",
  "answers" : [ {
    "id" : "1b2f94887a1a413bb008dfa5d6f76f06",
    "option" : "AWS Lambda cannot be invoked from a custom application. They can only be triggered by AWS supported event sources.",
    "isCorrect" : "false"
  }, {
    "id" : "b585ad4480114787a00334e24ac4e284",
    "option" : "Replicate the JAVA code easily onto AWS Lambda function with few modifications and use Lambda Invoke API with input passed as custom event.",
    "isCorrect" : "true"
  }, {
    "id" : "d442e3cc616349b280e6a2b7466d763e",
    "option" : "Trigger Lambda from AWS CloudWatch scheduled event and invoke CloudWatch API from your applications.",
    "isCorrect" : "false"
  }, {
    "id" : "fec3aaaa130e4576806d395972a429e1",
    "option" : "AWS Lambda is not designed to run backend applications. Better to use EC2 for that purpose.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nYou can invoke a Lambda function using a custom event through AWS Lambda's invoke API.\nOnly the function's owner or another AWS account that the owner has granted permission can invoke the function.\nhttps://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-userapp.html\n\n\nThe best recommendation would be option B: Replicate the Java code easily onto AWS Lambda function with few modifications and use Lambda Invoke API with input passed as custom event.\nAWS Lambda is a serverless computing service that allows developers to run code without having to worry about managing servers. It supports a variety of programming languages, including Java, and can be triggered by AWS supported event sources, including AWS services like S3, DynamoDB, and others.\nTo migrate the processes developed on Java programming language to Lambda, the first step would be to replicate the code into a Lambda function. This can be done with few modifications to ensure the code works well with the serverless environment.\nOnce the code is in the Lambda function, it can be invoked using the Lambda Invoke API. This API allows custom events to be passed as input to the function. In this case, the triggers from other applications can be used as custom events to invoke the Lambda function and run the processes.\nThis approach can help reduce costs incurred on EC2 instances as Lambda is a pay-per-use service, which means that you only pay for the amount of compute time your code consumes, and there are no upfront costs or minimum fees.\nOption A is incorrect because Lambda can be invoked from a custom application using the Lambda Invoke API.\nOption C is incorrect because triggering Lambda from AWS CloudWatch scheduled events and invoking CloudWatch API from the applications would not be the best approach as it requires additional steps and complexity.\nOption D is incorrect because Lambda is designed to run backend applications and can be a more cost-effective and scalable solution compared to using EC2 instances.\n\n"
}, {
  "id" : 637,
  "question" : "Which of the following are AWS CloudFront events that can trigger AWS Lambda@edge function? (choose 3 options)\n",
  "answers" : [ {
    "id" : "3bbe93d491f84086a6434b4cd8d6514a",
    "option" : "Viewer Request",
    "isCorrect" : "true"
  }, {
    "id" : "07e4875b9076431bba48723869030e93",
    "option" : "CloudFront Cache",
    "isCorrect" : "false"
  }, {
    "id" : "fbdab6cb07ed40a1aaac80611c7cae01",
    "option" : "Sender Request",
    "isCorrect" : "false"
  }, {
    "id" : "3a4fd2d4396542c1a020c059f1ae5209",
    "option" : "Origin Request",
    "isCorrect" : "true"
  }, {
    "id" : "3e7a38e079fd4fb3927be562cb1eb111",
    "option" : "Origin Response.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A, D, E.\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-cloudfront-trigger-events.html\n\n\nAWS CloudFront is a content delivery network service provided by AWS, that enables faster delivery of static and dynamic content, such as HTML, CSS, images, and videos, from a web application to end-users around the globe. AWS Lambda@Edge is an extension of AWS Lambda, that allows developers to run serverless functions in response to CloudFront events, and execute custom logic for each end-user request.\nThere are several CloudFront events that can trigger a Lambda@Edge function, and based on the event type, different actions can be taken. Here are the explanations of the three CloudFront events that can trigger a Lambda@Edge function:\nViewer Request: This event occurs when a viewer (end-user) sends a request to CloudFront, and before the request is forwarded to the origin server. A Lambda@Edge function triggered by this event can inspect and modify the viewer request, and either allow or deny the request based on custom business logic. For example, you can use this event to block requests from certain IP addresses or to redirect requests to a different URL. Origin Request: This event occurs when CloudFront sends a request to the origin server, on behalf of the viewer, to fetch the content. A Lambda@Edge function triggered by this event can modify the headers or parameters of the request before it is sent to the origin server. This can be used, for example, to add custom headers for authentication or to rewrite the URL to fetch a different version of the content. Origin Response: This event occurs when CloudFront receives a response from the origin server, and before the response is forwarded to the viewer. A Lambda@Edge function triggered by this event can modify the response body or headers, and add or remove content. For example, you can use this event to compress the response body, to add security headers, or to customize the content for each viewer based on their location.\nThe following CloudFront events are not valid triggers for Lambda@Edge:\nCloudFront Cache: This event occurs when CloudFront retrieves content from its cache, and is not related to a viewer request. Sender Request: This is not a valid CloudFront event.\n\n"
}, {
  "id" : 638,
  "question" : "You created an AWS Lambda function to process files uploaded to AWS S3 bucket.\nLambda function started receiving requests and working properly.\nYou have changed the code and uploaded new version of code to AWS Lambda function.\nWhat will happen to the requests sent right after the AWS lambda function update?\n",
  "answers" : [ {
    "id" : "ed21075fe3c443c196a8cea53de0d153",
    "option" : "Requests will queue until the changes are fully propagated. You could experience up to 5 minutes of wait during this period.",
    "isCorrect" : "false"
  }, {
    "id" : "fa195d8f881a44b3ac219f311ce66ce4",
    "option" : "Requests will be served by old version till you enable new version as latest.",
    "isCorrect" : "false"
  }, {
    "id" : "2a49e6bd69af45f290fc544831ece052",
    "option" : "When you have multiple versions of Lambda function, in the code you need to define which version of function to be used. Otherwise, requests would fail.",
    "isCorrect" : "false"
  }, {
    "id" : "4e303fe15527450ca7d25ff68b600321",
    "option" : "Requests might be served by old or new version for a brief period of less than one minute.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is not a valid statement.\nIt will not continually queue automatically or 5 minute wait time.\nOption B is not correct.\nBy default, whenever you update the code, it updates the LATEST version.\n\nOption C is not correct.\nThere is no need to define in code which version to be used.\nHowever, you can define which version to be used at the source which triggers Lambda function by providing version qualified ARN if you have published version.\n\n\nWhen you update the code of an AWS Lambda function, AWS creates a new version of the function. The new version does not immediately become the default version for incoming requests. Instead, it is published as a version and a new alias is created, which points to the new version. The old alias, which points to the old version of the function, is still active and serves incoming requests.\nTherefore, the correct answer is:\nB. Requests will be served by the old version until you enable the new version as the latest.\nWhen you are ready to start using the new version of the function, you can update the alias to point to the new version. Once the alias is updated, all incoming requests will be served by the new version.\nIt's important to note that when you have multiple versions of a Lambda function, you can specify which version or alias of the function you want to use in your code. If you do not specify a version or alias, the default version (which is the latest version) will be used. However, this is not related to the scenario in the question.\nIn summary, the requests sent right after updating the AWS Lambda function will be served by the old version until you update the alias to point to the new version.\n\n"
}, {
  "id" : 639,
  "question" : "Which of the following are poll-based event sources for AWS Lambda function? (choose 3 options)\n",
  "answers" : [ {
    "id" : "2417de7e06df417d8c3d49fda90526e0",
    "option" : "AWS SNS",
    "isCorrect" : "false"
  }, {
    "id" : "f7f46796fd3d48a7a5c4beeceef47c4f",
    "option" : "AWS Kinesis",
    "isCorrect" : "true"
  }, {
    "id" : "bb59eda464da484480a527d7135c3445",
    "option" : "AWS SQS",
    "isCorrect" : "true"
  }, {
    "id" : "02485aedaec74ad69d9d8f068b0da457",
    "option" : "AWS DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "228b5898e55049bca9f19d86c7e5ca74",
    "option" : "AWS CodePipeline.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B, C, D.\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-options.html?shortFooter=true#streaming-event-source-mapping\n\n\nSure, I'd be happy to help!\nAWS Lambda is a compute service provided by AWS that allows developers to run code in response to events and automatically manages the computing resources required by that code. One of the key benefits of Lambda is its ability to integrate with other AWS services to automatically trigger the execution of Lambda functions based on events happening in those services.\nThere are three main types of event sources for AWS Lambda: push-based, pull-based, and stream-based. Push-based event sources, such as AWS S3 and AWS CloudFormation, send events directly to a Lambda function when an action occurs. Stream-based event sources, such as Amazon Kinesis Data Streams and AWS DynamoDB Streams, send records to a Lambda function as they are generated. Pull-based event sources, on the other hand, require the Lambda function to poll the event source periodically to check for new events.\nNow, let's look at the options provided in the question and identify the three poll-based event sources for AWS Lambda:\nA. AWS SNS: AWS Simple Notification Service (SNS) is a push-based event source for AWS Lambda, so it is not a poll-based event source.\nB. AWS Kinesis: Amazon Kinesis Data Streams is a stream-based event source for AWS Lambda, so it is not a poll-based event source.\nC. AWS SQS: AWS Simple Queue Service (SQS) is a pull-based event source for AWS Lambda. A Lambda function can be configured to poll an SQS queue for new messages, and then execute the function with the contents of the message.\nD. AWS DynamoDB: AWS DynamoDB is a stream-based event source for AWS Lambda, so it is not a poll-based event source.\nE. AWS CodePipeline: AWS CodePipeline is a push-based event source for AWS Lambda, so it is not a poll-based event source.\nTherefore, the three poll-based event sources for AWS Lambda are AWS SQS, AWS EventBridge, and AWS Systems Manager.\n\n"
}, {
  "id" : 640,
  "question" : "You work for a team which has 10s of applications running on AWS EC2 instances.\nAll these applications would need a common backend processing job.\nYou have created an AWS Lambda function with name â€œapp-backend-jobâ€and published PROD version with version â€œ1â€ in order to make sure any changes to the function by anyone will not impact the PROD execution code.\nYou have shared the version qualified ARN to all the applications assuming requests would be sent to the specific version.\nHowever, due to frequent changes in requirements, you had to change the code of Lambda function many times and keep publishing versions.\nThis is causing a lot of overhead at the application level to update the Lambda function ARN each time you publish a new version.\nHow can you overcome this situation?\n",
  "answers" : [ {
    "id" : "db4569413bd2474582c946ebaaf5e091",
    "option" : "Create an alias, point it to PROD version and share the ARN with applications. When new version is published, change the alias to point to it.",
    "isCorrect" : "true"
  }, {
    "id" : "9f37db034f4342d1af5f1986977710bc",
    "option" : "Do not publish versions for every code change. Instead, update the published version so that ARN to be invoked will not change.",
    "isCorrect" : "false"
  }, {
    "id" : "73cea7608ea343d7b0089d37d32bf2e3",
    "option" : "Delete the old published version â€œ1â€ before publishing new version. This way when you publish, you will get the version ID as â€œ1â€ and the lambda version ARN will remain unchanged.",
    "isCorrect" : "false"
  }, {
    "id" : "427e499ff91a48d1adb725bf0056afbe",
    "option" : "Do not use versioning in this case. Always use $LATEST version and share its ARN with applications.You can update the codes of $LATEST version any number of times.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nBy using aliases, you can access the Lambda function an alias is pointing to (for example, to.\ninvoke the function) without the caller having to know the specific version the alias is pointing to.\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.html?shortFooter=true\nOption B is not correct.\nhttps://docs.aws.amazon.com/lambda/latest/dg/versioning-intro.html?shortFooter=true\nOption C is not correct.\nAlthough Option D sounds correct, it is not a recommended approach since $LATEST version can be changed by anyone who has access to it.\nAny code running in PRODUCTION mode and using.\n$LATEST version, there are chances that the configuration can be meddled and can cause unwanted issues.\nhttps://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html?shortFooter=true\n\n\nThe correct answer to this question is A. Create an alias, point it to PROD version and share the ARN with applications. When a new version is published, change the alias to point to it.\nExplanation:\nAWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. It allows you to create functions that can be triggered by various events, such as an HTTP request, S3 bucket upload, or an event from other AWS services. Lambda functions can be invoked using a unique Amazon Resource Name (ARN) that includes a version number.\nIn this scenario, the team has created an AWS Lambda function called â€œapp-backend-jobâ€ and published a PROD version with version number â€œ1â€. They have shared the version qualified ARN with all the applications, assuming requests would be sent to the specific version.\nHowever, due to frequent changes in requirements, the Lambda function code has to be changed many times, causing overhead at the application level to update the Lambda function ARN each time a new version is published.\nTo overcome this situation, the team can create an alias for the Lambda function and point it to the PROD version. An alias is a pointer to a specific version of a Lambda function, which allows you to decouple the function's version number from the ARN used to invoke it. By using an alias, you can share a single ARN with the applications and not worry about updating it every time a new version is published.\nWhen a new version of the Lambda function is published, the team can simply update the alias to point to the new version. This will ensure that all the applications continue to use the same ARN to invoke the function, and the alias will automatically route requests to the latest version of the function.\nOption B, \"Do not publish versions for every code change. Instead, update the published version so that ARN to be invoked will not change\" is not recommended because it does not allow you to track changes and roll back to a previous version if needed. It also makes it difficult to test and deploy new changes in a controlled manner.\nOption C, \"Delete the old published version â€œ1â€ before publishing a new version. This way when you publish, you will get the version ID as â€œ1â€ and the lambda version ARN will remain unchanged\" is also not recommended. Deleting old versions may cause issues if there are other applications or services still using that version. It also makes it difficult to roll back to a previous version if needed.\nOption D, \"Do not use versioning in this case. Always use $LATEST version and share its ARN with applications. You can update the codes of $LATEST version any number of times\" is also not recommended. Using $LATEST makes it difficult to track changes and roll back to a previous version if needed. It also does not provide the ability to create stable versions of the Lambda function that can be used in production environments.\n\n"
}, {
  "id" : 641,
  "question" : "Which of the following are correct ARNs for a Lambda function? (choose 3 options)\n",
  "answers" : [ {
    "id" : "c997a1f2a9e24c5092fd78de65d2a3a4",
    "option" : "arn:aws:lambda:aws-region:acct-id:function:helloworld:$LATEST",
    "isCorrect" : "true"
  }, {
    "id" : "bd0fad122656435e99ce58ca814f9437",
    "option" : "arn:aws:lambda:aws-region:acct-id:function:helloworld",
    "isCorrect" : "true"
  }, {
    "id" : "1959cc0ceaaa491b99a8d40f705747ec",
    "option" : "arn:aws:lambda:aws-region:acct-id:function:helloworld/$LATEST",
    "isCorrect" : "false"
  }, {
    "id" : "37d588ea5d76480d8c100e98f0d6a1df",
    "option" : "arn:aws:lambda:aws-region:acct-id:function:helloworld:PROD",
    "isCorrect" : "true"
  }, {
    "id" : "de2a117878e741a3b908817150f647b8",
    "option" : "arn:aws:lambda:aws-region:acct-id:function:helloworld/1",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B, D.\n\n\nSure, I'd be happy to help you out with this question!\nAn Amazon Resource Name (ARN) is a unique identifier that is used to identify and access AWS resources. In the case of Lambda functions, the ARN includes the function name, AWS region, AWS account ID, and version or alias (if applicable).\nLet's look at the options one by one:\nA. arn:aws:lambda:aws-region:acct-id:function:helloworld:$LATEST\nThis is a valid ARN for a Lambda function. The $LATEST refers to the latest version of the function code.\nB. arn:aws:lambda:aws-region:acct-id:function:helloworld\nThis is also a valid ARN for a Lambda function. It refers to the specific version of the function, as no version or alias is specified.\nC. arn:aws:lambda:aws-region:acct-id:function:helloworld/$LATEST\nThis is not a valid ARN for a Lambda function. The / character is not allowed in the function name.\nD. arn:aws:lambda:aws-region:acct-id:function:helloworld:PROD\nThis is a valid ARN for a Lambda function, but it refers to a specific alias (in this case, PROD) of the function. If the function does not have an alias named PROD, then this ARN is not valid.\nE. arn:aws:lambda:aws-region:acct-id:function:helloworld/1\nThis is not a valid ARN for a Lambda function. The / character is not allowed in the function name.\nTherefore, the correct answers are A, B, and D.\n\n"
}, {
  "id" : 642,
  "question" : "Which of the following is a valid AWS Lambda configuration?\n",
  "answers" : [ {
    "id" : "e235bf154cd44f0ba4a0f607c4a83f8d",
    "option" : "64 GB memory and 212 seconds timeout.",
    "isCorrect" : "false"
  }, {
    "id" : "e99643da11fc4a28b6f193a8af168c10",
    "option" : "1376 GB memory and 120 seconds timeout.",
    "isCorrect" : "false"
  }, {
    "id" : "61f5457cf26c429e9adf37b73ae7eaf6",
    "option" : "2112 MB memory and 100 seconds timeout.",
    "isCorrect" : "true"
  }, {
    "id" : "785bfdbcbf704f7e94bedd5875e19cd2",
    "option" : "3072 MB memory and 1300 seconds timeout.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOptions A and B are not correct because the units are in GB and NOT in MB.Option D is not correct.\nMaximum time can be 900 seconds only.\nPlease refer to the below link to get further information:\nhttps://docs.aws.amazon.com/lambda/latest/dg/limits.html.\n\n\nAWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. When creating a Lambda function, you can configure various parameters, including the memory size and timeout duration. The memory size controls the amount of memory allocated to the function, while the timeout duration determines the maximum amount of time the function can run before it times out.\nOut of the options provided, the valid AWS Lambda configuration is option C: 2112 MB memory and 100 seconds timeout.\nOption A has an excessive amount of memory (64 GB) and a high timeout duration (212 seconds), which is not a practical or cost-effective configuration for most Lambda functions.\nOption B has an unrealistically large amount of memory (1376 GB) and a shorter timeout duration (120 seconds). This configuration is not practical as it would be very expensive and could lead to resource wastage.\nOption D has a large memory size (3072 MB) but a very high timeout duration (1300 seconds), which again is not a practical or cost-effective configuration for most Lambda functions.\nTherefore, option C with 2112 MB memory and 100 seconds timeout is the most practical and cost-effective configuration for most Lambda functions.\n\n"
}, {
  "id" : 643,
  "question" : "Your organization has two accounts, for DEV and TEST.\nYou have certain user applications running on the TEST account would like to trigger AWS Lambda on the DEV account.\nWhat is the permission model which needs to be set to get this configuration working?\n",
  "answers" : [ {
    "id" : "fd1f990678b94270be180b959769bc48",
    "option" : "Add permission for TEST account on DEV accountâ€™s lambda function policy through AWS CLI.",
    "isCorrect" : "true"
  }, {
    "id" : "3fb495d765714452ae24f9f639b5d495",
    "option" : "Add permission for TEST account on DEV accountâ€™s lambda execution role policy through AWS Console.",
    "isCorrect" : "false"
  }, {
    "id" : "31b5b8ffe5be4cb48ba876a12d130e97",
    "option" : "Add permission for TEST account on DEV accountâ€™s lambda execution role policy through AWS CLI.",
    "isCorrect" : "false"
  }, {
    "id" : "5de9cf0f543e4a948be9b52a9a2c2ac8",
    "option" : "Add permission for TEST account on DEV accountâ€™s lambda function policy through AWS Console.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nhttps://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html?shortFooter=true#access-control-resource-based-example-cross-account-scenario\nOptions B, C are not correct.\nPermission needs to be added to the Lambda function policy to invoke the function, not on the execution role policy.\nOption D is not correct.\nLambda function policy cannot be edited from the AWS console.\nFor more information, please fere to the below mentioned AWS docs:\nhttps://docs.aws.amazon.com/lambda/latest/dg/access-control-resource-based.html\n\n\nIn order for an application running on the TEST account to trigger an AWS Lambda function on the DEV account, you need to grant permission to the TEST account to invoke the Lambda function on the DEV account.\nThe AWS Lambda function policy controls who can invoke the Lambda function. The policy must include an IAM role or user that has permission to invoke the Lambda function. You can add permissions to the Lambda function policy using either the AWS CLI or the AWS Management Console.\nOption A suggests adding permissions for the TEST account to the DEV account's Lambda function policy using the AWS CLI. This is incorrect because the Lambda function policy is only used to control access to the function itself, and not to control access to invoke the function.\nOption B suggests adding permissions for the TEST account to the DEV account's Lambda execution role policy through the AWS Management Console. This is partially correct. The Lambda execution role policy controls who or what can execute the Lambda function. By adding the TEST account to the execution role policy, you are granting permission for the TEST account to invoke the Lambda function on the DEV account. However, the question specifically asks for the permission model that needs to be set, and this answer only addresses part of the required permission model.\nOption C suggests adding permissions for the TEST account to the DEV account's Lambda execution role policy using the AWS CLI. This is the correct answer. The Lambda execution role policy controls who or what can execute the Lambda function, and adding the TEST account to the execution role policy using the AWS CLI will grant permission for the TEST account to invoke the Lambda function on the DEV account.\nOption D suggests adding permissions for the TEST account to the DEV account's Lambda function policy using the AWS Management Console. This is incorrect for the same reasons as Option A.\nIn summary, the correct answer is option C. You need to add permission for the TEST account on the DEV account's Lambda execution role policy through the AWS CLI to allow an application running on the TEST account to trigger an AWS Lambda function on the DEV account.\n\n"
}, {
  "id" : 644,
  "question" : "You are a solutions architect working for a social media company that provides a place for civil discussion of political and news-related events.\nDue to the ever-changing regulatory requirements and restrictions placed on social media apps that provide these services, you need to build your app in a very highly flexible environment that you can change instantly without updating code.\nYou have chosen to build the REST API endpoints used by your social media app user interface code using Lambda.\nHow can you securely configure your Lambda functions so that you can achieve the flexibility required by your app? (Select TWO)\n",
  "answers" : [ {
    "id" : "47b9e3747b994ca3b6ab130d5a08c646",
    "option" : "Pass environment variables to your Lambda function via the request header sent to your API Gateway methods.",
    "isCorrect" : "false"
  }, {
    "id" : "77dbf308637e44f1bc67fed16805ef75",
    "option" : "Configure your Lambda functions to use key configuration.",
    "isCorrect" : "true"
  }, {
    "id" : "eacfaa5eb5794dfd988b1ba25985eb22",
    "option" : "Use encryption helpers",
    "isCorrect" : "true"
  }, {
    "id" : "6ec4c721c94b4e128823962bedf35a35",
    "option" : "Use Lambda layers",
    "isCorrect" : "false"
  }, {
    "id" : "6edc795480bc42e18fa21c96c34059c4",
    "option" : "Use Lambda aliases.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: B and C.\nOption A is incorrect.\nSending environment variables to your Lambda function as request parameters would expose the environment variables as plain text.\nThis is not a secure approach.\nOption B is correct.\nLambda key configuration allows you to have your Lambda functions use an encryption key.\nYou create the key in AWS KMS.\nThe key is used to encrypt the environment variables that you can use to change your function without deploying any code.\nOption C is correct.\nEncryption helpers make your lambda function more secure by allowing you to encrypt your environment variables before they are sent to Lambda.\nOption D is incorrect.\nLambda layers are used to package common code such as libraries, configuration files, or custom runtime images.\nLayers will not give you the same flexibility as environment variables for use in managing change without deploying any code.\nOption E is incorrect.\nLambda aliases are used to refer to a specific version of your Lambda function.\nYou could switch between many versions of your Lambda function, but you would have to deploy new code to create a different version of your Lambda function.\nReferences:\nPlease see the AWS Lambda developer guide titled Data protection in AWS Lambda (https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html), the AWS Lambda developer guide titled Lambda concepts (https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-concepts.html#gettingstarted-concepts-layer), the AWS Lambda developer guide titled Lambda function aliases (https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html), and the AWS Lambda developer guide titled Using AWS Lambda environment variables (https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html)\n\nTo achieve the required flexibility and security for the REST API endpoints of a social media app, two possible solutions are:\nA. Pass environment variables to your Lambda function via the request header sent to your API Gateway methods.\nAPI Gateway allows you to map request parameters to Lambda function environment variables. You can use this feature to configure your Lambda functions securely without updating code. For example, you can set environment variables for database credentials or API keys, and map them to request parameters such as headers or query strings. This way, you can change the environment variables at any time without redeploying your Lambda function, and the changes will be effective immediately. This approach allows you to achieve flexibility while maintaining security because the environment variables are not exposed in the code and are only available to the Lambda function.\nB. Use Lambda aliases.\nLambda aliases provide a way to publish multiple versions of a Lambda function under a single Amazon Resource Name (ARN). You can use aliases to route traffic to specific versions of the function, allowing you to implement flexible and secure deployment strategies. For example, you can have a production alias that points to the latest stable version of your Lambda function, and a staging alias that points to a new version that is being tested. You can also configure different aliases to use different resource policies or environment variables, providing fine-grained control over the function's behavior. This approach allows you to achieve flexibility while maintaining security because you can control access to different versions of the function using IAM roles and policies.\nC, D, and E are incorrect options for this scenario:\nC. Use encryption helpers\nEncryption helpers are libraries that help you encrypt and decrypt data in your Lambda function. While encryption is important for securing data at rest and in transit, it does not address the requirement for flexibility in this scenario. Encryption helpers do not allow you to configure your Lambda function without updating code, and therefore do not meet the requirements of the question.\nD. Use Lambda layers\nLambda layers are a way to package and share libraries, custom runtimes, and other function dependencies across multiple Lambda functions. While Lambda layers can help you manage dependencies and reduce code duplication, they do not provide the required flexibility for this scenario. Lambda layers do not allow you to configure your Lambda function without updating code, and therefore do not meet the requirements of the question.\nE. Use key configuration\nKey configuration typically refers to the process of generating and managing API keys that are used to authenticate and authorize access to API Gateway endpoints. While API keys are an important part of securing API endpoints, they do not provide the required flexibility for this scenario. API keys are typically used to restrict access to specific endpoints, rather than to configure Lambda functions, and therefore do not meet the requirements of the question.\n\n"
}, {
  "id" : 645,
  "question" : "Which of the following AWS services can be used as target(s) while configuring a Dead Letter Queue? (Select Two)\n",
  "answers" : [ {
    "id" : "87429e316f5545eaa578faecc32b4ebc",
    "option" : "AWS SQS",
    "isCorrect" : "true"
  }, {
    "id" : "fdb1deebcfe648f7b220c0dea4f9d888",
    "option" : "AWS Kinesis",
    "isCorrect" : "false"
  }, {
    "id" : "10a96420cea844c38071b5483f9547e0",
    "option" : "AWS SNS",
    "isCorrect" : "true"
  }, {
    "id" : "47036cd3fa2245d5acee7c126b88ffbf",
    "option" : "AWS CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "0723f910ca84420b871fcea6e76a96b7",
    "option" : "AWS X-Ray.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A and C.\nhttps://docs.aws.amazon.com/lambda/latest/dg/dlq.html?shortFooter=true.\n\n\nThe Dead Letter Queue (DLQ) is a feature that allows you to set up a queue or topic to receive messages that cannot be delivered to their intended destination. When a message fails to be processed or delivered, the message is sent to the DLQ, which can be configured to trigger an action or notification, such as sending an email or alert.\nTo configure a Dead Letter Queue, you need to specify the target where the failed messages will be delivered. The target can be a queue or a topic in AWS. Therefore, the correct answer to this question is A and C - AWS SQS and AWS SNS.\nAWS SQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It allows you to send, store, and receive messages between software components, without losing messages or requiring other services to be available.\nAWS SNS (Simple Notification Service) is a fully managed pub/sub messaging service that enables you to decouple microservices, distributed systems, and serverless applications. It allows you to send messages or notifications to multiple recipients or subscribers, such as email, SMS, mobile push, and HTTP/S endpoints.\nAWS Kinesis, AWS CloudWatch, and AWS X-Ray are not valid targets for a Dead Letter Queue.\nAWS Kinesis is a platform for real-time data processing and analysis of streaming data at scale. It is not a message queuing service, and therefore, it is not a valid target for a Dead Letter Queue.\nAWS CloudWatch is a monitoring and observability service that enables you to collect, analyze, and act on metrics, logs, and traces from AWS services, applications, and resources. Although it can trigger alarms and notifications based on metric thresholds or log patterns, it is not a message queuing service, and therefore, it is not a valid target for a Dead Letter Queue.\nAWS X-Ray is a distributed tracing service that enables you to visualize, analyze, and debug the performance of your applications and microservices. It is not a message queuing service, and therefore, it is not a valid target for a Dead Letter Queue.\n\n"
}, {
  "id" : 646,
  "question" : "You are setting up AWS Lambda function to receive messages from SQS queue, process the message body and insert one record in MySQL RDS instance.\nYou have setup SQS event trigger as AWS Lambda function.\nHowever, for connecting to RDS instance, you need MySQL details such as hostname, username and password.\nWhere would you configure them?\n",
  "answers" : [ {
    "id" : "434dd461857d421cbf226c7c9d763d08",
    "option" : "Use environment variables to pass configuration. They are automatically encrypted by AWS default KMS keys and decrypted when used in Lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "0a90966ee77e4a6e9c810706bc05db5b",
    "option" : "Use environment variables to pass configuration. Use encryption helpers to encrypt sensitive information by your own KMS key. Decrypt the variable using decryption helper code provided in the console.",
    "isCorrect" : "true"
  }, {
    "id" : "4a862b3454c740f8bc13153351b7fc74",
    "option" : "Use properties file in AWS Lambda function for any such configuration. Properties files are encrypted by AWS in transit and at rest.",
    "isCorrect" : "false"
  }, {
    "id" : "80bdd519acb34ce2b6f0fc1f1e202d3e",
    "option" : "Store such configuration in AWS S3 bucket and enable encryption on S3 bucket. Perform S3 get object to get the configuration details in the Lambda function code.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nhttps://docs.aws.amazon.com/lambda/latest/dg/env_variables.html?shortFooter=true.\nOption A is not correct.\nThe statement is true, however the encryption only happens after deployment of lambda function.\nOption C is not a valid statement.\nOption D looks correct.\nHowever, out of given options, Option B is more valid and recommended.\n\n\nWhen setting up an AWS Lambda function to receive messages from an SQS queue, process the message body, and insert a record in MySQL RDS instance, you will need MySQL details such as hostname, username, and password to connect to the RDS instance. There are various ways to configure these details securely.\nOption A suggests using environment variables to pass configuration. AWS Lambda supports environment variables that can be used to pass configuration values to your function. These environment variables are automatically encrypted by AWS default KMS keys and decrypted when used in the Lambda function. This approach ensures that sensitive information is kept secure, and you don't have to worry about encrypting and decrypting data manually.\nOption B also suggests using environment variables to pass configuration, but this time, it recommends using encryption helpers to encrypt sensitive information by your own KMS key. You can then decrypt the variable using decryption helper code provided in the console. This approach gives you more control over the encryption of sensitive data, but it requires more effort to set up.\nOption C suggests using properties files in AWS Lambda function for any such configuration. Properties files are plain text files that contain key-value pairs of configuration values. These files can be encrypted by AWS in transit and at rest, ensuring that sensitive data is kept secure. This approach is similar to using environment variables, but it requires creating and managing properties files, which can be cumbersome.\nOption D suggests storing configuration details in an S3 bucket and enabling encryption on the S3 bucket. You can then perform S3 get object to get the configuration details in the Lambda function code. This approach gives you the flexibility to store configuration data separately from the Lambda function code, but it requires additional permissions to access the S3 bucket and retrieve the configuration details.\nIn summary, Option A and Option B are both viable solutions to configure MySQL details securely when setting up an AWS Lambda function to receive messages from an SQS queue, process the message body, and insert a record in MySQL RDS instance. Option C and Option D are also possible but require more effort to set up and manage.\n\n"
}, {
  "id" : 647,
  "question" : "Which of the following actions is required by Lambda execution role to write the logs into AWS CloudWatch? (choose 3 options)\n",
  "answers" : [ {
    "id" : "a07bc0bd918c4046aad69e0fe422b90d",
    "option" : "logs:CreateLogGroup",
    "isCorrect" : "true"
  }, {
    "id" : "1bc9ce1a5f47400b9adc70ce0869561d",
    "option" : "logs:GetLogEvents",
    "isCorrect" : "false"
  }, {
    "id" : "e106087d5e75436bb73d081d62251d03",
    "option" : "logs:CreateLogStream",
    "isCorrect" : "true"
  }, {
    "id" : "35e7872ac9ad4a408161972df7e2ab08",
    "option" : "logs:DescribeLogStreams",
    "isCorrect" : "false"
  }, {
    "id" : "07402ec3fb2f4d2e820ec633f494c9e7",
    "option" : "logs:PutLogEvents.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A, C, E.\nhttps://docs.aws.amazon.com/lambda/latest/dg/intro-permission-model.html?shortFooter=true#lambda-intro-execution-role.\n\n\nLambda functions can write logs to CloudWatch logs, which can help you troubleshoot issues with your application. To write logs to CloudWatch, a Lambda execution role must be granted specific permissions. The following are the required actions by Lambda execution role to write logs into AWS CloudWatch:\nlogs:CreateLogGroup: This permission is required to create a new log group in CloudWatch Logs. A log group is a collection of log streams that share the same retention, monitoring, and access control settings. This permission allows the Lambda function to create a new log group if one does not already exist. logs:CreateLogStream: This permission is required to create a new log stream within a log group. A log stream represents a sequence of log events that share the same source. This permission allows the Lambda function to create a new log stream for each instance of the function. logs:PutLogEvents: This permission is required to write log events to a log stream. This permission allows the Lambda function to write log events to the log stream created for each instance of the function.\nThe other two permissions mentioned in the question are not required to write logs to CloudWatch:\nlogs:GetLogEvents: This permission is used to retrieve log events from a log stream. It is not required for writing logs to CloudWatch. logs:DescribeLogStreams: This permission is used to retrieve information about the log streams in a log group. It is not required for writing logs to CloudWatch.\nIn summary, the Lambda execution role requires the logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents permissions to write logs to CloudWatch.\n\n"
}, {
  "id" : 648,
  "question" : "Which of the following options is not AWS CloudWatch metric for AWS Lambda function?\n",
  "answers" : [ {
    "id" : "ef2228c9a51e4a2fbb781ea3479784ca",
    "option" : "Memory",
    "isCorrect" : "true"
  }, {
    "id" : "0775b54313d844548c1f1bf88388e776",
    "option" : "Dead Letter Error",
    "isCorrect" : "false"
  }, {
    "id" : "37374279dfd941b68b6989fa6ad53262",
    "option" : "Duration",
    "isCorrect" : "false"
  }, {
    "id" : "5c6bc7d60bf44f0197b07feb18b2c3e4",
    "option" : "Invocations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nThe AWS/Lambda namespace includes the following metrics.\nhttps://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions-metrics.html\n\n\nAll the listed options are AWS CloudWatch metrics for AWS Lambda function except for the Dead Letter Error option, which is not a valid CloudWatch metric for AWS Lambda.\nHere's a detailed explanation of each option:\nA. Memory: This metric shows the amount of memory that the Lambda function uses during its execution. It is measured in megabytes (MB) and is helpful in optimizing the function's memory allocation.\nB. Dead Letter Error: This option is not a valid CloudWatch metric for AWS Lambda. However, a Dead Letter Queue (DLQ) is a feature that allows you to capture and store failed events that a Lambda function can't process. When a Lambda function fails to process an event, it can send the event to a DLQ. You can then analyze the failed events to identify and fix the root cause of the issue.\nC. Duration: This metric measures the amount of time that a Lambda function takes to execute. It is measured in milliseconds (ms) and is helpful in optimizing the function's performance.\nD. Invocations: This metric shows the number of times that a Lambda function is invoked. It is helpful in understanding the function's usage patterns and can help you optimize the function's scaling and resource allocation.\nIn summary, the correct answer is B, Dead Letter Error, as it is not a valid AWS CloudWatch metric for AWS Lambda function.\n\n"
}, {
  "id" : 649,
  "question" : "You are creating a production mode Lambda function.\nDue to auditing compliance, your organization stated that production-grade code must not be modified during its execution unless the modification goes through a change process.\nFor that, you decided to publish a version for PROD, create an alias, and use the alias ARN for invoking the Lambda function.\nHowever, your organization stated the code should not run if the version is $LATEST.\nHow would you achieve this? (choose 2 options)\n",
  "answers" : [ {
    "id" : "3717d84014ae4d3f8aac41a09dfa1368",
    "option" : "getFunctionVersion from Context object.",
    "isCorrect" : "true"
  }, {
    "id" : "d051f4955f0e4f0f8cf6bbbb776c8680",
    "option" : "Get invokedLambdaARN from the event object and find out the version from it.",
    "isCorrect" : "false"
  }, {
    "id" : "af89fefefde0406b9061c7cf109dc35b",
    "option" : "Use the AWS_LAMBDA_FUNCTION_VERSION environment variable.",
    "isCorrect" : "true"
  }, {
    "id" : "e1b88a47923747b0b9f616c84487b725",
    "option" : "Use the AWS_LAMBDA_FUNCTION_ALIAS environment variable.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C.\nFor more information on environment variables available to Lambda functions, refer to documentation here.\nhttps://docs.aws.amazon.com/lambda/latest/dg/env_variables.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/tutorial-env_cli.html\nOption B is not correct.\nThere is no parameter in any event source containing the ARN of the invoked Lambda function.\nOption D is not correct.\nThis is no environment variable for ALIAS.\n\n\nTo achieve the requirement of not allowing the code to run if the version is $LATEST, the following two options can be used:\nOption A: Get function version from Context object In this option, the getFunctionVersion method of the Context object can be used to retrieve the version of the Lambda function that is currently running. If the version is $LATEST, an error message can be thrown, and the execution can be stopped. This method returns the version number as a string.\nOption C: Use the AWS_LAMBDA_FUNCTION_VERSION environment variable In this option, the AWS_LAMBDA_FUNCTION_VERSION environment variable can be used to retrieve the version of the Lambda function that is currently running. This environment variable is set by AWS Lambda and is available to the Lambda function at runtime. If the value of this variable is $LATEST, an error message can be thrown, and the execution can be stopped.\nOption B and D are not correct options for this requirement. The Get invokedLambdaARN method in Option B returns the ARN of the Lambda function that triggered the current invocation, which does not provide the information about the version of the function. The AWS_LAMBDA_FUNCTION_ALIAS environment variable in Option D is used to retrieve the alias of the Lambda function that is currently running, which is not relevant for this requirement.\nIn summary, the two options to achieve the requirement of not allowing the code to run if the version is $LATEST are to use the getFunctionVersion method of the Context object or the AWS_LAMBDA_FUNCTION_VERSION environment variable.\n\n"
}, {
  "id" : 650,
  "question" : "Which of the following statement is not true with respect to the default retry behavior of the AWS Lambda function?\n",
  "answers" : [ {
    "id" : "daf717a3b3cc4b16b92c145069fe35d4",
    "option" : "With synchronous invocation, the invoking application receives a 429 error and is responsible for retries.",
    "isCorrect" : "true"
  }, {
    "id" : "ec654f19c70b4de3ac2ff91f5cf4a5f3",
    "option" : "With asynchronous invocation, If the function returns an error, Lambda attempts to run it two more times.",
    "isCorrect" : "false"
  }, {
    "id" : "3bf1b5a41c3e4c71923b32628eb738ad",
    "option" : "With Poll-based (or pull model) event sources that are stream-based, when a Lambda function invocation fails, AWS Lambda attempts to process the erring batch of records until the time the data expires, which can be up to seven days.",
    "isCorrect" : "false"
  }, {
    "id" : "215d58749eab464a94f85d2949004025",
    "option" : "With Poll-based event sources that are not stream-based, if the invocation fails or times out, the message will be returned to the queue and will be available for invocation once the Visibility Timeout period expires.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nSynchronous invocation - Lambda includes the\nFunctionError.\nfield in the response body, with details about the error in the\nX-Amz-Function-Error.\nheader.\nThe status code is 200 for function errors.\nEvent sources that aren't stream-based - Some of these event sources are set up to invoke a Lambda function synchronously, and others invoke it asynchronously.\nAccordingly, exceptions are handled as follows:\nSynchronous invocation - Lambda includes the\nFunctionError.\nfield in the response body, with details about the error in the\nX-Amz-Function-Error.\nheader.\nThe status code is 200 for function errors.\nLambda only returns error status codes if there is an issue with the request, function, or permissions that prevent the handler from processing the event.\nSee Invoke Errors for details.\nAsynchronous invocation - Lambda retries function errors twice.\nIf the function doesn't have enough capacity to handle all incoming requests, events might wait in the queue for hours or days to be sent to the function.\nYou can configure a dead-letter queue on the function to capture events that weren't successfully processed.\nFor more information, see Asynchronous invocation.\nLambda manages the function's asynchronous event queue and attempts to retry on errors.\nIf the function returns an error, Lambda attempts to run it two more times, with a one-minute wait between the first two attempts, and two minutes between the second and third attempts.\nFunction errors include errors returned by the function's code and errors returned by the function's runtime, such as timeouts.\nReferences:\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html\nhttps://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-max-retry-attempts-event-age-asynchronous-invocations/\nhttps://docs.aws.amazon.com/lambda/latest/dg/invocation-sync.html\n\nSure, I'd be happy to explain the default retry behavior of AWS Lambda in detail and which statement is not true.\nAWS Lambda provides automatic retry behavior for function invocations that result in certain errors. The default retry behavior varies depending on the invocation type and the event source that triggered the function.\nLet's examine each statement provided in the question:\nA. With synchronous invocation, the invoking application receives a 429 error and is responsible for retries.\nThis statement is true. With synchronous invocation, if a function returns a 429 error, it indicates that the function has reached its concurrency limit. In this case, the invoking application receives the 429 error and is responsible for retrying the invocation.\nB. With asynchronous invocation, If the function returns an error, Lambda attempts to run it two more times.\nThis statement is also true. With asynchronous invocation, if a function returns an error, Lambda automatically retries the invocation twice. If the function still fails after the third attempt, the event is discarded and the function's error is logged.\nC. With Poll-based (or pull model) event sources that are stream-based, when a Lambda function invocation fails, AWS Lambda attempts to process the erring batch of records until the time the data expires, which can be up to seven days.\nThis statement is true. With stream-based event sources, such as Amazon Kinesis or DynamoDB Streams, Lambda processes records in batches. If a batch of records fails to invoke the function successfully, AWS Lambda retries the batch until the data expires, which can be up to seven days.\nD. With Poll-based event sources that are not stream-based, if the invocation fails or times out, the message will be returned to the queue and will be available for invocation once the Visibility Timeout period expires.\nThis statement is the one that is NOT true. Poll-based event sources that are not stream-based, such as Amazon Simple Queue Service (SQS), follow a different retry behavior. If a function invocation fails or times out, the message remains in the queue and becomes available for another invocation after the Visibility Timeout period expires. It is up to the application to handle the retries in this case.\nSo, to sum up, the statement that is not true with respect to the default retry behavior of AWS Lambda is:\nD. With Poll-based event sources that are not stream-based, if the invocation fails or times out, the message will be returned to the queue and will be available for invocation once the Visibility Timeout period expires.\n\n"
}, {
  "id" : 651,
  "question" : "Which of the following statements are correct with respect to the instance store and EBS volume? Please select 2 correct options.\n",
  "answers" : [ {
    "id" : "820ea2c7ded245389167d54a5a564ee2",
    "option" : "Instance store backed EC2 instances will persist storage across instance stop, terminate, and failures.",
    "isCorrect" : "false"
  }, {
    "id" : "80724b4b20c44dceaab8c6b662665363",
    "option" : "EBS backed EC2 instances can persist storage across instance stop, terminate, and failures.",
    "isCorrect" : "true"
  }, {
    "id" : "c4de111b46c4487f9e1fa596183549b6",
    "option" : "Instance store backed EC2 instance will persist storage only during instance stop and start.",
    "isCorrect" : "false"
  }, {
    "id" : "342368a04ba54e1dad89e803c25fb9e6",
    "option" : "You cannot add instance store volumes once the EC2 instance is launched.",
    "isCorrect" : "true"
  }, {
    "id" : "d840d0d8e0e34da78eaa58200190ce3d",
    "option" : "All available EC2 instance types support instance store and EBS volumes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B, D.\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/InstanceStorage.html\nAmazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances.\nEBS volumes are highly available and reliable storage volumes attached to any running instance in the same Availability Zone.\nEBS volumes attached to an EC2 instance are exposed as storage volumes that persist independently from the life of the instance.\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AmazonEBS.html\nOption A is not correct.\nOption B is correct.\nOption C is not correct.\nInstance store persists during reboots, not during the stop and start of the instance.\nOption D is correct.\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/add-instance-store-\nOption E is not correct.\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/InstanceStorage.html#instance-\n\n\nInstance store and EBS volume are two types of storage options available for Amazon Elastic Compute Cloud (EC2) instances in Amazon Web Services (AWS). Here's a detailed explanation of the statements given in the question:\nA. Instance store backed EC2 instances will persist storage across instance stop, terminate, and failures.\nThis statement is incorrect. Instance store volumes are physically attached to the host computer that is running the EC2 instance. As a result, the data stored on instance store volumes is lost when the instance is stopped, terminated, or fails. Therefore, if you need to persist data, you should use an EBS volume instead.\nB. EBS backed EC2 instances can persist storage across instance stop, terminate, and failures.\nThis statement is correct. EBS volumes are network-attached storage devices that persist data independently of the EC2 instance's lifecycle. Therefore, data stored on EBS volumes persists even when the instance is stopped, terminated, or fails.\nC. Instance store backed EC2 instance will persist storage only during instance stop and start.\nThis statement is partially correct. Instance store volumes can persist data across instance stops and starts, but not across instance terminations or failures.\nD. You cannot add instance store volumes once the EC2 instance is launched.\nThis statement is correct. Once an EC2 instance is launched, you cannot add instance store volumes to it. Instance store volumes are physically attached to the host computer, so they must be specified at instance launch time.\nE. All available EC2 instance types support instance store and EBS volumes.\nThis statement is incorrect. Not all EC2 instance types support instance store volumes. Instance store volumes are available only on certain instance types, such as the C5d and M5d instances. However, all EC2 instance types support EBS volumes.\nIn summary, the correct statements in the question are B and D. EBS backed EC2 instances can persist storage across instance stop, terminate, and failures, and you cannot add instance store volumes once the EC2 instance is launched.\n\n"
}, {
  "id" : 652,
  "question" : "Your organization is planning to build a BigData project on AWS.\nThey need high data transfer rates for huge workloads to stream through with better performance.They are also looking for a cost-effective solution.\nWhich EBS storage type would you choose in this scenario?\n",
  "answers" : [ {
    "id" : "5122a04dd5294c12b6e58ecdd8bd46a4",
    "option" : "General Purpose SSD",
    "isCorrect" : "false"
  }, {
    "id" : "f56d323744af4da08e48d124716eb756",
    "option" : "Provisioned IOPS SSD",
    "isCorrect" : "false"
  }, {
    "id" : "79a47140c9474883aa828e8793db04b1",
    "option" : "Throughput Optimized HDD",
    "isCorrect" : "true"
  }, {
    "id" : "3c9e1e85e9cc46969eee02f8b6d13476",
    "option" : "Cold HDD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nAmazon EBS provides the following volume types, which differ in performance characteristics and price, so that you can tailor your storage performance and cost to the needs of your applications.\nThe volumes types fall into two categories:\nSSD-backed volumes optimized for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS.\nHDD-backed volumes optimized for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS.\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EBSVolumeTypes.html\n\n\nFor a BigData project on AWS that requires high data transfer rates for huge workloads with better performance, and cost-effectiveness, the recommended EBS storage type would be the \"Throughput Optimized HDD\" (C).\nThroughput Optimized HDDs are designed to provide low-cost storage with high throughput and are optimized for frequently accessed, large streaming workloads. This storage type is ideal for BigData workloads such as log processing, data warehousing, and big data analytics that require high-performance and low-cost storage.\nThe \"General Purpose SSD\" (A) is best suited for small to medium-sized workloads with moderate I/O requirements. Provisioned IOPS SSD (B) is designed for workloads that require consistent and high IOPS performance, such as large databases, NoSQL databases, and transactional workloads. These two options may not be cost-effective for the BigData project since they are more expensive than the Throughput Optimized HDD.\n\"Cold HDD\" (D) is designed for infrequent access and long-term storage, making it a poor choice for BigData workloads that require frequent access and high-performance storage.\nTherefore, the Throughput Optimized HDD is the most suitable and cost-effective option for a BigData project with high data transfer rates, huge workloads, and better performance.\n\n"
}, {
  "id" : 653,
  "question" : "You are working for a data management company which uses AWS platform to manage the data for various customers.\nThey are using AWS EBS backed EC2 instance with â€œDelete EBS volume on terminationâ€ checked.\nEC2 instances are used to run datastreamingapplicationwhich generates logs and are stored on EBS volumes.\nThe log files are critical for auditing purposes.\nHow would you protect the data stored on EBS volumes from accidental terminations of EC2 instances?\n",
  "answers" : [ {
    "id" : "483b324dfd574403936192f31a9d0d9a",
    "option" : "Every EBS volume will have a daily EBS snapshot created automatically by AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "d7a69c330ac3479aafc66cacbfb658ef",
    "option" : "Setup a Data LifeCycle Manager policy scheduler to create EBS snapshots for your EBS volumes.",
    "isCorrect" : "true"
  }, {
    "id" : "be04ff4c4c3e42a2b5e8959e0d9d209f",
    "option" : "When EC2 instance is terminated, it automatically creates a snapshot of EBS volume and then deletes the EBS volume.",
    "isCorrect" : "false"
  }, {
    "id" : "3089b9766acc4112a11dcd2e67eff29e",
    "option" : "Write a custom script on your EC2 instance and schedule it to back up the data onto AWS S3.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nYou can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.\nSnapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html\n\n\nThe correct answer is B. Setup a Data Lifecycle Manager policy scheduler to create EBS snapshots for your EBS volumes.\nExplanation:\nIn this scenario, the customer is using AWS EBS-backed EC2 instances with the \"Delete EBS volume on termination\" option checked. This means that when an EC2 instance is terminated, the associated EBS volume will also be deleted. However, the log files generated by the data streaming application are critical for auditing purposes and must be protected from accidental terminations.\nOption A suggests creating a daily EBS snapshot of every EBS volume automatically by AWS. While this would create a backup of the data, it is not a reliable solution to prevent accidental terminations. If an EC2 instance is terminated before the scheduled snapshot is taken, then the data would still be lost.\nOption C suggests that when an EC2 instance is terminated, it automatically creates a snapshot of the associated EBS volume and then deletes the EBS volume. However, this is not true. When an EC2 instance is terminated, the associated EBS volume is deleted without creating a snapshot.\nOption D suggests writing a custom script on the EC2 instance to back up the data onto AWS S3. While this would create a backup of the data, it is not a reliable solution to prevent accidental terminations. If the EC2 instance is terminated before the backup script can run, then the data would still be lost.\nOption B suggests setting up a Data Lifecycle Manager policy scheduler to create EBS snapshots for the EBS volumes. This is the correct solution to protect the data stored on the EBS volumes from accidental terminations. Data Lifecycle Manager is a service that allows you to create automated policies to manage the lifecycle of your AWS resources. You can create a policy to take EBS snapshots of the EBS volumes at regular intervals and retain them for a specified period. This way, even if an EC2 instance is terminated, the data on the EBS volume is protected by the snapshot, which can be used to restore the data in case of accidental terminations.\nTherefore, the correct answer is B. Setup a Data Lifecycle Manager policy scheduler to create EBS snapshots for your EBS volumes.\n\n"
}, {
  "id" : 654,
  "question" : "Which of the following is an action you cannot perform on an EBS snapshot?\n",
  "answers" : [ {
    "id" : "1c7d949a05724565805f6e7171a1d776",
    "option" : "Create Image from snapshot.",
    "isCorrect" : "false"
  }, {
    "id" : "53d3aecc82824d1b90d17a654b232af5",
    "option" : "Create EBS volume from snapshot.",
    "isCorrect" : "false"
  }, {
    "id" : "21720f9897534ba3bf530dd7e227acf8",
    "option" : "Share a snapshot with another AWS account.",
    "isCorrect" : "false"
  }, {
    "id" : "639530848cce40399f06d7128d937ced",
    "option" : "Provide permissions to snapshots based on AWS Account Names.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is a snapshot action.\n\nOption B is a snapshot action.\nOption C is a snapshot action.\nOption D is the correct answer.\nWe can give permissions to snapshots based on AWS Account Numbers and not based on AWS Account names.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modifying-snapshot-permissions.html\n\n\nAnswer: A. Create Image from snapshot.\nExplanation:\nEBS snapshots are point-in-time copies of an EBS volume. They can be used to create new EBS volumes or to restore an existing volume to a previous state. Snapshots are stored in Amazon S3 and can be used to migrate data between regions, copy data across accounts, and protect data for long-term retention.\nThe following actions can be performed on an EBS snapshot:\nA. Create EBS volume from snapshot: You can create a new EBS volume from a snapshot. The new volume will be an exact copy of the original volume at the time the snapshot was taken.\nB. Share a snapshot with another AWS account: You can share a snapshot with another AWS account. The other account will be able to create a new volume from the snapshot or use it to launch an EC2 instance.\nC. Provide permissions to snapshots based on AWS Account Names: You can provide permissions to snapshots based on AWS Account Names. This allows you to control which AWS accounts can access the snapshot and what actions they can perform on it.\nHowever, you cannot create an Amazon Machine Image (AMI) directly from an EBS snapshot. An AMI is a pre-configured virtual machine image, which includes the operating system, application server, and any additional software required to run the application. To create an AMI, you need to launch an EC2 instance from the snapshot and then create an image of the instance.\nTherefore, the correct answer is A. Create Image from snapshot.\n\n"
}, {
  "id" : 655,
  "question" : "You are working as an AWS architect in your organization.\nAn application is being developed on an AWS EC2 instance and would need a local volume with low latency to handle database workloads.\nThey figured out the Provisioned IOPS SSD volume type suits best.\nHowever, when the application team is launching an EC2 instance, they found an option named â€œEBS-optimized.â€ They reached out to you asking the purpose of EBS optimized instances.\nWhat do you suggest?\n",
  "answers" : [ {
    "id" : "0f9bebdc6d42436883df2149b2877f22",
    "option" : "Amazon EBSâ€“optimized instance provides additional, dedicated capacity for Amazon EBS I/O.",
    "isCorrect" : "true"
  }, {
    "id" : "c26b511712964d66bce3de0c7a150702",
    "option" : "Amazon EBS-optimized instance comes with instance store ephemeral storage which provides faster throughput.",
    "isCorrect" : "false"
  }, {
    "id" : "7d3f0bae8dcc429cb48951d4adb882cb",
    "option" : "EBS-optimized is a configuration on the EBS volume, not an option on EC2 instance.",
    "isCorrect" : "false"
  }, {
    "id" : "475f56360b0a4812876e6bc557e8be87",
    "option" : "Amazon EBS-optimized instances cannot have Provisioned IOPS SSD volume types. They only work with General Purpose SSD, Throughput optimized HDD, Cold HDD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nAn Amazon EBS-optimized instance uses an optimized configuration stack and provides additional, dedicated capacity for Amazon EBS I/O.\nThis optimization provides the best performance for your EBS volumes by minimizing contention between Amazon EBS I/O and other traffic from your instance.\nEBS-optimized instances deliver dedicated bandwidth to Amazon EBS, with options between 425 Mbps and 14,000 Mbps, depending on the instance type you use.\nWhen attached to an EBS-optimized instance, General Purpose SSD (gp2) volumes are designed to deliver within 10% of their baseline and burst performance 99% of the time in a given year.\nProvisioned IOPS SSD (io1) volumes are designed to deliver within 10% of their provisioned performance 99.9% of the time in a given year.\nBoth Throughput Optimized HDD (st1) and Cold HDD (sc1) guarantee performance consistency of 90% of burst throughput 99% of the time in a given year.\nNon- compliant periods are approximately uniformly distributed, targeting 99% of the expected total throughput each hour.\nFor more information on EBS-optimized instances, refer to the documentation here.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSOptimized.html\n\nSure, I'd be happy to help!\nWhen launching an EC2 instance, you may notice an option called \"EBS-optimized\" in the configuration. This option is related to the performance of Elastic Block Store (EBS) volumes attached to your EC2 instance.\nEBS is a service that provides block-level storage volumes for use with EC2 instances. EBS volumes can be attached to EC2 instances and can be used like a physical hard drive. EBS volumes can be either General Purpose SSD, Provisioned IOPS SSD, Throughput Optimized HDD, or Cold HDD, depending on your specific requirements.\nThe \"EBS-optimized\" option in EC2 configuration provides additional, dedicated capacity for EBS I/O. This means that the EC2 instance is optimized to handle EBS volumes and their workloads. This optimization is achieved through the use of additional network capacity dedicated to EBS I/O.\nBy selecting the \"EBS-optimized\" option, you can improve the performance of your EBS volumes and reduce the impact of contention with other network traffic. This option is particularly useful if your application has high I/O demands or requires low latency access to EBS volumes.\nHowever, it's worth noting that selecting the \"EBS-optimized\" option does not affect the type of EBS volume you can use. You can still choose between General Purpose SSD, Provisioned IOPS SSD, Throughput Optimized HDD, or Cold HDD volumes based on your specific requirements.\nIn summary, the correct answer is A: Amazon EBS-optimized instance provides additional, dedicated capacity for Amazon EBS I/O.\n\n"
}, {
  "id" : 656,
  "question" : "Which of the following is not true with respect to EBS volume encryption?\n",
  "answers" : [ {
    "id" : "5f16843ccf55448b8558d84ed4569fce",
    "option" : "Encrypts data at rest inside the volume.",
    "isCorrect" : "false"
  }, {
    "id" : "348352b33dad4ddba00ee4275a8ee819",
    "option" : "Encrypts all data moving between the volume and the instance.",
    "isCorrect" : "false"
  }, {
    "id" : "181dc11007634118ae84fcadb4fd984b",
    "option" : "Encrypts all snapshots created from the volume.",
    "isCorrect" : "false"
  }, {
    "id" : "120622f82d5c4b1c9640a3811191bb97",
    "option" : "Encrypted EBS volumes are not supported on all current generation instance types.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOptions A, B, C are true statements but the question asks for the false statement.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html\nOption D is a false statement, so it is correct.\nSupported instance types:\nAmazon EBS encryption is available on all current generation instance types and the following previous generation instance types: A1, C3, cr1.8xlarge, G2, I2, M3, and R3.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_s%20upported_instances\n\n\nEBS volume encryption is a feature of Amazon Elastic Block Store (EBS) that enables you to encrypt data at rest inside the volume. This feature provides an additional layer of security for your data by encrypting all data stored on an EBS volume.\nThe correct answer to the question is D. Encrypted EBS volumes are not supported on all current generation instance types.\nExplanation of each answer:\nA. Encrypts data at rest inside the volume: This statement is true. EBS volume encryption encrypts all data stored on an EBS volume at rest using the industry-standard AES-256 encryption algorithm. This encryption provides data protection even in the case of unauthorized access to the underlying storage.\nB. Encrypts all data moving between the volume and the instance: This statement is false. EBS volume encryption only encrypts data at rest inside the volume. It does not encrypt data in transit between the volume and the instance. For encrypting data in transit, you can use SSL/TLS encryption or IPsec VPN.\nC. Encrypts all snapshots created from the volume: This statement is true. When you create a snapshot of an encrypted EBS volume, the snapshot is also encrypted using the same encryption key as the original volume. You can also copy encrypted snapshots to another region, and the snapshot will remain encrypted.\nD. Encrypted EBS volumes are not supported on all current generation instance types: This statement is true. Not all current generation instance types support encrypted EBS volumes. You should check the documentation for the specific instance type you plan to use to determine whether it supports encrypted volumes. If the instance type does not support encrypted volumes, you can still use encrypted snapshots to store encrypted data.\nIn summary, EBS volume encryption encrypts data at rest inside the volume, encrypts snapshots created from the volume, but does not encrypt data in transit between the volume and the instance. Encrypted EBS volumes are not supported on all current generation instance types.\n\n"
}, {
  "id" : 657,
  "question" : "Which of the following statements is true with respect to encryption?\n",
  "answers" : [ {
    "id" : "da8c482b7cf34c09b18c5e1af5263f33",
    "option" : "You can enable encryption when creating a snapshot from an unencrypted volume.",
    "isCorrect" : "false"
  }, {
    "id" : "f050bdb5f05b41858fb0d4a49ee887bb",
    "option" : "You can enable encryption while copying a snapshot from an unencrypted snapshot.",
    "isCorrect" : "true"
  }, {
    "id" : "419867aa152f408e8d068a1ea69369eb",
    "option" : "You can disable encryption while creating a snapshot from encrypted volume.",
    "isCorrect" : "false"
  }, {
    "id" : "f487c844ecd24e3388e906f0aef4170a",
    "option" : "You can disable encryption while copying a snapshot from an encrypted snapshot.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_c\nOption Cand option A is not correct.\nOption A is wrong because You cannot enable Encryption when creating a snapshot from an unencrypted volume.\nYou can only view whether the volume is encrypted or not.\nOption D is not correct.\nWe cannot disable encryption while performing a copy of a snapshot.\n\n\nOut of the given options, the correct statement with respect to encryption is A. You can enable encryption when creating a snapshot from an unencrypted volume.\nHere is a detailed explanation for each option:\nA. You can enable encryption when creating a snapshot from an unencrypted volume: This statement is true. Amazon Web Services (AWS) allows you to create an encrypted snapshot from an unencrypted volume. You can choose to encrypt the snapshot at the time of creation. AWS uses AWS KMS (Key Management Service) to encrypt the snapshot. When you create an encrypted snapshot, AWS KMS generates a unique data key for that snapshot, which is then used to encrypt the data in the snapshot. You can also choose to use your own KMS key to encrypt the snapshot.\nB. You can enable encryption while copying a snapshot from an unencrypted snapshot: This statement is false. Once a snapshot is created, you cannot enable encryption for it. If you want to create an encrypted snapshot, you need to create a new snapshot from the unencrypted volume and choose to encrypt it at the time of creation.\nC. You can disable encryption while creating a snapshot from an encrypted volume: This statement is false. Once a volume is encrypted, all snapshots created from that volume are also encrypted. You cannot disable encryption for an encrypted volume or its snapshots.\nD. You can disable encryption while copying a snapshot from an encrypted snapshot: This statement is false. Once a snapshot is encrypted, all subsequent snapshots created from it are also encrypted. You cannot disable encryption for an encrypted snapshot or its subsequent snapshots.\nIn summary, the only true statement is that you can enable encryption when creating a snapshot from an unencrypted volume.\n\n"
}, {
  "id" : 658,
  "question" : "You are working as an architect in your organization's Linux environment.\nAn application team uses EBS volume for their database workloads which would need a high throughput, low latency and maximum I/O.\nThey chose Provisioned IOPS EBS volume for better performance.\nHowever, after setting up the entire application, they notice the performance is not up to the mark.\nWhich performance tuning mechanisms would you suggest? Please choose 3 correct options.\n",
  "answers" : [ {
    "id" : "38a0427fb5be472f906e96f4579ab71e",
    "option" : "Use EBS-Optimized Instances.",
    "isCorrect" : "true"
  }, {
    "id" : "a86a7fe1719a49e48cb3d6cece865aba",
    "option" : "Use Throughput optimized HDD volume types for low latency and maximum I/O.",
    "isCorrect" : "false"
  }, {
    "id" : "6e9efcf7e18f44f4beadfc872721a324",
    "option" : "Use a Modern Linux Kernel.",
    "isCorrect" : "true"
  }, {
    "id" : "2126a350fa2b411590ba21bfd96744f7",
    "option" : "Use RAID 0 to Maximize Utilization of Instance Resources.",
    "isCorrect" : "true"
  }, {
    "id" : "5e765f8fed984b49a2a47ea54ed8094a",
    "option" : "Performance of Provisioned IOPS SSD volume increase with volume size. Increase EBS volume size.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C, D\nOption A is correct.\nOption B is not correct.\nProvisioned IOPS SSD volume type is more suitable for database workloads.\nOption C is correct.\nOption D is correct.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSPerformance.html#tips\nOption E is not correct.\nThe statement is applicable to General Purpose SSD, not for Provisioned IOPS SSD.https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_gp2\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#EBSVolumeTypes_piops\n\n\nSure, I'd be happy to provide a detailed explanation of the correct options for performance tuning in this scenario.\nThe application team has already chosen Provisioned IOPS EBS volumes to improve performance for their database workloads, but they are experiencing suboptimal performance even after setting up the entire application. Here are the three correct options for performance tuning in this scenario:\nOption A: Use EBS-Optimized Instances EBS-optimized instances are instances that are optimized for using EBS volumes and provide dedicated throughput between Amazon EC2 instances and EBS volumes. This can help to improve the performance of EBS volumes by ensuring that there is sufficient network bandwidth available for EBS I/O operations. Therefore, using EBS-optimized instances can help improve the performance of Provisioned IOPS EBS volumes in this scenario.\nOption D: Use RAID 0 to Maximize Utilization of Instance Resources Using RAID 0 can help maximize the utilization of instance resources and improve the performance of EBS volumes. RAID 0 distributes data across multiple disks, which can help to increase the throughput of the storage system. By using RAID 0 with multiple EBS volumes, the application team can achieve higher performance and better utilize the resources of their EC2 instances.\nOption E: Performance of Provisioned IOPS SSD volume increase with volume size. Increase EBS volume size. The performance of Provisioned IOPS SSD volumes increases with volume size. Provisioned IOPS EBS volumes are designed to provide predictable performance, but their performance is also limited by their size. By increasing the size of the EBS volume, the application team can improve the performance of the volume. Therefore, increasing the size of the EBS volume can help improve the performance of the database workloads in this scenario.\nOption B: Use Throughput optimized HDD volume types for low latency and maximum I/O. This option is not correct in this scenario because the application team is already using Provisioned IOPS EBS volumes, which are designed to provide high throughput and low latency. Throughput optimized HDD volume types are designed for big data and other workloads that require high throughput, but they are not suitable for database workloads that require low latency and high I/O operations.\nOption C: Use a Modern Linux Kernel. This option is not correct in this scenario because the Linux kernel version used by the application team is not likely to be the cause of suboptimal performance for their database workloads on Provisioned IOPS EBS volumes. While a modern Linux kernel can help improve overall system performance and security, it is not directly related to the performance of EBS volumes.\nTherefore, the three correct options for performance tuning in this scenario are:\nUse EBS-Optimized Instances Use RAID 0 to Maximize Utilization of Instance Resources Increase EBS volume size.\n\n"
}, {
  "id" : 659,
  "question" : "Which of the following is not an AWS CloudWatch metric for EBS Volumes?\n",
  "answers" : [ {
    "id" : "e412672b8f584fdabdbc170812a52053",
    "option" : "VolumeReadBytes",
    "isCorrect" : "false"
  }, {
    "id" : "e97ee65abcbd4f28a1c5c2d910d35d60",
    "option" : "VolumeWriteOps",
    "isCorrect" : "false"
  }, {
    "id" : "32646da570694204ba5202ac9b54145c",
    "option" : "VolumeThroughputPercentage",
    "isCorrect" : "false"
  }, {
    "id" : "e1e3715eea0145e9ba2174848def316f",
    "option" : "VolumeRemainingSize.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-volume-status.html#using_cloudwatch_ebs\n\n\nThe correct answer is D. VolumeRemainingSize is not an AWS CloudWatch metric for EBS Volumes.\nExplanation:\nAmazon Elastic Block Store (Amazon EBS) provides block-level storage volumes for use with EC2 instances. AWS CloudWatch is a monitoring service that provides metrics and logs for resources in AWS.\nWhen monitoring Amazon EBS volumes, AWS CloudWatch provides the following metrics:\nA. VolumeReadBytes: The total number of bytes that have been read from the volume.\nB. VolumeWriteOps: The total number of write operations that have been performed on the volume.\nC. VolumeThroughputPercentage: The percentage of I/O operations per second (IOPS) delivered from the total IOPS provisioned for an EBS volume.\nD. VolumeRemainingSize: This is not an AWS CloudWatch metric for EBS Volumes. It is not possible to monitor the remaining size of an EBS volume using CloudWatch.\nIn summary, CloudWatch provides metrics for monitoring the I/O operations and throughput of EBS volumes but does not provide a metric for monitoring the remaining size of an EBS volume.\n\n"
}, {
  "id" : 660,
  "question" : "You are using AWS EC2 linux instance for log processing which would require high throughput.\nYou chose Throughput optimized HDD storage with 500 GB in size.\nYou deployed your application to production mode and it is running as expected.\nAfter a month, you see anincrease in log files and you are fast approaching the 500 GB size and running out of space onthe EBS volume.\nWhich of the following is a best approach to mitigate the situation with minimal configuration?\n",
  "answers" : [ {
    "id" : "8a0321f017244324b6d62fe5c3dfc42a",
    "option" : "Add a new EBS volume, mount on EC2 instance and configure your application accordingly.",
    "isCorrect" : "false"
  }, {
    "id" : "b99a59a4c0ff466fabc58837e0c0d63d",
    "option" : "Increase the size of the existing EBS volume.",
    "isCorrect" : "true"
  }, {
    "id" : "5b5fff525cfe43328caee34c82756c1b",
    "option" : "EBS volume size cannot be changed. Build purging logic for your old log files",
    "isCorrect" : "false"
  }, {
    "id" : "b6e02ca307064fa7b36c7957bfb30d96",
    "option" : "You can have only one EBS volume per instance. Snapshot existing EBS volume, detach current volume, create a new volume from snapshot with bigger size and attached to EC2 instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAlthough option A looks correct, it is not the best approach since it involves more configuration and changes to the application to point to a new EBS mount point.\nOption B is correct.\nYou can modify the volume and increase in volume size.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-modify-volume.html\nOption C is incorrect.\nThe reason for Option C to be wrong is, we can increase the volume size but the option mentioned about \"EBS volume size cannot be changed' is the reason.\nOption D is not correct.\nYou can attach multiple EBS volumes to the single EC2 instances.\n\n\nWhen using AWS EC2 instances for log processing, it is important to consider the storage requirements and make sure that enough space is available to store the log files. In this case, the instance was initially configured with a 500 GB Throughput Optimized HDD storage. However, after a month, the log files have increased, and the current storage is about to run out of space. To mitigate this situation, there are several options available.\nA. Add a new EBS volume, mount on EC2 instance and configure your application accordingly. This option involves adding a new EBS volume to the EC2 instance and configuring the application to use the new volume for storing log files. This approach can be a quick solution to the problem as it involves minimal configuration changes. However, it requires monitoring the space usage of both volumes to prevent running out of space again.\nB. Increase the size of the existing EBS volume. Another option is to increase the size of the existing EBS volume. This can be done by modifying the volume size from the AWS Management Console or using the AWS CLI. This option requires stopping the instance to modify the volume, which may result in some downtime. Additionally, it may not be possible to increase the size beyond a certain limit, depending on the instance type and EBS volume type.\nC. EBS volume size cannot be changed. Build purging logic for your old log files This option involves building purging logic to remove old log files and free up space on the existing EBS volume. While this option can help mitigate the situation with minimal configuration changes, it may not be a long-term solution. If the log files continue to increase in size, the instance may still run out of space.\nD. You can have only one EBS volume per instance. Snapshot existing EBS volume, detach current volume, create a new volume from snapshot with bigger size and attached to EC2 instance. This option involves taking a snapshot of the existing EBS volume, detaching the current volume, creating a new volume from the snapshot with a larger size, and attaching the new volume to the EC2 instance. This approach requires some downtime to detach and attach the volumes but can be a reliable long-term solution.\nIn summary, the best approach to mitigate the situation with minimal configuration changes would be to add a new EBS volume and configure the application accordingly (option A). However, if a long-term solution is needed, it would be best to take a snapshot of the existing EBS volume and create a new volume with a larger size (option D).\n\n"
}, {
  "id" : 661,
  "question" : "Company ABC has an AWS setup and planning to use Amazon SQS for queuing messages.\nThe design is such that two applications will receive the same message in the queue and process it.\nOnce applications would have read the message, it should be deleted.\nHowever, when the 2nd application makes the ReceiveMessage API call, the message is not getting returned.\nWhich of the following could be reasons? (Choose 2 options)\n",
  "answers" : [ {
    "id" : "a0268454a6aa46c88833411da560a825",
    "option" : "Application 2 is making a call before Visibility Timeout elapsed which was set by application 1 ReceiveMessage call.",
    "isCorrect" : "true"
  }, {
    "id" : "344ab168139146d1aa8d06a9594815ec",
    "option" : "Amazon SQS deletes the message once it has been responded via the ReceiveMessage call from Application 1.",
    "isCorrect" : "false"
  }, {
    "id" : "020f9ce702bb4455b47aed9edc90e6bb",
    "option" : "Application 1 had deleted the message after it had been processed before Visibility Timeout elapsed.",
    "isCorrect" : "true"
  }, {
    "id" : "2105717def0245e987af9433b48952e9",
    "option" : "Application 2 does not have access to the message it is trying to receive.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, C.\nOption A is correct.\nImmediately after a message is received, it remains in the queue.\nTo prevent other consumers from processing the message again, Amazon SQS sets a visibility timeout, a period of time during which Amazon SQS prevents other consumers from receiving and processing the message.\nThe default visibility timeout for a message in 30 seconds.\nThe minimum is 0 seconds.\nThe maximum is 12 hours.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\nOption B is not correct.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html\nOption C is correct.\nOnce the message is deleted from Amazon SQS Queue, it will not be available anymore.\nOption D is not correct.\nPermission exists on the Queue level, not on the message level.\nFor more information on SQS actions, refer to documentation here.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/list_amazonsqs.html\n\n\nThe correct answers are A and C.\nExplanation:\nAmazon Simple Queue Service (SQS) is a fully-managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It allows multiple applications to read messages from a single queue, but the following two issues may occur:\nA. Application 2 is making a call before Visibility Timeout elapsed which was set by application 1 ReceiveMessage call.\nWhen an application receives a message from an SQS queue using the ReceiveMessage API call, it sets a visibility timeout for that message. The visibility timeout is the amount of time that the message remains hidden from other applications after an application receives it. This timeout gives the first application time to process the message without the second application receiving it.\nIf the second application tries to receive the message before the visibility timeout has expired, it will not be able to receive the message, and the message will not be returned to the queue. In this case, the second application should wait until the visibility timeout has expired before attempting to receive the message.\nB. Amazon SQS deletes the message once it has been responded via the ReceiveMessage call from Application 1.\nAmazon SQS does not automatically delete a message when an application receives it. Instead, the application must explicitly delete the message from the queue after it has processed it. Therefore, this answer is incorrect.\nC. Application 1 had deleted the message after it had been processed before Visibility Timeout elapsed.\nIf the first application deletes the message before the visibility timeout has expired, the message will not be visible to the second application, and the second application will not be able to receive the message. In this case, the second application should wait until the visibility timeout has expired before attempting to receive the message.\nD. Application 2 does not have access to the message it is trying to receive.\nIf the second application does not have access to the message it is trying to receive, it will not be able to receive the message. However, this situation can occur due to permission issues, and it is not relevant to the scenario described in the question. Therefore, this answer is incorrect.\nIn summary, both options A and C could be reasons why the second application is not receiving the message, and the correct answers are A and C.\n\n"
}, {
  "id" : 662,
  "question" : "Your organization is using Amazon SQS as an enterprise message queuing platform.\n100s of applications read the queues every few seconds to process the messages and delete them as soon as they are written into the queues.\nLooking at the number of requests being sent to Amazon SQS APIs, your management is concerned about the pricing that will be incurred.\nAs an architect, how would you reduce pricing without compromising on time in this scenario? Please select 2 correct answers.\n",
  "answers" : [ {
    "id" : "ead80ab15a0242a1bc2c75a8d4f297d6",
    "option" : "Once successfully written, Amazon SQS messages are only available after 1 minute. Ask applications to increase the delay between calls to 1 minute. This reduces the number of API calls made.",
    "isCorrect" : "false"
  }, {
    "id" : "8e7545c0cbdd4c83b27cdc657955f5c9",
    "option" : "Use Amazon SQS Long Polling.",
    "isCorrect" : "true"
  }, {
    "id" : "3b5397c977ce4abc8490123ae6dfff8a",
    "option" : "Send DeleteMessage requests in batch.",
    "isCorrect" : "true"
  }, {
    "id" : "9d30f061c71c4ca0b2e4d55e8056881d",
    "option" : "Use Amazon SQS Short Polling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B, C.\nOption A is incorrect.\nThere is no such limitation on AWS SQS queues.\nOption B is correct.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html\nOption C is correct.\nOption D is not correct.\nShort polling does not guarantee a return of the message, and you have to repeat the call until you receive the message.\nThat does not reduce any costs.\nhttps://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html\n\n\nAmazon Simple Queue Service (Amazon SQS) is a managed message queuing service that enables decoupling and scaling of distributed systems and applications. It is a cost-effective messaging service that can be used to transmit any volume of data, at any level of throughput, without losing messages or requiring other services to be available all the time.\nIn the given scenario, where hundreds of applications are reading and processing messages from SQS queues every few seconds, it is natural to expect a large number of requests being sent to Amazon SQS APIs, leading to cost concerns. To reduce pricing without compromising on time, the following options can be considered:\nA. Once successfully written, Amazon SQS messages are only available after 1 minute. Ask applications to increase the delay between calls to 1 minute. This reduces the number of API calls made.\nThis option is not suitable in the given scenario as the applications need to process messages frequently and deleting messages as soon as they are processed is crucial to avoid message duplication and other issues. Increasing the delay between calls to 1 minute may lead to slower processing and reduced system performance.\nB. Use Amazon SQS Long Polling.\nLong Polling is a feature in Amazon SQS that allows applications to receive messages from a queue with reduced cost and increased efficiency. In Long Polling, an application can specify a WaitTimeSeconds parameter to receive messages only when they are available, reducing the number of empty responses and hence, the number of requests sent to SQS APIs. Long Polling can also reduce the cost by minimizing the use of Short Polling, which involves sending a request to SQS every few seconds to check if there are new messages.\nC. Send DeleteMessage requests in batch.\nIn the given scenario, applications are deleting messages from SQS queues as soon as they are processed. Instead of sending individual DeleteMessage requests for each message, the applications can send DeleteMessageBatch requests to delete multiple messages in a single request, thereby reducing the number of requests sent to SQS APIs and hence, the cost incurred.\nD. Use Amazon SQS Short Polling.\nShort Polling is the default mode of operation in Amazon SQS, where an application sends a request to SQS every few seconds to check if there are new messages in the queue. Short Polling can lead to increased cost and reduced efficiency as it results in empty responses when there are no new messages, leading to unnecessary API requests. Hence, using Short Polling is not recommended in the given scenario.\nTo conclude, options B and C are the correct answers to reduce pricing without compromising on time in the given scenario. Option A is not suitable, and option D is not recommended.\n\n"
}, {
  "id" : 663,
  "question" : "Which of the following statements is not correct with respect to AWS SQS?\n",
  "answers" : [ {
    "id" : "e07a68e9242d4e2294da56a74bda50c5",
    "option" : "Amazon SQS can trigger a lambda function.",
    "isCorrect" : "false"
  }, {
    "id" : "a305cdf8af7145b9992e599a761fa200",
    "option" : "To select the message to delete, use the ReceiptHandle of the message, not the MessageId which you receive when you send the message.",
    "isCorrect" : "false"
  }, {
    "id" : "4bc41699e15746f39972e73759fd3c7b",
    "option" : "Use dead letter queues to isolate messages that can`t be processed for later analysis.",
    "isCorrect" : "false"
  }, {
    "id" : "062d316f4e064d18b61ac37ea23b0de4",
    "option" : "All messages in the Amazon SQS queue are encrypted by default.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nNote: In the question, it is mentioned to select the wrong Option as the answer.\nHere.\nOption A, B and C are correct statements but not the correct options.\nOption A is a correct statement.\nOption B is the correct statement.\nOption C is the correct statement.\nOption D is not a correct statement.\nAmazon SQS does not encrypt messages by default.\nThe option needs to be selected by the customer to enable encryption on the Queue messages.\nhttps://aws.amazon.com/blogs/aws/new-server-side-encryption-for-amazon-simple-queue-service-sqs/\n\n\nThe correct answer is D. All messages in the Amazon SQS queue are encrypted by default.\nExplanation: Amazon SQS (Simple Queue Service) is a fully managed message queuing service provided by Amazon Web Services (AWS). It enables decoupling and scaling of distributed systems and applications by allowing them to communicate asynchronously through messages.\nA. Amazon SQS can trigger a Lambda function: Amazon SQS can be used to trigger AWS Lambda functions. AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. By configuring an SQS queue as an event source for Lambda, you can invoke a Lambda function to process messages in the queue automatically.\nB. Use ReceiptHandle to delete messages: When you send a message to an SQS queue, the service returns a MessageId that you can use to identify the message in the queue. However, when you want to delete a message from the queue, you need to use the ReceiptHandle that SQS returns with the message. The ReceiptHandle is a unique identifier that is associated with the act of receiving the message, and it is necessary to delete the message from the queue.\nC. Use Dead Letter Queues: A Dead Letter Queue (DLQ) is a queue where messages are sent when they can't be processed successfully by the primary queue. This can occur due to errors in the application code, lack of resources, or other issues. By using DLQ, you can isolate problematic messages for later analysis and take corrective action. Amazon SQS supports the use of DLQs to help you identify and diagnose the root cause of message processing failures.\nD. Messages in Amazon SQS are not encrypted by default: SQS messages are not encrypted by default. However, you can choose to encrypt your messages by enabling the Server-Side Encryption (SSE) feature on your SQS queue. This feature uses AWS Key Management Service (KMS) to encrypt your messages before storing them in the SQS queue.\nIn conclusion, the incorrect statement with respect to AWS SQS is D, as messages in the Amazon SQS queue are not encrypted by default.\n\n"
}, {
  "id" : 664,
  "question" : "Which of the following is not a feature of AWS Security Token Service?\n",
  "answers" : [ {
    "id" : "a61a8028be764f7b91a6f60463c632ca",
    "option" : "STS enables you to request temporary, limited-privilege credentials.",
    "isCorrect" : "false"
  }, {
    "id" : "f2d5eeb41dee4a1f88e77048e573a153",
    "option" : "STS enables users to assume role.",
    "isCorrect" : "false"
  }, {
    "id" : "2aa145917e79416ea48cbab32a0dde09",
    "option" : "STS generates Git Credentials for IAM users.",
    "isCorrect" : "true"
  }, {
    "id" : "4567fc17f7554bb1a1a213c0b6af07ea",
    "option" : "By default, AWS STS is available as a global service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOptions A, B, D are incorrect.\nOption A is a true statement.\nOption B is a true statement.\nSTS â€œAssumeRoleâ€ action will enable users to assume a role.https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html.\nOption C is a false statement, so correct option.\nWith Git credentials, you can generate a static user name and password in the Identity and Access Management (IAM) console that you can use to access AWS CodeCommit repositories from the command line, Git CLI, or any Git tool that supports HTTPS authentication.\nhttps://aws.amazon.com/blogs/devops/introducing-git-credentials-a-simple-way-to-connect-to-aws-codecommit-repositories-using-a-static-user-name-and-password/\nOption D is a true statement because by default, AWS Security Token Service (AWS STS) is available as a global service, and all AWS STS requests go to a single endpoint at\nhttps://sts.amazonaws.com\nGlobal requests map to the US East (N.\nVirginia) Region.\nAWS recommends using Regional AWS STS endpoints instead of the global endpoint to reduce latency, build in redundancy, and increase session token validity.\nhttps://docs.aws.amazon.com/STS/latest/APIReference/welcome.html\n\n\nAWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users). These credentials enable you to carry out authorized AWS operations for a limited period. This helps in securing your AWS resources by minimizing the scope of access and authorization.\nOption A is correct. STS enables you to request temporary, limited-privilege credentials. This is a key feature of STS that allows you to provide temporary access to users or services that require access to AWS resources. These credentials can be used to access AWS resources for a specified period of time, after which they expire automatically.\nOption B is correct. STS enables users to assume roles. This allows you to delegate access to AWS resources to IAM users, applications, or services that don't normally have access to those resources. When a user assumes a role, they are granted temporary security credentials that provide the permissions associated with the role.\nOption C is incorrect. STS does not generate Git Credentials for IAM users. Git Credentials are used to authenticate with Git repositories, and are not related to AWS IAM or STS.\nOption D is correct. By default, AWS STS is available as a global service. This means that you can use STS to generate temporary credentials for AWS resources in any region where the service is available.\nIn summary, Option C is not a feature of AWS Security Token Service.\n\n"
}, {
  "id" : 665,
  "question" : "Your organization AWS Setup has an AWS S3 bucket which stores confidential documents which can be only downloaded by users authenticated and authorized via your application.\nYou do not want to create IAM users for each of these users and as a best practice you have decided to generate AWS STS Federated User temporary credentials each time when a download request is made and then use the credentials to generate presigned URL and redirect user for download.\nHowever, when user is trying to access the presigned URL, they are getting Access Denied Error.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "6f656af19dba455f9d6ea4db0b942230",
    "option" : "AWS STS service must be given access in S3 bucket ACL.",
    "isCorrect" : "false"
  }, {
    "id" : "d78c4fd108cf49d78301f6f1995c9390",
    "option" : "IAM User used to generate Federated User credentials does not have access on S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "de620f3d19bb4f5b8b48413ecf9d7dbb",
    "option" : "IAM Role used to generate Federated User credentials does not have access on S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "4c7bd50c818e430c8e11dad04a35de19",
    "option" : "Your application must be whitelisted in AWS STS service to perform FederatedUser action.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is not a correct statement.\nOption B is correct.\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_GetFederationToken.html\nOption C is not correct.\nYou can generated FederatedUser credentials using an IAM User, not using an IAM Role.\nOption D is not a correct statement.\n\n\nThe most likely reason for the Access Denied Error is option C: IAM Role used to generate Federated User credentials does not have access on S3 bucket.\nWhen generating temporary security credentials using AWS Security Token Service (STS), you need to specify an IAM role that defines the permissions for the federated user. This IAM role must have the necessary permissions to access the S3 bucket that stores the confidential documents.\nHere are some steps to check if the IAM role has access to the S3 bucket:\nGo to the IAM console and select the IAM role that is used to generate the temporary security credentials. Check if the IAM role has a policy that allows access to the S3 bucket. The policy should have an action that grants access to the S3 bucket, such as \"s3:GetObject\", \"s3:PutObject\", or \"s3:ListBucket\". If the IAM role doesn't have a policy that grants access to the S3 bucket, you need to create a new policy or modify the existing policy to include the necessary permissions. After creating or modifying the policy, attach it to the IAM role. Test the presigned URL again to see if the Access Denied Error has been resolved.\nOption A (AWS STS service must be given access in S3 bucket ACL) is not correct because the ACL (Access Control List) of the S3 bucket only controls access to the bucket itself, not to the AWS STS service.\nOption B (IAM User used to generate Federated User credentials does not have access on S3 bucket) is not correct because IAM users are not used to generate temporary security credentials for federated users. IAM users are used to manage access to AWS resources, but they are not suitable for granting access to external users.\nOption D (Your application must be whitelisted in AWS STS service to perform FederatedUser action) is not correct because there is no requirement to whitelist an application in the AWS STS service. However, you do need to make sure that the application is configured to use the temporary security credentials generated by the AWS STS service.\n\n"
}, {
  "id" : 666,
  "question" : "Your organization has an AWS setup and planning to build Single Sign On for users to authenticate with on-premise Microsoft Active Directory Federation Services (ADFS) and let users login to AWS console using AWS STS Enterprise Identity Federation.\nWhich of the following API should be used with AWS STS service after you authenticate with your on-premise?\n",
  "answers" : [ {
    "id" : "0b85889edb814d168f10fff348543625",
    "option" : "AssumeRoleWithSAML",
    "isCorrect" : "true"
  }, {
    "id" : "14f6ea30fc6b4cf2a4ab29bca21280c6",
    "option" : "GetFederationToken",
    "isCorrect" : "false"
  }, {
    "id" : "34e7010c05994ec9a68797813f2eba59",
    "option" : "AssumeRoleWithWebIdentity",
    "isCorrect" : "false"
  }, {
    "id" : "8bd69eb565d9411baafd6a1009c4c397",
    "option" : "GetCallerIdentity.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nhttps://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithSAML.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html\n\n\nThe correct API that should be used with AWS STS service after authenticating with your on-premise Microsoft Active Directory Federation Services (ADFS) is AssumeRoleWithSAML. Therefore, the answer is A.\nHere's a detailed explanation:\nAWS Security Token Service (STS) is a web service that enables you to request temporary, limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users that you authenticate (federated users).\nSingle Sign-On (SSO) allows users to access multiple applications with a single set of credentials. AWS supports SSO through the use of Security Assertion Markup Language (SAML), which is an XML-based standard for exchanging authentication and authorization data between parties.\nTo use SAML-based SSO with AWS, you need to configure an identity provider (IDP) that supports SAML 2.0, such as Microsoft Active Directory Federation Services (ADFS), and map SAML attributes to IAM roles in AWS.\nOnce the SSO setup is complete, users can access AWS services and resources using the AWS Management Console or AWS command line interface (CLI) using temporary security credentials obtained via the AssumeRoleWithSAML API. This API call requests AWS to assume an IAM role based on the SAML assertion provided by the IDP.\nThe AssumeRoleWithSAML API call takes the following parameters:\nRoleArn: The Amazon Resource Name (ARN) of the IAM role to assume PrincipalArn: The ARN of the SAML provider in IAM that describes the IdP SAMLAssertion: The base-64 encoded SAML assertion provided by the IdP\nThe GetCallerIdentity API call returns the AWS account ID and IAM user or role that made the request, and is not used for SSO.\nThe AssumeRoleWithWebIdentity API call is used when you want to grant access to AWS resources to users who are authenticated outside of AWS with a web identity provider, such as Amazon, Facebook, or Google.\nThe GetFederationToken API call is used to obtain temporary security credentials for IAM users who require temporary access to resources. It is not used for SSO.\nTherefore, the correct answer is A. AssumeRoleWithSAML should be used with AWS STS service after authenticating with your on-premise Microsoft Active Directory Federation Services (ADFS) to enable SSO with AWS.\n\n"
}, {
  "id" : 667,
  "question" : "Your organization has an AWS Setup and wants to have the history of all API calls and the responses given out by AWS SNS for audit purposes.\nWhich of the following logging mechanisms would you configure?\n",
  "answers" : [ {
    "id" : "97e4f2c22fe244ed942f07b7a826d483",
    "option" : "Enable SNS logging to S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "9a9b5d9368424325b68a72213abb503d",
    "option" : "Enable X-ray logging for SNS.",
    "isCorrect" : "false"
  }, {
    "id" : "465a923aecdd4b5b84416d58e9d39aca",
    "option" : "Enable CloudTrail logging for SNS.",
    "isCorrect" : "true"
  }, {
    "id" : "3b51908406f748c6a14b6aa4833332ad",
    "option" : "Enable CloudWatch logging for SNS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOptions A and B are not correct.\nOption D is not correct.\nAmazon SNS and CloudWatch are integrated so you can collect, view, and analyze metrics for every active Amazon SNS notification.\nOnce you have configured CloudWatch for Amazon SNS, you can gain better insight into the performance of your Amazon SNS topics, push notifications, and SMS deliveries.\nBut it does not provide information on the API calls made to SNS.\n\n\nThe correct answer for this question is C. Enable CloudTrail logging for SNS.\nExplanation:\nAWS CloudTrail is a service that logs, monitors, and retains account activity related to actions across your AWS infrastructure. CloudTrail provides a history of AWS API calls for your account, including API calls made through AWS Management Console, AWS SDKs, command-line tools, and other AWS services.\nTo monitor the activity for SNS, you can enable CloudTrail logs for SNS. When you enable CloudTrail logging for SNS, it creates log files that capture the activity for SNS API calls, such as creating a topic, subscribing to a topic, and publishing to a topic.\nThe logs contain information about who made the API call, the time the API call was made, which API was called, what parameters were passed to the API, and the response returned by the API. These logs are delivered to an S3 bucket that you specify.\nEnabling SNS logging to S3 bucket (Option A) is not a valid option since SNS does not provide a direct way of logging to S3 bucket.\nX-ray logging (Option B) is a debugging tool used to identify and diagnose errors in distributed applications. It is not specifically designed for audit purposes and does not provide complete details about API calls and responses like CloudTrail does.\nCloudWatch logging (Option D) is used to monitor and collect log data from different resources, including EC2 instances, Lambda functions, VPC flow logs, and more. However, it does not provide a complete history of API calls like CloudTrail does.\nTherefore, the correct answer is C. Enable CloudTrail logging for SNS.\n\n"
}, {
  "id" : 668,
  "question" : "Which of the following are the valid delivery protocols for receiving notifications from AWS SNS? (Choose 3 options)\n",
  "answers" : [ {
    "id" : "a09132e0e32f4f70bec61e9ea310d229",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "b583cfed68dc45879c4f4b2e3c6b6965",
    "option" : "AWS SQS",
    "isCorrect" : "true"
  }, {
    "id" : "8e37dbffeb3c4a82a8f57cbbfe47a17d",
    "option" : "SMS",
    "isCorrect" : "true"
  }, {
    "id" : "1b41a9c8d96e449487cbda7416ffc428",
    "option" : "Email-XML",
    "isCorrect" : "false"
  }, {
    "id" : "9723355f9af94aaeab9bd0884e1c3156",
    "option" : "AWS MQ.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B, C.\n\n\nAmazon SNS (Simple Notification Service) is a fully managed messaging service that enables the sending of messages from publishers to subscribers or other endpoints. SNS supports several delivery protocols that can be used to deliver messages to the intended recipients. The valid delivery protocols supported by AWS SNS are:\nA. AWS Lambda: AWS Lambda is a serverless computing service provided by AWS. It allows the execution of code in response to events, including events generated by SNS. Lambda can be used as a subscriber endpoint for SNS to receive notifications.\nB. AWS SQS: AWS SQS (Simple Queue Service) is a fully managed message queuing service that enables decoupling and scaling of distributed systems. SNS can deliver messages to an SQS queue, and the messages can be retrieved by the consumer application from the queue.\nC. SMS: SNS supports sending text messages to mobile devices using SMS (Short Message Service) delivery protocol. This allows developers to reach their users even if they are not connected to the internet.\nD. Email-XML: SNS can also deliver messages to an email address using the Email-XML delivery protocol. In this protocol, SNS sends the message in an XML format to the email address specified as the subscriber endpoint.\nE. AWS MQ: AWS MQ (Managed Message Broker) is a fully managed message broker service that supports different messaging protocols such as AMQP (Advanced Message Queuing Protocol), MQTT (Message Queuing Telemetry Transport), and STOMP (Streaming Text Oriented Messaging Protocol). SNS can deliver messages to an AWS MQ queue.\nTherefore, the valid delivery protocols for receiving notifications from AWS SNS are AWS Lambda, AWS SQS, and SMS.\n\n"
}, {
  "id" : 669,
  "question" : "You are an architect in your company.\nYou have configured an SNS topic to send emails to a group of users regarding the CloudWatch alarms on the resource usages and outages.\nYou were requested by your head of department to exclude him from those alarms except for critical system outages.\nHow efficiently can you achieve this?\n",
  "answers" : [ {
    "id" : "11354b24d5354c19805e636d70d42e7f",
    "option" : "Create a new topic and subscribe only head of department email address. Create a new CloudWatch alarm only for critical outages and send messages to the new Topic.",
    "isCorrect" : "false"
  }, {
    "id" : "7574963bf5854aaaa083ac2a659dc7e8",
    "option" : "Configure another option on the AWS CloudWatch alarm to send a direct email to the head of the department.",
    "isCorrect" : "false"
  }, {
    "id" : "e5968fd2614744b8b63eb1ce57107160",
    "option" : "Add filter policy to head of department subscription.",
    "isCorrect" : "true"
  }, {
    "id" : "324497fdcfa3408aaee47f66ead16785",
    "option" : "For head of department subscription, select AWS Lambda function which contains code to identify critical system outages and send email using AWS SES.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is not correct.\nAlthough it looks correct, it is not an efficient solution.\nOption B is not correct.\nThere is no such option on AWS CloudWatch alarms.\nOption C is correct.\n########\nAmazon SNS Message Filtering.\nBy default, a subscriber of an Amazon SNS topic receives every message published to the topic.\nA subscriber assigns a filter policy to the topic subscription to receive only a subset of the messages.\nA filter policy is a simple JSON object.\nThe policy contains attributes that define which messages the subscriber receives.\nWhen you publish a message to a topic, Amazon SNS compares the message attributes to the filter policy's attributes for each of the topic's subscriptions.\nIf there is a match between the attributes, Amazon SNS sends the message to the subscriber.\nOtherwise, Amazon SNS skips the subscriber without sending the message to it.\nIf a subscription lacks a filter policy, the subscription receives every message published to its topic.\nWith filter policies, you can simplify your usage of Amazon SNS by consolidating your message filtering criteria into your topic subscriptions.\nWith this consolidation, you can offload the message filtering logic from subscribers and the message routing logic from publishers.\nTherefore, you don't need to filter messages by creating a separate topic for each filtering condition.\nInstead, you can use a single topic, and you can differentiate your messages with attributes.\nEach subscriber receives and processes only those messages accepted by its filter policy.\nFor example, you could use a single topic to publish all messages generated by transactions from your online retail site.\nTo each message, you could assign an attribute that indicates the type of transaction, such as\norder_placed.\n,\norder_cancelled.\n, or\norder_declined.\nBy creating subscriptions with filter policies, you can route each message to the queue meant to process the message's transaction type.\nFor a tutorial demonstrating how to implement message filtering with the AWS Management Console, see Filter Messages Published to Topics.\nThis tutorial shows how to apply filter policies to route messages to separate Amazon SQS queues.\n########\nOption D is not correct.\nAlthough it looks correct, it is not an efficient solution.\n\nThe requirement is to exclude the head of department from receiving CloudWatch alarms regarding resource usage but to receive alerts only for critical system outages. This can be achieved through the following methods:\nA. Create a new topic and subscribe only head of department email address. Create a new CloudWatch alarm only for critical outages and send messages to the new Topic. This method involves creating a new SNS topic, subscribing only the head of department email address to the new topic, and creating a new CloudWatch alarm only for critical system outages. The messages related to critical system outages will be sent to the new topic, and the head of the department will receive alerts only related to critical system outages.\nB. Configure another option on the AWS CloudWatch alarm to send a direct email to the head of the department. This method involves configuring an additional action on the existing CloudWatch alarm, which sends a direct email to the head of department email address for critical system outages. However, this method does not exclude the head of the department from receiving CloudWatch alarms regarding resource usage.\nC. Add filter policy to head of department subscription. This method involves adding a filter policy to the existing head of department subscription, which filters out the messages related to resource usage and only allows the messages related to critical system outages. However, this method requires the messages to be published to the same topic, which may not be efficient if there are a large number of subscriptions.\nD. For head of department subscription, select AWS Lambda function which contains code to identify critical system outages and send email using AWS SES. This method involves creating an AWS Lambda function that identifies critical system outages and sends an email using Amazon SES. The head of the department can be subscribed to this Lambda function, which will send alerts only for critical system outages. However, this method requires additional setup and configuration of the Lambda function.\nAmong these methods, option A is the most efficient way to achieve the requirement. It involves creating a new topic and CloudWatch alarm, which requires minimal setup and configuration changes, and ensures that the head of the department receives alerts only for critical system outages.\n\n"
}, {
  "id" : 670,
  "question" : "Which of the following is not an item of message attribute in AWS SNS?\n",
  "answers" : [ {
    "id" : "aa72e900770a47288b002d5f8b472a99",
    "option" : "Name",
    "isCorrect" : "false"
  }, {
    "id" : "8e6a6d83597a40c7b6f57d4a2be5426f",
    "option" : "Type",
    "isCorrect" : "false"
  }, {
    "id" : "c58be976e05f4b20ace3aa6d7cf65a8c",
    "option" : "Value",
    "isCorrect" : "false"
  }, {
    "id" : "1b1e243ea241426fac1081441985b47c",
    "option" : "MessageID.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nhttps://docs.aws.amazon.com/sns/latest/dg/SNSMessageAttributes.html\n\n\nAWS SNS (Simple Notification Service) is a fully-managed messaging service that allows you to send and receive messages from various types of endpoints or clients. SNS uses a publish-subscribe model where publishers send messages to topics, and subscribers receive messages from topics.\nWhen publishing a message to an SNS topic, you can include optional message attributes. These attributes provide metadata about the message, such as its format, type, and structure. The message attributes are key-value pairs that can be used to filter messages based on the attributes' values.\nLet's take a look at the options provided:\nA. Name: This is not an item of message attribute in AWS SNS. There is no attribute named \"Name\" in SNS.\nB. Type: This is an item of message attribute in AWS SNS. The Type attribute specifies the data type of the message attribute value. It can be String, Number, or Binary.\nC. Value: This is an item of message attribute in AWS SNS. The Value attribute is the actual value of the message attribute.\nD. MessageID: This is not an item of message attribute in AWS SNS. The MessageID is a unique identifier for the message, which is generated by SNS when the message is published.\nTherefore, the correct answer is A. Name.\n\n"
}, {
  "id" : 671,
  "question" : "Which of the following components effectively allows a user to set up AutoScaling on EC2 instances for a web-based application? (Select TWO)\n",
  "answers" : [ {
    "id" : "bcaa79526891412188b955f5418014f1",
    "option" : "Launch Configuration",
    "isCorrect" : "true"
  }, {
    "id" : "4f3ce309644343f28ec6ca9d19b5afa7",
    "option" : "AWS Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "7f2428404e7846ec803d8e1a6917d2b7",
    "option" : "Auto Scaling Group",
    "isCorrect" : "true"
  }, {
    "id" : "ca1dae328deb4cffa860e4c458972ed4",
    "option" : "Elastic IP.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A and C.\nOption A is correct.\nA launch configuration specifies the type of EC2 instance that Amazon EC2 Auto Scaling creates for you.\nYou create the launch configuration by including information such as the ID of the Amazon Machine Image (AMI) to use, the instance type, the key pair, security groups, and block device mapping.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\nOption B is incorrect.\nLambda functions are not required to set up auto-scaling for EC2 instances.\nOption C is correct.\nAn Auto Scaling group is a collection of EC2 instances and the core of Amazon EC2 Auto Scaling.\nWhen you create an Auto Scaling group, you include information such as the subnets for the instances and the number of instances the group must maintain at all times.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/AutoScalingGroup.html\nOption D is incorrect.\nElastic IP is nowhere used to set up auto-scaling for EC2 instances.\n\n\nThe correct answers are A. Launch Configuration and C. Auto Scaling Group.\nAuto Scaling is an AWS service that allows you to automatically adjust the number of EC2 instances in an Auto Scaling group based on the demand for your application. The purpose of Auto Scaling is to maintain the application's performance, availability, and cost-effectiveness by automatically adding or removing EC2 instances as needed.\nHere's how Launch Configuration and Auto Scaling Group enable Auto Scaling for a web-based application:\nLaunch Configuration: A Launch Configuration is a template that defines the configuration settings for a set of EC2 instances, including the AMI (Amazon Machine Image), instance type, security groups, and block device mappings. When you create a Launch Configuration, you specify the number of instances to be launched in the Auto Scaling Group, and the Launch Configuration is used to launch new instances as needed. Auto Scaling Group: An Auto Scaling Group is a collection of EC2 instances that are created from a common Launch Configuration. When you create an Auto Scaling Group, you specify the minimum and maximum number of instances that you want to have running, and the desired capacity for the group. The Auto Scaling Group then launches instances from the Launch Configuration as needed to maintain the desired capacity.\nTogether, Launch Configuration and Auto Scaling Group enable you to set up Auto Scaling for a web-based application. When traffic to your application increases, the Auto Scaling Group will automatically launch new EC2 instances based on the Launch Configuration, and when traffic decreases, it will terminate instances to maintain the desired capacity.\nAWS Lambda is a serverless computing service that allows you to run code without provisioning or managing servers. While it can be used to trigger Auto Scaling actions, it does not directly enable Auto Scaling on EC2 instances.\nElastic IP is a static, public IP address that you can assign to your EC2 instances. While it can be useful for creating a static endpoint for your application, it does not enable Auto Scaling on EC2 instances.\n\n"
}, {
  "id" : 672,
  "question" : "Your organization had set up Auto Scaling for an EC2 instance.\nThey intend to launch one additional new instance with same configuration automatically when the workload increases and shut it down automatically when the workload is back to normal.\nHowever, they have applied operating system patches to the main instance for security reasons and would like this to be reflected when the Auto Scaling group launches a new EC2 instance.\nWhat actions would you take in this scenario?\n",
  "answers" : [ {
    "id" : "b519c86f974944479e5323961c80afee",
    "option" : "Auto Scaling group will launch new EC2 instance from the main instance latest snapshot. New instance will have updated patches.",
    "isCorrect" : "false"
  }, {
    "id" : "20e8500a5c9149f49834ef037ea1b59b",
    "option" : "Create an image out of main EC2 instance and update Auto Scaling group configuration with new image AMI I.",
    "isCorrect" : "false"
  }, {
    "id" : "36c7c6cc97f948a2ac33ad98ba5931e3",
    "option" : "Create an image out of main EC2 instance and update Launch Configuration with new image AMI I.",
    "isCorrect" : "false"
  }, {
    "id" : "2b21da2756334640851c937419f58d60",
    "option" : "Create an image out of main EC2 instance, create a new Launch Configuration with new image AMI ID, update Auto Scaling group with new Launch Configuration ID.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is not correct.\nAuto Scaling group launches new instances based on the configuration defined in Launch Configuration.\nAMI ID is one of the configuration parameter which defines the type of instance to be launched when auto-scaling logic is executed.\nAMI ID is set during the creation of the launch configuration and cannot be modified.\nSo, the auto-scaling group will not launch a new instance based on the latest image of the main instance.\nOption B is not correct.\nAMI ID is a configuration on Launch Configuration, not Auto Scaling Group.\nOption C is not correct and Option D is correct.\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/change-launch-config.html\n\n\nIn this scenario, the organization wants to ensure that any new instance launched by the Auto Scaling group has the latest security patches that were applied to the main EC2 instance.\nOption A, which suggests that the Auto Scaling group will launch a new instance from the main instance's latest snapshot, is not a suitable solution in this scenario because snapshots capture only the disk contents of the instance, not the state of the instance, including the installed patches.\nOption B suggests creating an image (AMI) out of the main EC2 instance and updating the Auto Scaling group configuration with the new image AMI. This option is partially correct, but it doesn't provide a clear way to update the patches on the new instances. Creating a new image AMI I based on the main EC2 instance will capture its state, including the installed patches, but the AMI needs to be regularly updated with the latest security patches.\nOption C is more appropriate. It suggests creating an image out of the main EC2 instance and updating the Launch Configuration with the new image AMI I. This way, any new instances launched by the Auto Scaling group will have the latest patches applied to the main EC2 instance.\nOption D is the best solution in this scenario. It suggests creating an image out of the main EC2 instance, creating a new Launch Configuration with the new image AMI ID, and updating the Auto Scaling group with the new Launch Configuration ID. This option allows you to keep the existing Launch Configuration and update it with the new image AMI ID, which helps avoid any configuration errors that may arise from creating a new Launch Configuration from scratch.\nIn summary, the correct option in this scenario is option D. You should create a new image AMI based on the main EC2 instance, create a new Launch Configuration with the new image AMI ID, and update the Auto Scaling group with the new Launch Configuration ID. This approach ensures that any new instances launched by the Auto Scaling group have the latest security patches applied to the main EC2 instance.\n\n"
}, {
  "id" : 673,
  "question" : "Which of the following is not a default metric type for Auto Scaling Group policy?\n",
  "answers" : [ {
    "id" : "d12b5789f92e4be087afaeae44217ec1",
    "option" : "Average CPU Utilization",
    "isCorrect" : "false"
  }, {
    "id" : "e4089157cf394b0a9d30a9e00c4d964b",
    "option" : "Memory Utilization",
    "isCorrect" : "true"
  }, {
    "id" : "8191026f20a64a6dbc2d14fbac122985",
    "option" : "Network In",
    "isCorrect" : "false"
  }, {
    "id" : "cb2708782640400db8636529bb81cd14",
    "option" : "Network Out.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nFollowing are the default metric types available for Simple Policy and Step Policy.\n\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html\n\n\nAuto Scaling is a service offered by Amazon Web Services (AWS) that helps to ensure that the number of EC2 instances running matches the current demand for the application.\nWhen configuring Auto Scaling, you can define one or more policies that determine when to add or remove instances based on certain conditions or metrics. Metrics are used to monitor the health of your application and make decisions about scaling actions.\nThere are several metric types available for use with Auto Scaling Group policies, including:\nAverage CPU Utilization Average Network In Average Network Out Average Disk Read Ops Average Disk Write Ops Sum Request Count Sum HTTP 4xx Error Count Sum HTTP 5xx Error Count\nThe answer to the question is B. Memory Utilization, as it is not a default metric type for Auto Scaling Group policy.\nHowever, you can use custom CloudWatch metrics for your Auto Scaling Group policies, which allows you to track any metric that is available through CloudWatch, including memory utilization.\nTo summarize, the default metric types for Auto Scaling Group policy are average CPU utilization, average network in, average network out, average disk read ops, sum request count, sum HTTP 4xx error count, and sum HTTP 5xx error count. However, you can use custom CloudWatch metrics to track other metrics, such as memory utilization.\n\n"
}, {
  "id" : 674,
  "question" : "In your organization, an application team sets up an autoscaling group configuration (with a simple scaling policy) to launch a new instance when CPU utilization reaches 85%\nHowever, at times, when the EC2 instance comes into in-service, it reports Unhealthy status immediately.\nAs the replacement of the unhealthy instance by another instance (launched by the Autoscaling group) takes more than 15 minutes, the unhealthy instance has to be removed manually.\nWhat do you think is the reason behind this?\n",
  "answers" : [ {
    "id" : "e9fa2f1cfdb6439d817c33f0418841aa",
    "option" : "Auto scaling policy alarm incorrectly configured.",
    "isCorrect" : "false"
  }, {
    "id" : "d82c32a9009f4e928e0d5bdab8050343",
    "option" : "Health Check Grace Period set to 20 minutes.",
    "isCorrect" : "true"
  }, {
    "id" : "4be83929508146e38f681d802a3f1c9a",
    "option" : "Termination policy set to Do Not Terminate instances.",
    "isCorrect" : "false"
  }, {
    "id" : "c14c0f6776994df88af69a66b305aec0",
    "option" : "Launch Configuration is not configured to report Unhealthy status.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect because the Instance health status is not determined by the CloudWatch alarms.\nOption B is correct.\nOption C is incorrect because the termination policy does not have a â€œDo Not Terminateâ€ option.\nOption D is not a correct statement.\n\n\nThe most probable reason behind an EC2 instance reporting an unhealthy status immediately after being launched by an autoscaling group is that the Health Check Grace Period is set to a value that is less than the time taken by the application to start up and become ready to receive traffic.\nWhen a new EC2 instance is launched, it takes some time for the application to start up, configure itself, and become ready to receive traffic. During this time, the application is not yet fully functional, and traffic sent to it might result in errors or unexpected behavior.\nTo prevent this, the Elastic Load Balancer (ELB) or Target Group associated with the autoscaling group performs health checks on the instances. By default, the Health Check Grace Period is set to 300 seconds (5 minutes), during which time the instance is considered to be in a \"warm-up\" state and is not checked for health.\nHowever, if the application takes longer than 5 minutes to start up, it might report an unhealthy status immediately after the grace period ends. This would cause the autoscaling group to terminate the instance and launch a new one, which would go through the same cycle again.\nTo solve this problem, the Health Check Grace Period needs to be increased to a value that allows the application to start up and become ready to receive traffic. This can be done by modifying the autoscaling group configuration and setting the HealthCheckGracePeriod parameter to a higher value, such as 10 or 15 minutes.\nTherefore, the answer to the question is B. Health Check Grace Period set to 20 minutes.\n\n"
}, {
  "id" : 675,
  "question" : "Which of the following statement is incorrect, provided there is an auto-scaling group setup using launch configuration, with a default termination policy for scale in?\n",
  "answers" : [ {
    "id" : "6d300448ef48445e8c4d9a811bbb2dbf",
    "option" : "Determine unprotected instances in the selected Availability Zone which uses the oldest launch configuration. If there is one such instance, terminate it.",
    "isCorrect" : "false"
  }, {
    "id" : "e1844ea1e3c84e4b8beb223c3b5771a7",
    "option" : "Determine unprotected instances in the selected Availability Zone use the newest launch configuration. If there is one such instance, terminate it.",
    "isCorrect" : "true"
  }, {
    "id" : "3ac0ef494f044050a69ce93195e1a22d",
    "option" : "If multiple instances use the oldest launch configuration, determine which unprotected instances are closest to the next billing hour and terminate it.",
    "isCorrect" : "false"
  }, {
    "id" : "e4b4d2f59a4044458288d22a1643f7d8",
    "option" : "If there is more than one unprotected instance closest to the next billing hour, select one of the instances randomly.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption B is the only false statement, the rest of them are true statements.\nThe default termination policy applies multiple termination criteria before selecting an instance to terminate.\nWhen Amazon EC2 Auto Scaling terminates instances, it first determines which Availability Zones have the most instances, and it finds at least one instance that is not protected from scale-in.\nDetermine whether any of the instances eligible for termination use the oldest launch template or configuration:\n[For Auto Scaling groups that use a launch template]\nDetermine whether any of the instances use the oldest launch template unless there are instances that use a launch configuration.\nAmazon EC2 Auto Scaling terminates instances that use a launch configuration before it terminates instances that use a launch template.\n[For Auto Scaling groups that use a launch configuration]\nDetermine whether any of the instances use the oldest launch configuration.\nAfter applying the preceding criteria, if there are multiple unprotected instances to terminate, determine which instances are closest to the next billing hour.\nIf there are multiple unprotected instances closest to the next billing hour, terminate one of these instances at random.\nReference:\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-termination-policies.html\n\n\nAuto Scaling group is a service provided by AWS which enables users to launch or terminate EC2 instances automatically based on pre-defined rules, in order to handle changes in demand for an application or service.\nWhen creating an Auto Scaling group, users can define a Launch Configuration, which defines the configuration of the instances launched by the Auto Scaling group. Additionally, users can define a Termination Policy, which specifies the order and criteria for selecting instances to terminate when scaling in.\nThe provided options are all related to the default termination policy for scale in. The default termination policy selects instances for termination based on the criteria listed below:\nA. Determine unprotected instances in the selected Availability Zone which uses the oldest launch configuration. If there is one such instance, terminate it.\nThis statement is correct. The default termination policy selects the unprotected instances in the selected Availability Zone which use the oldest launch configuration for termination.\nB. Determine unprotected instances in the selected Availability Zone use the newest launch configuration. If there is one such instance, terminate it.\nThis statement is incorrect. The default termination policy selects the instances which use the oldest launch configuration for termination, not the newest.\nC. If multiple instances use the oldest launch configuration, determine which unprotected instances are closest to the next billing hour and terminate it.\nThis statement is correct. If multiple instances use the oldest launch configuration, the default termination policy selects the unprotected instances which are closest to the next billing hour for termination.\nD. If there is more than one unprotected instance closest to the next billing hour, select one of the instances randomly.\nThis statement is correct. If there is more than one unprotected instance closest to the next billing hour, the default termination policy selects one of the instances randomly for termination.\nIn conclusion, the correct statement is A, while the incorrect statement is B. The default termination policy selects unprotected instances using the oldest launch configuration for termination, not the newest.\n\n"
}, {
  "id" : 676,
  "question" : "An eCommerce website is hosted on two EC2 instances that sit behind an Elastic Load Balancer.\nThe response time of the website has slowed dramatically.\nThe analytics team found that one of the EC2 instances failed, leaving only one instance running.\nWhat is the best course of action to prevent this from happening in the future?\n",
  "answers" : [ {
    "id" : "88352ad1b6f94f158832c17105b6d030",
    "option" : "Change the instance size to the maximum available to compensate for the failure.",
    "isCorrect" : "false"
  }, {
    "id" : "6d1114ceb42947ff9da0be15571d6206",
    "option" : "Use CloudWatch to monitor the instances via VPC Flow Logs.",
    "isCorrect" : "false"
  }, {
    "id" : "2ae38f174c354434a1087e5a0170bd44",
    "option" : "Configure the ELB to perform health checks on the EC2 instances and implement auto-scaling.",
    "isCorrect" : "true"
  }, {
    "id" : "67ff5a1688854d2a9da0df3725605366",
    "option" : "Replicate the existing configuration in several regions for failover.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nOption C is correct.\nUsing the elastic load balancer to perform health checks will determine whether or not to remove a non- or underperforming instance and have the auto-scaling group launch a new instance.\nOption A is incorrect because increasing the instance size doesn't prevent the failure of one or both instances.\nTherefore the website can still become slow or unavailable.\nOption B is incorrect because monitoring the VPC flow logs for the VPC will capture VPC traffic, not traffic for the EC2 instance.\nOption D is incorrect because replicating the same two instance deployment may not prevent the failure of instances and could still result in the website becoming slow or unavailable.\nReferences:\nhttps://media.amazonwebservices.com/AWS_Building_Fault_Tolerant_Applications.pdf\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html#working-with-flow-logs\n\nThe correct answer is C. Configure the ELB to perform health checks on the EC2 instances and implement auto-scaling.\nExplanation: When an EC2 instance fails, the Elastic Load Balancer should automatically detect the failure and stop sending traffic to the failed instance. However, in this case, the website's response time has slowed dramatically, which suggests that the Elastic Load Balancer failed to detect the failure of the EC2 instance.\nTo prevent this from happening in the future, the Elastic Load Balancer should be configured to perform health checks on the EC2 instances. Health checks periodically verify that the instances are running and responding correctly, and remove any failed instances from the load balancer's rotation.\nIn addition to configuring health checks, the solution should implement auto-scaling. Auto-scaling automatically adjusts the number of EC2 instances based on the demand of the application. When the demand is low, it reduces the number of instances, and when the demand is high, it increases the number of instances. By doing so, it ensures that the application can handle varying levels of traffic.\nChanging the instance size to the maximum available to compensate for the failure (option A) is not the best course of action as it is an expensive and inefficient solution. It will only increase the cost of running the application and may lead to overprovisioning.\nUsing CloudWatch to monitor the instances via VPC Flow Logs (option B) is a good practice for troubleshooting network connectivity and security issues. However, it is not directly related to preventing an instance failure or ensuring high availability.\nReplicating the existing configuration in several regions for failover (option D) is a viable solution for achieving high availability and disaster recovery. However, it is not the best option in this scenario as it requires additional resources and complexity. Implementing auto-scaling and health checks is a simpler and more cost-effective solution to prevent instance failures.\n\n"
}, {
  "id" : 677,
  "question" : "In an AWS Setup of a company, a web-based application has a fleet of 10 EC2 instances.\n7 EC2 instances are present in Availability Zone A, whereas 3 EC2 instances are in Availability Zone B.\nThe percentage (%) of requests received in Availability Zone B is greater than the percentage (%) of requests in Availability Zone A.\nWhat can be done at the architecture level to balance the load across the two availability zones?\n",
  "answers" : [ {
    "id" : "20f0c0d1c3344cb5b518a0d4a3bab395",
    "option" : "Use X-forwarded headers",
    "isCorrect" : "false"
  }, {
    "id" : "a3ba81c4697b4e17b631e212e4a8a244",
    "option" : "Enable â€œsplit traffic equallyâ€ checkbox under load balancer configuration.",
    "isCorrect" : "false"
  }, {
    "id" : "36d1e2de2c5d4acb90fb24b3230d2890",
    "option" : "Use Cross-Zone load balancing.",
    "isCorrect" : "true"
  }, {
    "id" : "275d4ee8827e47f6ad365737b3682a66",
    "option" : "Use Sticky Sessions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nCross-zone load balancing.\nThe nodes for your load balancer distribute requests from clients to registered targets.\nWhen cross-zone load balancing is enabled, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones.\nWhen cross-zone load balancing is disabled, each load balancer node distributes traffic only across the registered targets in its Availability Zone.\nOption A is incorrect because X-forwarded headers are not used to distribute the traffic.\nOption B is incorrect because it is just a distractor, there is no such configuration.\nOption D is incorrect because the sticky session feature is used to enable the load balancer to bind a user's session to a specific target.\nThis ensures that all requests from the user during the session are sent to the same target.\nReference:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html\n\n\nThe correct answer is C. Use Cross-Zone load balancing.\nExplanation: In this scenario, the web-based application has a fleet of 10 EC2 instances, with 7 instances in Availability Zone A and 3 instances in Availability Zone B. Additionally, the percentage of requests received in Availability Zone B is greater than the percentage of requests in Availability Zone A. To balance the load across the two availability zones, we need to use a load balancer.\nA load balancer distributes incoming traffic across multiple instances to ensure that no single instance is overwhelmed. In Amazon Web Services (AWS), we can use Elastic Load Balancer (ELB) to distribute traffic across EC2 instances in multiple availability zones.\nThere are two types of load balancing available in AWS:\nInternal Load Balancer (ILB) - It distributes traffic within the same VPC. External Load Balancer (ELB) - It distributes traffic from the internet to the VPC.\nIn this scenario, we can use an External Load Balancer (ELB) to distribute traffic across the two availability zones. The ELB can be configured to distribute traffic evenly across all instances in both availability zones. There are different types of load balancing algorithms available in AWS, such as round-robin, least connections, and IP hash. We can choose an appropriate algorithm based on our requirements.\nThe options provided are: A. Use X-forwarded headers - This option is not related to load balancing. X-forwarded headers are used to identify the originating IP address of a client connecting to a web server through a load balancer or proxy server.\nB. Enable â€œsplit traffic equallyâ€ checkbox under load balancer configuration - This option is not available in AWS. It seems like an incorrect answer.\nC. Use Cross-Zone load balancing - This is the correct answer. Cross-Zone load balancing distributes traffic evenly across all instances in both availability zones. It allows the ELB to distribute traffic across all instances, even if they are not in the same availability zone as the ELB.\nD. Use Sticky Sessions - Sticky Sessions allow the ELB to route requests from a particular client to the same EC2 instance every time. This option is not recommended in scenarios where we need to distribute traffic evenly across all instances.\nTherefore, the correct answer is C. Use Cross-Zone load balancing.\n\n"
}, {
  "id" : 678,
  "question" : "Which of the following are features for monitoring application load balancer? Choose the 3 correct options.\n",
  "answers" : [ {
    "id" : "af46a8f3d085478cbc77900c963b2615",
    "option" : "CloudWatch metrics",
    "isCorrect" : "true"
  }, {
    "id" : "4046c49b5e1c488f829c243d94cb706f",
    "option" : "Request tracing",
    "isCorrect" : "true"
  }, {
    "id" : "e9a8637dc17b4e13b0fde92fbf39e998",
    "option" : "VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "9e8eb339096b427083a4a7c96bbd69e0",
    "option" : "CloudTrail logs",
    "isCorrect" : "true"
  }, {
    "id" : "ede438625f574ef2b39f3e00008006af",
    "option" : "EC2 Flow Logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B, D.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-monitoring.html\n\n\nThe correct options for monitoring an Application Load Balancer (ALB) in AWS are:\nA. CloudWatch metrics C. VPC Flow Logs E. EC2 Flow Logs.\nHere's a detailed explanation for each option:\nA. CloudWatch Metrics: Amazon CloudWatch is a monitoring service that provides metrics on the performance of various AWS resources, including ALBs. With CloudWatch, you can monitor the request count, latency, and other metrics for an ALB. These metrics can be used to set alarms and automate actions based on certain thresholds.\nC. VPC Flow Logs: Amazon VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. By enabling VPC Flow Logs for your VPC, you can capture information about the traffic going to and from the ALB. This information can be used for security analysis, troubleshooting, and compliance auditing.\nE. EC2 Flow Logs: Similar to VPC Flow Logs, EC2 Flow Logs capture information about the IP traffic going to and from network interfaces of EC2 instances. Since the ALB is built on top of EC2 instances, you can use EC2 Flow Logs to capture information about the traffic going to and from the ALB. This information can be used for the same purposes as VPC Flow Logs.\nB. Request Tracing: Request tracing is not a feature for monitoring an ALB. However, AWS offers AWS X-Ray, which is a service that provides request tracing for distributed applications, including those that use an ALB. With X-Ray, you can trace requests across multiple services and get a comprehensive view of how they interact with each other. While this is useful for troubleshooting and performance optimization, it is not a feature for monitoring an ALB.\nD. CloudTrail Logs: AWS CloudTrail is a service that provides logs of API calls made to various AWS resources. While CloudTrail can provide some information about the ALB, it is not a feature for monitoring the ALB itself. CloudTrail logs can be used for security analysis and compliance auditing, but they do not provide the same level of visibility into the performance of the ALB as CloudWatch metrics, VPC Flow Logs, or EC2 Flow Logs.\n\n"
}, {
  "id" : 679,
  "question" : "You have created a VPC with an application load balancer and selected two EC2 instances as targets.\nHowever, when you are trying to make a request to the internet-facing load balancer, the request fails.\nWhat could be the reason?\n",
  "answers" : [ {
    "id" : "fe8e92c2e0f746deb027d6f529552213",
    "option" : "The route table associated with the load balancer`s subnet does not have a route to the internet gateway.",
    "isCorrect" : "true"
  }, {
    "id" : "84266ccb96c049bda694f825850b4077",
    "option" : "Target EC2 instances are in a public subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "ac84644e1c314a8a95937f937e718d9b",
    "option" : "There is no elastic IP address attached to the load balancer.",
    "isCorrect" : "false"
  }, {
    "id" : "ae1897305e034892a20705ea832820e0",
    "option" : "Cross-zone load balancing is not enabled.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is correct because there must be a route in a route table to the internet gateway for internet connectivity\nOption B is incorrect because this does not result in the failure mentioned in the question.\nOption C is incorrect because any instances in the VPC must either have a public IP address or an attached Elastic IP address.\nAnd for the internet-facing load balancer, AWS manages the underlying IP addresses.\nSo no need for an Elastic IP address.\nOption D is incorrect because cross-zone load balancing is not a reason to cause the failure.\nhttps://aws.amazon.com/premiumsupport/knowledge-center/create-attach-igw-vpc/\n\n\nThe most likely reason for the request to fail when accessing the internet-facing application load balancer (ALB) is that the associated subnet's route table does not have a route to the internet gateway.\nA route to the internet gateway is necessary for internet traffic to reach the EC2 instances through the ALB. Therefore, to resolve this issue, you should add a route to the subnet's route table that points to the internet gateway.\nOption B (Target EC2 instances are in a public subnet) is not a reason for the request to fail because instances in a public subnet can be accessed from the internet.\nOption C (No Elastic IP address attached to the load balancer) is not a requirement for an ALB. However, if you need to provide a static IP address for the ALB, then you can associate an Elastic IP address with it.\nOption D (Cross-zone load balancing is not enabled) affects the distribution of traffic across the target instances within the same Availability Zone. However, it is not a cause of the request to fail.\n\n"
}, {
  "id" : 680,
  "question" : "A small company started using EBS backed EC2 instances due to the cost improvements over running their own servers.\nThe company's policy is to stop the development servers over the weekend and restart them each week.\nThe first time the servers were brought back.\nNone of the developers were able to SSH into them.\nWhat did the server most likely overlook?\n",
  "answers" : [ {
    "id" : "b1a56f7aa3f549548c9679b7f402e5ba",
    "option" : "The associated Elastic IP address has changed and the SSH configurations were not updated.",
    "isCorrect" : "false"
  }, {
    "id" : "3835bfcfb2c94c67b0d2e039b834a5d1",
    "option" : "The security group for a stopped instance needs to be reassigned after start.",
    "isCorrect" : "false"
  }, {
    "id" : "c2cc689388e44dfa8b795ffcb2a8f9e2",
    "option" : "The public IPv4 address has changed on server restart and the SSH configurations were not updated.",
    "isCorrect" : "true"
  }, {
    "id" : "87a146a70f034a6e8fe99b4511d69410",
    "option" : "EBS backed EC2 instances cannot be stopped and were automatically terminated.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nCorrect:\nC.\nThe instance retains its private IPv4 addresses and any IPv6 addresses when stopped and restarted.\nAWS releases the public IPv4 address and assigns a new one when it's restarted.\nIncorrect:\nA.\nAn EC2 instance retains its associated Elastic IP addresses.\nB.\nSecurity groups do not need to be reassigned to instances that are restarted.\nD.\nEBS backed instances are the only instance type that can be started and stopped.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html\n\nThe most likely issue that the developers faced when they tried to SSH into the EBS-backed EC2 instances after the servers were restarted is that the public IPv4 address has changed on server restart, and the SSH configurations were not updated.\nWhen an EC2 instance is stopped and then started again, the public IPv4 address assigned to the instance may change. This is because Amazon EC2 releases the public IPv4 address associated with the instance when it is stopped. When the instance is started again, a new public IPv4 address is assigned to it.\nIn this case, the developers were not able to SSH into the instance because they were trying to connect to the old public IPv4 address that was associated with the instance before it was stopped. To fix this issue, the developers need to update the SSH configurations to use the new public IPv4 address assigned to the instance after it was restarted.\nOption A, which suggests that the associated Elastic IP address has changed and the SSH configurations were not updated, is incorrect because Elastic IP addresses are static IP addresses that can be associated with an instance and remain the same even if the instance is stopped and started again. Therefore, if an Elastic IP address was used to connect to the instance before it was stopped, the same Elastic IP address should work after the instance is started again.\nOption B, which suggests that the security group for a stopped instance needs to be reassigned after start, is incorrect because the security group assigned to the instance remains the same even if the instance is stopped and started again.\nOption D, which suggests that EBS-backed EC2 instances cannot be stopped and are automatically terminated, is incorrect because EBS-backed EC2 instances can be stopped and started again. When an EBS-backed EC2 instance is stopped, the instance's root EBS volume is preserved, and the instance can be started again with the same data and configuration.\n\n"
}, {
  "id" : 681,
  "question" : "Which of the following are the main functions of AWS Route 53? (SELECT THREE)\n",
  "answers" : [ {
    "id" : "90161b9e0b0545acba58e44f96a7e280",
    "option" : "Register domain names",
    "isCorrect" : "true"
  }, {
    "id" : "810999579fd54bac806b29a80c82e22d",
    "option" : "Route internet traffic to the resources for your domain",
    "isCorrect" : "true"
  }, {
    "id" : "7fe9a64ec2de4c2a95020350aab215aa",
    "option" : "Load-balance traffic among individual AWS resource instances",
    "isCorrect" : "false"
  }, {
    "id" : "f24c2e4c85264f0b945771302ebcfdd9",
    "option" : "Check the health of your resources",
    "isCorrect" : "true"
  }, {
    "id" : "d6e858cab8b5409489c0a1fc50a1f5ca",
    "option" : "Auto Scale your resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B, D.\nBreakdown.\nOption C is incorrect because Route 53 is not used for load-balancing traffic among individual AWS resource instances (like a load balancer) rather we integrate it with resources for added functionality (https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/integration-with-other-services.html)\nOption E is incorrect.\nIn AWS, Autoscaling is used to scale underlying resources up or down based on pre-determined or dynamic factors.\n\n\nAWS Route 53 is a highly scalable domain name system (DNS) service that is designed to give developers and businesses a reliable way to route end users to internet applications by converting domain names like www.example.com into IP addresses that computers use to identify each other on the internet. The service provides the following main functions:\nA. Register domain names: With Route 53, you can easily register a new domain name or transfer an existing one to AWS. It provides a simple registration process and the ability to manage your domains in a single place. You can also use Route 53 to register domains that are hosted outside of AWS.\nB. Route internet traffic to the resources for your domain: Route 53 lets you route traffic to your internet applications by using DNS, which maps human-readable domain names to IP addresses. Route 53 can route traffic to any AWS resource, including EC2 instances, S3 buckets, and load balancers.\nC. Load-balance traffic among individual AWS resource instances: Route 53 can also act as a global load balancer, which allows you to distribute traffic across multiple AWS resources in different regions or availability zones. This helps to improve the availability and performance of your applications by distributing traffic to the resources that are closest to your end users.\nD. Check the health of your resources: Route 53 can monitor the health of your resources by sending requests to them and checking for expected responses. If a resource fails a health check, Route 53 can stop routing traffic to it until it becomes healthy again. This helps to ensure that your applications are highly available and responsive.\nE. Auto Scale your resources: Route 53 can also be used in conjunction with other AWS services like Auto Scaling to help you automatically scale your resources up or down in response to changes in demand. When you use Route 53 with Auto Scaling, you can automatically adjust the DNS settings for your application to reflect the current state of your resources.\n\n"
}, {
  "id" : 682,
  "question" : "Your organization had created an S3 bucket for static website hosting.\nThey had created and configured all necessary components for the static website with the S3 website endpoint http://example-bucket.com.s3-website-us-east-2.amazonaws.com.\nThey would like to get the website served through the domain name www.example-bucket.com which is already registered.\nYou want to create a record in Route 53 to route to the S3 website endpoint.\nWhich type of record set you need to create?\n",
  "answers" : [ {
    "id" : "e9268d49f7614bdc8f4ac20f4f90ff56",
    "option" : "A - IPv4 Address with Alias=NO",
    "isCorrect" : "false"
  }, {
    "id" : "fd5b2ebad54946b8b6e8ef98d0e75d58",
    "option" : "A - IPv4 Address with Alias=YES",
    "isCorrect" : "true"
  }, {
    "id" : "d620137320bd4253ba7e856a897f783f",
    "option" : "CNAME - Canonical Name with ALIAS=NO",
    "isCorrect" : "false"
  }, {
    "id" : "6d9bacf90af84c6994e2e43b31c7aa6a",
    "option" : "CNAME - Canonical Name with ALIAS=YES.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAWS Documentation:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html#root-domain-walkthrough-s3-tasks\nWe can't create a CNAME record for the Parent, Naked, or Apex domain, whereas we can use an alias record to point to the parent domain.\nBelow scenarios given are not possible.\nwww.example-bucket.com CNAME http://example-bucket.com.s3-website-us-east-2.amazonaws.com.\nor\nexample-bucket.com CNAME http://example-bucket.com.s3-website-us-east-2.amazonaws.com.\nBut with an alias, they are possible.\nexample-bucket.com Alias(A) http://example-bucket.com.s3-website-us-east-2.amazonaws.com.\nOption A is incorrect as explained above A record Alias should be yes.\nOptions C and D are incorrect because a CNAME record set is mainly used to route the traffic to another DNS name.\nBut it cannot route it to an S3 website endpoint.\n\n\nTo route traffic from the registered domain name www.example-bucket.com to the S3 website endpoint http://example-bucket.com.s3-website-us-east-2.amazonaws.com, we need to create a record in Route 53.\nRoute 53 is a highly scalable and available DNS web service provided by AWS. It allows you to manage your domain names and the IP addresses and resources associated with them. It provides a variety of record types, and the record type we should use in this scenario is the A record or the CNAME record.\nThe A record maps a domain name to an IPv4 address, and the CNAME record maps a domain name to another domain name. In this scenario, since we want to route traffic from the registered domain name www.example-bucket.com to the S3 website endpoint, we need to use a CNAME record.\nNow the question is, should we use an ALIAS or NO-ALIAS CNAME record?\nThe ALIAS record is a special type of record that allows you to map your domain name to AWS resources, such as an S3 bucket, CloudFront distribution, or Elastic Load Balancer, without using a CNAME record. It provides seamless integration between Route 53 and other AWS services and ensures that the DNS name resolution works correctly in all cases.\nIn this scenario, since we want to route traffic from the registered domain name www.example-bucket.com to the S3 website endpoint, we should use an ALIAS CNAME record.\nOption B: A - IPv4 Address with Alias=YES is incorrect because an A record maps a domain name to an IPv4 address, and we need to map a domain name to another domain name in this scenario.\nOption A: A - IPv4 Address with Alias=NO is incorrect because it does not allow us to map a domain name to an S3 bucket or any other AWS resource. We need to use a CNAME record for that purpose.\nOption C: CNAME - Canonical Name with ALIAS=NO is incorrect because it does not provide the seamless integration that the ALIAS CNAME record provides. Also, it does not work correctly if the domain name is the root domain, such as example-bucket.com (without www).\nTherefore, the correct answer is option D: CNAME - Canonical Name with ALIAS=YES.\n\n"
}, {
  "id" : 683,
  "question" : "Which of the following is not an AWS service that AWS Route 53 can route traffic to?\n",
  "answers" : [ {
    "id" : "b2a021dae40c41b0b929a86a3f2296cd",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "23b4293a9e0f41b58e1fbdc79ebf2266",
    "option" : "Elastic Load Balancing",
    "isCorrect" : "false"
  }, {
    "id" : "e6e5dfda95724a38b2ce0682e7ff53af",
    "option" : "Amazon RDS",
    "isCorrect" : "false"
  }, {
    "id" : "45d6027b894443c1bbd5bb6687bf5d46",
    "option" : "Amazon CloudWatch.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\n\nAmazon Route 53 integrates with Amazon CloudWatch for monitoring, not for routing traffic.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/integration-with-other-services.html\n\n\nAWS Route 53 is a highly available and scalable cloud-based Domain Name System (DNS) web service that can route traffic to a variety of AWS services. It can also route traffic to non-AWS resources, such as on-premises servers or resources hosted with other cloud providers.\nAmong the given options, the correct answer to the question is option D - Amazon CloudWatch. AWS Route 53 cannot route traffic to Amazon CloudWatch.\nHere is a brief overview of the other options and how Route 53 can route traffic to them:\nA. Amazon CloudFront: This is a content delivery network (CDN) service that distributes content such as HTML, CSS, and JavaScript files, videos, and other web assets globally. AWS Route 53 can route traffic to Amazon CloudFront by creating an alias record that points to the CloudFront distribution.\nB. Elastic Load Balancing: This service distributes incoming traffic across multiple EC2 instances or containers in multiple Availability Zones to ensure high availability and fault tolerance. AWS Route 53 can route traffic to Elastic Load Balancing by creating an alias record that points to the load balancer.\nC. Amazon RDS: This is a relational database service that provides managed database solutions for various database engines, such as MySQL, Oracle, SQL Server, and PostgreSQL. AWS Route 53 can route traffic to Amazon RDS by creating a CNAME record that points to the RDS endpoint.\nD. Amazon CloudWatch: This is a monitoring and observability service that collects and analyzes metrics, logs, and events from various AWS resources and applications. AWS Route 53 cannot route traffic to Amazon CloudWatch since it is not a resource that can receive incoming traffic.\nIn conclusion, AWS Route 53 is a powerful DNS web service that can route traffic to a variety of AWS services and non-AWS resources. When using Route 53 to route traffic to AWS services, it is important to understand which services can and cannot receive incoming traffic.\n\n"
}, {
  "id" : 684,
  "question" : "Your organization had set up a web application on AWS VPC with 4 EC2 instances in a private subnet.\nThey had configured an elastic load balancer to distribute traffic between all 4 EC2 instances.\nThey decided to route traffic from the internet to the elastic load balancer resource via the domain, â€œwww.example-web-application.comâ€ they had already registered.\nWhich type of record set do you need to create in Route 53?\n",
  "answers" : [ {
    "id" : "6295ac79180846338356490276ec6a9f",
    "option" : "A - IPv4 Address with Alias=NO and select the ELB resource",
    "isCorrect" : "false"
  }, {
    "id" : "e0ee3580bfe74e059e288f75f49395e3",
    "option" : "A - IPv4 Address with Alias=YES and select the ELB resource",
    "isCorrect" : "true"
  }, {
    "id" : "8efafa7a3c324ca793bfd51e83139ff5",
    "option" : "CNAME - Canonical Name with ALIAS=NO and select the ELB IP addresses",
    "isCorrect" : "false"
  }, {
    "id" : "8e88f040955e46c98fa368b8f4b28520",
    "option" : "CNAME - Canonical Name with ALIAS=YES and select the ELB resource.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nWhile using an Alias record to route traffic to AWS resources, Route 53 automatically recognizes the change in the resource eg an IP change in a Load Balancer will prompt route 53 to automatically send queries to the new IP address.\nAlias routing comes at no additional cost as compared to CNAME routing which incurs cost depending on the nature of its routing queries.\nA CNAME record redirects DNS queries for a record name regardless of the record type specified in the DNS query, such as A or AAAA.Option A is incorrect because for A record, when Alias is not enabled, an IP address needs to be configured.\nFor an ELB resource, this is not applicable.\nOption B is CORRECT because when Alias is enabled in A record, you can select the ELB resource in the target.\nRoute 53 responds to a DNS query only when the name of the alias record (such as xyz.example.com) and the type of the alias record (such as A or AAAA) match the name and type in the DNS query.\nOption C is incorrect because, for a CNAME record, you need to configure a DNS name instead of an IP address.\nOption D is incorrect because the CNAME Alias record does not support ELB resources.\n'CNAME record with Alias=Yes' can work for the scenario given in the question, but it is not best suitable.\nPlease check the youtube link given below.\nhttps://www.youtube.com/watch?v=YPNzeuNd-lA\nReferences:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-choosing-alias-non-alias.html\n\n\nTo route traffic from the internet to an Elastic Load Balancer (ELB) resource via a domain registered with Route 53, you would need to create a DNS record set in Route 53 that maps the domain name to the ELB resource.\nThere are two types of record sets that can be created in Route 53 for this purpose: A record sets and CNAME record sets.\nA record sets map a domain name to an IPv4 address, while CNAME record sets map a domain name to another domain name.\nIn this case, since the ELB is the target resource, we can rule out creating a CNAME record set that maps the domain name to the ELB's IP address (Option C). The reason for this is that an ELB is a highly available, scalable service and the IP addresses associated with the ELB can change frequently, making it difficult to manage DNS records.\nTherefore, we would want to create an A record set or an ALIAS record set that maps the domain name to the ELB resource.\nOption A suggests creating an A record set with Alias=NO and selecting the ELB resource. However, this is not the recommended approach for mapping a domain name to an ELB resource. The reason for this is that A record sets are not dynamic and cannot be updated if the ELB's IP address changes.\nOption B suggests creating an A record set with Alias=YES and selecting the ELB resource. This is the recommended approach for mapping a domain name to an ELB resource. The Alias option allows you to create a dynamic record set that automatically maps to the current IP addresses associated with the ELB.\nOption D suggests creating a CNAME record set with Alias=YES and selecting the ELB resource. However, CNAME record sets cannot be used to map a domain name to an ELB resource directly.\nTherefore, the correct answer is Option B - A - IPv4 Address with Alias=YES and select the ELB resource.\n\n"
}, {
  "id" : 685,
  "question" : "You have a web application hosted on AWS VPC with a single EC2 instance with Auto Scaling enabled.\nYou have also assigned elastic IP address to the EC2 instance.\nWhen you access the elastic IP address, you are able to successfully connect to your web application.\nYou decided to route requests to your application from a custom domain through Route 53\nYou have performed the setup on Route 53\nHowever, when you access your custom domain name from internet, you get â€œServer Not Foundâ€ error.\nWhich of the following could be a reason?\n",
  "answers" : [ {
    "id" : "b00c1fad955843e382454e260fa5239f",
    "option" : "Route 53 service is for internal application routing. It does not support routing traffic from internet.",
    "isCorrect" : "false"
  }, {
    "id" : "f5969be772ef41afbde2e1926f5b0f6a",
    "option" : "You must configure elastic load balancer in order to use Route 53 for web application hosting.",
    "isCorrect" : "false"
  }, {
    "id" : "74106cb612454df182a92801e34064da",
    "option" : "IP address configured in Route 53 DNS record set might be incorrect.",
    "isCorrect" : "true"
  }, {
    "id" : "c89e6635edca439c8bfe1188a1e7f9ec",
    "option" : "The resource on EC2 instance that you`re routing traffic to is unavailable.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A and B are not valid statements.\nOption C is correct.\nAlthough option D looks correct, the question states the connection to web application was successful when connected through elastic IP address.\nSo this option is not the cause of failure.\n\n\nThe correct answer is C. IP address configured in Route 53 DNS record set might be incorrect.\nExplanation:\nWhen you access your web application through the elastic IP address, you are directly accessing the EC2 instance. However, when you want to access it through a custom domain name using Route 53, you need to configure a DNS record set in Route 53 that points to the elastic IP address of your EC2 instance.\nIf the IP address configured in the DNS record set is incorrect or invalid, you will not be able to access your web application using the custom domain name. This could be the reason why you are getting the â€œServer Not Foundâ€ error.\nOption A is incorrect as Route 53 is a public DNS service provided by AWS and can be used to route traffic from the internet to your application running in a VPC.\nOption B is also incorrect because you can use Route 53 to route traffic to a single EC2 instance with an Elastic IP address without using an Elastic Load Balancer.\nOption D is also a possibility, but before checking the resource on EC2 instance, you should verify the DNS configuration in Route 53. If the DNS configuration is correct, then you can check the resource on the EC2 instance to see if it is available or not.\nIn conclusion, the most likely reason for the \"Server Not Found\" error is an incorrect IP address configured in the DNS record set in Route 53.\n\n"
}, {
  "id" : 686,
  "question" : "Which of the following are correct options for logging and monitoring AWS Route 53 service? (Select TWO)\n",
  "answers" : [ {
    "id" : "d9ea12c6bc934b1cb5eb1a23735000d4",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "true"
  }, {
    "id" : "bc167ee9cca244c5b9c542ebfe2d03c2",
    "option" : "AWS VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "0761172aee374b5b85db96296ea30958",
    "option" : "AWS Route 53 dashboard",
    "isCorrect" : "false"
  }, {
    "id" : "54788cefd2a4491fb3876142fefa3822",
    "option" : "Access logs in S3",
    "isCorrect" : "false"
  }, {
    "id" : "e16b094f90424eb4a6863873937130c3",
    "option" : "AWS CloudTrail.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A and E.\nOption A is correct.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-health-checks.html\nOption B is incorrect because VPC Flow logs are for logging the network traffic going in/coming out of a specific VPC.\nRoute 53 is not a VPC specific service.\nOption C is incorrect because Route53 Dashboard is not used for \"logging and monitoring\"\nStatus of domain registration, lists of domains, etc.\nare neither metrics (time series of magnitudes with dimensions/units) nor logs (text messages written by applications).\nOption D is incorrect because Route 53 does not log directly into AWS S3\nHowever, you can export the CloudWatch or CloudTrail logs into S3.\nOption E is correct.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/logging-using-cloudtrail.html\n\n\nThe correct options for logging and monitoring AWS Route 53 service are A. Amazon CloudWatch and E. AWS CloudTrail.\nA. Amazon CloudWatch is a monitoring service that can be used to monitor Route 53 health checks and DNS queries. It can also be used to create alarms based on Route 53 metrics, such as latency and health check status. These alarms can be used to alert you when Route 53 is not performing as expected.\nE. AWS CloudTrail is a logging service that can be used to log API calls made to Route 53. This can be used to audit changes made to Route 53 resources and to troubleshoot issues that may arise.\nB. AWS VPC Flow Logs are used to capture information about IP traffic going to and from network interfaces in a VPC. This is not applicable to Route 53.\nC. AWS Route 53 dashboard is a web-based console that allows you to manage and configure Route 53 resources. It is not used for logging and monitoring.\nD. Access logs in S3 are used to log requests made to S3 buckets. This is not applicable to Route 53.\nIn summary, Amazon CloudWatch and AWS CloudTrail are the correct options for logging and monitoring AWS Route 53 service.\n\n"
}, {
  "id" : 687,
  "question" : "You have launched an RDS instance in your VPC.\nThe endpoint that is assigned to your DB instance is a long, partially random, alphanumeric string, for example, myexampledb.a1b2c3d4wxyz.us-west-2.rds.amazonaws.com.\nYour organization wants to use a name that's easier to remember.\nSo you registered a domain name using the Route53 service.\nWhich type of record set do you need to create?\n",
  "answers" : [ {
    "id" : "1115c32046cf42bea5fc56438a660b7f",
    "option" : "A - IPv4 Address with Alias=NO",
    "isCorrect" : "false"
  }, {
    "id" : "05a00b3f0e3748c098df6bc180a6adf3",
    "option" : "A - IPv4 Address with Alias=YES",
    "isCorrect" : "false"
  }, {
    "id" : "dfdcab1f1db44b2d8465e949b516567d",
    "option" : "CNAME - Canonical Name with ALIAS=NO",
    "isCorrect" : "true"
  }, {
    "id" : "cded2174b17b4d10b2580160bc50b5f1",
    "option" : "CNAME - Canonical Name with ALIAS=YES.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-rds-db.html\nNote:\nA Record (Address Record) is typically used when an IP address to name conversion is required.\nThis is most commonly used.\nA record points a name to specific IP.\nIf you want tests.com to point to the server 10.120.13.14, you will configure an A record.\ntests.com A 10.120.13.14\nA CNAME record (Canonical record) points a name to another name.\nTypically a complex name with alphanumeric characters can be shortened to a user understandable format.\nblog.tests.com CNAMEblog235_github_repoABCD.io.net.\nIn the question, the AWS generated resource name is complex i.e myexambledb.a1b2c3d4.......\nThis can be converted to a user-friendly name by using a CNAME record.\n\n\nThe correct answer for this question is D. CNAME - Canonical Name with ALIAS=YES.\nWhen you register a domain name using Amazon Route 53, you can create a DNS record set that maps your domain name to the endpoint of your RDS instance. The endpoint of an RDS instance is a long, partially random, alphanumeric string that is difficult to remember.\nTo map your domain name to the RDS endpoint, you need to create a CNAME record set in your Route 53 hosted zone. A CNAME (Canonical Name) record is a type of DNS record that maps an alias name to a true or canonical domain name.\nThe CNAME record set should have an alias target that points to the RDS endpoint. In Route 53, you can create an ALIAS record set that functions similarly to a CNAME record set, but with some additional benefits, such as automatic health checks and the ability to map to other AWS resources like Elastic Load Balancers.\nHowever, ALIAS record sets are only available for certain types of AWS resources, and RDS instances are not one of them. Therefore, the correct answer is D. CNAME - Canonical Name with ALIAS=YES.\nNote that you cannot create a record set of type A - IPv4 Address because RDS endpoints are domain names, not IP addresses. Additionally, the Alias option must be set to YES to create an alias record that maps to the RDS endpoint.\n\n"
}, {
  "id" : 688,
  "question" : "In AWS Route 53 record set, which of the following is not a routing policy?\n",
  "answers" : [ {
    "id" : "474121f8933744959161b5ba5acd9b70",
    "option" : "Weighted routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "36cd4e0fe5964e0a9a39a21afc49cd42",
    "option" : "Geolocation routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "6cf6b4ca2f1b4ac3ae3b9a57e689f616",
    "option" : "Failover routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "8ef56a171d8d43bd86581559fab2ddbc",
    "option" : "Distributed routing policy.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOptions A, B, C are valid routing policies for AWS Route 53.\nFollowing are list of routing policies.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html.\n\n\nIn AWS Route 53, there are four types of routing policies available to choose from to control how the DNS responds to queries from users:\nWeighted Routing Policy: Allows distributing traffic across multiple resources in proportion to their assigned weights. You can assign a weight to each record, and Route 53 responds to queries based on the probability of each record being returned. Geolocation Routing Policy: Allows directing traffic to specific resources based on the geographic location of the user who made the DNS query. You can create a record set for each geographic location, and Route 53 will respond to queries from that location with the appropriate record. Failover Routing Policy: Allows redirecting traffic from an unhealthy resource to a healthy resource. You can create a primary record for the healthy resource and a secondary record for the backup resource. Route 53 monitors the health of the primary resource, and if it becomes unavailable, Route 53 automatically responds to DNS queries with the secondary record. Latency-based Routing Policy: Allows directing traffic to the resource that provides the lowest latency for the user. Route 53 measures the latency between the user and each resource and responds to DNS queries with the resource that provides the lowest latency.\nThere is no such routing policy as \"Distributed Routing Policy\" in AWS Route 53. Therefore, the correct answer to the given question is option D.\n\n"
}, {
  "id" : 689,
  "question" : "You are planning to launch a web based application in two different regions within US on AWS due to your organization compliance policies.\nYou have setup 2 EC2 instances attached to an elastic load balancer in us-east-1\nYou have replicated the same setup in us-west-1\nNow you have two load balancers which needs to listen traffic from internet.\nYou would want to split the requests equally between both load balancers from a domain name hosted on your AWS Route 53\nHow should you configure your Route 53 record sets?\n",
  "answers" : [ {
    "id" : "575f0328f3fd4d08b31c70f1cb702a32",
    "option" : "Create two record sets, one each for us-east-1 and us-west-1 load balancers. Set weighted routing policy with weights as 1 and 2 respectively.",
    "isCorrect" : "false"
  }, {
    "id" : "a404ffcd6aad4ef2b0c8406927ee4489",
    "option" : "Create two record sets, one each for us-east-1 and us-west-1 load balancers. Set weighted routing policy with weights as 1 and 1 respectively.",
    "isCorrect" : "true"
  }, {
    "id" : "50f4b8b8f9184c30be4ae2b552dfd19f",
    "option" : "Create one record set and select both load balancers as Alias Targets. Set weighted routing policy with weights as 1 and 2 respectively.",
    "isCorrect" : "false"
  }, {
    "id" : "57cac32d28054ac886d50da1dca25168",
    "option" : "Create one record set and select both load balancers as Alias Targets. Set weighted routing policy with weights as 1 and 1 respectively.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted.\nOption A is not correct.\nAccording to above screen shot, with weights 1 and 2, the request distribution is 1:2 which is not equal.\nOption B is correct as the distribution ratio would be 1:1\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/TutorialManagingOver100WRR.html\nOptions C and D are not correct.\nA record set can only have one Alias Target.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-alias.html#rrsets-values-alias-alias-target\n\n\nSure, I can provide a detailed explanation of the answer options for this question.\nScenario: The scenario involves launching a web-based application in two different regions within the US on AWS. Two EC2 instances have been set up in us-east-1 and us-west-1, respectively, and both are attached to an Elastic Load Balancer. Two load balancers have been replicated to cater to each region.\nRequirement: The requirement is to split the requests equally between both load balancers from a domain name hosted on AWS Route 53.\nAnswer Option A: This answer option suggests creating two record sets, one for each load balancer in us-east-1 and us-west-1. The weighted routing policy would be set with weights as 1 and 2, respectively.\nExplanation: Weighted routing policy is used to distribute traffic across multiple resources in proportion to their assigned weights. In this case, the weight assigned to the us-west-1 load balancer is higher than the us-east-1 load balancer. As a result, the traffic distribution would not be equal, and the requirement of splitting requests equally would not be met. Hence, this answer option is incorrect.\nAnswer Option B: This answer option suggests creating two record sets, one for each load balancer in us-east-1 and us-west-1. The weighted routing policy would be set with weights as 1 and 1, respectively.\nExplanation: This option satisfies the requirement of splitting requests equally between both load balancers. By setting the weights as 1 and 1, the Route 53 service would distribute the traffic equally between both load balancers, ensuring that the requests are split equally. Therefore, this answer option is correct.\nAnswer Option C: This answer option suggests creating a single record set and selecting both load balancers as Alias Targets. The weighted routing policy would be set with weights as 1 and 2, respectively.\nExplanation: This option is incorrect as the weighted routing policy is not required to achieve the requirement of splitting requests equally between both load balancers. Moreover, selecting both load balancers as Alias Targets with different weights will result in an unequal distribution of traffic, which does not meet the requirement. Therefore, this answer option is incorrect.\nAnswer Option D: This answer option suggests creating a single record set and selecting both load balancers as Alias Targets. The weighted routing policy would be set with weights as 1 and 1, respectively.\nExplanation: This option is the same as Option B, which satisfies the requirement of splitting requests equally between both load balancers. By setting the weights as 1 and 1, the Route 53 service would distribute the traffic equally between both load balancers, ensuring that the requests are split equally. Therefore, this answer option is correct.\nIn summary, the correct answer to this question is B. Create two record sets, one each for us-east-1 and us-west-1 load balancers. Set weighted routing policy with weights as 1 and 1, respectively.\n\n"
}, {
  "id" : 690,
  "question" : "Which of the following types can be monitored for health checks by AWS Route 53? Choose 2 correct options.\n",
  "answers" : [ {
    "id" : "d55365864ce944a29114939896093fd6",
    "option" : "Endpoints",
    "isCorrect" : "true"
  }, {
    "id" : "55c1ad17773845e79478165c3db30b52",
    "option" : "State of CloudWatch alarm",
    "isCorrect" : "true"
  }, {
    "id" : "9660c8288e3146e6a18cdc2b7c827bf7",
    "option" : "EC2 instance health checks",
    "isCorrect" : "false"
  }, {
    "id" : "1ff1be16fc5745eebf804c9a25d2f998",
    "option" : "DNS service health checks.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/health-checks-types.html\n\n\nAWS Route 53 is a DNS web service provided by Amazon Web Services. It is used for registering domain names, routing internet traffic to the appropriate resources, and also for monitoring the health of these resources.\nRoute 53 can perform health checks to determine the availability and status of resources that it is responsible for. These health checks can be used to automate failover and recovery of resources in the event of an outage.\nNow, let's discuss the given options:\nA. Endpoints: Route 53 can monitor endpoints such as IP addresses, domain names, and URLs, and check their availability and response time. These endpoints can be located anywhere on the internet or within a VPC.\nC. EC2 instance health checks: Route 53 can also monitor the health of EC2 instances by performing checks on their status using the Amazon EC2 API. This allows Route 53 to route traffic only to healthy instances, ensuring high availability and reliability of applications running on EC2.\nD. DNS service health checks: Route 53 can perform health checks on DNS services, such as checking the availability and response time of DNS servers. This helps to ensure that DNS resolution is fast and reliable.\nB. State of CloudWatch alarm: Route 53 does not monitor the state of CloudWatch alarms. CloudWatch is a monitoring service that provides data and insights into AWS resources, but it is not directly related to Route 53's health checking capabilities.\nTherefore, the correct answers are A. Endpoints and C. EC2 instance health checks.\n\n"
} ]