[ {
  "id" : 1,
  "question" : "Which AWS service provides infrastructure security optimization recommendations?\n",
  "answers" : [ {
    "id" : "7d06ec98870946a1b9aa6e9cc40f9db4",
    "option" : "AWS Application Programming Interface(API)",
    "isCorrect" : "false"
  }, {
    "id" : "f3c2fcbe56ed4a19a79b5eaf32d4466f",
    "option" : "Reserved Instances",
    "isCorrect" : "false"
  }, {
    "id" : "d3ffdee5d9ea4b2fa4ddc44758cc8af2",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "true"
  }, {
    "id" : "6deeee197b8c4445bfb48083ac956d24",
    "option" : "Amazon Elastic Compute Cloud (Amazon EC2) SpotFleet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS documentation mentions the following:\nAn online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment, Trusted Advisor provides real time guidance to help you provision your resources following AWS best practices.\nFor more information on the AWS Trusted Advisor, please refer to the below URL:\nhttps://aws.amazon.com/premiumsupport/trustedadvisor/\nChoices A, B, and D are incorrect.\nThey are not related to infrastructure security optimization.\n\nThe correct answer is C. AWS Trusted Advisor.\nAWS Trusted Advisor is a service provided by AWS that offers guidance to optimize AWS infrastructure for security, cost, performance, and fault tolerance. Trusted Advisor provides real-time feedback and recommendations based on the current usage and configuration of AWS services. It leverages AWS best practices and checks for over 100 different criteria across several categories such as Cost Optimization, Performance, Security, Fault Tolerance, and Service Limits.\nRegarding the specific question about infrastructure security optimization recommendations, Trusted Advisor can provide guidance on several security-related topics, including:\nSecurity Groups: checks for unused and overly permissive rules in security groups. IAM: evaluates the use of MFA, least privilege, unused access keys, and more. Network ACLs: checks for unused and overly permissive rules in network access control lists. Amazon S3: checks for publicly accessible buckets, encryption settings, and more. AWS WAF: checks for outdated and unused rules, and security misconfigurations.\nIn summary, AWS Trusted Advisor provides security optimization recommendations by analyzing the configuration of AWS infrastructure for security vulnerabilities, and providing guidance based on AWS best practices.\n\n"
}, {
  "id" : 2,
  "question" : "A file-sharing service uses Amazon S3 to store files uploaded by users.\nFiles are accessed with random frequency.\nPopular ones are downloaded every day whilst others not so often and some rarely.\nWhat is the most cost-effective Amazon S3 object storage class to implement?\n",
  "answers" : [ {
    "id" : "659e8157fc244c8bb0d8a834a7ae48c5",
    "option" : "Amazon S3 Standard",
    "isCorrect" : "false"
  }, {
    "id" : "23635ded82c14cd385e7e2b1d09aa19c",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "77469604e87644148db5a75912c12d54",
    "option" : "Amazon S3 One Zone-Infrequently Accessed",
    "isCorrect" : "false"
  }, {
    "id" : "c8b12ab861f64481825cee80a21894ff",
    "option" : "Amazon S3 Intelligent-Tiering.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nS3 Intelligent-Tiering is a new Amazon S3 storage class designed for customers who want to optimize storage costs automatically when data access patterns change, without performance impact or operational overhead.\nS3 Intelligent-Tiering is the first cloud object storage class that delivers automatic cost savings by moving data between two access tiers - frequent access and infrequent access - when access patterns change, and is ideal for data with unknown or changing access patterns.\nS3 Intelligent-Tiering stores objects in two access tiers: one tier optimized for frequent access and another lower-cost tier optimized for infrequent access.\nFor a small monthly monitoring and automation fee per object, S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier.\nThere are no retrieval fees in S3 Intelligent-Tiering.\nIf an object in the infrequent access tier is accessed later, it is automatically moved back to the frequent access tier.\nNo additional tiering fees apply when objects are moved between access tiers within the S3 Intelligent-Tiering storage class.\nS3 Intelligent-Tiering is designed for 99.9% availability and 99.999999999% durability, and offers the same low latency and high throughput performance of S3 Standard.\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/s3-intelligent-tiering/\nOption A is incorrect because Amazon S3 Standard would be an inefficient class for storing those objects that will be accessed rarely.\nOption B is incorrect because storing objects that are frequently accessed in Amazon S3 Glacier would present operational bottlenecks since these objects would not be available instantly.\nhttps://aws.amazon.com/s3/storage-classes/\nOption C is incorrect because storing those objects that are rarely accessed and those that would be accessed frequently in Amazon S3 One Zone-Infrequently Accessed would be inefficient.\n\nBased on the access patterns provided in the question, the most cost-effective Amazon S3 object storage class to implement is Amazon S3 One Zone-Infrequently Accessed (C).\nAmazon S3 offers different storage classes with varying levels of performance, durability, and cost. The four options provided in the question are:\nA. Amazon S3 Standard: This is the default storage class and provides high durability, availability, and performance. It is suitable for frequently accessed data and has the highest cost compared to other storage classes.\nB. Amazon S3 Glacier: This storage class is designed for data archiving and long-term backup. It has low cost but has a retrieval time of several hours, making it unsuitable for frequently accessed data.\nC. Amazon S3 One Zone-Infrequently Accessed: This storage class is designed for infrequently accessed data that can be recreated if lost. It is less expensive than Amazon S3 Standard and is stored in a single availability zone, reducing its durability compared to other storage classes.\nD. Amazon S3 Intelligent-Tiering: This storage class automatically moves data between two tiers based on changing access patterns. It is designed for data with unknown or changing access patterns and has a higher cost compared to Amazon S3 Standard and One Zone-Infrequently Accessed.\nBased on the access patterns provided in the question, it is clear that not all files are accessed frequently. Popular files are downloaded every day, while others are accessed less frequently or rarely. Amazon S3 One Zone-Infrequently Accessed is the most cost-effective storage class in this scenario since it provides durability for data that can be recreated if lost and has a lower cost than Amazon S3 Standard. Although it is less durable than other storage classes, it is still suitable for storing files that are not critical and can be easily replaced if lost.\nIn conclusion, Amazon S3 One Zone-Infrequently Accessed (C) is the most cost-effective Amazon S3 object storage class to implement for a file-sharing service that stores files uploaded by users and has random access patterns.\n\n"
}, {
  "id" : 3,
  "question" : "Which AWS service can be deployed to enhance read performance for applications while reading data from NoSQL database?\n",
  "answers" : [ {
    "id" : "52e540056db54ea1a29521c1e1c32630",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "44833d3429f24c64bb6e4572bc14634d",
    "option" : "Amazon DynamoDB Accelerator",
    "isCorrect" : "true"
  }, {
    "id" : "6418b0a2903d4cc3be15e69466b2849f",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "35373c767a264d0b91571c8c91a64ea8",
    "option" : "AWS Greengrass.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon DynamoDB Accelerator (DAX) is a caching service for DynamoDB which can be deployed in VPC in a region where DynamoDB is deployed.\nFor read-heavy applications, DAX can be deployed to increase throughput by providing in-memory caching.\nOption A is incorrect because Amazon Route 53 is an AWS DNS service and cannot improve the performance of DynamoDB.Option C is incorrect because Amazon CloudFront is a global content delivery network that cannot be applied to a DynamoDB table.\nOption D is incorrect because AWS Greengrass is data caching software for connected devices.\nFor more information on caching solutions with AWS, refer to the following URL:\nhttps://aws.amazon.com/caching/aws-caching/\n\nThe AWS service that can be deployed to enhance read performance for applications while reading data from NoSQL database is Amazon DynamoDB Accelerator, also known as DynamoDB Accelerator or DAX.\nDynamoDB is a NoSQL database service provided by AWS, which provides fast and flexible querying capabilities with high scalability and performance. However, as the data volume grows, it may take longer to fetch the data and read operations may suffer from latency issues.\nDynamoDB Accelerator (DAX) is an in-memory cache that sits between the application and DynamoDB database, enabling fast, predictable, and consistent performance for read-intensive applications. DAX caches frequently accessed data in memory, thereby reducing the number of read requests to the DynamoDB database.\nDAX is fully managed by AWS and is compatible with DynamoDB API, which means that applications can use the same API calls as they would with DynamoDB, but with improved performance. DAX is also highly available and can be deployed in multiple availability zones to ensure business continuity and disaster recovery.\nTherefore, the correct answer is B. Amazon DynamoDB Accelerator. The other options mentioned in the question are not related to enhancing read performance for applications while reading data from NoSQL database.\nAmazon Route 53 is a highly available and scalable DNS service that can be used to route traffic to different AWS services, including DynamoDB, but it does not directly enhance read performance.\nAmazon CloudFront is a content delivery network (CDN) that can be used to cache and distribute content globally, including static and dynamic content from web applications, but it is not designed to improve read performance of NoSQL databases.\nAWS Greengrass is a software that extends AWS cloud capabilities to local devices and helps to process and analyze data at the edge, but it is not related to enhancing read performance for applications while reading data from NoSQL database.\n\n"
}, {
  "id" : 4,
  "question" : "An organization utilizes a software suite that consists of a multitude of underlying microservices hosted on the cloud.\nThe application is frequently giving runtime errors.\nWhich service will help in the troubleshooting process?\n",
  "answers" : [ {
    "id" : "5fbc021de6ae488f913fdbb79f74519d",
    "option" : "AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "c8b4624ca8064513acb8806daced1f12",
    "option" : "AWS CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "e82ad445cd4743798b46a85209ab8e1e",
    "option" : "AWS X-Ray",
    "isCorrect" : "true"
  }, {
    "id" : "0612153651e6490cb809b4520b6d6f88",
    "option" : "Amazon OpenSearch Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS X-Ray is a service that collects data about requests that your application serves and provides tools that you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.\nAWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\nhttps://aws.amazon.com/xray/\nOption A is INCORRECT because AWS CloudTrail primarily records user or API activity, â€˜who has done what.' It logs, continuously monitors, and retains account activity related to actions across AWS infrastructure.\nCloudTrail provides event history in the AWS account activity but NOT that of the interaction of software microservices within a suite.\nhttps://aws.amazon.com/cloudtrail/\nOption B is INCORRECT because AWS CloudWatch does the primary function of monitoring and NOT debugging.\nIt collates data and actionable insights to monitor applications.\nIt also responds to system-wide performance changes, optimizes resource utilization, and gets a unified view of operational health.\nHowever, the service does neither debug nor logs errors that occur amongst software microservices within a suite.\nhttps://aws.amazon.com/cloudwatch/\nOption D is INCORRECT because Amazon OpenSearch Service is a managed service that makes it easy to deploy, operate, and scale OpenSearch clusters in the AWS Cloud.\nIt automatically detects and replaces failed OpenSearch Service nodes, reducing the overhead associated with self-managed infrastructures.\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\n\nThe service that can help in the troubleshooting process of a software suite consisting of multiple microservices hosted on the cloud, and that frequently gives runtime errors is AWS X-Ray.\nAWS X-Ray is a service that allows developers to analyze and debug distributed applications, such as those consisting of multiple microservices. It provides an end-to-end view of requests as they travel through the system, showing how each microservice contributes to the overall request processing time. This makes it easier to identify bottlenecks and issues that may be causing runtime errors.\nIn addition to providing an overall view of the system, AWS X-Ray also provides detailed trace data for each request, showing the exact path taken by the request through the microservices and the time taken at each step. This allows developers to identify specific issues and drill down into the root cause of the runtime errors.\nAWS CloudTrail is a service that provides a record of all API calls made in an AWS account. It is useful for auditing and compliance purposes but does not provide the detailed information needed for troubleshooting runtime errors in a distributed application.\nAWS CloudWatch is a monitoring service that can be used to monitor the performance of AWS resources, such as EC2 instances and RDS databases. While it can be used to monitor the performance of microservices, it does not provide the end-to-end view of request processing that is needed for troubleshooting runtime errors.\nAmazon OpenSearch Service is a managed search and analytics service that can be used to index and search data. It is not directly related to the troubleshooting of runtime errors in a distributed application.\nTherefore, the best answer to this question is C. AWS X-Ray.\n\n"
}, {
  "id" : 5,
  "question" : "According to the AWS, what is the benefit of Elasticity?\n",
  "answers" : [ {
    "id" : "ea3973722f3348ae9c3508a242c1a868",
    "option" : "Minimize storage requirements by reducing logging and auditing activities",
    "isCorrect" : "false"
  }, {
    "id" : "3d9e0cc2ea914c1585b7405568dd0323",
    "option" : "Create systems that scale to the required capacity based on changes in demand",
    "isCorrect" : "true"
  }, {
    "id" : "3ea753eb9a2e4b1da77f3c01ee2b9c24",
    "option" : "Enable AWS to automatically select the most cost-effective services.",
    "isCorrect" : "false"
  }, {
    "id" : "a535db36d3f94a60bad4e8f563b0174d",
    "option" : "Accelerate the design process because recovery from failure is automated, reducing the need for testing.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe concept of Elasticity is the means of an application having the ability to scale up and scale down based on demand.\nAn example of such a service is the Autoscaling service.\nFor more information on AWS Autoscaling service, please refer to the below URL:\nhttps://aws.amazon.com/autoscaling/\nA, C and D are incorrect.\nElasticity will not have positive effects on storage, cost or design agility.\n\nThe correct answer is B. Elasticity is one of the key benefits of cloud computing and AWS. It refers to the ability of a system to automatically scale up or down resources based on changes in demand, ensuring that the system can handle varying workloads and traffic spikes without compromising performance or availability.\nIn practice, elasticity allows organizations to provision resources dynamically, as needed, instead of overprovisioning upfront and paying for unused capacity. For example, if an online store experiences a surge in traffic during the holiday season, it can automatically scale up its compute and storage resources to handle the increased demand. Conversely, if the traffic subsides, the system can scale down resources to reduce costs.\nElasticity is achieved through a combination of automation and monitoring. AWS provides a range of services that support elasticity, such as Amazon EC2 Auto Scaling, Amazon S3, and Amazon CloudFront. These services enable organizations to define scaling policies, set thresholds, and configure triggers that automatically adjust resources based on workload patterns.\nBy leveraging elasticity, organizations can achieve greater efficiency, agility, and cost savings in their IT operations. They can respond faster to changing business needs, improve performance and availability, and avoid overprovisioning and wasted resources. Overall, elasticity is a key enabler of cloud computing and a fundamental benefit of AWS.\n\n"
}, {
  "id" : 6,
  "question" : "Which tool can you use to forecast your AWS spending?\n",
  "answers" : [ {
    "id" : "afb26fad1ddf475a8b96fb45674e0537",
    "option" : "AWS Organizations",
    "isCorrect" : "false"
  }, {
    "id" : "27f76dd7ee8649b1aca3a94eccdada5a",
    "option" : "Amazon Dev Pay",
    "isCorrect" : "false"
  }, {
    "id" : "391a7c525e05476fa666c250bd6ce670",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "ee8d7cefc62b44549954ff20b8309cd8",
    "option" : "AWS Cost Explorer.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nCost Explorer is a free tool that you can use to view your costs.\nYou can view data up to the last 12 months.\nYou can forecast how much you are likely to spend for the next 12 months and get recommendations for what Reserved Instances to purchase.\nYou can use Cost Explorer to see patterns in how much you spend on AWS resources over time, identify areas that need further inquiry, and see trends that you can use to understand your costs.\nYou also can specify time ranges for the data and view time data by day or by month.\nFor more information on the AWS Cost Explorer, please refer to the below URL:\nhttp://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-explorer-what-is.html\nA, B and C are incorrect.\nThese services do not relate to billing and cost.\n\nThe tool you can use to forecast your AWS spending is AWS Cost Explorer.\nAWS Cost Explorer is a powerful tool that helps you to visualize and manage your AWS spending. With this tool, you can generate reports and get insights into your costs and usage patterns. You can also forecast your future costs based on your historical data.\nBy using Cost Explorer, you can:\nAnalyze your AWS costs: Cost Explorer provides you with a detailed breakdown of your AWS costs by service, region, and resource type. You can also create custom reports and filter your data to get more granular insights. Monitor your spending: Cost Explorer enables you to set up custom cost and usage alarms to alert you when your spending exceeds your budget. You can also track your spending over time and get recommendations on how to optimize your costs. Forecast your costs: Cost Explorer allows you to forecast your future costs based on your historical data. This helps you to plan your budget and avoid any unexpected surprises.\nAWS Organizations is a management tool that helps you to consolidate multiple AWS accounts into a single organization. Amazon Dev Pay is a discontinued service that allowed developers to sell their applications on the AWS Marketplace. AWS Trusted Advisor is a tool that provides recommendations on how to optimize your AWS resources and improve your security and performance. While these services can be helpful for managing your AWS environment, they do not provide cost forecasting capabilities like AWS Cost Explorer does.\n\n"
}, {
  "id" : 7,
  "question" : "Which of the following is an optional Security layer attached to a subnet within a VPC for controlling traffic in &amp; out of the VPC?\n",
  "answers" : [ {
    "id" : "ead70212388f45cb9d26475af462df1a",
    "option" : "VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "b288fa0b4c3c4eafb98fd9e3471538bd",
    "option" : "Web Application Firewall",
    "isCorrect" : "false"
  }, {
    "id" : "acde5e11eef0483da94f948538ed903e",
    "option" : "Security Group",
    "isCorrect" : "false"
  }, {
    "id" : "fe91e14d9c704b6bbc1cd636523ab73b",
    "option" : "Network ACL.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nNetwork ACL can be additionally configured on subnet level to control traffic in &amp; out of the VPC.Option A is incorrect.\nVPC Flow Logs will capture information about IP traffic in &amp; out of VPC.\nThis will not be used for controlling purposes.\nOption B is incorrect.\nWeb Application Firewall (WAF) can be configured to protect web applications from common security threats.\nIt can be deployed on devices such as Amazon CloudFront, Application Load Balancer and Amazon API Gateway.\nOption C is incorrect.\nSecurity Groups are attached at instance level &amp; not at the subnet level.\nFor more information on security within VPC, refer to the following URL:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison\n\nThe correct answer is D. Network ACL.\nNetwork ACL stands for Network Access Control List. It is an optional security layer that can be attached to a subnet within a Virtual Private Cloud (VPC) in AWS. The purpose of a Network ACL is to control traffic in and out of the subnet.\nA Network ACL acts as a firewall at the subnet level, controlling inbound and outbound traffic by allowing or denying traffic based on rules that you define. It is stateless, which means that each rule applies to both inbound and outbound traffic separately. In other words, if you want to allow traffic to enter a subnet, you need to create both an inbound rule and an outbound rule.\nA Network ACL operates at the subnet level, which means that it can control traffic between instances in different subnets within the same VPC. However, it cannot control traffic between different VPCs or between a VPC and the internet. To control traffic between different VPCs or between a VPC and the internet, you need to use a different security layer, such as a VPN connection, an internet gateway, or a NAT gateway.\nIn contrast, Security Groups are another security layer that are attached to instances and control inbound and outbound traffic at the instance level. They are stateful, which means that they automatically allow return traffic that is related to outbound traffic. Web Application Firewall is also a security layer that is used to protect web applications from common web exploits.\nVPC Flow Logs is a monitoring feature that captures information about the IP traffic going to and from network interfaces in a VPC. It can be used to troubleshoot network connectivity issues, monitor the traffic flow, and detect security threats.\nTherefore, the correct answer is D. Network ACL, as it is an optional security layer that can be attached to a subnet within a VPC for controlling traffic in and out of the VPC.\n\n"
}, {
  "id" : 8,
  "question" : "Which of the following is a customer responsibility under AWS Shared Responsibility Model?\n",
  "answers" : [ {
    "id" : "aba8d59a8a424a988a6a34ea2414e40a",
    "option" : "Patching of host OS deployed on Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "8b3b3be47668444babd12312f3efdf89",
    "option" : "Logical Access controls for underlying infrastructure.",
    "isCorrect" : "false"
  }, {
    "id" : "ea9afcd0e37d4f62bdabd37ff72f807f",
    "option" : "Physical security of the facilities.",
    "isCorrect" : "false"
  }, {
    "id" : "2e5b3748944848a3a54545824badcf94",
    "option" : "Patching of guest OS deployed on Amazon EC2 instance.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nUnder the AWS shared responsibility model, AWS takes care of infrastructure configuration &amp; management while customers must take care of the resources they launched within AWS.\nOption A is incorrect.\nAmazon S3 is part of the infrastructure layer &amp; Patching of host OS/Configuration for Amazon S3 is responsibility of AWS.\nOption B is incorrect.\nAWS has the responsibility for the Logical Access controls for the underlying infrastructure.\nOption C is incorrect.\nPhysical Security of the facilities is AWS responsibility.\nFor more information on Shared responsibility model, refer to the following URL:\nhttps://aws.amazon.com/compliance/shared-responsibility-model\n\nThe AWS Shared Responsibility Model defines the division of security and compliance responsibilities between AWS and its customers. Under this model, AWS is responsible for the security of the cloud infrastructure, while customers are responsible for security in the cloud, such as the security of their applications and data.\nAmong the options given, the correct answer is D: Patching of guest OS deployed on Amazon EC2 instance.\nThis is because AWS is responsible for the security of the underlying infrastructure of EC2 instances, including the physical security of the facilities, as well as the logical access controls for the infrastructure. However, customers are responsible for managing and securing the operating systems and applications running on the EC2 instances, which includes keeping the guest OS up to date with security patches and updates.\nOption A, patching of host OS deployed on Amazon S3, is incorrect because S3 is an object storage service and does not have a host OS.\nOption B, logical access controls for underlying infrastructure, is incorrect because AWS is responsible for this aspect of security, including network and infrastructure security controls.\nOption C, physical security of the facilities, is also incorrect because AWS is responsible for the physical security of its data centers and infrastructure, including measures like surveillance, access controls, and environmental controls.\n\n"
}, {
  "id" : 9,
  "question" : "What is the AWS feature that enables fast, easy and secure transfers of files over long distances between your client and your Amazon S3 bucket?\n",
  "answers" : [ {
    "id" : "4042932770cc4e058c394c21d3693c4f",
    "option" : "File Transfer",
    "isCorrect" : "false"
  }, {
    "id" : "4b15241f08ff433890171136b3974bfd",
    "option" : "HTTP Transfer",
    "isCorrect" : "false"
  }, {
    "id" : "4b16d93f8aed460cb15cff4abba170ee",
    "option" : "Amazon S3 Transfer Acceleration",
    "isCorrect" : "true"
  }, {
    "id" : "db9f6e1e66214ee0acc71efaa319cfda",
    "option" : "S3 Acceleration.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAmazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket.\nTransfer Acceleration takes advantage of Amazon CloudFront's globally distributed edge locations.\nAs the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\nFor more information on S3 transfer acceleration, please visit the Link:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nOptions A, B and D are incorrect.\nThese features deal with transferring data but not between clients and an S3 bucket.\n\nThe correct answer is C. Amazon S3 Transfer Acceleration.\nAmazon S3 Transfer Acceleration is a feature provided by AWS to enable fast, easy, and secure transfers of files over long distances between a client and an Amazon S3 bucket.\nWhen a client uploads or downloads an object to/from an S3 bucket, the data must travel over the internet from the client to the AWS data center where the S3 bucket is located. This can lead to slow transfer speeds, particularly for larger files or when the distance between the client and the S3 bucket is significant.\nTo overcome this, AWS Transfer Acceleration leverages Amazon CloudFront's globally distributed edge locations. These edge locations act as intermediate endpoints for transferring data between the client and the S3 bucket. When a client uploads or downloads an object, the data is first sent to the nearest edge location. From there, it is transferred over an optimized network path to the S3 bucket.\nThis results in faster transfer speeds, as the data travels over a more optimized network path, and the use of AWS edge locations reduces the latency of the connection. Additionally, Transfer Acceleration automatically detects and routes around internet congestion, ensuring that data transfers are reliable and predictable.\nTo use Transfer Acceleration, clients can simply enable the feature for their S3 bucket, and then use a specially formatted URL to access the bucket. The URL includes the AWS region, bucket name, and the word \"accelerate\". For example, instead of accessing the bucket using the standard URL \"https://s3.amazonaws.com/mybucket/myobject\", the accelerated URL would be \"https://mybucket.s3-accelerate.amazonaws.com/myobject\".\nIn summary, Amazon S3 Transfer Acceleration is a feature provided by AWS that enables fast, easy, and secure transfers of files over long distances between a client and an S3 bucket. By leveraging AWS edge locations, it reduces latency and optimizes network paths to provide reliable and predictable transfer speeds.\n\n"
}, {
  "id" : 10,
  "question" : "Which of the following services can be used to optimize performance for global users to transfer large-sized data objects to a centralized Amazon S3 bucket in us-west-1 region?\n",
  "answers" : [ {
    "id" : "a96412e23d504718a62c5c7a560cd9c4",
    "option" : "Enable S3 Transfer Acceleration on Amazon S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "3c950410e5cf4157a531f050cae30d18",
    "option" : "Use Amazon CloudFront Put/Post commands",
    "isCorrect" : "false"
  }, {
    "id" : "6b075eed75714c538120d4634193b86b",
    "option" : "Use Multipart upload",
    "isCorrect" : "false"
  }, {
    "id" : "c85f906c07dd49e6b12fbe75df4816d7",
    "option" : "Use Amazon ElastiCache.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nS3 Transfer Acceleration can optimise performance for data transfer between users &amp; objects in Amazon S3 bucket.\nTransfer acceleration uses CloudFront edge location to provide accelerated data transfer to users.\nOption B is incorrect as Amazon CloudFront Put/Post commands can be used for small-sized objects but for large-sized data objects, S3 Transfer Acceleration provides better performance.\nOption C is incorrect as users should use Multipart uploads for all data objects exceeding 100 megabytes.\nBut for better performance, S3 transfer acceleration should be enabled.\nOption D is incorrect as for global users accessing S3 bucket, S3 Transfer Acceleration is a better choice.\nFor more information on Amazon S3 Transfer Acceleration, refer to the following URLs:\nhttps://aws.amazon.com/s3/faqs/#s3ta\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\n\nThe service that can be used to optimize performance for global users to transfer large-sized data objects to a centralized Amazon S3 bucket in us-west-1 region is:\nA. Enable S3 Transfer Acceleration on Amazon S3 bucket.\nAmazon S3 Transfer Acceleration is a service that enables fast, easy, and secure transfers of files over long distances between a client and an S3 bucket. With this service, Amazon S3 utilizes the globally distributed Amazon CloudFront network to accelerate transfers to and from S3 buckets. When a user uploads data to an S3 bucket through S3 Transfer Acceleration, the data is first routed to the closest Amazon CloudFront edge location. From there, it is sent to the S3 bucket over an optimized network path.\nThis service is ideal for transferring large files, as it provides an optimized network path for data transfer, reducing the time required for data transmission. It also helps to ensure that data is transmitted securely.\nB. Amazon CloudFront Put/Post commands\nAmazon CloudFront is a content delivery network (CDN) service that can be used to distribute content to users worldwide. However, it does not provide direct upload capabilities to Amazon S3. CloudFront can be used to distribute content, but the content must first be uploaded to S3 using other methods. So, this option is not suitable for the given scenario.\nC. Multipart upload\nMultipart upload is a feature of Amazon S3 that enables faster, more reliable uploads of large objects. It breaks up large files into smaller parts, which are uploaded in parallel. This feature is useful when uploading files larger than 100 MB. However, it does not help in optimizing the transfer of files to a centralized Amazon S3 bucket in us-west-1 region.\nD. Amazon ElastiCache\nAmazon ElastiCache is a web service that makes it easy to deploy and run an in-memory cache in the cloud. It is used to improve the performance of web applications by caching frequently accessed data in memory. However, it is not related to optimizing the transfer of large-sized data objects to a centralized Amazon S3 bucket in us-west-1 region.\nIn summary, the correct answer is A. Enable S3 Transfer Acceleration on Amazon S3 bucket.\n\n"
}, {
  "id" : 11,
  "question" : "There is a requirement to store objects.\nThe objects must be downloadable via a URL.\nWhich storage option would you choose?\n",
  "answers" : [ {
    "id" : "126891e96a3d4b3d8dbc4cceb51c24af",
    "option" : "Amazon S3",
    "isCorrect" : "true"
  }, {
    "id" : "b99eecd320e94946a9a12ec64f9c023e",
    "option" : "Amazon Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "1989560852c04d3eac5699038f190df7",
    "option" : "Amazon Storage Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "dadfe52afc284be99ebfffe30b57cabd",
    "option" : "Amazon EBS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nAmazon S3 is the perfect storage option.\nIt also provides the facility of assigning a URL to each object which can be used to download the object.\nFor more information on AWS S3, please visit the Link:\nhttps://aws.amazon.com/s3/\nB is incorrect.\nGlacier is for archival and long-term storage.\nThis question is to check the user understanding of AWS S3 service terminology and use cases.\nObjects are stored in S3 and should be downloadable via a URL.\nIt's not possible with EBS.\n\nThe most appropriate option for this scenario would be Amazon S3 (Simple Storage Service).\nAmazon S3 is a scalable and durable cloud-based object storage service that can store and retrieve any amount of data from anywhere on the web. It is designed to provide 99.999999999% durability, and it provides easy access to the stored objects via URLs.\nWhen an object is stored in Amazon S3, it is assigned a unique URL that can be used to access the object from anywhere on the web. This makes it easy to share and distribute objects with others. Additionally, Amazon S3 provides configurable access controls that can be used to restrict access to the stored objects to only authorized users.\nAmazon Glacier is designed for long-term archival storage of data that is rarely accessed. It provides low-cost storage, but the retrieval times can be long (hours to days). This option is not ideal if objects need to be downloadable via a URL.\nAmazon Storage Gateway provides hybrid storage between on-premises environments and the AWS Cloud, enabling the use of cloud storage with existing on-premises applications. However, this option is not ideal if objects need to be downloadable via a URL.\nAmazon EBS (Elastic Block Store) is a block-level storage service designed to provide persistent storage for Amazon EC2 instances. It is used to store data that requires low-latency access, such as operating system or application data. It is not designed for storing and sharing objects via URLs.\nTherefore, the best option for storing and sharing objects via URLs is Amazon S3.\n\n"
}, {
  "id" : 12,
  "question" : "There is a requirement to host a database server for a minimum period of one year.\nWhich of the following would result in the least cost?\n",
  "answers" : [ {
    "id" : "dc26b58ec54b4b1c80c48dec8983b820",
    "option" : "Spot Instances",
    "isCorrect" : "false"
  }, {
    "id" : "9d80538b46314f1494ffef3c5ce7a337",
    "option" : "On-Demand",
    "isCorrect" : "false"
  }, {
    "id" : "9baa50795e7c4c2cb631a3971e270d28",
    "option" : "No Upfront costs Reserved",
    "isCorrect" : "false"
  }, {
    "id" : "eef3f74fecbf4a9490c5f8cfd6af4c87",
    "option" : "Partial Upfront costs Reserved.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nIf the database is going to be used for a minimum of one year at least, it is better to get Reserved Instances.\nYou can save on costs if you use partial upfront options.\nFor more information on AWS Reserved Instances, please visit the Link:\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\nA is incorrect.\nSpot instances can be terminated with fluctuations in market prices.\nUnless the question specifies a use case where high availability is not a requirement, this cannot be assumed.\nB is incorrect.\nOn-Demand is not the most cost-efficient solution.\nC is incorrect.\nNo upfront payment is required.\nHowever, it's a costlier option than Partial/All upfront payment.\nFor more information on the Reserved Instances Payment option, please check below AWS Docs:\nhttps://docs.aws.amazon.com/aws-technical-content/latest/cost-optimization-reservation-models/reserved-instance-payment-options.html\nNote:\nReserved Instances do not renew automatically.\nWhen they expire, you can continue using the EC2 instance without interruption.\nBut you are charged On-Demand rates.\nIn the above example, when the Reserved Instances that cover the T2 and C4 instances expire, you go back to paying the On-Demand rates until you terminate the instances or purchase new Reserved Instances that match the instance attributes.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\n\nTo host a database server for a minimum period of one year, the least cost-effective option would be to choose a Reserved Instance (RI) plan. Reserved Instances allow customers to reserve capacity for a one or three-year term, providing a significant discount over the On-Demand price.\nThe pricing options for Reserved Instances include No Upfront, Partial Upfront, and All Upfront. With the No Upfront option, you pay nothing upfront but commit to paying for the instance over the term of the reservation. With the Partial Upfront option, you pay a portion of the cost upfront and commit to paying for the instance over the term of the reservation. With the All Upfront option, you pay the full amount upfront and receive the maximum discount.\nChoosing a No Upfront cost Reserved Instance would result in the least cost, as it requires no upfront payment, and the discount is applied over the term of the reservation. However, this option may result in slightly higher hourly rates compared to the Partial or All Upfront options.\nSpot Instances are instances that are available at a significant discount compared to On-Demand prices but can be terminated by AWS with little notice when demand exceeds supply. Spot Instances can be a cost-effective option for short-term workloads, but they are not recommended for workloads that need to be available for extended periods.\nOn-Demand instances are instances that you pay for by the hour with no upfront cost or long-term commitment. While they provide maximum flexibility, On-Demand instances are the most expensive pricing option and are not recommended for long-term workloads.\nIn summary, the best option for hosting a database server for a minimum period of one year is to choose a No Upfront cost Reserved Instance. This option provides the greatest cost savings over the one-year term while requiring no upfront payment.\n\n"
}, {
  "id" : 13,
  "question" : "During an organization's information systems audit, the administrator is requested to provide a dossier of security and compliance reports and online service agreements between the organization and AWS.\nWhich service can they utilize to acquire this information?\n",
  "answers" : [ {
    "id" : "cd26a81e035f432fb1e2ee7c1fbb8542",
    "option" : "AWS Artifact",
    "isCorrect" : "true"
  }, {
    "id" : "521e89b73b1a4deea2c2c74981f35c09",
    "option" : "AWS Resource Center",
    "isCorrect" : "false"
  }, {
    "id" : "7d765abee7414a54af8c15f48cb40b43",
    "option" : "AWS Service Catalog",
    "isCorrect" : "false"
  }, {
    "id" : "24f5841769bc47e28d56a6d0037f6fed",
    "option" : "AWS Directory Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Artifact is a comprehensive resource center to have access to the AWS' auditor-issued reports and security and compliance documentation from several renowned independent standard organizations.\nhttps://aws.amazon.com/artifact/\nOption B is INCORRECT.\nAWS Resource Center is arepository of tutorials, whitepapers, digital training, and project use cases that aid in learning the core concepts of Amazon Web Services.\nhttps://aws.amazon.com/getting-started/\nOption C is INCORRECT.\nAWS Service Catalog allows organizations to create and save their own IT service catalogs for further use.\nBut they have to be approved by AWS.\nIT service catalogs can be multi-tiered application architectures.\nhttps://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\nOption D is INCORRECT.\nAWS Directory Service is an AWS tool that provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory with other AWS services.\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\n\nThe service that an administrator can utilize to acquire security and compliance reports and online service agreements between an organization and AWS is AWS Artifact.\nAWS Artifact is a centralized repository of all the compliance documentation and reports required to satisfy various regulatory and compliance requirements. It provides on-demand access to AWS's documentation and audit reports for a range of compliance programs, including SOC, ISO, PCI, HIPAA, and more.\nBy using AWS Artifact, administrators can obtain various documents and reports, including service agreements, system architecture diagrams, compliance reports, and third-party assessments of AWS's compliance with various regulatory standards. Additionally, AWS Artifact allows users to easily manage and audit their own compliance with AWS services.\nAWS Resource Center is a portal for accessing technical and business resources to help users with their AWS adoption and operations, but it does not provide compliance documentation or reports.\nAWS Service Catalog allows administrators to create and manage catalogs of IT services that are approved for use on AWS, while AWS Directory Service is a managed service that makes it easy to connect AWS resources to an on-premises directory.\nTherefore, the correct answer to the given question is A. AWS Artifact.\n\n"
}, {
  "id" : 14,
  "question" : "A new department has recently joined the organization and the administrator needs to compose access permissions for the group of users.\nGiven that they have various roles and access needs, what is the best-practice approach when granting access?\n",
  "answers" : [ {
    "id" : "4c4f16cac8e14ec89b4bab1d220c9d52",
    "option" : "After gathering information on their access needs, the administrator should allow every user to access the most common resources and privileges on the system.",
    "isCorrect" : "false"
  }, {
    "id" : "26edb2f29b954ff2adf688f6fa96d2b2",
    "option" : "The administrator should grant all users the same permissions and then grant more upon request.",
    "isCorrect" : "false"
  }, {
    "id" : "ac11afbe83334b01a3c66fb38fafce47",
    "option" : "The administrator should grant all users the least privilege and add more privileges to only to those who need it.",
    "isCorrect" : "true"
  }, {
    "id" : "2ca1c33f2a7749b6800d090cae8c6659",
    "option" : "Users should have no access and be granted temporary access on the occasions that they need to execute a task.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe best-practice for AWS Identity Access Management (IAM) is to grant the least amount of permissions on the system only to execute the required tasks of the user's role.\nAdditional permissions can be granted per user according to the tasks they wish to perform on the system.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege\nOption A is incorrect because granting users access to the most common resources presents security vulnerabilities, especially from those who have access to resources they do not need.\nOption B is incorrect because granting users the same privileges on the system means other users might get access to resources they do not need to carry out their job functions.\nThis presents a security risk.\nOption D is incorrect because the users are part of the organisation; it will be cumbersome for the administrator to create temporal access passes for internal staff constantly.\n\nWhen granting access permissions for a new department, the best practice approach is to grant each user the least privilege necessary to perform their job functions. This means that users should only be given access to the specific resources and privileges that they need to do their work, and no more.\nOption A, which involves granting every user access to the most common resources and privileges, is not ideal because it can result in users having access to resources that they don't need, which can increase the risk of security breaches or accidental damage to the system.\nOption B, which involves granting all users the same permissions and then granting more upon request, can be inefficient and time-consuming for the administrator, as they will have to field numerous requests for additional privileges. Moreover, it can lead to users having unnecessary access in the interim period while they wait for their request to be granted.\nOption D, which involves granting users temporary access on an as-needed basis, can be an effective approach for highly sensitive or critical resources. However, it can be impractical for everyday work and can result in delays and inefficiencies if users have to wait for access every time they need to perform a task.\nTherefore, option C, which involves granting all users the least privilege and adding more privileges only to those who need it, is the best-practice approach for granting access permissions. This approach ensures that users only have access to the resources they need to do their jobs, which reduces the risk of security breaches or accidental damage to the system. It also simplifies access management for the administrator and reduces the number of requests for additional privileges.\n\n"
}, {
  "id" : 15,
  "question" : "There is an external audit being carried out on your company.\nThe IT auditor needs to have a log of 'who made the requests' to the AWS resources in the company's account.\nWhich of the below services can assist in providing these details?\n",
  "answers" : [ {
    "id" : "017683b485234527b632cdb3b876d6d2",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "80b41d85e309420ab591fb6e15bdee67",
    "option" : "AWS CloudTrail",
    "isCorrect" : "true"
  }, {
    "id" : "4990147d28ad4f6293ebd4dfd78edff6",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "64b61454b95b480e97365d06fe1f64f6",
    "option" : "AWS SNS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nUsing CloudTrail, one can monitor all the API activity conducted on all AWS services.\nThe AWS Documentation additionally mentions the following.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nCloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nThis event history simplifies security analysis, resource change tracking, and troubleshooting.\nFor more information on AWS Cloudtrail, please refer to the below URL:\nhttps://aws.amazon.com/cloudtrail/\n\nThe service that can assist in providing details about who made the requests to the AWS resources in the company's account is AWS CloudTrail (Option B).\nAWS CloudTrail is a web service that records AWS API calls made in a user's account and delivers log files containing those calls to an Amazon S3 bucket or CloudWatch Logs stream. CloudTrail logs are very useful in auditing, compliance, and security scenarios. CloudTrail records all the API calls made by users, roles, or services within the account and provides information about the identity of the entity that made the call, the time the call was made, and the parameters used in the API call.\nIn this scenario, the IT auditor needs to have a log of who made the requests to the AWS resources in the company's account. By enabling CloudTrail, the auditor can get this information. CloudTrail logs will capture all API requests made to AWS resources in the company's account and provide information about the identity of the entity that made the request. The logs can then be analyzed to identify who made the request to AWS resources.\nAWS CloudWatch (Option A) is a monitoring service that provides metrics and logs about AWS resources and applications. It can also provide alerts and notifications when thresholds are reached. However, CloudWatch is not specifically designed for tracking API calls or logging who made those calls.\nAWS EC2 (Option C) is a web service that provides resizable compute capacity in the cloud. It is a service for launching and managing virtual machines. It does not provide any logging or monitoring capabilities for API calls or users.\nAWS SNS (Option D) is a messaging service that enables message delivery to a variety of endpoints including email, SMS, mobile push, and HTTP endpoints. It is not designed for logging or monitoring API calls or users.\nTherefore, the correct option is B - AWS CloudTrail.\n\n"
}, {
  "id" : 16,
  "question" : "Which of the following features can be used to preview changes to be made to an AWS resource which will be deployed using the AWS CloudFormation template?\n",
  "answers" : [ {
    "id" : "f46188784d1c4b069dd1bd46efcca286",
    "option" : "AWS CloudFormation Drift Detection",
    "isCorrect" : "false"
  }, {
    "id" : "48f52d52c85a4006beefc4eb23098878",
    "option" : "AWS CloudFormation Change Sets",
    "isCorrect" : "true"
  }, {
    "id" : "f6cd2827e7c74a4ea483b03ff22252de",
    "option" : "AWS CloudFormation Stack Sets",
    "isCorrect" : "false"
  }, {
    "id" : "a32fc9475594440e92bf88d7202fd5a3",
    "option" : "AWS CloudFormation Intrinsic Functions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS CloudFormation Change Set can be used to preview changes to AWS resources when a stack is executed.\nOption A is incorrect as AWS CloudFormation Drift Detection is used to detect any changes made to resources outside of CloudFormation templates.\nIt would not be able to preview changes that will be made by CloudFormation Templates.\nOption C is incorrect as these are groups of stacks that are managed together.\nOption D is incorrect as these Intrinsic Functions are used for assigning values to properties in CloudFormation templates.\nFor more information on AWS CloudFormation, refer to the following URL:\nhttps://aws.amazon.com/cloudformation/features/\n\nThe feature that can be used to preview changes to be made to an AWS resource which will be deployed using the AWS CloudFormation template is AWS CloudFormation Change Sets.\nAWS CloudFormation is a service that allows users to model and provision AWS resources with an easy-to-use template. Change Sets, a feature of AWS CloudFormation, helps to preview the changes that will occur to your AWS infrastructure before you deploy them.\nWhen creating a Change Set, AWS CloudFormation analyzes the differences between the current stack and the updated template. The Change Set then provides a detailed report that outlines the proposed changes, such as which resources will be created, modified, or deleted, and the estimated impact on your resources.\nChange Sets can help you to evaluate the potential impact of changes to your infrastructure, review the changes before applying them, and avoid any unexpected changes that could lead to service disruptions or other problems.\nAWS CloudFormation Drift Detection is another feature that can be used to detect any differences between a stack's actual configuration and its expected configuration. However, it is not used for previewing changes to be made to a resource.\nAWS CloudFormation Stack Sets is a feature that allows you to deploy CloudFormation stacks across multiple accounts and regions. It is also not used for previewing changes.\nAWS CloudFormation Intrinsic Functions are built-in functions that can be used in a CloudFormation template to create dynamic templates. They are not used for previewing changes either.\n\n"
}, {
  "id" : 17,
  "question" : "Which of the following is the responsibility of the customer to ensure the availability and backup of the EBS volumes?\n",
  "answers" : [ {
    "id" : "e8ff87c0ddac4c2e91ff31dc8c98b2ae",
    "option" : "Delete the data and create a new EBS volume.",
    "isCorrect" : "false"
  }, {
    "id" : "3a0aae8e06c04b55a7db07da84df51fe",
    "option" : "Create EBS snapshots.",
    "isCorrect" : "true"
  }, {
    "id" : "756df4470f824fc692743807ba129b5b",
    "option" : "Attach new volumes to EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "d100da6f556743088710fa4c0deb60de",
    "option" : "Create copies of EBS Volumes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nSnapshots are incremental backups, which means that only the blocks on the device that have changed after your most recent snapshot are saved.\nWhen you create an EBS volume based on a snapshot, the new volume begins as an exact replica of the original volume that was used to create the snapshot.\nThe replicated volume loads data in the background so that you can begin using it immediately.\nOption A is incorrect because there is no need for backup of the volumes if data is already deleted.\nOption C is incorrect because attaching more EBS volumes doesn't ensure availability, if there is no snapshot then the volume cannot be available to a different availability zone.\nOption D is incorrect EBS volumes cannot be copied, they can only be replicated using snapshots.\nFor more information on EBS Snapshots, please refer to the below URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\n\nThe correct answer is B. Create EBS snapshots.\nExplanation: Amazon Elastic Block Store (EBS) is a block-level storage service that provides persistent block-level storage volumes for use with Amazon EC2 instances. Amazon EBS volumes are designed for durability and availability, but it is still the customer's responsibility to ensure data protection and availability in case of failure.\nEBS snapshots are point-in-time copies of EBS volumes, which are stored in Amazon S3. They can be used to protect data against accidental deletion, data corruption, or other issues. EBS snapshots are incremental, meaning that only the blocks that have changed since the last snapshot are saved. This can help to reduce costs and time required for backups.\nTherefore, creating EBS snapshots is the responsibility of the customer to ensure the availability and backup of the EBS volumes. This practice provides data protection and allows for the recovery of data in case of a disaster.\nA. Deleting the data and creating a new EBS volume is not a backup solution and does not provide data protection or availability in case of a failure.\nC. Attaching new volumes to EC2 instances is not a backup solution and does not provide data protection or availability in case of a failure.\nD. Creating copies of EBS volumes is not a backup solution and does not provide data protection or availability in case of a failure.\n\n"
}, {
  "id" : 18,
  "question" : "When designing a highly available architecture, what is the difference between vertical scaling (scaling-up) and horizontal scaling (scaling-out)?\n",
  "answers" : [ {
    "id" : "7ac27400559940e0bd3129bc1146af53",
    "option" : "Scaling up provides for high availability whilst scaling out brings fault-tolerance.",
    "isCorrect" : "false"
  }, {
    "id" : "c2381b504dd041bbad2e018a05747f59",
    "option" : "Scaling out is not cost-effective compared to scaling up.",
    "isCorrect" : "false"
  }, {
    "id" : "6e05bbd8ca4d4e218d1a63c2ade972af",
    "option" : "Scaling up adds more resources to an instance, scaling out adds more instances.",
    "isCorrect" : "true"
  }, {
    "id" : "e77d227b6d0243d99893d835d098dfa8",
    "option" : "Autoscaling groups require scaling up whilst launch configurations use scaling out.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nIn high availability architectures, Autoscaling is used to give elasticity to the design.\nHorizontal scaling (scaling-out) uses Autoscaling groups to increase processing capacity in response to changes in preset threshold parameters.\nIt could involve adding more EC2 instances of a web server.\nVertical scaling (scaling-up), which can create a single point of failure, involves adding more resources to a particular instance to meet demand.\nhttps://docs.aws.amazon.com/autoscaling/plans/userguide/what-is-aws-auto-scaling.html\nOption A is INCORRECT.\nScaling-up does not provide high availability.\nAdding more resources to one instance is often not a best-practice in architecture design.\nOption B is INCORRECT.\nScaling-out is cost-effective since it involves adding more resources in response to demand and reducing resources (scaling down) when demand is low.\nOption D is INCORRECT.\nAll Autoscaling groups require a launch configuration based on what resources would be provisioned or deprovisioned to meet predefined parameters.\n\nWhen designing a highly available architecture, scaling is an essential factor to consider to ensure that the system can handle increasing traffic and workload demands. There are two types of scaling: vertical scaling (scaling up) and horizontal scaling (scaling out).\nVertical scaling involves adding more resources such as CPU, memory, and storage capacity to a single server or instance. This type of scaling is often referred to as \"scaling up\" because it increases the power of a single machine.\nHorizontal scaling, on the other hand, involves adding more instances or servers to distribute the workload across multiple machines. This type of scaling is also referred to as \"scaling out\" because it increases the number of machines in the system.\nNow let's go over the options:\nA. Scaling up provides for high availability whilst scaling out brings fault-tolerance. This answer is incorrect. Both scaling up and scaling out can contribute to high availability and fault-tolerance. However, the approaches differ in how they achieve this.\nVertical scaling (scaling up) can provide high availability by adding more resources to a single server, which can help the server handle higher loads without crashing. But, vertical scaling does not inherently provide fault-tolerance since all resources are concentrated on a single machine, and if it fails, the entire system goes down.\nHorizontal scaling (scaling out) provides fault-tolerance by spreading the workload across multiple instances, and if one instance fails, the other instances can take over the load. Horizontal scaling can also provide high availability by allowing the system to handle more traffic since there are more instances available to process requests.\nB. Scaling out is not cost-effective compared to scaling up. This answer is also incorrect. In general, horizontal scaling (scaling out) can be more cost-effective than vertical scaling (scaling up) since it relies on commodity hardware, which is often less expensive than high-end hardware used for vertical scaling. Additionally, scaling out provides greater flexibility since you can add or remove instances as needed, whereas scaling up requires replacing hardware with more powerful components, which can be costly.\nC. Scaling up adds more resources to an instance, scaling out adds more instances. This answer is correct. As explained earlier, vertical scaling (scaling up) adds more resources to a single instance, while horizontal scaling (scaling out) adds more instances to the system.\nD. Autoscaling groups require scaling up whilst launch configurations use scaling out. This answer is incorrect. Both autoscaling groups and launch configurations can be used for either vertical scaling (scaling up) or horizontal scaling (scaling out). Autoscaling groups can scale out by adding more instances to the group based on traffic demand, and launch configurations can be used to create multiple instances with the same configuration. Both approaches can be used for vertical scaling by increasing the resources of a single instance.\n\n"
}, {
  "id" : 19,
  "question" : "Your company is planning to move to the AWS Cloud.\nYou need to give a presentation on the cost perspective when moving existing resources to the AWS Cloud.\nConsidering Amazon EC2, which of the following is an advantage from the cost perspective?\n",
  "answers" : [ {
    "id" : "7aec23b8826c40f4b539e548d8765953",
    "option" : "Having the ability of automated backups of the EC2 instance, so that you donâ€™t need to worry about the maintenance costs.",
    "isCorrect" : "false"
  }, {
    "id" : "8448388e9ca7481fa914efcf2ddad836",
    "option" : "The ability to choose low cost AMIâ€™s to prepare the EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "6aa0f691f00c4cdfb5adb115b387cb75",
    "option" : "The ability to only pay for what you use.",
    "isCorrect" : "true"
  }, {
    "id" : "6c9366f4c70b424491e957a9ee8f0dad",
    "option" : "Ability to tag instances to reduce the overall cost.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOne of the advantages of EC2 Instances is the per-second billing concept.\nThis is also given in the AWS documentation.\nWith per-second billing, you pay for only what you use.\nIt takes the cost of unused minutes and seconds in an hour off of the bill.\nSo, you can focus on improving your applications instead of maximizing usage to the hour especially if you manage instances running for irregular periods of time, such as dev/testing, data processing, analytics, batch processing and gaming applications.\nFor more information on EC2 Pricing, please refer to the below URL:\nhttps://aws.amazon.com/ec2/pricing/\n\nSure, I'd be happy to provide a detailed explanation of the advantages of Amazon EC2 from a cost perspective.\nAmazon Elastic Compute Cloud (EC2) is a web service that provides resizable compute capacity in the cloud, allowing you to scale your computing resources up or down as your requirements change. Here are some of the advantages of Amazon EC2 from a cost perspective:\nA. Having the ability of automated backups of the EC2 instance, so that you don't need to worry about the maintenance costs.\nAutomated backups can help you reduce maintenance costs by eliminating the need to manually create and manage backups. Amazon EC2 provides automated backup capabilities that allow you to take regular snapshots of your instances, which can be used to restore data or recreate the entire instance if necessary. By using automated backups, you can save time and resources while also ensuring that your data is protected.\nB. The ability to choose low-cost AMIs to prepare the EC2 Instances.\nAmazon Machine Images (AMIs) are pre-configured virtual machine images that you can use to launch EC2 instances. You can choose from a wide variety of AMIs, including low-cost options, which can help you reduce your overall EC2 costs. By selecting the appropriate AMI, you can ensure that you are only paying for the features and resources that you actually need.\nC. The ability to only pay for what you use.\nOne of the primary advantages of cloud computing is the ability to pay only for the resources that you use. With Amazon EC2, you can choose from a variety of pricing models, including On-Demand, Reserved Instances, and Spot Instances. This allows you to select the most cost-effective option for your specific use case. Additionally, Amazon EC2 provides tools and services to help you monitor and optimize your usage, which can help you further reduce costs.\nD. Ability to tag instances to reduce the overall cost.\nTags are metadata that you can assign to your Amazon EC2 instances to help you categorize and manage your resources. By using tags, you can easily identify and manage your instances, which can help you optimize your usage and reduce costs. For example, you can use tags to identify instances that are not being used or that are overprovisioned, and then take action to reduce or eliminate those resources.\nIn conclusion, Amazon EC2 provides a number of cost-saving advantages, including automated backups, low-cost AMIs, pay-as-you-go pricing, and tagging capabilities. By taking advantage of these features, you can optimize your usage and reduce your overall costs when moving resources to the AWS Cloud.\n\n"
}, {
  "id" : 20,
  "question" : "When designing a system, you use the principle of â€œdesign for failure and nothing will failâ€\nWhich of the following services/features of AWS can assist in supporting this design principle? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "a5be195dddfb4969b965eab8d32809c2",
    "option" : "Availability Zones",
    "isCorrect" : "true"
  }, {
    "id" : "82cbd7948f44456e98811f7b5abd8a18",
    "option" : "Regions",
    "isCorrect" : "true"
  }, {
    "id" : "d9707141b35647f4a9ec3cc45abfcdd1",
    "option" : "Elastic Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "8e0892a5a87343b38e95aa2bc645a283",
    "option" : "Pay as you go.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A, B and C.\nEach AZ is a set of one or more data centers.\nBy deploying your AWS resources to multiple Availability zones, you are designing with failure in mind.\nSo if one AZ were to go down, the other AZ's would still be up and running.\nHence your application would be more fault-tolerant.\nFor disaster recovery scenarios, one can move or make resources run in other regions.\nAnd finally, one can use the Elastic Load Balancer to distribute load to multiple backend instances within a particular region.\nFor more information on AWS Regions and AZ's, please refer to the below URL:\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html\n\nThe principle of \"design for failure and nothing will fail\" means that when designing a system, you assume that failure is inevitable and design the system in a way that it can tolerate and recover from failures. This principle is also known as \"resiliency\" in the context of cloud computing. AWS provides several services and features that can assist in supporting this design principle, including:\nA. Availability Zones: AWS provides multiple Availability Zones (AZs) in each region. An Availability Zone is a separate data center that is isolated from other AZs in terms of power, networking, and cooling. By deploying your application across multiple AZs, you can ensure that your application remains available even if one AZ goes down due to a disaster, outage, or maintenance activity. AWS provides a 99.99% uptime SLA for each AZ.\nB. Regions: AWS has multiple regions across the world, each consisting of multiple Availability Zones. By deploying your application across multiple regions, you can ensure that your application remains available even if one region goes down due to a disaster, outage, or other event. However, deploying across multiple regions can introduce higher latency and higher costs due to data transfer fees.\nC. Elastic Load Balancer: AWS provides Elastic Load Balancers (ELBs) that can distribute traffic across multiple instances of your application running in different AZs. By using ELBs, you can ensure that traffic is directed to healthy instances and avoid overloading any single instance. ELBs can also automatically detect and redirect traffic away from unhealthy instances, thereby ensuring high availability of your application.\nD. Pay as you go: \"Pay as you go\" is not a service or feature of AWS that directly supports the principle of \"design for failure.\" It is a pricing model that allows you to pay only for the resources that you consume, without any upfront costs or long-term commitments. This pricing model can be beneficial when designing a resilient architecture, as you can provision resources across multiple AZs or regions without worrying about underutilization or overprovisioning.\nIn summary, the three services/features of AWS that can assist in supporting the \"design for failure\" principle are Availability Zones, Regions, and Elastic Load Balancers.\n\n"
}, {
  "id" : 21,
  "question" : "Your design team is planning to design an application that will be hosted on the AWS Cloud.\nOne of their main non-functional requirements is given below: Reduce inter-dependencies so failures do not impact other components. Which of the following concepts does this requirement relate to?\n",
  "answers" : [ {
    "id" : "a2ede1e32453405a85b734576094e27a",
    "option" : "Integration",
    "isCorrect" : "false"
  }, {
    "id" : "639ffd560e3e47beb7b15f0620c6ed28",
    "option" : "Decoupling",
    "isCorrect" : "true"
  }, {
    "id" : "c15c9201da434e43ad33ea81961b563a",
    "option" : "Aggregation",
    "isCorrect" : "false"
  }, {
    "id" : "29c93130bbe04982bd9b13c65627fae1",
    "option" : "Segregation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe entire concept of decoupling components ensures that the different components of applications can be managed and maintained separately.\nIf all components are tightly coupled, the entire application would go down when one component goes down.\nHence it is always a better practice to decouple application components.\nFor more information on a decoupled architecture, please refer to the below URL:\nhttp://whatis.techtarget.com/definition/decoupled-architecture\n\nThe non-functional requirement of reducing inter-dependencies so failures do not impact other components relates to the concept of decoupling.\nDecoupling refers to the practice of designing an application or system in a way that minimizes dependencies between its components. This approach makes the system more flexible and resilient because it reduces the impact of failures in one component on other components.\nIn the context of AWS Cloud, decoupling is typically achieved by using loosely coupled services that communicate through well-defined APIs. For example, AWS Lambda, Amazon Simple Notification Service (SNS), and Amazon Simple Queue Service (SQS) are commonly used services that help to decouple components in an AWS-based application.\nBy using these services, developers can create a more modular architecture that isolates failures and improves the overall reliability of the application. Additionally, decoupling can also improve scalability, as it allows developers to scale individual components independently without affecting the rest of the system.\nIn summary, decoupling is an important concept in designing applications for the AWS Cloud because it helps to reduce inter-dependencies and improves the reliability and scalability of the system.\n\n"
}, {
  "id" : 22,
  "question" : "Which of the following security requirements are managed by AWS? Select 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "0048a1d9231d480b92aad8629b4c40ba",
    "option" : "Password Policies",
    "isCorrect" : "false"
  }, {
    "id" : "727970af18a04bbd809e643ccd8e396b",
    "option" : "User permissions",
    "isCorrect" : "false"
  }, {
    "id" : "075a92fab3af4eb99d5e7ee5587de474",
    "option" : "Physical security",
    "isCorrect" : "true"
  }, {
    "id" : "6960e8b93f9b4dd0b38fb865b4eb61d4",
    "option" : "Disk disposal",
    "isCorrect" : "true"
  }, {
    "id" : "ac282865b94e4bfb915083cf454b2a39",
    "option" : "Hardware patching.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C, D and E.\nAs per the Shared Responsibility Model, the Patching of the underlying hardware and physical security of AWS resources is the responsibility of AWS.\nFor more information on AWS Shared Responsibility Model, please refer to the below URL-\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\nDisk disposal-\nStorage Device Decommissioning: When a storage device has reached the end of its useful life, AWS procedures include a decommissioning process designed to prevent customer data from being exposed to unauthorized individuals.\nAWS uses the techniques detailed in DoD 5220.22-M (â€œNational Industrial Security Program Operating Manual â€œ) or NIST 800-88 (â€œGuidelines for Media Sanitizationâ€) to destroy data as part of the decommissioning process.\nAll decommissioned magnetic storage devices are degaussed and physically destroyed in accordance with industry-standard practices.\nFor more information on Disk disposal, please refer to the below URL-\nhttps://d0.awsstatic.com/whitepapers/aws-security-whitepaper.pdf\n\nAWS is responsible for the security of the cloud, which includes the physical infrastructure and the underlying software. However, AWS customers are responsible for securing the applications and data they deploy in the cloud. The shared responsibility model outlines the division of security responsibilities between AWS and its customers.\nOut of the given options, the following three security requirements are managed by AWS:\nPhysical security: AWS is responsible for the physical security of its data centers, including the protection of the buildings, servers, and networking equipment. AWS data centers are built to meet the most stringent physical security standards, and they have multiple layers of security controls, such as biometric authentication, video surveillance, and 24/7 onsite security staff. Hardware patching: AWS manages the underlying infrastructure of the cloud, including the servers, storage, and networking equipment. AWS performs routine maintenance and updates to ensure that the infrastructure is secure and up-to-date. Disk disposal: AWS ensures secure disk disposal by overwriting the storage media with random data before reusing it, and physically destroying it if necessary. This ensures that no data can be recovered from the storage media, even if it falls into the wrong hands.\nThe other two options, password policies and user permissions, are the responsibility of the AWS customer. AWS provides tools and services to help customers manage user access and authentication, but it is ultimately up to the customer to ensure that their password policies and user permissions are secure. Similarly, AWS provides guidelines for disk disposal, but the customer is responsible for implementing these guidelines for their own data.\n\n"
}, {
  "id" : 23,
  "question" : "You are planning on deploying a video-based application onto the AWS Cloud.\nUsers across the world will access these videos.\nWhich of the below services can help efficiently stream the content to the users across the globe?\n",
  "answers" : [ {
    "id" : "a71a2e30ea7648ef814e5c2274fdbf18",
    "option" : "Amazon SES",
    "isCorrect" : "false"
  }, {
    "id" : "9103b124f97f4690adfc1641f3a843fc",
    "option" : "Amazon Cloudtrail",
    "isCorrect" : "false"
  }, {
    "id" : "4d869eeb27e34abeb3b065edbc62a820",
    "option" : "Amazon CloudFront",
    "isCorrect" : "true"
  }, {
    "id" : "bf3001de191d47a2bc9d3c4b3cf27bfb",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following:\nAmazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds.\nLike other AWS services, Amazon CloudFront is a self-service, pay-per-use offering, requiring no long term commitments or minimum fees.\nWith CloudFront, your files are delivered to end-users using a global network of edge locations.\nFor more information on CloudFront, please visit the link:\nhttps://aws.amazon.com/cloudfront/\n\nThe service that can efficiently stream content to users across the globe is Amazon CloudFront.\nAmazon CloudFront is a content delivery network (CDN) that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, and no minimum usage commitments. It integrates with other AWS services such as Amazon S3, Elastic Load Balancing, and AWS Shield for enhanced performance, security, and availability.\nBy using Amazon CloudFront, you can cache your video content at edge locations around the world, which are geographically closer to your users. This ensures that users can access the video content with low latency and high transfer speeds, reducing buffering and improving user experience.\nAmazon SES (Simple Email Service) is an email service used for sending bulk and transactional emails, but it's not relevant to streaming video content.\nAmazon CloudTrail is a service that logs, monitors, and retains account activity related to actions across your AWS infrastructure, but it's not relevant to streaming video content.\nAmazon S3 (Simple Storage Service) is an object storage service that can store and retrieve any amount of data from anywhere, but it's not specifically designed for streaming video content to end-users globally.\nTherefore, the correct answer is C. Amazon CloudFront.\n\n"
}, {
  "id" : 24,
  "question" : "Which of the following AWS services can be used to retrieve configuration changes made to AWS resources causing operational issues?\n",
  "answers" : [ {
    "id" : "d350a64f39c94e408ed6c45e86bc90f4",
    "option" : "Amazon Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "83968631027c45eaab9213e744a9b6fc",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "2c38c43dd72546738a2b6e06efbf4c26",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "6d137e3d45364ecaaf2c3840d3e24b39",
    "option" : "AWS Config.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Config can be used to audit, evaluate configurations of AWS resources.\nIf there are any operational issues, AWS config can be used to retrieve configurational changes made to AWS resources that may have caused these issues.\nOption A is incorrect as Amazon Inspector can be used to analyze potential security threats for an Amazon EC2 instance against an assessment template with predefined rules.\nIt does not provide historical data for configurational changes done to AWS resources.\nOption B is incorrect as AWS CloudFormation provided templates to provision and configure resources in AWS.\nOption C is incorrect as AWS Trusted Advisor can help optimize resources with AWS cloud with respect to cost, security, performance, fault tolerance, and service limits.\nIt does not provide historical data for configurational changes done to AWS resources.\nFor more information on AWS Config, refer to the following URL:\nhttps://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\n\nThe AWS service that can be used to retrieve configuration changes made to AWS resources causing operational issues is AWS Config (Option D).\nAWS Config is a fully managed service that provides you with an inventory of your AWS resources, the history of configurations and changes, and configuration compliance against rules specified by you. It continuously tracks and records changes made to AWS resources and configuration item details, which can be accessed through the AWS Config console or API.\nIn the case of operational issues, AWS Config can be used to retrieve configuration changes made to AWS resources that could have caused the issue. You can view the configuration history for a particular resource, and identify any changes that were made leading up to the issue.\nOption A, Amazon Inspector, is a security assessment service that helps in identifying vulnerabilities and deviations from best practices in applications running on AWS. It does not provide configuration change tracking.\nOption B, AWS CloudFormation, is a service that helps in creating and managing AWS resources through templates. It does not provide configuration change tracking.\nOption C, AWS Trusted Advisor, provides real-time guidance to help you optimize your resources for performance, security, and cost. It does not provide configuration change tracking.\nTherefore, the correct answer is Option D, AWS Config.\n\n"
}, {
  "id" : 25,
  "question" : "An organization runs several EC2 instances inside a VPC using three subnets, one for Development, one for Test and one for Production.\nThe Security team has some concerns about the VPC configuration.\nIt requires to restrict the communication across the EC2 instances using Security Groups. Which of the following options is true for Security Groups?\n",
  "answers" : [ {
    "id" : "4c2eb03cf3b94c7b83bcfd79e31bbc75",
    "option" : "You can change a Security Group associated to an instance if the instance state is stopped or running.",
    "isCorrect" : "true"
  }, {
    "id" : "e29efe0d4f254c4fb3adaefb160d23a0",
    "option" : "You can change a Security Group associated to an instance if the instance state is stopped but not if the instance state is running.",
    "isCorrect" : "false"
  }, {
    "id" : "0e3e627a90854800a8e5a29b0a1b12d5",
    "option" : "You can change a Security Group only if there are no instances associated to it.",
    "isCorrect" : "false"
  }, {
    "id" : "20f97bd2e9c94bc3a9692530b24874d9",
    "option" : "The only Security Group you can change is the Default Security Group.",
    "isCorrect" : "false"
  }, {
    "id" : "49fc2c067f4b4931a43059cff6673062",
    "option" : "None of the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT because the AWS documentation mentions it in the section calledâ€œChanging an Instance's Security Groupâ€ using the following sentence: â€œAfter you launch an instance into a VPC, you can change the security groups that are associated with the instance.\nYou can change the security groups for an instance when the instance is in the running or stopped state.â€\nOption B, C, D and E are INCORRECT as a consequence of A.Diagram: none.\nReferences:\nhttps://docs.aws.amazon.com/en_pv/vpc/latest/userguide/VPC_SecurityGroups.html\n\nThe correct answer is A: You can change a Security Group associated with an instance if the instance state is stopped or running.\nExplanation:\nA Security Group acts as a virtual firewall that controls the inbound and outbound traffic for one or more instances in a VPC. You can think of a security group as a set of rules that define what type of traffic is allowed or denied for an instance. Security Groups are stateful, which means that if you allow inbound traffic, the return traffic is automatically allowed, regardless of any rules in the outbound security group.\nIn this scenario, the Security team requires to restrict the communication across the EC2 instances using Security Groups. By default, all traffic is allowed across the subnets, and all instances within the same VPC can communicate with each other. Therefore, to restrict communication, you need to create and apply Security Groups to each EC2 instance.\nNow, let's analyze the given options:\nA. You can change a Security Group associated to an instance if the instance state is stopped or running.\nThis option is true. You can change the security group associated with an instance at any time, even if the instance is running. However, the changes to the security group will take effect immediately only if the instance is stopped, or the changes will take effect when the instance is restarted if the instance is running.\nB. You can change a Security Group associated to an instance if the instance state is stopped but not if the instance state is running.\nThis option is incorrect. You can change the Security Group associated with an instance, whether it is running or stopped.\nC. You can change a Security Group only if there are no instances associated with it.\nThis option is incorrect. You can change the Security Group rules at any time, even if instances are associated with it. However, the changes to the security group will apply to all instances associated with it.\nD. The only Security Group you can change is the Default Security Group.\nThis option is incorrect. You can change any Security Group, including the Default Security Group, at any time.\nE. None of the above.\nThis option is incorrect. Option A is correct, and therefore, it is the answer.\n\n"
}, {
  "id" : 26,
  "question" : "Your company is planning to pay for an AWS Support plan.\nThey have the following requirements as far as the support plan goes: 24x7 access to Cloud Support Engineers via email, chat &amp; phone Response time of less than 15 minutes for any business-critical system faults Which of the following plans will suffice to keep in mind the above requirement?\n",
  "answers" : [ {
    "id" : "359ed523623249aebdb184f3875c5874",
    "option" : "Basic",
    "isCorrect" : "false"
  }, {
    "id" : "178991d63dba408e8931ea7ce758b13d",
    "option" : "Developer",
    "isCorrect" : "false"
  }, {
    "id" : "9e1ef9d5a8794088938a75523ad16902",
    "option" : "Business",
    "isCorrect" : "false"
  }, {
    "id" : "9241f49f35264a8bafcffa6a8e95eb0d",
    "option" : "Enterprise.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nAs per the AWS document, there is no critical support available for Basic, Developer and Business plans.\nEnterprise plan has critical support within 15 minutes.\nThe question mentions less than 15 minutes for critical faults.\nHence the correct answer is Enterprise.\nFor more information on the support plans, please refer to the following Link:\nhttps://aws.amazon.com/premiumsupport/compare-plans/\n\n\nBased on the given requirements, the suitable AWS Support plan would be the Enterprise plan (Option D).\nExplanation:\nAWS offers four different support plans: Basic, Developer, Business, and Enterprise. Each plan offers different features, benefits, and costs.\nThe Basic plan offers 24x7 customer service, documentation, whitepapers, and support forums. However, it does not provide any technical support, and the response time is not specified.\nThe Developer plan offers all the features of the Basic plan, along with 12-hour response times for support cases and limited technical support.\nThe Business plan offers all the features of the Developer plan, along with 24x7 phone, email, and chat support, unlimited technical support, and one-hour response times for critical system issues.\nThe Enterprise plan offers all the features of the Business plan, along with a dedicated Technical Account Manager (TAM), infrastructure event management, and a 15-minute response time for critical system issues.\nTherefore, based on the given requirements, the Enterprise plan would be the best option, as it offers 24x7 phone, email, and chat support, unlimited technical support, and a 15-minute response time for critical system issues, which meets the requirement of less than 15 minutes for business-critical system faults.\n\n"
}, {
  "id" : 27,
  "question" : "Your company wants to move an existing Oracle database to the AWS Cloud.\nWhich of the following services can help facilitate this move?\n",
  "answers" : [ {
    "id" : "9e1d2bd52eb5425d8abd713a8897a105",
    "option" : "AWS Database Migration Service",
    "isCorrect" : "true"
  }, {
    "id" : "91df8e0743724746a4fa36364aad9cd0",
    "option" : "AWS VM Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "adf75466c37a415aa9fc24344d7231e5",
    "option" : "AWS Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "1a44d2f885e0445c849dc41d93184df5",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS Database Migration Service helps you migrate databases to AWS quickly and securely.\nThe source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.\nThe AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases.\nFor more information on AWS Database migration, please refer to the below URL:\nhttps://aws.amazon.com/dms/\n\nThe correct answer is A. AWS Database Migration Service.\nExplanation:\nWhen migrating an existing Oracle database to AWS, there are several services that can help with the process. AWS Database Migration Service (DMS) is a fully managed service that makes it easy to migrate databases to AWS. With DMS, you can migrate data from Oracle databases to Amazon RDS, Amazon Aurora, and Amazon Redshift.\nDMS supports several migration scenarios, including homogeneous migrations, where the source and target database engines are the same, and heterogeneous migrations, where the source and target database engines are different.\nDMS can also perform continuous data replication, enabling you to keep your databases in sync between your on-premises data center and AWS. This can be useful if you want to migrate your database to AWS but still need to keep it running in your on-premises environment for a period of time.\nAWS VM Migration Service (B) is a service that can help you migrate your existing virtual machines to AWS. However, it is not specifically designed for database migrations, and may not be the best choice for migrating an Oracle database.\nAWS Inspector (C) is a service that helps you improve the security and compliance of your applications running on AWS. It is not directly related to database migration.\nAWS Trusted Advisor (D) is a service that provides recommendations to help you optimize your AWS infrastructure for performance, security, and cost. While it can be useful for optimizing your database migration, it is not specifically designed for database migration.\nIn summary, AWS Database Migration Service (DMS) is the best choice for migrating an existing Oracle database to AWS.\n\n"
}, {
  "id" : 28,
  "question" : "On which of the following resources does Amazon Inspector perform network accessibility checks?\n",
  "answers" : [ {
    "id" : "81f78b293e3e4092bfa8ac2edb66c3dc",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "5eb26fab0a6643a18b305fee9f287822",
    "option" : "Amazon VPN",
    "isCorrect" : "false"
  }, {
    "id" : "a6cd8285218c4af4a40f81bbd1690e05",
    "option" : "Amazon EC2 instance",
    "isCorrect" : "true"
  }, {
    "id" : "53c5d329745b437e8409ed730c467e54",
    "option" : "Amazon VPC.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon Inspector provides two types of packages.\nNetwork reachability rules package checks network accessibility checks on Amazon EC2 instance.\nHost assessment rules package checks vulnerabilities on Amazon EC2 instance.\nOptions A, B &amp; D are incorrect as Amazon Inspector performs network accessibility checks on Amazon EC2 instance, not on Amazon CloudFront, Amazon VPN or Amazon VPC.For more information on Amazon Inspector, refer to the following URL:\nhttps://aws.amazon.com/inspector/faqs/\n\nAmazon Inspector is a security service provided by Amazon Web Services (AWS) that helps identify security issues within your AWS resources, including your Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Simple Storage Service (Amazon S3) buckets, and more. It works by performing assessments against the resources and identifying vulnerabilities or deviations from best practices.\nRegarding the question, Amazon Inspector performs network accessibility checks on Amazon EC2 instances. These checks are designed to identify potential security issues related to the network configuration of an EC2 instance, including open ports or services that may be vulnerable to attacks.\nAmazon CloudFront is a content delivery network service that speeds up the delivery of your static and dynamic web content, but it's not related to network security checks.\nAmazon VPN is a managed virtual private network service that allows you to securely connect to AWS resources from your on-premises network or another VPC.\nAmazon VPC is a virtual private cloud service that lets you launch AWS resources into a virtual network that you define. While VPCs have their own set of security controls, Amazon Inspector does not perform network accessibility checks on VPCs themselves.\nIn summary, Amazon Inspector performs network accessibility checks on Amazon EC2 instances, making sure they are configured securely, and no vulnerabilities or deviations exist in their network settings.\n\n"
}, {
  "id" : 29,
  "question" : "Which of the following services helps to achieve the computing elasticity in AWS?\n",
  "answers" : [ {
    "id" : "a4425230c774484ea005a86725a490e4",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "79e039a9ecc242a585ebab70b58a3b08",
    "option" : "VPC Endpoint",
    "isCorrect" : "false"
  }, {
    "id" : "8cdac543ddc946059f6d3e59b9cd3eab",
    "option" : "AWS EC2 Auto Scaling Group",
    "isCorrect" : "true"
  }, {
    "id" : "d776adfecb3442028905d308a8675a0d",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAWS EC2 Auto Scaling Group achieves the computing elasticity by scaling up/down the EC2 instances based on demand.\nFor more information on the AWS Autoscaling service, please refer to the below URL:\nhttps://aws.amazon.com/autoscaling/\n\nThe correct answer is C. AWS EC2 Auto Scaling Group.\nExplanation:\nElasticity is the ability to scale resources up or down in response to changing workload demands. AWS EC2 Auto Scaling Group is a service that helps achieve elasticity in AWS by automatically adjusting the number of Amazon Elastic Compute Cloud (EC2) instances in response to changes in demand for your application. EC2 Auto Scaling Group allows you to set up a group of EC2 instances that work together to handle incoming traffic.\nWhen traffic increases, EC2 Auto Scaling Group can launch additional EC2 instances to handle the extra load. When traffic decreases, EC2 Auto Scaling Group can terminate the unnecessary instances to save costs. This automatic adjustment of resources ensures that your application is always running smoothly and cost-effectively.\nAWS RDS is a managed relational database service that helps you set up, operate, and scale a relational database in the cloud. It provides scalability and high availability for your database, but it does not help achieve computing elasticity.\nVPC Endpoint is a service that enables private connectivity between a VPC and supported AWS services. It helps to secure communication between your VPC and AWS services but it does not contribute to computing elasticity.\nAmazon S3 is an object storage service that provides scalable storage for data backup, archival, and analytics. It provides high durability, availability, and performance for your data, but it does not help achieve computing elasticity.\nIn conclusion, the correct service that helps achieve computing elasticity in AWS is AWS EC2 Auto Scaling Group.\n\n"
}, {
  "id" : 30,
  "question" : "A live online game uses DynamoDB instances in the backend to store real-time scores of the participants as they compete against each other from various parts of the world.\nWhich data consistency option is the most appropriate to implement?\n",
  "answers" : [ {
    "id" : "b5a17b4aa4024b8ba7e4d40c60a4fab6",
    "option" : "Strongly consistent",
    "isCorrect" : "true"
  }, {
    "id" : "b79d204739d149d590cef54831df10a5",
    "option" : "Eventually consistent",
    "isCorrect" : "false"
  }, {
    "id" : "c252454ce7ea4954811cc0d29c1e9975",
    "option" : "Strong Eventual consistency",
    "isCorrect" : "false"
  }, {
    "id" : "bb35e50f27b845dfba547b9ebbe06809",
    "option" : "Optimistic consistency.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nSince the gamers are from geographically distinct locations, the data will need to be immediately readable within a second as soon as it is written.\nTherefore strongly consistency is needed.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\nOption B is INCORRECT because the scenarios outline that the participants of the game are live.\nIt will not suffice if any of them get updates on scores in less than real-time.\nOption C is INCORRECT because strong eventual consistency is not applicable in DynamoDB.Option D is INCORRECT because only two data consistency models are available with the DynamoDB service.\nOptimistic consistency is not supported.\n\nIn this scenario, the online game is using DynamoDB as its backend data store to store real-time scores of the participants. Since DynamoDB is a highly available and scalable NoSQL database service provided by AWS, it offers different data consistency options to choose from.\nData consistency refers to the level of accuracy and correctness of the data returned by a database system. DynamoDB provides four types of data consistency models, namely, strongly consistent, eventually consistent, strongly eventual consistency, and optimistic consistency.\nIn the context of the given scenario, the most appropriate data consistency option to implement is \"strongly consistent\" (Option A). Here's why:\nStrongly Consistent: In a strongly consistent system, every read operation is guaranteed to return the most up-to-date data. This means that every read operation will see the latest changes made to the data, and there is no delay or lag in the retrieval of data. Strong consistency is the most stringent and reliable consistency model available in DynamoDB.\nSince the online game is live and real-time, it is crucial to ensure that the scores are accurately updated in real-time, and there is no delay or lag in the retrieval of data. Thus, implementing strongly consistent data consistency ensures that the game's scores are accurately updated in real-time, and every read operation returns the most up-to-date data.\nEventually Consistent: In an eventually consistent system, every read operation is not guaranteed to return the most up-to-date data. There is a slight delay in the retrieval of data, and multiple read operations might return different versions of data. This consistency model is suitable for systems where the data is not time-sensitive, and the system can tolerate some inconsistency for a short period.\nStrong Eventual Consistency: Strong eventual consistency is a variation of eventual consistency where the system guarantees that all updates will propagate to all nodes eventually. This model is suitable for systems that can tolerate some inconsistency for a short period, but eventually, all nodes will have the same version of data.\nOptimistic Consistency: Optimistic consistency is a consistency model where the system assumes that there will be no conflicts in the data and allows multiple writes to proceed concurrently. The system then detects and resolves any conflicts that might occur after the writes are complete. This model is suitable for systems where conflicts are rare, and the system can tolerate some inconsistency for a short period.\nIn conclusion, the most appropriate data consistency option to implement in the given scenario is strongly consistent. This ensures that every read operation returns the most up-to-date data and the scores are accurately updated in real-time, which is critical for a live online game.\n\n"
}, {
  "id" : 31,
  "question" : "Which of the following services allows you to analyze EC2 Instances against pre-defined security templates to check for vulnerabilities?\n",
  "answers" : [ {
    "id" : "d7caaba207c34b118fd430dffb2f3e14",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "092b9ee339de458f9a89eda44dbdc88d",
    "option" : "AWS Inspector",
    "isCorrect" : "true"
  }, {
    "id" : "d3041083ff1341e5ba242cb1c4651fb0",
    "option" : "AWS WAF",
    "isCorrect" : "false"
  }, {
    "id" : "0196fe10bd9f4f57bc0063333955eb0a",
    "option" : "AWS Shield.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon Inspector enables you to analyze the behavior of your AWS resources and helps you to identify potential security issues.\nUsing Amazon Inspector, you can define a collection of AWS resources that you want to include in an assessment target.\nYou can then create an assessment template and launch a security assessment run of this target.\nFor more information on AWS Inspector, please refer to the below URL:\nhttps://docs.aws.amazon.com/inspector/latest/userguide/inspector_introduction.html\n\nThe correct answer is B. AWS Inspector.\nAWS Inspector is a security service that helps improve the security and compliance of applications running on Amazon EC2 instances by analyzing the behavior and configuration of those instances against predefined security rules. It does this by running various security assessments against EC2 instances, including vulnerability scanning, network security, and host security assessments.\nUsing AWS Inspector, you can define security rules packages called assessment templates that contain a set of rules to be evaluated against your EC2 instances. AWS Inspector provides a set of predefined assessment templates that can help you get started quickly. These templates are based on industry best practices and regulatory requirements, such as PCI DSS, HIPAA, and others.\nWhen you run an assessment using AWS Inspector, it will generate a report that includes a list of findings, which are identified security issues on your EC2 instances. Each finding includes a description of the issue, its severity level, and recommendations for remediation.\nIn summary, AWS Inspector is a security service that allows you to assess the security of your EC2 instances against predefined security templates and helps you identify and remediate potential security issues.\n\n"
}, {
  "id" : 32,
  "question" : "You are planning to serve a web application on the AWS Platform by using EC2 Instances.\nWhich of the below principles would you adopt to ensure that even if some of the EC2 Instances crash, you still have a working application?\n",
  "answers" : [ {
    "id" : "3261694708ff49139f48fd1d2769695a",
    "option" : "Using a scalable system",
    "isCorrect" : "false"
  }, {
    "id" : "4ad5a996ac1f48d0a52531732a53a6a1",
    "option" : "Using an elastic system",
    "isCorrect" : "false"
  }, {
    "id" : "458e4e5934a44046ab40aa2b866c7a4a",
    "option" : "Using a regional system",
    "isCorrect" : "false"
  }, {
    "id" : "edbd258681f94540a7406290ab93de18",
    "option" : "Using a fault tolerant system.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nA fault-tolerant system is one that ensures that the entire system works as expected, even there are issues.\nFor more information on designing fault-tolerant applications in AWS, please refer to the below URL:\nhttps://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf?did=wp_card&amp;trk=wp_card\nhttps://aws.amazon.com/premiumsupport/knowledge-center/autoscaling-fault-tolerance-load-balancer/\nhttps://aws.amazon.com/whitepapers/?whitepapers-main.sort-by=item.additionalFields.sortDate&amp;whitepapers-main.sort-order=desc\n\nTo ensure that a web application is always available and functional, it is essential to adopt a fault-tolerant system that can handle potential failures. A fault-tolerant system is designed to continue operating even if some of its components fail. Therefore, the answer to this question is D, using a fault-tolerant system.\nEC2 instances are virtual servers that can be launched and terminated as required, and they can run a wide range of applications. However, they are prone to failure due to factors such as hardware issues, software crashes, or network disruptions. To ensure that a web application hosted on EC2 instances continues to function even when some instances fail, a fault-tolerant system is necessary.\nA fault-tolerant system is built with redundant components and mechanisms that can detect and handle failures quickly. In the case of EC2 instances, a fault-tolerant system could include:\nAuto Scaling groups - Auto Scaling groups allow you to launch multiple EC2 instances in different Availability Zones within a region. This approach ensures that if one instance fails, the others can still handle the incoming traffic. Load Balancers - Load balancers can distribute incoming traffic to multiple EC2 instances, ensuring that the workload is evenly distributed. If one instance fails, the load balancer can redirect traffic to other instances. Elastic IPs - Elastic IPs are static IP addresses that can be assigned to EC2 instances. If an instance fails, you can quickly reassign the Elastic IP to another instance, ensuring that the application is still accessible from the same IP address. Multi-AZ RDS instances - If your web application relies on a database, using a multi-AZ RDS instance can help ensure that your data remains available even if the primary database instance fails. Multi-AZ RDS creates a secondary database instance in a different Availability Zone, ensuring that data is replicated in near-real-time.\nIn conclusion, a fault-tolerant system is necessary to ensure that a web application hosted on EC2 instances is always available and functional, even if some instances fail. By adopting a fault-tolerant system with redundant components and mechanisms, you can ensure that your web application remains accessible and operational, which is crucial for maintaining user satisfaction and minimizing downtime.\n\n"
}, {
  "id" : 33,
  "question" : "Which of the following are best practices when designing cloud-based systems? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "082bfc9de58547fa888fcb808ef54fd6",
    "option" : "Build Tightly-coupled components.",
    "isCorrect" : "false"
  }, {
    "id" : "2a2b5d1c0d9c4e8cbfc6bce8848e9673",
    "option" : "Build loosely-coupled components.",
    "isCorrect" : "true"
  }, {
    "id" : "1f91620b2c304d51ab4299e8b5db225a",
    "option" : "Assume everything will fail.",
    "isCorrect" : "true"
  }, {
    "id" : "0f0f010088f2413b89f97c7cd553d5a5",
    "option" : "Use as many services as possible.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nAlways build components that are loosely coupled.\nThis is so that even if one component does fail, the entire system does not fail.\nIf you build with the assumption that everything will fail, you will ensure that the right measures are taken to build a highly available and fault-tolerant system.\nOption D is incorrect because using multiple services increases cost and operational burden, rather use less and efficient services like serverless storage services and serverless compute services.\nFor more information on a well-architected framework, please refer to the below URL:\nhttps://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf\n\nThe best practices for designing cloud-based systems involve building loosely-coupled components and assuming everything will fail.\nBuild Loosely-coupled components: The cloud-based systems should be designed using loosely-coupled components, which means that the components should not be highly dependent on each other. This provides flexibility in the system, enabling the components to be easily replaced, upgraded or modified without affecting the entire system. This design principle allows for increased scalability, reliability, and agility of the system. Assume Everything Will Fail: In a cloud-based system, there are multiple services and components working together. Therefore, it is necessary to assume that some of these components will fail at some point in time. By assuming this, the system can be designed in a way that ensures fault tolerance, high availability, and disaster recovery. To achieve this, components can be replicated in different availability zones or regions to ensure that even if one component fails, the system will continue to operate without disruption. Avoid Tightly-coupled components: Tightly-coupled components can create issues when scaling the system, making it challenging to replace, upgrade, or modify individual components without impacting the entire system. This approach is not suitable for cloud-based systems as it can increase the risk of system downtime and limit the scalability and agility of the system. Use Only the Necessary Services: Using as many services as possible may seem like a good idea, but it can have several negative implications. It can lead to higher costs, increased complexity, and reduced agility, as there may be too many components to manage. Therefore, it is important to use only the necessary services that are required to achieve the desired outcomes.\nIn summary, to design a cloud-based system that is scalable, reliable, and fault-tolerant, it is necessary to use loosely-coupled components and assume that everything will fail. It is also essential to avoid tightly-coupled components and use only the necessary services to reduce complexity and costs.\n\n"
}, {
  "id" : 34,
  "question" : "Which services allow the customer to retain full administrative privileges of the underlying virtual infrastructure?\n",
  "answers" : [ {
    "id" : "86117e07e3f24a86b4a7bed62045ec2a",
    "option" : "Amazon EC2",
    "isCorrect" : "true"
  }, {
    "id" : "1525c11c868a4ef285d1f5fe38c8d87c",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "0f02811f40774b56b5548e1d9c326d89",
    "option" : "Amazon Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "0a556ca72c84425c8dab01f652078694",
    "option" : "Amazon DynamoDB.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nAll of the other services are all managed by AWS as serverless components.\nOnly you have complete control over the EC2 service.\nFor more information on AWS EC2, please refer to the below URL:\nhttps://aws.amazon.com/ec2/\n\nOut of the given options, Amazon EC2 is the service that allows the customer to retain full administrative privileges of the underlying virtual infrastructure.\nAmazon EC2 (Elastic Compute Cloud) is a web service that provides resizable compute capacity in the cloud. Customers can launch virtual machines, called instances, which run a variety of operating systems, including Windows and Linux. EC2 allows customers to have full control over the virtual infrastructure, including root access to instances, the ability to customize the networking and security settings, and the ability to install and configure any software needed.\nOn the other hand, Amazon S3 (Simple Storage Service) is an object storage service that offers industry-leading scalability, data availability, security, and performance. However, S3 does not provide full administrative access to the underlying virtual infrastructure.\nAmazon Lambda is a serverless compute service that allows customers to run code without provisioning or managing servers. Customers upload their code to Lambda and it automatically scales to handle the request volume. Lambda does not provide full administrative access to the underlying virtual infrastructure.\nAmazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance. Customers can create tables, store data, and query the data using a range of query types. However, DynamoDB does not provide full administrative access to the underlying virtual infrastructure.\nIn summary, the service that allows the customer to retain full administrative privileges of the underlying virtual infrastructure is Amazon EC2.\n\n"
}, {
  "id" : 35,
  "question" : "You have a mission-critical application that must be globally available at all times.\nIf this is the case, which of the below deployment mechanisms would you employ?\n",
  "answers" : [ {
    "id" : "94b5cc8ab71f403a93ce8f80ac0a8605",
    "option" : "Deployment to multiple edge locations",
    "isCorrect" : "false"
  }, {
    "id" : "6c611fdcffce4ab5af4e27aa12d81a8c",
    "option" : "Deployment to multiple Availability Zones",
    "isCorrect" : "false"
  }, {
    "id" : "0680731ced4643cbb637561157ed01a9",
    "option" : "Deployment to multiple Data Centers",
    "isCorrect" : "false"
  }, {
    "id" : "11b3f7e450604678a8c51867a17338ba",
    "option" : "Deployment to multiple Regions.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nRegions represent different geographical locations and are suitable for hosting your application across multiple regions for disaster recovery.\nFor more information on AWS Regions, please refer to the below URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html\n\nTo ensure global availability for a mission-critical application, the recommended deployment mechanism is to deploy the application to multiple AWS Regions.\nAWS Regions are separate geographic areas with multiple Availability Zones in each region. Each Availability Zone is an isolated location with its own power, cooling, and networking facilities, designed to be highly available and fault-tolerant. Deploying an application across multiple regions can help ensure that the application is available even if one or more regions experience a service outage or disaster.\nOption A, deployment to multiple edge locations, is not the best choice for global availability because edge locations are designed to cache content and reduce latency for end-users accessing content. Edge locations are not designed for high availability or fault tolerance.\nOption B, deployment to multiple Availability Zones, is a good option for high availability within a single region, but it does not provide global availability. While deploying an application across multiple Availability Zones can help ensure availability in the event of an outage in one Availability Zone, it does not provide redundancy across multiple regions.\nOption C, deployment to multiple data centers, is similar to option D, deployment to multiple regions. However, deploying an application across multiple data centers typically requires significant upfront capital investment and ongoing maintenance costs, whereas deploying an application across multiple AWS Regions is a more cost-effective and scalable option.\nTherefore, the correct answer is D, deployment to multiple Regions, to ensure global availability for a mission-critical application.\n\n"
}, {
  "id" : 36,
  "question" : "Which of the following can be used to protect against DDoS attacks? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "de42c2e72b9140fa97ef0bdb33b4790b",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "81359de8665e432faee38c51258fa799",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "47ae1a5cd545426bb8c5fd5a8f8e5821",
    "option" : "AWS Shield",
    "isCorrect" : "true"
  }, {
    "id" : "a9eed0a3f5584a2eb0f51290fe50f985",
    "option" : "AWS Shield Advanced.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nThe AWS Documentation mentions the following:\nAWS Shield - All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge.\nAWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications.\nAWS Shield Advanced - For higher levels of protection against attacks targeting your web applications running on Amazon EC2, Elastic Load Balancing (ELB), CloudFront, and Route 53 resources, you can subscribe to AWS Shield Advanced.\nAWS Shield Advanced provides expanded DDoS attack protection for these resources.\nFor more information on AWS Shield, please refer to the below URL:\nhttps://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html\n\nThe correct answers are C. AWS Shield and D. AWS Shield Advanced.\nExplanation:\nA Distributed Denial of Service (DDoS) attack is a malicious attempt to disrupt normal traffic of a targeted server, service or network by overwhelming the target or its surrounding infrastructure with a flood of Internet traffic.\nTo protect against DDoS attacks, AWS provides two services - AWS Shield and AWS Shield Advanced.\nAWS Shield is a free service that provides DDoS protection to all AWS customers. It helps in detecting and mitigating DDoS attacks on AWS infrastructure, including Elastic Load Balancers, CloudFront distributions, and Amazon Route 53 hosted zones. AWS Shield can protect your applications from the most common, frequently occurring network and transport layer DDoS attacks.\nAWS Shield Advanced provides additional DDoS protection to customers who require a higher level of protection against more sophisticated attacks. AWS Shield Advanced is a paid service that provides access to 24/7 DDoS response team, advanced metrics and reports, and cost protection against usage spikes. AWS Shield Advanced can also protect against application layer attacks and supports integration with AWS WAF, AWS Firewall Manager, and Amazon CloudFront.\nAWS EC2 and RDS are not designed to protect against DDoS attacks. AWS EC2 is a virtual server in the cloud designed to run applications and services, and AWS RDS is a managed database service. While they have some security features, they are not specifically designed to mitigate DDoS attacks.\n\n"
}, {
  "id" : 37,
  "question" : "Which of the following is a serverless compute offering from AWS?\n",
  "answers" : [ {
    "id" : "b020d9e660784e58b1dfed252f995596",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "4faee27b35664fdba146d47d0ff6b8f9",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "abf53d3a993d40bfbce463dad6538e83",
    "option" : "AWS SNS",
    "isCorrect" : "false"
  }, {
    "id" : "250aa1985b8846be975c21b6e01ffbd1",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers.\nAWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.\nFor more information on AWS Lambda, please refer to the below URL:\nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html\n\nThe correct answer is B. AWS Lambda.\nExplanation:\nAWS Lambda is a serverless compute offering from AWS that allows you to run your code without the need to provision or manage servers. With AWS Lambda, you only pay for the compute time that you consume, making it a cost-effective solution for running small, event-driven applications or for executing individual functions within larger applications.\nHere are brief descriptions of the other options:\nA. AWS EC2: Amazon Elastic Compute Cloud (EC2) is a web service that provides resizable compute capacity in the cloud. It is not a serverless offering, as it requires you to provision and manage virtual machines (EC2 instances) to run your applications.\nC. AWS SNS: Amazon Simple Notification Service (SNS) is a fully managed messaging service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It is not a compute offering, but rather a messaging service.\nD. AWS SQS: Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. Like SNS, it is not a compute offering, but rather a messaging service.\nIn summary, AWS Lambda is the only serverless compute offering among the options provided.\n\n"
}, {
  "id" : 38,
  "question" : "Which of the following security services can be used to detect users' personal credit card numbers from data stored in Amazon S3?\n",
  "answers" : [ {
    "id" : "833d0077fd244f08b6d2b4fbec173eef",
    "option" : "Amazon Macie",
    "isCorrect" : "true"
  }, {
    "id" : "0719d87814a04e9ead59658eb9934722",
    "option" : "Amazon GuardDuty",
    "isCorrect" : "false"
  }, {
    "id" : "71a8818f439141e380b6405ba4bbd86e",
    "option" : "Amazon Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "ca63a48c0d3f4e39ad00549967bc5a68",
    "option" : "AWS Shield.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon Macie is a managed security service which can be used to detect personally identifiable information (PII) such as names, password, Credit card numbers from large amounts of data stored in Amazon S3 bucket.\nOption B is incorrect as Amazon GuardDuty is used to identify threats by analyzing events from AWS CloudTrail, VPC Flow Logs, and DNS Logs.\nIt cannot be used to detect PII from data stored in the Amazon S3 bucket.\nOption C is incorrect as Amazon Inspector can analyze potential security threats for an Amazon EC2 instance against an assessment template with predefined rules.\nOption D is incorrect as AWS Shield provides protection against DDOS attacks.\nFor more information on Amazon Macie, refer to the following URLs:\nhttps://aws.amazon.com/macie/features/\n\nThe correct answer is A. Amazon Macie.\nAmazon Macie is a security service provided by AWS that uses machine learning and pattern matching to automatically discover, classify, and protect sensitive data stored in Amazon S3. Macie can detect various types of sensitive data, such as credit card numbers, social security numbers, and personally identifiable information (PII).\nMacie can help you to understand your data better, manage your data security, and comply with data protection regulations. It works by continuously monitoring the data stored in S3 buckets, identifying sensitive data, and assigning a risk score to each piece of data based on its sensitivity and exposure.\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior in your AWS account. It does not specifically focus on detecting sensitive data in S3 buckets.\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It does not specifically focus on detecting sensitive data in S3 buckets.\nAWS Shield is a managed service that provides protection against DDoS attacks on web applications running on AWS. It does not specifically focus on detecting sensitive data in S3 buckets.\nTherefore, the correct answer is Amazon Macie.\n\n"
}, {
  "id" : 39,
  "question" : "An online streaming company is prohibited from broadcasting its content in certain countries and regions in the world.\nWhich Amazon Route 53 routing policy would be the most suitable in guaranteeing their compliance?\n",
  "answers" : [ {
    "id" : "b353ae09ba0d41b8babd43f5c79b7273",
    "option" : "Geoproximity",
    "isCorrect" : "false"
  }, {
    "id" : "9306b514076b46779f7b7390f4a9c1df",
    "option" : "Geolocation",
    "isCorrect" : "true"
  }, {
    "id" : "94598b18f46349bc98bb848dea4bf182",
    "option" : "Multi-value answer",
    "isCorrect" : "false"
  }, {
    "id" : "a6e85b8c2cf24af9be5106e7fec8144f",
    "option" : "Failover.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon Route 53 geolocation routing policy makes it possible for different types of content to be served depending on the browser's geographical location.\nIn this use case, the streaming company can serve a restriction message if Amazon Route 53 detects origin requests from prohibited countries.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\nOption A is INCORRECT because geo-proximity allows for DNS traffic to be routed in accordance with a bias or preset preference rule.\nThis allows the user to be served with content from resources closest to their geographical location.\nThis routing manipulates DNS traffic flow only.\nThis routing policy is not the most suitable.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity\nOption C is INCORRECT because a multi-value answer primarily addresses the quality of service and resources queried in DNS requests.\nThis routing policy is not the most suitable.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-multivalue\nOption D is INCORRECT because failover allows for the automatic switch to healthy DNS resources if another becomes unavailable.\nIt will not allow for the preferential serving of content based on the geographical location.\nThis routing policy is not the most suitable.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-multivalue\n\nThe most suitable Amazon Route 53 routing policy in guaranteeing compliance for an online streaming company that is prohibited from broadcasting its content in certain countries and regions in the world would be the Geolocation routing policy.\nGeolocation routing policy allows you to route traffic based on the location of your users. It uses the location of the user's DNS resolver to determine the appropriate response to return. You can specify different responses based on the country or continent that the user is located in. This would allow the online streaming company to block access to their content for users located in the countries or regions where their content is prohibited.\nThe other routing policies mentioned are:\nGeoproximity routing policy: This policy routes traffic based on the location of your resources and the location of your users. It is useful when you have resources that are geographically distributed, and you want to route traffic to the closest available resource. However, it is not useful in guaranteeing compliance as it does not allow for blocking traffic based on location. Multi-value answer routing policy: This policy returns multiple values for a DNS query, which can be useful for load balancing or providing fault tolerance. However, it does not allow for blocking traffic based on location. Failover routing policy: This policy is used to route traffic to a standby resource when the primary resource is unavailable. It is useful for high availability scenarios but does not allow for blocking traffic based on location.\nTherefore, the most suitable Amazon Route 53 routing policy in guaranteeing compliance for an online streaming company that is prohibited from broadcasting its content in certain countries and regions in the world would be the Geolocation routing policy.\n\n"
}, {
  "id" : 40,
  "question" : "Which AWS service provides a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability?\n",
  "answers" : [ {
    "id" : "1f68d63090944a73bed5a59fbd1776be",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "696ce1cc714140aa854e458b5098353b",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "6f74c04a062c440d8080b1cb724370e0",
    "option" : "Oracle RDS",
    "isCorrect" : "false"
  }, {
    "id" : "d6631ae39eab4a5781edb47a62aac9ec",
    "option" : "Elastic Map Reduce.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: - B.\nDynamoDB is a fully managed NoSQL offering provided by AWS.\nIt is now available in most regions for users to consume.\nFor more information on AWS DynamoDB, please refer to the below URL:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\n\nThe correct answer is B. DynamoDB.\nExplanation: DynamoDB is a fully managed NoSQL database service provided by AWS. It provides fast and predictable performance with seamless scalability. DynamoDB is designed to be highly available and durable, with automatic scaling to handle high traffic and throughput. It is a key-value and document database that allows users to store and retrieve data using a flexible data model.\nAWS RDS (A) is a managed relational database service that supports multiple relational database engines such as MySQL, PostgreSQL, Oracle, Microsoft SQL Server, and Amazon Aurora. It is not a NoSQL database service.\nOracle RDS (C) is a managed relational database service that supports only Oracle Database. It is not a NoSQL database service.\nElastic MapReduce (D) is a managed Hadoop framework that allows users to process large amounts of data using MapReduce programming model. It is not a NoSQL database service.\nTherefore, the correct answer is B. DynamoDB.\n\n"
}, {
  "id" : 41,
  "question" : "In the AWS Billing and Management service, which tool can provide usage-based forecasts of estimated billing costs and usage for the coming months?\n",
  "answers" : [ {
    "id" : "bce4895beb124ee4a840f93b30896030",
    "option" : "AWS Cost Explorer",
    "isCorrect" : "true"
  }, {
    "id" : "1a328e686a2c443bba42f96e67d9c200",
    "option" : "AWS Bills",
    "isCorrect" : "false"
  }, {
    "id" : "679304cc444e42b1bb4fae21c7fdd194",
    "option" : "AWS Reports",
    "isCorrect" : "false"
  }, {
    "id" : "671f6cd0c8a048289c037cbff99d7a6b",
    "option" : "Cost &amp; Usage Reports.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Cost Explorer can create user-defined custom forecasts for future usage patterns.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ce-forecast.html\nhttps://aws.amazon.com/about-aws/whats-new/2019/07/usage-based-forecasting-in-aws-cost-explorer/\nOption B is INCORRECT because AWS Bills will list the historical costs that would have been incurred over the past month with granular options.\nThe tool will not give the usage-based forecasts as specified in the question.\nOption C is INCORRECT because AWS Reports will give a composite overview of costs and usage.\nThe tool gives a granular perspective of usage and billing but without usage-based forecasts.\nOption D is INCORRECT because AWS Reports and Cost &amp; Usage Reports are the same tool.\nOption.\nC.\nexplanation outlines why it is inaccurate as a response to the question.\n\nThe tool in AWS Billing and Management service that provides usage-based forecasts of estimated billing costs and usage for the coming months is AWS Cost Explorer (option A).\nAWS Cost Explorer is a powerful cost management tool that provides visualization and analysis of your AWS spending. It allows you to view, understand and manage your AWS costs and usage over time, and provides actionable insights to help you optimize your spending.\nOne of the key features of AWS Cost Explorer is its ability to provide usage-based forecasts of estimated billing costs and usage for the coming months. This is done by analyzing your historical usage and cost data, and using this data to project your future usage and costs.\nTo access AWS Cost Explorer, you must have an AWS account and the necessary permissions to access the service. Once you have access, you can use the Cost Explorer dashboard to view cost and usage reports, create custom reports, set up budgets and alerts, and explore cost-saving opportunities.\nIn contrast, AWS Bills (option B) is simply a list of your current and past bills, while AWS Reports (option C) provides detailed reports on your AWS usage and costs, but does not offer cost forecasting. Cost & Usage Reports (option D) is a tool that allows you to download detailed data about your AWS usage and costs, but it also does not provide cost forecasting.\n\n"
}, {
  "id" : 42,
  "question" : "Which of the following AWS managed database service provides processing power that is up to 5X faster than a traditional MySQL database?\n",
  "answers" : [ {
    "id" : "f0d9d322f405478ca93fbdcc02dde543",
    "option" : "MariaDB",
    "isCorrect" : "false"
  }, {
    "id" : "199a5292629f4210ae8f227ab528454b",
    "option" : "Aurora",
    "isCorrect" : "true"
  }, {
    "id" : "822779f1f363449a8cd486ea9cb399ff",
    "option" : "PostgreSQL",
    "isCorrect" : "false"
  }, {
    "id" : "85810b6192984297b594f012c6485ca4",
    "option" : "DynamoDB.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAmazon Aurora (Aurora) is a fully managed, MySQL- and PostgreSQL-compatible, relational database engine.\nIt combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases.\nIt delivers up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.\nFor more information on AWS Aurora, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Overview.html\n\nThe correct answer is B. Aurora.\nAmazon Aurora is an AWS-managed relational database service that is fully compatible with MySQL and PostgreSQL, but provides better performance, availability, and scalability. It uses a distributed architecture that can scale out to meet the demands of a growing workload, while also providing fault tolerance and automatic failover.\nAurora provides processing power that is up to 5 times faster than a traditional MySQL database, making it a great choice for applications that require high performance and low latency. Aurora also offers features such as automated backups, point-in-time recovery, and read replicas, which can help improve database availability and reliability.\nMariaDB is an open-source database management system that is a fork of MySQL. It is not an AWS-managed service, but it can be used on EC2 instances. PostgreSQL is another open-source relational database management system that is fully supported on AWS, but it is not known for its high performance. DynamoDB is a fully managed NoSQL database service provided by AWS that can scale to support high traffic applications, but it is not a relational database management system and is not comparable to Aurora or other RDBMS services.\n\n"
}, {
  "id" : 43,
  "question" : "In the AWS Well-Architected Framework, which of the following is NOT a Security design principle to design solutions in AWS?\n",
  "answers" : [ {
    "id" : "3852f56363d24b09b8f19617c036fc45",
    "option" : "Apply Security only at the edge of the network.",
    "isCorrect" : "true"
  }, {
    "id" : "9c3ca7f2c4444943ac3fb2a1259428a1",
    "option" : "Protect Data at rest &amp; in transit.",
    "isCorrect" : "false"
  }, {
    "id" : "052a17c022f94fdda52a5cde39c06c4a",
    "option" : "Implement a strong Identity foundation.",
    "isCorrect" : "false"
  }, {
    "id" : "6d9a99917cd947b791322196e60d596c",
    "option" : "Enable Traceability.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nSecurity needs to be applied at all network layers, like edge of network, VPC, all instances &amp; application with the VPC.\nApplying Security controls at the edge of the network is not an efficient security control &amp; against security design principles.\nAs per AWS Well-Architected Framework, the following are the design principles for security in the cloud:\nÂ· Implement a strong identity foundation.\nÂ· Enable traceability.\nÂ· Apply security at all layers.\nÂ· Automate security best practices.\nÂ· Protect data in transit and at rest.\nÂ· Keep people away from data.\nÂ· Prepare for security events.\nOptions B, C, &amp; D are incorrect as these are part of security design principles that need to be followed while implementing security controls in the cloud.\nFor more information on Security Design Principle with AWS Well-Architected Framework, refer to the following URL:\nhttps://docs.aws.amazon.com/wellarchitected/latest/framework/sec-design.html\n\nThe AWS Well-Architected Framework provides a set of best practices and guidelines for designing and operating reliable, secure, efficient, and cost-effective systems in the AWS Cloud. The framework is based on five pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization.\nIn the Security pillar of the AWS Well-Architected Framework, the following design principles are recommended:\nA. Apply Security at all layers B. Enable traceability C. Automate security best practices D. Protect data in transit and at rest E. Keep people away from data F. Prepare for security events\nTherefore, the answer to the given question is A. \"Apply Security only at the edge of the network\" as it is not a recommended Security design principle in the AWS Well-Architected Framework.\nApplying security only at the edge of the network is a traditional security model that assumes that the perimeter of the network is secure and that attackers are outside the network. However, with the increasing number of threats, this model is no longer sufficient. Applying security at all layers means implementing security controls at the application, data, and network layers. By doing so, security is enforced regardless of the location of the user or the application.\nProtecting data at rest and in transit means implementing encryption and other security controls to ensure that data is not compromised while it is stored or transmitted. Implementing a strong identity foundation means establishing a secure and reliable authentication and authorization mechanism. Enabling traceability means logging and monitoring all activities to detect and respond to security events.\n\n"
}, {
  "id" : 44,
  "question" : "Which of the following options is TRUE for AWS Database Migration Service (AWS DMS)?\n",
  "answers" : [ {
    "id" : "9a3b9b50858d4046ab19381e2ca12358",
    "option" : "AWS DMS can migrate databases from on-premise to AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "2688d28643474a82b172517b8c58abe5",
    "option" : "AWS DMS can migrate databases from AWS to on-premise.",
    "isCorrect" : "false"
  }, {
    "id" : "1ec255182091492fad8c4fa746390347",
    "option" : "AWS DMS can migrate databases from EC2 to Amazon RDS.",
    "isCorrect" : "false"
  }, {
    "id" : "5c268bfc593640c5825bce5e6bdf43de",
    "option" : "AWS DMS can have Amazon Redshift and Amazon DynamoDB as target databases.",
    "isCorrect" : "false"
  }, {
    "id" : "4c468d784c9e4c45b579f643128a5b64",
    "option" : "All the above.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: E.\nAll the options are CORRECT.\nOptions are clearly described in the AWS DMS documentation at the link below.\nOption A is TRUE and is the â€œmost commonâ€ way to use AWS DMS.\nOption B is TRUE and can be used to create a copy (or migrate) a database from AWS to the on-premise data center.\nOption C is TRUE and can be used to migrate the IaaS solution (e.g., generated from a lift-and-shift wave) to a managed service like Amazon RDS.\nOption D is TRUE, according to AWS documentation.\nDiagram: none.\nReferences:\nhttps://aws.amazon.com/dms/\nhttps://aws.amazon.com/dms/faqs/\n\nThe correct answer is E, which means that all the given options are true for AWS Database Migration Service (AWS DMS).\nAWS Database Migration Service (DMS) is a managed service that makes it easy to migrate data from one database to another. It supports both homogenous and heterogeneous migrations, meaning that you can migrate data between databases that are running on the same platform, or between different platforms.\nLet's look at each option in detail:\nA. AWS DMS can migrate databases from on-premise to AWS: This option is true. AWS DMS can migrate data from on-premise databases to Amazon Web Services (AWS) databases, including Amazon RDS, Amazon Aurora, Amazon Redshift, and Amazon DynamoDB.\nB. AWS DMS can migrate databases from AWS to on-premise: This option is true. AWS DMS can migrate data from AWS databases, including Amazon RDS, Amazon Aurora, Amazon Redshift, and Amazon DynamoDB, to on-premise databases.\nC. AWS DMS can migrate databases from EC2 to Amazon RDS: This option is also true. AWS DMS can migrate data from databases running on Amazon Elastic Compute Cloud (EC2) instances to Amazon RDS, which is a managed relational database service.\nD. AWS DMS can have Amazon Redshift and Amazon DynamoDB as target databases: This option is true as well. AWS DMS can migrate data to several target databases, including Amazon RDS, Amazon Aurora, Amazon Redshift, and Amazon DynamoDB.\nTherefore, all the options given in the question are true for AWS DMS, making option E the correct answer.\n\n"
}, {
  "id" : 45,
  "question" : "Which of the following services helps in governance, compliance, and risk auditing in AWS?\n",
  "answers" : [ {
    "id" : "707723a69d174bdabb090969a04c07e1",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "40c25ba2b4cb4596be64ca13f204b5a3",
    "option" : "AWS CloudTrail",
    "isCorrect" : "true"
  }, {
    "id" : "37c2c0ab4b9e434ea552d95ea842e976",
    "option" : "AWS CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "b33b3dff7b7e4f1fbe5bde83d25d365e",
    "option" : "AWS SNS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nCloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nThis event history simplifies security analysis, resource change tracking, and troubleshooting.\nFor more information on AWS CloudTrail, please refer to the below URL:\nhttps://aws.amazon.com/cloudtrail/\n\nThe service in AWS that helps in governance, compliance, and risk auditing is AWS CloudTrail (Option B).\nAWS CloudTrail is a service that provides a record of all AWS API calls made in your account. It captures detailed information about who made the API call, when it was made, which services were accessed, and what actions were performed. This information is stored in a log file, which can be analyzed and used to track changes and troubleshoot operational issues.\nThe AWS CloudTrail log file can be used for governance, compliance, and risk auditing purposes. For example, it can help you meet compliance requirements by providing an audit trail of all API calls made in your account. It can also help you identify security risks by alerting you to unusual API activity or unauthorized access attempts.\nAWS CloudFormation (Option A) is a service that provides templates for creating and managing AWS resources. It is used for infrastructure as code, allowing users to create, manage, and provision resources in an automated and repeatable manner.\nAWS CloudWatch (Option C) is a monitoring and observability service that provides metrics, logs, and alarms for AWS resources and applications. It can be used to monitor resource utilization, detect and diagnose operational issues, and take automated actions in response to changes in the environment.\nAWS SNS (Option D) is a messaging and notification service that can be used to send text or email messages to subscribers. It is used for event-driven architectures, allowing users to publish and subscribe to messages that are triggered by events in their AWS environment. It can be used to automate workflows and notify users of changes in their environment.\nWhile these services may have some security and compliance features, they are not specifically designed for governance, compliance, and risk auditing purposes like AWS CloudTrail.\n\n"
}, {
  "id" : 46,
  "question" : "Project team enhancing the security features of a banking application, requires implementing a threat detection service that continuously monitors malicious activities and unauthorized behaviors to protect AWS accounts, workloads, and data stored in Amazon S3\nWhich AWS services should the project team select?\n",
  "answers" : [ {
    "id" : "66fe1474219c4104af1ad8ca922c233d",
    "option" : "AWS Shield",
    "isCorrect" : "false"
  }, {
    "id" : "216c98fd8ee0481db88d87bfe89438ba",
    "option" : "AWS Firewall Manager",
    "isCorrect" : "false"
  }, {
    "id" : "e891deb704ae46c795b750c179586274",
    "option" : "Amazon GuardDuty",
    "isCorrect" : "true"
  }, {
    "id" : "f13687a9df0844dfaf904319ee0c679b",
    "option" : "Amazon Inspector.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT.\nAWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\nOption B is INCORRECT.\nAWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization.\nOption C is CORRECT.\nAmazon GuardDuty is a threat detection service that continuously monitors malicious activities and unauthorized behaviors to protect your AWS accounts, workloads, and data stored in Amazon S3.\nOption D is INCORRECT.\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.\nReference:\nhttps://aws.amazon.com/guardduty/\nhttps://aws.amazon.com/firewall-manager/\nhttps://aws.amazon.com/shield/\nhttps://aws.amazon.com/inspector/\n\nThe project team wants to implement a threat detection service that continuously monitors malicious activities and unauthorized behaviors to protect AWS accounts, workloads, and data stored in Amazon S3. To achieve this goal, the team should consider using Amazon GuardDuty.\nAmazon GuardDuty is a threat detection service that continuously monitors the AWS accounts and workloads for malicious activity and unauthorized behavior. It analyzes data from multiple sources, such as VPC flow logs, AWS CloudTrail, and DNS logs, to identify potential security threats. Once a threat is detected, GuardDuty generates an alert with detailed information about the threat and provides recommendations for remediation.\nAWS Shield is a managed DDoS protection service that safeguards web applications running on AWS. It provides protection against network and application layer attacks by automatically detecting and mitigating DDoS attacks. However, it does not provide continuous monitoring of malicious activities and unauthorized behaviors, which is the requirement of the project team.\nAWS Firewall Manager is a security management service that centralizes the management of AWS WAF rules across multiple accounts and resources. It allows administrators to apply WAF rules across multiple accounts and resources from a single location. However, it does not provide continuous monitoring of malicious activities and unauthorized behaviors, which is the requirement of the project team.\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It analyzes the configuration of applications and their underlying infrastructure to identify potential security issues. However, it does not provide continuous monitoring of malicious activities and unauthorized behaviors, which is the requirement of the project team.\nTherefore, Amazon GuardDuty is the most appropriate service for the project team to select to continuously monitor malicious activities and unauthorized behaviors to protect AWS accounts, workloads, and data stored in Amazon S3.\n\n"
}, {
  "id" : 47,
  "question" : "Which of the following are benefits of the AWS's Relational Database Service (RDS)? Choose the 2 correct answers from the options below.\n",
  "answers" : [ {
    "id" : "94ea4a0496544ab29be10078e2a89ec1",
    "option" : "Automated patches and backups",
    "isCorrect" : "true"
  }, {
    "id" : "c4a8ecac7e1947c7b0fa5fc9525858ba",
    "option" : "DB owner can resize the capacity accordingly",
    "isCorrect" : "true"
  }, {
    "id" : "3639917711f9445196e581702744e74d",
    "option" : "It allows you to store unstructured data",
    "isCorrect" : "false"
  }, {
    "id" : "606eb3c7611545a2a0dc9cd46a42d5e1",
    "option" : "It allows you to store NoSQL data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS Documentation mentions the following:\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud.\nIt provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.\nIt frees you to focus on your applications.\nSo you can give them the fast performance, high availability, security and compatibility they need.\nFor more information on AWS RDS, please visit the URL:\nhttps://aws.amazon.com/rds/\n\nThe correct answers are A and B.\nA) Automated patches and backups: One of the key benefits of AWS's Relational Database Service (RDS) is that it provides automated patches and backups. RDS takes care of managing and maintaining the database, including applying security patches, updates, and backups. This can save a lot of time and effort for developers and database administrators, who can focus on other aspects of their applications.\nB) DB owner can resize the capacity accordingly: Another benefit of RDS is that it allows the database owner to resize the capacity of the database according to their needs. This means that the owner can increase or decrease the size of the database based on the application's usage and demands. This can help reduce costs by only paying for the resources needed at any given time.\nC) It allows you to store unstructured data: The statement \"It allows you to store unstructured data\" is incorrect. RDS is a relational database service, which means it is designed to store structured data in a tabular format with defined relationships between tables. If you need to store unstructured data, you might consider using Amazon S3 or another suitable service.\nD) It allows you to store NoSQL data: The statement \"It allows you to store NoSQL data\" is also incorrect. While AWS does offer a NoSQL database service called Amazon DynamoDB, RDS is not designed to store NoSQL data. RDS is a relational database service and supports SQL-based database engines such as MySQL, PostgreSQL, Oracle, and Microsoft SQL Server.\n\n"
}, {
  "id" : 48,
  "question" : "Which AWS product provides a unified user interface, enabling easy management of software development activities in one place, along with, quick development, build, and deployment of applications on AWS?\n",
  "answers" : [ {
    "id" : "46fee058005b4d378ab7aa2cd74ba06a",
    "option" : "Amazon CodeGuru.",
    "isCorrect" : "false"
  }, {
    "id" : "98062222aca04c4e968ee0043ca2a6c8",
    "option" : "AWS CodeBuild",
    "isCorrect" : "false"
  }, {
    "id" : "7492ab63229547e382df203446b7f385",
    "option" : "AWS CodeArtifact",
    "isCorrect" : "false"
  }, {
    "id" : "92a27f1e060e4cbfaeddb623d27137fa",
    "option" : "AWS CodeStar.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nAmazon CodeGuru is a developer tool powered by machine learning that provides intelligent recommendations for improving code quality and identifying an application's most expensive lines of code.\nOption B is INCORRECT.\nAWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.\nOption C is INCORRECT.\nAWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process.\nOption D is CORRECT.\nAWS CodeStar enables you to develop, build, and deploy applications on AWS quickly.\nAWS CodeStar provides a unified user interface, enabling you to manage your software development activities in one place easily.\nReference:\nhttps://aws.amazon.com/codeguru/\nhttps://aws.amazon.com/codeartifact/\nhttps://aws.amazon.com/codebuild/\nhttps://aws.amazon.com/codestar/\n\nThe AWS product that provides a unified user interface, enabling easy management of software development activities in one place, along with quick development, build, and deployment of applications on AWS is AWS CodeStar (Option D).\nAWS CodeStar is a fully managed service that simplifies the development and deployment of applications on AWS. It provides a unified user interface that allows developers to manage their entire software development lifecycle in one place. CodeStar supports various programming languages, frameworks, and AWS services, making it easy for developers to start and manage their projects quickly.\nSome key features of AWS CodeStar include:\nProject Templates: AWS CodeStar provides project templates for popular languages and frameworks, including Java, Python, Ruby, Node.js, and more. Developers can choose a template that best suits their project needs and start coding right away. Integrated Tools: CodeStar integrates with various AWS services such as AWS CodeCommit, AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy. This integration enables developers to easily build, test, and deploy their applications on AWS. Team Management: AWS CodeStar provides tools for managing team members, permissions, and access control. This feature allows developers to collaborate seamlessly with their team members on a project. Monitoring: CodeStar also provides monitoring tools that enable developers to monitor their applications' performance and health. This feature helps developers detect and resolve issues quickly.\nIn summary, AWS CodeStar provides a unified platform for managing the entire software development lifecycle in one place. With its project templates, integrated tools, team management, and monitoring capabilities, CodeStar makes it easy for developers to develop, build, and deploy applications on AWS.\n\n"
}, {
  "id" : 49,
  "question" : "If you want to take a backup of an EBS Volume, what would you do?\n",
  "answers" : [ {
    "id" : "6c5552097706486f861e55aee85b2f19",
    "option" : "Store the EBS volume in S3.",
    "isCorrect" : "false"
  }, {
    "id" : "972ab1f9d5a2437ba300410c3e9cb847",
    "option" : "Store the EBS volume in an RDS database.",
    "isCorrect" : "false"
  }, {
    "id" : "bca3bb64edc34463ba0b4220da32a4b2",
    "option" : "Create an EBS snapshot.",
    "isCorrect" : "true"
  }, {
    "id" : "688fc8272d104175ad7b9906cf8e42f8",
    "option" : "Store the EBS volume in DynamoD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following:\nYou can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.\nFor more information on EBS Snapshots, please visit the link:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\n\nIf you want to take a backup of an EBS Volume in AWS, the appropriate action would be to create an EBS snapshot. Therefore, the correct answer is option C, \"Create an EBS snapshot.\"\nHere is a more detailed explanation of EBS snapshots and how they work:\nAmazon Elastic Block Store (EBS) is a storage service that provides persistent block-level storage volumes for use with EC2 instances. An EBS volume is a network-attached storage device that can be attached to an EC2 instance and used like a physical hard drive. EBS volumes are durable and reliable, but they are not immune to data loss. To protect your data against accidental deletion, hardware failure, or other types of data loss, you can create a backup of your EBS volume by taking an EBS snapshot.\nAn EBS snapshot is a point-in-time copy of an EBS volume. It captures the data on the volume, including all of its blocks, and stores it in Amazon S3 as an Amazon EBS snapshot. The snapshot is incremental, meaning that only the blocks that have changed since the last snapshot are stored. This makes it efficient to create and store snapshots, as well as restore them when needed.\nTo create an EBS snapshot, you can use the AWS Management Console, the AWS Command Line Interface (CLI), or the AWS SDKs. When you create an EBS snapshot, you can specify which EBS volume you want to back up, and you can also add tags to the snapshot to help you identify it later. Once the snapshot is created, it is stored in Amazon S3, which provides durability and availability for the snapshot.\nWith an EBS snapshot, you can restore the EBS volume to a previous state or create a new EBS volume from the snapshot. You can also share the snapshot with other AWS accounts, copy it to a different region, or encrypt it using AWS Key Management Service (KMS) for added security.\nIn summary, to take a backup of an EBS volume in AWS, you would create an EBS snapshot, which captures a point-in-time copy of the volume and stores it in Amazon S3.\n\n"
}, {
  "id" : 50,
  "question" : "Your Security Team has some security concerns about the application data stored on S3\nThe team requires you to introduce two improvements: (i) add â€œencryption at restâ€ and (ii) give them the possibility to monitor who has accessed the data and when the data have been accessed. Which of the following AWS solution would you adopt to satisfy the requirement?\n",
  "answers" : [ {
    "id" : "ea80728bf3bf4b20a7fb7fc975fb87b5",
    "option" : "AWS Certificate Manager with CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "85dbd386891e4705aab36cd984784a58",
    "option" : "Server-Side Encryption managed by S3 (SSE-S3) with CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "c90ae49667d2456daad63eeadab9a7f1",
    "option" : "Server-Side Encryption managed by customer (SSE-C) with CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "c42595d8d81c44a4879c2f8ccf14a84d",
    "option" : "Server-Side Encryption managed by KMS (SSE-KMS) with CloudTrail.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nAmazon S3 is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3\nCloudTrail logs successful operations and attempted calls that failed, such as when the caller is denied access to a resource.\nOperations on KMS keys in other accounts are logged in both the caller account and the KMS key owner account.\nOption A is INCORRECT AWS Certificate Manager is not a solution for encryption at rest.\nIt is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates.\nHence it is a solution for â€œencryption in transitâ€, not an â€œencryption at rest.â€\nOption B is INCORRECT because SSE-S3 does â€œencryption/decryption at restâ€, but it does not offer monitoring capabilities (who/when encrypts/decrypts).\nOption C is INCORRECT because SSE-C does â€œencryption/decryption at restâ€, but it does not offer monitoring capabilities (who/when encrypts/decrypts).\nOption D is CORRECT because SSE-KMS does â€œencryption/decryption at restâ€ and does offer monitoring capabilities.\nCloudTrail captures all API calls to AWS KMS as events, including calls from the AWS KMS console, AWS KMS APIs, the AWS Command Line Interface (AWS CLI), and AWS Tools for PowerShell.\nReferences:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html#sse\nhttps://docs.aws.amazon.com/kms/latest/developerguide/logging-using-cloudtrail.html\n\nTo satisfy the security team's requirements, you would need to introduce both encryption at rest and access monitoring for S3 data. AWS offers several solutions to achieve these requirements, but the best choice depends on specific needs, regulatory compliance, and costs.\nOne solution is to use server-side encryption (SSE) managed by AWS Key Management Service (KMS) and CloudTrail. SSE-KMS provides an added layer of security for data at rest by encrypting the data with keys managed by KMS. KMS provides granular access controls to allow users to encrypt and decrypt data securely, and these keys are auditable via CloudTrail.\nCloudTrail allows users to monitor and log API activity in S3, including data access events such as GET, PUT, and DELETE requests. CloudTrail can track who made requests, when requests occurred, and the source IP address. With SSE-KMS, KMS manages the encryption keys, and these keys are also logged in CloudTrail, making it easier to trace data access events.\nTherefore, the correct answer is D. Server-Side Encryption managed by KMS (SSE-KMS) with CloudTrail. Option D provides both encryption at rest and monitoring of data access events, meeting the security team's requirements.\n\n"
}, {
  "id" : 51,
  "question" : "Which of the following statements regarding Amazon Athena are accurate? Select TWO.\n",
  "answers" : [ {
    "id" : "29375b18c5e84c858866237ceba47cc4",
    "option" : "Amazon Athena queries data directly from Amazon S3 and there are no additional data storage commitments beyond the object storage.",
    "isCorrect" : "true"
  }, {
    "id" : "efebbb784f0049f5836739a533722362",
    "option" : "Amazon Athena is not suitable for complex analysis such as large joins, window functions and arrays.",
    "isCorrect" : "false"
  }, {
    "id" : "cebcd4b5f2aa438c9582ac302ea8258b",
    "option" : "Amazon Athena resources are allocated in accordance to processing and memory requirements prior to deployment.",
    "isCorrect" : "false"
  }, {
    "id" : "c16ded65abe7416eb5442e25f28eabda",
    "option" : "Amazon Athena is compatible with data formats such as CSV, JSON, ORC, AVRO and Parquet",
    "isCorrect" : "true"
  }, {
    "id" : "11d2e14f27f245b9ade7f4f99160d232",
    "option" : "Amazon Athena uses a variety of query languages including SQL, LDAP, JPQL as well as CQL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A, D.\nAmazon Athena a serverless query service that does not need to build databases on dedicated Elastic Block Store (EBS) volumes.\nInstead, it builds tables from data read directly from Amazon S3 buckets.\nAmazon Athena does not store any of the data.\nThe service is compatible with the regular data formats that include CSV, JSON, ORC, AVRO and Parquet.\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html\nOption B is incorrect because Amazon Athena can query Big Data, complex analysis such as large joins, window functions and arrays.\nOption C is incorrect because Amazon Athena is serverless.\nThus the service scales following the resource demands.\nNo prior resource planning is necessary.\nOption E is incorrect because Amazon Athena uses SQL only.\n\nThe correct statements regarding Amazon Athena are A and D.\nA) Amazon Athena queries data directly from Amazon S3 and there are no additional data storage commitments beyond the object storage.\nAmazon Athena is a serverless interactive query service that allows users to analyze data stored in Amazon S3 using SQL. Athena uses S3 as its underlying data store, and queries data directly from S3. This means that there are no additional data storage commitments beyond the object storage in S3. Users are only charged for the amount of data scanned by their queries, making Athena a cost-effective option for ad-hoc querying of large datasets.\nD) Amazon Athena is compatible with data formats such as CSV, JSON, ORC, AVRO and Parquet\nAmazon Athena supports a variety of data formats, including CSV, JSON, ORC, AVRO, and Parquet. This means that users can store their data in the format that best suits their needs, and then query it using Athena's SQL interface. Athena also supports nested data types and arrays, making it easy to query semi-structured data.\nB) Amazon Athena is not suitable for complex analysis such as large joins, window functions and arrays.\nThis statement is incorrect. Amazon Athena is designed to handle complex queries, including large joins, window functions, and arrays. It uses a distributed SQL engine that scales automatically to handle large datasets and complex queries. However, the performance of Athena queries can be impacted by the size and complexity of the data, so it is important to optimize queries and use appropriate partitioning and compression techniques.\nC) Amazon Athena resources are allocated in accordance to processing and memory requirements prior to deployment.\nThis statement is also incorrect. Amazon Athena is a serverless service, which means that users do not need to provision or manage any resources. Athena automatically scales up or down to handle the workload, and users are only charged for the amount of data scanned by their queries. However, users can specify certain parameters for their queries, such as the amount of memory to use for sorting and the maximum number of concurrent queries.\nE) Amazon Athena uses a variety of query languages including SQL, LDAP, JPQL as well as CQL.\nThis statement is also incorrect. Amazon Athena uses only one query language, which is SQL. It does not support LDAP, JPQL, or CQL.\n\n"
}, {
  "id" : 52,
  "question" : "An administrator is running a large deployment of AWS resources that are spread across several AWS Regions.\nThey would like to keep track of configuration changes on all the resources and maintain a configuration inventory.\nWhat is the best service they can use?\n",
  "answers" : [ {
    "id" : "a0bfa899d1a749c5adad242e46dbbc17",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "4c65c3e948564a6fb2c0fa0895178f3b",
    "option" : "Stacks and Templates",
    "isCorrect" : "false"
  }, {
    "id" : "6c8e3326f2764a8ab614dfabec2d785c",
    "option" : "AWS Backup",
    "isCorrect" : "false"
  }, {
    "id" : "d7b90c60038945ddbde339a9e8e5c96a",
    "option" : "AWS Config.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Config will meet the scenario requirements.\nThe service allows the administrator to monitor and record configuration changes on AWS resources in their account.\nThe service also allows the administrator to create a resource configuration inventory.\nhttps://aws.amazon.com/config/\nOption A is incorrect because AWS CloudFormation will allow the administrator to create templates of resources such as EC2 instances and RDS instances but not the actual configurations in these resources.\nOption B is incorrect because Templates and Stacks form the basis of AWS CloudFormation.\nThey aid in the automated deployment of whole environments but not the applications that run in them.\nOption C is incorrect because AWS Backup is a fully managed service that allows the administrator to back up data in the cloud and on-premises.\nThe service is not the most appropriate to monitor and record resource configuration changes.\n\nThe best service for an administrator to keep track of configuration changes on all resources and maintain a configuration inventory in a large deployment of AWS resources spread across several AWS regions is AWS Config (option D).\nAWS Config is a fully managed service that provides a detailed inventory of AWS resources and their current configuration. It continuously records and evaluates changes to AWS resource configurations and provides visibility into resource relationships, enabling administrators to track changes and monitor compliance.\nAWS Config enables the creation of rules to ensure compliance with internal policies, industry regulations, and best practices, which can help prevent security and compliance issues. With AWS Config, administrators can set up notifications, view detailed resource configuration history, and troubleshoot configuration issues, all within a centralized dashboard.\nOption A, AWS CloudFormation, is a service that allows administrators to automate the deployment and management of AWS resources. While it can be used to create and manage templates for infrastructure as code, it does not provide the configuration inventory or change tracking capabilities that AWS Config does.\nOption B, Stacks and Templates, is not a service but rather a feature of AWS CloudFormation. Stacks refer to a collection of AWS resources that are created and managed together as a single unit. Templates refer to the JSON or YAML files used to define the infrastructure as code.\nOption C, AWS Backup, is a service that provides centralized backup and recovery of AWS resources. While it can be used to backup and restore resources, it does not provide the configuration inventory or change tracking capabilities that AWS Config does.\n\n"
}, {
  "id" : 53,
  "question" : "A company does not want to manage their database.\nWhich of the following services is a fully managed NoSQL database provided by AWS?\n",
  "answers" : [ {
    "id" : "48575f2c192a4534bccbc2662d4cbc22",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "347272051a994e0ab72850d8e66c6520",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "3ffeef97af43475caeab60f806d11cda",
    "option" : "Oracle RDS",
    "isCorrect" : "false"
  }, {
    "id" : "7d3eff1abd4249789d06e3a7ca157700",
    "option" : "Elastic Map Reduce.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nDynamoDB is a fully managed NoSQL offering provided by AWS.\nIt is now available in most regions for users to consume.\nFor more information on AWS DynamoDB, visit the below link:\nhttps://aws.amazon.com/dynamodb/\n\nThe correct answer is B. DynamoDB.\nExplanation:\nAWS provides several database services, including RDS (Relational Database Service) and DynamoDB. RDS is a managed service for relational databases, while DynamoDB is a fully managed NoSQL database service provided by AWS.\nNoSQL databases are non-relational databases that allow for flexible and scalable data storage, making them a popular choice for web applications and other modern use cases.\nDynamoDB is fully managed by AWS, which means that the user does not need to worry about tasks like database setup, patching, and software updates. This frees up the user to focus on developing their application rather than managing their database infrastructure.\nAWS RDS is a managed service for relational databases like MySQL, Oracle, and PostgreSQL, providing similar benefits to DynamoDB, but for a different type of database. Oracle RDS is a specific instance of RDS that provides a managed service for Oracle databases.\nElastic Map Reduce is a managed service that provides a scalable and distributed framework for processing big data. It is not a database service and is not relevant to the question at hand.\n\n"
}, {
  "id" : 54,
  "question" : "Whilst working on a collaborative project, an administrator would like to record the initial configuration and several authorized changes that engineers make to the route table of a VPC.\nWhat is the best method to achieve this?\n",
  "answers" : [ {
    "id" : "03954f86a0614120b6453fb5f3bc6f2d",
    "option" : "Use of AWS Config",
    "isCorrect" : "true"
  }, {
    "id" : "7ada43a7109c49d8af179f9d9b681767",
    "option" : "Use of VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "52ad4db4acc84cdda8b7c07e99d05a83",
    "option" : "Use of AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "70976963d2bb4656a1cba5af511f631a",
    "option" : "Use of an AWS Lambda function that is triggered to save a log file to an S3 bucket each time configuration changes are made.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Config can be used to keep track of configuration changes on AWS resources, keeping multiple date-stamped versions in a reviewable history.\nThis makes it the best method to meet the scenario requirements.\nhttps://aws.amazon.com/config/\nOption B is incorrect because VPC flow logs will only capture IP traffic-related information passing through and from network interfaces within the VPC.\nVPC flow logs will not be able to capture configuration changes made to route tables.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\nOption C is incorrect because AWS CloudTrail will capture identity access activity, event history into the AWS environment.\nRecording the actions and API calls are not best suited to keep a record of configurations.\nhttps://aws.amazon.com/cloudtrail/\nOption D is incorrect because using a Lambda function to write configuration changes might meet the requirements, but it would not be the best method.\nAWS Config can deliver what is needed with much less administrative input.\n\nTo record the initial configuration and authorized changes made to the route table of a VPC in a collaborative project, the best method is to use AWS CloudTrail.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of AWS account activities. It records API calls and events made in an AWS account and delivers the log files to an Amazon S3 bucket, CloudWatch Logs, or a partner security information and event management (SIEM) tool. CloudTrail logs provide valuable information about the activities that occur in your AWS account, including who made the request, when they made it, and what resources were affected.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides a detailed inventory of all resources and their configuration history. However, AWS Config is not the best method for recording changes made to a specific resource, such as a route table, because it does not provide detailed information about the API calls and events made to that resource.\nVPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. They can help you troubleshoot connectivity issues and monitor network traffic. However, they do not provide information about changes made to the route table.\nAn AWS Lambda function can be used to save a log file to an S3 bucket each time configuration changes are made, but this requires writing custom code to monitor changes and may not be as reliable as using a service like CloudTrail.\nTherefore, the best method for recording the initial configuration and authorized changes made to the route table of a VPC in a collaborative project is to use AWS CloudTrail.\n\n"
}, {
  "id" : 55,
  "question" : "Which of the following is a template that contains the software configuration to launch an ec2 instance?\n",
  "answers" : [ {
    "id" : "2471a63241a240a49c49fb6f44634060",
    "option" : "EBS Volumes",
    "isCorrect" : "false"
  }, {
    "id" : "c05a4773c3034ee688be6e43215cfcb3",
    "option" : "AMI",
    "isCorrect" : "true"
  }, {
    "id" : "05711df77b574fee853374f2da95b4e2",
    "option" : "EC2 Snapshot",
    "isCorrect" : "false"
  }, {
    "id" : "a25772691dbd4af6b660f105b5e3c822",
    "option" : "EBS Snapshot.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAn Amazon Machine Image (AMI) provides the information required to launch an instance, which is a virtual server in the cloud.\nYou specify an AMI when you launch an instance, and you can launch as many instances from the AMI as you need.\nYou can also launch instances from as many different AMIs as you need.\nFor more information on Amazon Machine Images, please refer to the following link:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nThe correct answer is B. AMI (Amazon Machine Image).\nAn Amazon Machine Image (AMI) is a pre-configured virtual machine image used to create an EC2 instance. It contains the necessary information to launch an EC2 instance, such as the operating system, application server, and any additional software required to run the instance.\nAn AMI provides a template for launching an EC2 instance, which includes the instance type, security groups, and storage volumes. By using an AMI, you can easily create an instance that is pre-configured to run specific software and applications.\nIn summary, an AMI is a template that contains the software configuration to launch an EC2 instance. Therefore, option B is the correct answer to the question.\nOption A, EBS Volumes, is incorrect because EBS volumes are a type of storage that can be attached to an EC2 instance, but they do not contain the software configuration required to launch an instance.\nOption C, EC2 Snapshot, is also incorrect because an EC2 Snapshot is a point-in-time copy of an EBS volume. It can be used to create a new EBS volume or restore an existing one, but it does not contain the software configuration required to launch an EC2 instance.\nOption D, EBS Snapshot, is similar to option C and is incorrect for the same reasons.\n\n"
}, {
  "id" : 56,
  "question" : "Security and Compliance is a shared responsibility between AWS and the customer.\nWhich amongst the below-listed options are AWS responsibilities?(Select TWO.)\n",
  "answers" : [ {
    "id" : "6103c3b8e7124ce69c8173949df24981",
    "option" : "Perform all the necessary security configuration and management tasks for Amazon Elastic Compute Cloud (Amazon EC2).",
    "isCorrect" : "false"
  }, {
    "id" : "01be8a7975244ad8aeb42385ddee58b9",
    "option" : "Patch management of the guest OS and applications",
    "isCorrect" : "false"
  }, {
    "id" : "625acd00d13648beb0dec50b36458047",
    "option" : "Security of the data in the AWS cloud",
    "isCorrect" : "false"
  }, {
    "id" : "1a6e043c62bd4ee088330abe38041ca3",
    "option" : "Security of the AWS cloud",
    "isCorrect" : "true"
  }, {
    "id" : "55bf282bf5464d97ad47817acd6c5811",
    "option" : "Patch management within the AWS infrastructure.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D and E.\nOption A is INCORRECT because Amazon Elastic Compute Cloud (Amazon EC2) is categorized as Infrastructure as a Service (IaaS)\nHence this is the customer's responsibility.\nOption B is INCORRECT because AWS is responsible for patching and fixing flaws within the infrastructure.\nBut customers are responsible for patching their guest OS and applications.\nOption C is INCORRECT as Security of the data in the cloud is the customer's responsibility.\nOption D is CORRECT as security of the cloud is AWS's responsibility.\nOption E is CORRECT.\nAWS is responsible for patching and fixing flaws within the infrastructure.\nReference:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\n\nWhen it comes to security and compliance, AWS and its customers share responsibility. The division of responsibility depends on the AWS service being used. Below is an explanation of AWS's responsibilities:\nD. Security of the AWS cloud: AWS is responsible for the security of the cloud infrastructure that includes physical security of data centers, network security, and security of hardware that runs AWS services.\nE. Patch management within the AWS infrastructure: AWS is responsible for patching and securing the infrastructure that runs AWS services.\nRegarding the other options:\nA. Perform all the necessary security configuration and management tasks for Amazon Elastic Compute Cloud (Amazon EC2): This responsibility lies with the customer. The customer is responsible for configuring and securing their own Amazon EC2 instances.\nB. Patch management of the guest OS and applications: This responsibility lies with the customer. The customer is responsible for patching and securing their own guest operating systems and applications.\nC. Security of the data in the AWS cloud: This responsibility is shared between AWS and the customer. AWS provides tools and services to secure data in the cloud, but the customer is responsible for ensuring their data is appropriately secured and compliant with regulations.\nIn summary, while AWS is responsible for the security of the cloud infrastructure and patch management within the AWS infrastructure, customers are responsible for securing their own instances, guest operating systems, applications, and data in the cloud.\n\n"
}, {
  "id" : 57,
  "question" : "Which of the following tools can be used to check service limits for resources launched within AWS Cloud Infrastructure?\n",
  "answers" : [ {
    "id" : "08b0c1af43b64353ad021fe81cbc6be7",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "84518eadbb2148b2953cba4f9aec3e28",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "true"
  }, {
    "id" : "271bced5b5aa4c439aed4b0125c0d532",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "a5d2e94505d943aaa4468a6269094f5e",
    "option" : "AWS CloudTrail.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nAWS Trusted Advisor checks for service usage for all the resources within AWS Cloud and provides notifications.\nOption A is incorrect as AWS Config can be used to audit, evaluate configurations of AWS resources.\nBut it does not check service limits for resources.\nOption C is incorrect as Amazon CloudWatch monitors AWS resources and applications on these resources.\nBut it does not check service limits for resources.\nOption D is incorrect as AWS CloudTrail is a logging service, recording activity made to AWS resources.\nBut it does not check service limits for resources.\nFor more information on AWS Trusted Advisor, refer to the following URL:\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/\n\nThe correct answer to this question is B. AWS Trusted Advisor.\nAWS Trusted Advisor is a tool that provides guidance on best practices and optimization for your AWS resources. One of the features of AWS Trusted Advisor is the ability to check your account for service limits, also known as quotas, which are the maximum number of resources you can launch within a specific AWS service.\nWhen you use AWS Trusted Advisor to check your account for service limits, it provides you with a list of the services you are using and their current limits. This information can help you plan your infrastructure and make informed decisions about scaling your resources.\nAWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. It is primarily used to track changes to resources over time and enforce compliance with security policies. While it may provide some insight into service limits, it is not designed to specifically check for them.\nAmazon CloudWatch is a monitoring service that provides data and actionable insights for your AWS resources. It allows you to collect and track metrics, collect and monitor log files, and set alarms. Like AWS Config, it may provide some information on resource usage, but it is not specifically designed to check for service limits.\nAWS CloudTrail is a service that provides a history of AWS API calls and related events for your account. It is primarily used for compliance, auditing, and security purposes. It may provide information on resource usage, but, like AWS Config and Amazon CloudWatch, it is not specifically designed to check for service limits.\nIn summary, while all of these services may provide some information on resource usage, AWS Trusted Advisor is the tool specifically designed to check for service limits for resources launched within AWS Cloud Infrastructure.\n\n"
}, {
  "id" : 58,
  "question" : "Which of the following accurately describes a typical use case in which the AWS CodePipeline service can be utilized?\n",
  "answers" : [ {
    "id" : "c70c8a718b8c4eff90d7ba3dc6388516",
    "option" : "To compose code in an integrated development environment that enables developers to run, test and debug components of a dynamic microservice.",
    "isCorrect" : "false"
  }, {
    "id" : "95eb4491e88148ec98700cd010db2c44",
    "option" : "To compile and deploy a microservice onto Amazon EC2 instances or AWS Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "1810d905a36a4f529c3efbb47f841497",
    "option" : "To securely share code, collaborate on source code, version control and store binaries on an AWS fully-managed platform that scales seamlessly.",
    "isCorrect" : "false"
  }, {
    "id" : "e5815d3fd1be4100a96513144e8e9f2b",
    "option" : "To orchestrate and automate the various phases involved in the release of application updates in-line with a predefined release model.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer:D.\nThe question is looking for a typical use case for AWS CodePipeline.\nOption D is the most appropriate because AWS CodePipeline is typically utilized when orchestrating and automating the various phases involved in the release of application updates in-line with a release model that the developer defines.\nhttps://aws.amazon.com/codepipeline/\nOption A is INCORRECT because composing code in an integrated development environment that enables developers to run, test, and debug components of a dynamic microservice is the typical AWS Cloud9 IDE function.\nOption B is INCORRECT because compiling and deploying microservices on Amazon EC2 instances or AWS Lambda functions are the typical functions of AWS CodeDeploy.\nOption C is INCORRECT because securely sharing code, collaborating on source code, version control and storing binaries on an AWS fully-managed platform describe the functions of CodeCommit.\n\nOption D is the correct answer.\nAWS CodePipeline is a fully-managed service that allows developers to build, test, and deploy their applications to the cloud in a streamlined and automated way. It is primarily designed to automate the release process of software applications by creating a continuous delivery pipeline that can help developers and teams deliver new features, updates, and fixes to their applications rapidly and consistently.\nOption A describes an integrated development environment (IDE) that enables developers to write, test and debug code. While an IDE is a critical tool for software development, it is not the same as CodePipeline, which is focused on automating the software delivery process.\nOption B describes a task that can be accomplished using CodeDeploy, a separate AWS service that is designed to automate code deployment to EC2 instances and Lambda functions. CodeDeploy is one of the tools that can be used as part of a CodePipeline workflow.\nOption C describes a combination of services like AWS CodeCommit, CodeBuild and CodeArtifact which can be used for source code management, building binaries, and storing artifacts. However, CodePipeline offers more than just code collaboration, version control, and artifact storage. It is a comprehensive release orchestration service that can help automate the entire release process from building and testing to deploying and monitoring the application.\nTherefore, option D is the most accurate description of a typical use case for AWS CodePipeline, which is to orchestrate and automate the various phases involved in the release of application updates in-line with a predefined release model.\n\n"
}, {
  "id" : 59,
  "question" : "Your organization has planned to decommission its data centers.\nYou have a task to migrate hundreds of TB of archived data and 22 TB of active data to S3 and EFS, as per the designed solution.\nWhich of the below service will be best suited for this task?\n",
  "answers" : [ {
    "id" : "1c7c70213e464602b8c5e06b27b130cb",
    "option" : "AWS Data Sync",
    "isCorrect" : "true"
  }, {
    "id" : "c04b44e276ec4a1e97b8bc588c67a6c0",
    "option" : "AWS Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "26287c1cd94945c899a6b38413f1a81f",
    "option" : "AWS Data Pipeline",
    "isCorrect" : "false"
  }, {
    "id" : "585570feb0234efa92b5e624339205d8",
    "option" : "AWS Migration Hub.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAWS Data Sync is a simple and fast way to move huge amounts of data (hundreds of terabytes) between on-prem storage to S3, EFS, FSx.\nOption B is INCORRECT.\nAWS Direct Connect is an offering that helps run workloads that are heavy on bandwidth in AWS.\nAWS Direct Connect enables private and dedicated connections between the on-premises network and AWS.\nAWS Data Sync could be used over the internet or Direct Connect.\nOption C is INCORRECT.\nAWS Data Pipeline is a web service that facilitates data processing and movement between various AWS services (like compute and storage)\nData pipeline also works well with data sources that are on-premise.\nIn the given data migration scenario, data sync is a more apt choice.\nOption D is INCORRECT.\nAWS Migration Hub is a service that facilitates discovery of the existing applications and IT assets and provides a view to better plan and track application migrations.\nReference:\nhttps://aws.amazon.com/datasync/\nhttps://aws.amazon.com/directconnect/\nhttps://aws.amazon.com/datapipeline/\nhttps://aws.amazon.com/migration-hub/\n\nThe best-suited service for migrating hundreds of TB of archived data and 22 TB of active data to S3 and EFS is AWS Data Sync.\nAWS Data Sync is a data transfer service that enables users to move large amounts of data into and out of AWS services, including Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server, quickly and securely. Data Sync makes it simple and easy to automate and accelerate data transfers over the internet or AWS Direct Connect, with no impact on the application performance.\nAWS Data Sync allows you to copy data between on-premises storage and AWS, between AWS services, or from one AWS region to another. It also provides features such as data validation, encryption, and network optimization to ensure fast and secure data transfers.\nAWS Direct Connect, on the other hand, is a networking service that enables you to establish a dedicated network connection between your on-premises data center and AWS. It is primarily used to provide a more consistent network experience and better performance for workloads that require low latency and high bandwidth.\nAWS Data Pipeline is a data processing service that allows you to move and process data from various sources to various destinations, such as Amazon S3, Amazon Redshift, or DynamoDB. It enables you to schedule, execute, and monitor data processing workflows, and provides support for various data formats and technologies.\nAWS Migration Hub is a service that provides a central location to track the progress of application migrations to AWS. It enables you to discover, assess, and migrate applications and data from on-premises to AWS or between AWS environments, using various migration tools and services.\nTherefore, in summary, AWS Data Sync is the best-suited service for migrating hundreds of TB of archived data and 22 TB of active data to S3 and EFS, as it is specifically designed for fast and secure data transfer between various storage services.\n\n"
}, {
  "id" : 60,
  "question" : "To maximize user satisfaction, you are asked to improve the performance of the application for local and global users.\nAs part of this initiative, you must monitor the application endpoint health and route traffic to the most appropriate application endpoint.\nWhich service will you prefer to use?\n",
  "answers" : [ {
    "id" : "0e56e4e7bf694523828d5899af067275",
    "option" : "Amazon Global Accelerator",
    "isCorrect" : "true"
  }, {
    "id" : "075f5d4b21064cda9ec515cfde150e5e",
    "option" : "Amazon DAX accelerator",
    "isCorrect" : "false"
  }, {
    "id" : "9628d8dfbe97499fbe856b72fc220b92",
    "option" : "Amazon S3 transfer acceleration",
    "isCorrect" : "false"
  }, {
    "id" : "69053b04a66e46a491470d0288e214f7",
    "option" : "AWS Direct Connect.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nGlobal accelerator is a networking service that utilizes AWS global network to optimize the â€œuser to applicationâ€ path.\nThe performance benefits realized by the use of the Global accelerator can be tested using a speed comparison tool provided by AWS.\nGlobal accelerator differs from S3 transfer acceleration and DynamoDB accelerator.\nS3 transfer acceleration, accelerates the transfers of files to the S3 bucket by utilizing edge locations.\nFully managed DynamoDB Accelerator (DAX) is a highly available in-memory cache for Dynamodb.\nOption A is CORRECT.\nRefer to the explanation above.\nOption B is INCORRECT.\nRefer to the explanation above.\nOption C is INCORRECT.\nRefer to the explanation above.\nOption D is INCORRECT.\nAWS Direct Connect is an AWS offering that simplifies setting up dedicated network connectivity between AWS and on-premises infrastructure.\nhttps://aws.amazon.com/global-accelerator/\nhttps://aws.amazon.com/dynamodb/dax/\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nhttps://aws.amazon.com/directconnect/\n\nThe service that can help to monitor the application endpoint health and route traffic to the most appropriate endpoint to maximize user satisfaction is Amazon Global Accelerator.\nAmazon Global Accelerator is a networking service that directs user traffic to optimal endpoints over the AWS global network. It helps to improve the availability and performance of applications for local and global users.\nAmazon Global Accelerator uses anycast IP addresses that are assigned to the accelerator endpoints. These IP addresses allow traffic to be routed to the optimal endpoint based on the network conditions. This ensures that users are always directed to the best performing endpoint, which improves the overall application performance.\nIn addition, Amazon Global Accelerator monitors the health of the endpoints and automatically routes traffic to healthy endpoints. This helps to ensure that users are not directed to unhealthy endpoints, which can result in a degraded user experience.\nTo summarize, Amazon Global Accelerator is the preferred service for monitoring the application endpoint health and routing traffic to the most appropriate endpoint for maximizing user satisfaction.\n\n"
}, {
  "id" : 61,
  "question" : "Which AWS service offering uses machine learning and graph theory capability on automatically collected log data to help you conduct faster and efficient security investigations?\n",
  "answers" : [ {
    "id" : "002014c345c8415bb8a74d94e2d36883",
    "option" : "Amazon Macie",
    "isCorrect" : "false"
  }, {
    "id" : "f9d9ea308bc64724b78d355bf4787f3b",
    "option" : "Amazon Detective",
    "isCorrect" : "true"
  }, {
    "id" : "5d7190ed7a254a79b4b73d2542c5ae4f",
    "option" : "AWS Artifact",
    "isCorrect" : "false"
  }, {
    "id" : "11b58b0b42e14cf7915e3c15358e3d24",
    "option" : "Amazon GuardDuty.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nAmazon Macie is a fully managed service from AWS that provides data security and privacy by utilizing Amazon's machine learning and pattern matching capabilities.\nOption B is CORRECT.\nAmazon Detective is a security service that uses machine learning capabilities on the automatically collected log data to help customers perform efficient and fast security investigations.\nOption C is INCORRECT.\nAWS Artifact is a central resource for all the information about compliance.\nAWS artifact provides on-demand access to compliance reports at no additional cost.\nOption D is INCORRECT.\nAmazon GuardDuty performs continuous monitoring to protect AWS account, S3 data and workloads from any malicious, unauthorized activities.\nhttps://aws.amazon.com/macie/\nhttps://aws.amazon.com/detective/faqs/\nhttps://aws.amazon.com/artifact/\nhttps://aws.amazon.com/guardduty/\n\nThe correct answer to this question is B. Amazon Detective.\nAmazon Detective is an AWS service offering that uses machine learning and graph theory capability on automatically collected log data to help you conduct faster and more efficient security investigations. It is designed to simplify the process of analyzing and visualizing data from disparate sources, making it easier to identify potential security threats and quickly take action to address them.\nBy automatically collecting and analyzing data from AWS CloudTrail, Amazon VPC Flow Logs, and other sources, Amazon Detective creates a unified view of your resources, making it easier to identify and investigate security incidents. It uses machine learning to identify patterns and anomalies in your data, allowing you to quickly pinpoint the root cause of any issues.\nIn addition to its machine learning and graph theory capabilities, Amazon Detective also includes a number of built-in visualizations that help you easily explore and understand your data. These include interactive diagrams that show the relationships between your resources, as well as timelines and histograms that allow you to quickly identify trends and patterns.\nOverall, Amazon Detective is a powerful tool for anyone looking to improve their security posture in the cloud. By leveraging machine learning and graph theory to automatically analyze and visualize log data, it helps you quickly identify and respond to potential threats, allowing you to better protect your resources and maintain the integrity of your environment.\n\n"
}, {
  "id" : 62,
  "question" : "An administrator is tasked to configure privileges for the new joiners in the department.\nThe admin is selectively granting privileges and ensuring that not all the team members can access all the resources. Which principle is the administrator following?\n",
  "answers" : [ {
    "id" : "e9a78bc4dc2549a0b5b51e0d964eec9b",
    "option" : "Principle of privileged users",
    "isCorrect" : "false"
  }, {
    "id" : "d41e2cfef6634bc0b54528d1d276db6c",
    "option" : "Least privilege of group principle",
    "isCorrect" : "false"
  }, {
    "id" : "a0f7e97e2f8747d38823039879c97138",
    "option" : "Best practices of permission advisory",
    "isCorrect" : "false"
  }, {
    "id" : "94190803c4304750b0dbeb40c83f98d6",
    "option" : "Principle of least privilege.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nThis is not a valid option.\nOption B is INCORRECT.\nThis is not a valid option.\nOption C is INCORRECT.\nThis is not a valid option.\nOption D is CORRECT.\nThe administrator follows the â€œPrinciple of least privilegeâ€ as not all the privileges are granted to all the new joiners.\nThe privileges are being selectively granted.\nReference:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege\n\nThe principle that the administrator is following is the Principle of Least Privilege, also known as the Principle of Least Authority. This principle states that a user or process should be given only the minimum level of access or privileges necessary to perform their job or task.\nIn this scenario, the administrator is selectively granting privileges to the new joiners in the department and ensuring that not all team members can access all resources. By doing so, the administrator is limiting the exposure of the system to any potential security threats or breaches, as well as reducing the risk of accidental changes or errors made by users.\nThe Principle of Least Privilege is a fundamental security concept that is applied across various industries and technologies, including cloud computing. By following this principle, administrators can minimize the attack surface of their systems, limit the potential damage of any security incidents, and maintain the confidentiality, integrity, and availability of their resources.\nThe other answer options are incorrect or not relevant to this scenario. The Principle of Privileged Users implies that certain users have elevated privileges that give them access to all resources, which is not the case in this scenario. The Best Practices of Permission Advisory is not a known security principle or concept. Finally, the Least Privilege of Group Principle is not a commonly recognized security principle and is not applicable to this scenario.\n\n"
}, {
  "id" : 63,
  "question" : "You are asked to suggest an appropriate Amazon S3 storage class for â€œdata with unknown/changing access patternâ€\nWhich one would you suggest?\n",
  "answers" : [ {
    "id" : "ab1bf7acd4014cb9b2b0191e5fb03907",
    "option" : "Amazon S3 Intelligent-Tiering",
    "isCorrect" : "true"
  }, {
    "id" : "c5a46e64b1194843a38786e575b4b7a9",
    "option" : "Amazon S3 Standard",
    "isCorrect" : "false"
  }, {
    "id" : "61c79419f4554feb996dcaf48336727f",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "e9d5885b04a8453a9be93026515cf1a4",
    "option" : "Amazon S3 Standard-Infrequent Access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAmazon S3 Intelligent-Tiering is best suited for data with â€œunknown/changing access patternâ€.\nOption B is INCORRECT.\nS3 standard is ideal for general-purpose storage of frequently accessed data.\nOption C is INCORRECT.\nAmazon S3 Glacier is preferable for archival of data for a long term.\nOption D is INCORRECT.\nAmazon S3 Standard-Infrequent Access is better suited for less frequently accessed, long-lived data.\nReference:\nhttps://aws.amazon.com/s3/storage-classes/\n\nFor data with an unknown or changing access pattern, the best Amazon S3 storage class to use would be Amazon S3 Intelligent-Tiering.\nAmazon S3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers: one tier optimized for frequent access and another for infrequent access. This allows you to automatically and seamlessly optimize costs for data with unknown or changing access patterns, as Intelligent-Tiering can detect and respond to changes in access patterns.\nFor example, if a file is frequently accessed at first but then access decreases over time, Intelligent-Tiering can automatically move that file to the infrequent access tier to save costs. Conversely, if a file is accessed infrequently at first but then access increases, Intelligent-Tiering can move that file to the frequent access tier to ensure faster access times.\nIn contrast, the other S3 storage classes listed would not be as appropriate for data with an unknown or changing access pattern:\nAmazon S3 Standard is optimized for frequently accessed data and would not be cost-effective for infrequently accessed data. Amazon S3 Glacier is optimized for long-term data archival and retrieval, so it would not be suitable for data that is frequently accessed or for which access patterns are unknown. Amazon S3 Standard-Infrequent Access is optimized for infrequently accessed data but does not have the automatic tiering feature of Intelligent-Tiering, so it may not be as effective in optimizing costs for data with unknown or changing access patterns.\n\n"
}, {
  "id" : 64,
  "question" : "A â€œMember AWS accountâ€ in an AWS Organization (using consolidated billing) wants to receive a cost breakdown report (product-wise daily report) so that the analysis of cost and usage could be done. Where can this report be configured to be delivered?\n",
  "answers" : [ {
    "id" : "22f6b40c53a74e5e965cf771a489a163",
    "option" : "S3 bucket owned by the member account",
    "isCorrect" : "false"
  }, {
    "id" : "98fc632d272c4304b65fe806e32a7879",
    "option" : "S3 bucket owned by the master account",
    "isCorrect" : "true"
  }, {
    "id" : "7ad717d1d20d48d98b8b0cda1a43e3e8",
    "option" : "AWS Management Console",
    "isCorrect" : "false"
  }, {
    "id" : "bc01b9b8d43d43dfbeb7da12ea661dbb",
    "option" : "Amazon Athena.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAs the consolidated billing feature is being used in AWS organizations, the S3 bucket where the report could be configured to be received should be owned by the master account in the organization.\nBilling reports cannot be received in S3 buckets owned by member accounts.\nThe report delivered to the S3 bucket owned by the master account could be ingested to Amazon Athena.\nAfter that, the data in the S3 bucket can be analyzed using standard SQL queries.\nAWS Management Console is a centralized management and governance console for all the AWS products.\nOption A is INCORRECT.\nOption B is CORRECT.\nOption C is INCORRECT.\nOption D is INCORRECT.\nReference:\nhttps://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html\nhttps://aws.amazon.com/athena/\nhttps://aws.amazon.com/console/\n\nIn an AWS Organization with consolidated billing, all the member accounts roll up their usage and billing charges to the designated payer account (i.e., master account) for payment. The payer account can access the billing data of all the linked accounts and generate a detailed billing report that provides insights into the cost and usage data of each linked account.\nThe cost breakdown report (also known as Product-wise daily report) is a billing report that breaks down the AWS costs by product or service on a daily basis. It provides detailed information on the usage and charges for each AWS service, making it easier to analyze the costs and optimize the usage.\nNow, the question is about where this report can be configured to be delivered for a member account. There are a few options provided in the answer choices, and let's see which one is correct.\nA. S3 bucket owned by the member account: The member account can create an S3 bucket and configure the billing report to be delivered to that bucket. However, this option is not valid for a consolidated billing scenario where the billing data is collected and paid by the payer account. Therefore, this option is not correct.\nB. S3 bucket owned by the master account: The payer account (i.e., master account) can create an S3 bucket and configure the billing report to be delivered to that bucket. Since the payer account is responsible for paying the bills of all the linked accounts, it makes sense to have the billing report delivered to an S3 bucket owned by the payer account. Therefore, this option is correct.\nC. AWS Management Console: The AWS Management Console is a web-based interface that allows users to access and manage their AWS resources. However, the console is not a valid option for delivering billing reports. Therefore, this option is not correct.\nD. Amazon Athena: Amazon Athena is a serverless, interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. However, it is not a valid option for delivering billing reports. Therefore, this option is not correct.\nIn summary, the correct answer is B. S3 bucket owned by the master account. The payer account can create an S3 bucket and configure the billing report to be delivered to that bucket, which allows the member accounts to access the report and analyze their cost and usage data.\n\n"
}, {
  "id" : 65,
  "question" : "Which AWS service allows: 1- Running Docker containers 2- Simple API calls to launch and stop container-based applications.\n",
  "answers" : [ {
    "id" : "2f3ef5ad23cc49aabc1868245115bd49",
    "option" : "AWS Docker Manager",
    "isCorrect" : "false"
  }, {
    "id" : "6948f5e36cfd45ef9f2cf94c90ecc661",
    "option" : "AWS EKS",
    "isCorrect" : "false"
  }, {
    "id" : "6af07b14d4cd49c68393a5663cee5d12",
    "option" : "Amazon Elastic Container Service",
    "isCorrect" : "true"
  }, {
    "id" : "c896c3e0959447ab9d2c3bace9ee2ceb",
    "option" : "AWS Fargate.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nAWS EKS (Elastic Kubernetes Service): AWS EKS is a managed service that simplifies Kubernetes deployment on AWS by eliminating the need to install, operate, and/or maintain its own Kubernetes control plane.\nAmazon Elastic Container Service: AWS ECS is a container management service that facilitates containers' management on the cluster, including running and stopping the containers.\nThe container-based applications could be launched/stopped using simple API calls.\nAWS Fargate: AWS Fargate is an â€œECS and EKS compatibleâ€ serverless compute engine for containers.\nOption A is INCORRECT.\nAWS Docker Manager is an invalid option.\nOption B is INCORRECT.\nOption C is CORRECT.\nOption D is INCORRECT.\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\nhttps://aws.amazon.com/fargate/\n\nThe AWS service that allows running Docker containers and simple API calls to launch and stop container-based applications is Amazon Elastic Container Service (ECS).\nAmazon Elastic Container Service (ECS) is a fully managed container orchestration service that supports Docker containers. ECS allows you to easily run, scale, and manage Docker containers on a cluster of EC2 instances or using AWS Fargate.\nWith ECS, you can create a task definition that describes how to launch and run a Docker container, including specifying the Docker image, CPU and memory requirements, ports, and other configurations. You can then use the ECS console or API to create and manage tasks, services, and clusters.\nECS also provides a simple API for launching and stopping container-based applications. The ECS API is accessible through the AWS Management Console, CLI, and SDKs, allowing you to automate container management tasks and integrate ECS with your existing applications and infrastructure.\nIn contrast, the other options listed in the question are not correct:\nAWS Docker Manager does not exist as an AWS service. AWS EKS (Elastic Kubernetes Service) is a managed Kubernetes service that allows you to run and manage Kubernetes clusters on AWS. While Kubernetes can also run Docker containers, it is not as simple as ECS for launching and managing container-based applications. AWS Fargate is a serverless compute engine for containers that allows you to run containers without having to manage the underlying EC2 instances. While Fargate can run Docker containers, it is not as flexible as ECS for customizing the container environment and scaling containers on a cluster of EC2 instances.\n\n"
}, {
  "id" : 66,
  "question" : "Which AWS services can be used to store files? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "10d67f0a68fe4b09bf9a3df912d79963",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "d1a9f332105a4939b49bbc333a718c40",
    "option" : "Amazon Simple Storage Service (Amazon S3)",
    "isCorrect" : "true"
  }, {
    "id" : "e42235b389cf47f7bcc76a036f4fa32f",
    "option" : "Amazon Elastic Block Store (Amazon EBS)",
    "isCorrect" : "true"
  }, {
    "id" : "06b1112b9c8f47f4ba2a12c3433e924e",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "c8dc66e74aee43b88883cb26f954750c",
    "option" : "Amazon Athena.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe AWS documentation mentions the following:\nAmazon S3 is object storage built to store and retrieve any amount of data from anywhere - web sites and mobile apps, corporate applications and data from IoT sensors or devices.\nIt is designed to deliver 99.999999999% durability.\nIt stores data for millions of applications used by market leaders in every industry.\nFor more information on the Simple Storage Service, please refer to the below URL:\nhttps://aws.amazon.com/s3/\nAmazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for use with Amazon EC2 instances in the AWS Cloud.\nEach Amazon EBS volume is automatically replicated within its Availability Zone to protect you from component failure, offering high availability and durability.\nFor more information on Amazon EBS, please refer to the below URL:\nhttps://aws.amazon.com/ebs/\nAnswer A is incorrect.\nAmazon CloudWatch is used for performance monitoring.\nAnswer D is incorrect.\nAWS Config is used to audit and monitor configuration changes.\nAnswer E is incorrect.\nAmazon Athena is a serverless query service used to analyze BigData stored in S3.\n\nThe two AWS services that can be used to store files are Amazon Simple Storage Service (Amazon S3) and Amazon Elastic Block Store (Amazon EBS).\nAmazon S3: It is a highly scalable, durable, and secure object storage service that can store and retrieve any amount of data from anywhere on the web. It is designed to store and retrieve data from websites, mobile applications, corporate applications, and data archives. Amazon S3 provides easy-to-use management features and supports multiple access methods, including REST, SOAP, and AWS SDKs. Amazon EBS: It is a block-level storage service that provides persistent block-level storage volumes for use with Amazon EC2 instances. Amazon EBS volumes are designed for use with EC2 instances and offer low-latency access to data. They can be attached to and detached from running instances and offer options for backup and replication to ensure data durability.\nAmazon CloudWatch is a monitoring service that can be used to collect and track metrics, collect and monitor log files, and set alarms. AWS Config is a service that helps you assess, audit, and evaluate the configuration of your AWS resources. Amazon Athena is a serverless query service that enables you to analyze data in Amazon S3 using standard SQL. None of these services can be used to store files.\n\n"
}, {
  "id" : 67,
  "question" : "When an administrator is looking to deploy shared file access Linux-based workloads which will require up to petabytes of data stores, what is the best-suited file storage option to use?\n",
  "answers" : [ {
    "id" : "8dd58cf84a1a42328f13ef3e611ab7a2",
    "option" : "Amazon EFS",
    "isCorrect" : "true"
  }, {
    "id" : "7a41d35128cb44c1822988b45ac80df2",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "72a8545e38144c5d85eb38ca3bd8f456",
    "option" : "AWS Snowball",
    "isCorrect" : "false"
  }, {
    "id" : "170b40fd5cf340b5abbc0ca669105b9b",
    "option" : "Amazon EBS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon Elastic File Storage (EFS) is the best-suited file storage option for the described scenario.\nIt is designed for shared file access and scaling to petabyte data store.\nhttps://aws.amazon.com/efs/when-to-choose-efs/\nOption B is incorrect because Amazon S3 is an object data store which is not suitable for deploying Linux-based workloads as the scenario outlines.\nOption C is incorrect because AWS Snowball is a data transport solution and data migration which is not suitable for deploying shared file access builds.\nhttps://aws.amazon.com/snowball/\nOption D is incorrect because Amazon Elastic Block Store is block storage service for access by an EC2 instance but without the capability of a share file access.\nApplications that utilize persistent or dedicated block storage for a single instance can use Amazon EBS storage.\nhttps://aws.amazon.com/efs/when-to-choose-efs/\n\nThe best-suited file storage option for Linux-based workloads that require up to petabytes of data stores is Amazon EFS (Elastic File System).\nAmazon EFS is a scalable, fully-managed, and highly available cloud-based file storage service. It allows multiple EC2 instances to access a shared file system at the same time, enabling it to provide file storage for Linux-based applications that need shared access to file data.\nAmazon S3 (Simple Storage Service) is an object-based storage service that is ideal for storing and retrieving large amounts of unstructured data. It is not suitable for storing and accessing files that require frequent read and write operations, and it does not provide a file system interface that is required by Linux-based applications.\nAWS Snowball is a physical storage device that allows you to transfer large amounts of data from your on-premises data center to the cloud. It is not a suitable option for file storage.\nAmazon EBS (Elastic Block Store) is a block storage service that is used to store data for EC2 instances. It is not designed for file storage, and it is not suitable for shared access.\nIn summary, Amazon EFS is the best-suited file storage option for Linux-based workloads that require up to petabytes of data stores, as it provides a scalable and fully-managed file storage system with shared access capabilities.\n\n"
}, {
  "id" : 68,
  "question" : "Which of the following is a prerequisite for using AWS OpsWorks to manage applications on servers at the customer data centres (on-premises compute servers)?\n",
  "answers" : [ {
    "id" : "be852c3b15464a299ca9136d0a4059dc",
    "option" : "Servers should be running on either Linux or Windows OS with connectivity to AWS public endpoints.",
    "isCorrect" : "false"
  }, {
    "id" : "75d9db608082442da1ce810ccd81054d",
    "option" : "Servers should be running Linux OS with connectivity to AWS private endpoints.",
    "isCorrect" : "false"
  }, {
    "id" : "0edb004443b84617b41a4003dccdf785",
    "option" : "Servers should be running Windows OS with connectivity to AWS private endpoints.",
    "isCorrect" : "false"
  }, {
    "id" : "8ac13bd710c44ac7bfd07358f374f3ec",
    "option" : "Servers should be running Linux OS with connectivity to AWS public endpoints.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nIt is given in the AWS documentation that: You can use AWS OpsWorks Stacks to configure and manage both Linux and Windows EC2 instances.\nBut there is no need for EC2 instance, the question focuses on compute servers on the data centres means outside of AWS.\nA VPC endpoint enables private connections between your VPC and supported AWS services and VPC endpoint services powered by AWS PrivateLink.\nVPC endpoints are virtual devices.\nTo use AWS OpsWorks for servers in customer data centres, the servers should have Linux operating systems with an OpsWorks Stacks agent installed and connectivity to AWS Public endpoints.\nUsing AWS OpsWorks Stacks to create Amazon EC2 instances, you can also register instances with a Linux stack that were created outside of AWS OpsWorks Stacks.\nHowever, they must be running one of the supported Linux distributions.\nYou cannot register Amazon EC2 or on-premises Windows instances.\nOption A is incorrect because servers deployed at customer data centres only supports Linux OS, not both.\nOption B is incorrect because servers deployed at customer data centres should have connectivity to AWS public endpoints instead of private endpoints.\nOptions C is incorrect because on-premise servers should have Linux OS instead of Windows OS.\nFor more information on AWS OpsWorks, refer to the following URLs:\nhttps://aws.amazon.com/opsworks/stacks/features/\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-os.html\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html\n\nAWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet, which are automation platforms that help you automate infrastructure tasks, deploy applications, and manage servers. With AWS OpsWorks, you can use Chef and Puppet to automate how servers are configured, deployed, and managed across your Amazon EC2 instances or your on-premises compute servers.\nTo manage applications on servers at the customer data centres (on-premises compute servers) using AWS OpsWorks, the servers should meet the following prerequisites:\nThe servers should be running either Linux or Windows OS. The servers should have connectivity to AWS endpoints.\nOption A states that the servers should be running on either Linux or Windows OS with connectivity to AWS public endpoints. This option is incorrect because it mentions AWS public endpoints, which are internet-facing endpoints used for accessing AWS services from the internet. On-premises compute servers do not typically have direct connectivity to AWS public endpoints, and they usually require a VPN or Direct Connect connection to AWS.\nOption B states that the servers should be running Linux OS with connectivity to AWS private endpoints. This option is incorrect because it mentions AWS private endpoints, which are used to access AWS services over a private connection rather than the internet. Private endpoints are typically used for accessing AWS services from within a VPC, and they are not relevant to on-premises compute servers.\nOption C states that the servers should be running Windows OS with connectivity to AWS private endpoints. This option is incorrect for the same reasons as option B.\nOption D states that the servers should be running Linux OS with connectivity to AWS public endpoints. This option is incorrect because it mentions AWS public endpoints, which are not typically accessible from on-premises compute servers.\nTherefore, the correct answer is A. Servers should be running on either Linux or Windows OS with connectivity to AWS public endpoints.\n\n"
}, {
  "id" : 69,
  "question" : "Which AWS service provides infrastructure security optimization recommendations?\n",
  "answers" : [ {
    "id" : "6f6122f2f1d344688552abae44ada914",
    "option" : "AWS Application Programming Interface(API)",
    "isCorrect" : "false"
  }, {
    "id" : "22262f73f0e845478fad2ee4bd2bd626",
    "option" : "Reserved Instances",
    "isCorrect" : "false"
  }, {
    "id" : "33effb817ec54c6eb35b710abf42779a",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "true"
  }, {
    "id" : "804eba58ba344f4294a5797a16202b3b",
    "option" : "Amazon Elastic Compute Cloud (Amazon EC2) SpotFleet.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS documentation mentions the following:\nAn online resource to help you reduce cost, increase performance, and improve security by optimizing your AWS environment, Trusted Advisor provides real time guidance to help you provision your resources following AWS best practices.\nFor more information on the AWS Trusted Advisor, please refer to the below URL:\nhttps://aws.amazon.com/premiumsupport/trustedadvisor/\nChoices A, B, and D are incorrect.\nThey are not related to infrastructure security optimization.\n\nThe correct answer is C. AWS Trusted Advisor.\nAWS Trusted Advisor is a service that provides best practices and recommendations for improving security, performance, and cost optimization of your AWS infrastructure. It offers multiple checks and recommendations across various categories such as cost optimization, security, fault tolerance, and performance improvement.\nThe security category of AWS Trusted Advisor provides recommendations for optimizing the security of your AWS infrastructure. It checks your infrastructure for known security vulnerabilities and recommends steps to remediate those vulnerabilities. It also recommends security best practices for your AWS account, including IAM security, security group configuration, and network security.\nIn addition to security, AWS Trusted Advisor also provides recommendations for cost optimization, performance improvement, and fault tolerance. For example, it can recommend resizing underutilized resources, identifying idle Elastic Load Balancers, and checking for the availability of the latest version of Amazon RDS database engines.\nIn conclusion, AWS Trusted Advisor provides infrastructure security optimization recommendations, as well as recommendations for cost optimization, performance improvement, and fault tolerance.\n\n"
}, {
  "id" : 70,
  "question" : "A department in an organization has a stipulated monthly expenditure limit on their AWS account and is anxious about exceeding it.\nHow can they allay this concern in the best possible way?\n",
  "answers" : [ {
    "id" : "f8e230bd015a4867ad6c45cf4deb8169",
    "option" : "Regularly review their Billing and Cost management dashboard during the course of the month in the management console.",
    "isCorrect" : "false"
  }, {
    "id" : "677a57862a264af7a75de971afa5e331",
    "option" : "Under Billing Preferences &gt; Cost Management Preferences, they should tick the Receive Free Tier Usage Alerts checkbox.",
    "isCorrect" : "false"
  }, {
    "id" : "d8e63608c55743688b3ab58d9fd6fa19",
    "option" : "In AWS CloudWatch they ought to create an alarm that triggers each time the services bill surpasses the limit.",
    "isCorrect" : "false"
  }, {
    "id" : "857b6d4bba624b2780d3c372425a9858",
    "option" : "In AWS Budgets, creating an email alert based on the budget parameters would suffice.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Budgets provides a useful feature of setting custom budgets that prompt users when their costs or usage are forecasted to exceed.\nThe forecast aspect gives a buffer period in advance when alerting the user.\nBudgets can be tracked monthly, quarterly, or yearly, and have customizable start and end dates.\nAlerts can be sent via email and/or Amazon Simple Notification Service (SNS) topic.\nhttps://aws.amazon.com/aws-cost-management/aws-budgets/\nOption A is INCORRECT because the regular review will not stop or alert the department if their service bill exceeds their stipulated budget.\nhttps://docs.aws.amazon.com/account-billing/index.html\nOption B is INCORRECT because selecting the Receive Free Tier Usage Alerts checkbox would notify the department each time their service bills go out of the free-tier range only, not when it approaches the limit.\nhttps://aws.amazon.com/about-aws/whats-new/2017/12/aws-free-tier-usage-alerts-automatically-notify-you-when-you-are-forecasted-to-exceed-your-aws-service-usage-limits/\nOption C is INCORRECT because configuring an alarm in AWS CloudWatch that triggers after exceeding the bill will not meet the requirements of staying within the desired budget.\nThe alarm triggers when the account billing exceeds the threshold specified.\nIt triggers only when actual billing exceeds the threshold.\nIt does not use projections based on the usage so far in the month.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html\n\nThe best possible way to allay the department's concern of exceeding the stipulated monthly expenditure limit on their AWS account is by setting up a budget and creating email alerts using AWS Budgets.\nAWS Budgets is a service that enables users to set custom cost and usage budgets for their AWS resources. With AWS Budgets, users can create a budget that tracks their actual expenditure against their set limit and provides email alerts when the limit is reached.\nOption A, regularly reviewing the Billing and Cost management dashboard, can be time-consuming and may not provide immediate alerts when the limit is reached. It is a good practice to monitor the dashboard regularly, but it may not be the best solution for a department anxious about exceeding their monthly expenditure limit.\nOption B, ticking the Receive Free Tier Usage Alerts checkbox, will only provide alerts for free tier usage and will not help in tracking actual expenditure against the monthly limit.\nOption C, creating an alarm in AWS CloudWatch that triggers each time the services bill surpasses the limit, can be an effective solution, but it requires some setup and configuration. AWS Budgets, on the other hand, provides a simpler solution with email alerts and does not require much configuration.\nTherefore, option D, creating an email alert based on the budget parameters using AWS Budgets, is the best possible way to allay the department's concern about exceeding the monthly expenditure limit on their AWS account.\n\n"
}, {
  "id" : 71,
  "question" : "A company needs to know which user was responsible for terminating several critical Amazon Elastic Compute Cloud (EC2) Instances.\nWhere can the customer find this information?\n",
  "answers" : [ {
    "id" : "385f475959d6405e936d43248e59b374",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "2b5fac9d385d4c058bad73c6576d0ad5",
    "option" : "Amazon EC2 instance usage report",
    "isCorrect" : "false"
  }, {
    "id" : "11153c897e7b485ab47a15aca4cfd832",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "909c460818ed4dd18b4a5bae14092323",
    "option" : "AWS CloudTrail logs.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nUsing CloudTrail, one can monitor all the API activity conducted on all AWS services.\nThe AWS Documentation additionally mentions the following.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nCloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nThis event history simplifies security analysis, resource change tracking, and troubleshooting.\nFor more information on AWS Cloudtrail, please refer to the below URL:\nhttps://aws.amazon.com/cloudtrail/\nAnswers A, B and C are incorrect.\nCloudtrail is the most appropriate place to monitor activity in AWS.\n\nThe correct answer is D, AWS CloudTrail logs.\nAWS CloudTrail is a service that records all API calls made in your AWS account by any user, including console sign-in events, AWS Management Console actions, and AWS service API calls. CloudTrail enables you to identify which user or resource made a particular API call, when they made it, and from which IP address or source.\nIn this scenario, the company needs to know which user terminated the EC2 instances. This information is available in the AWS CloudTrail logs. By searching the logs, the company can filter for the event type \"TerminateInstances\" and find out which user initiated the action. The CloudTrail logs provide a complete audit trail of all activity within an AWS account, making it an essential tool for security, compliance, and troubleshooting.\nThe other options listed are not relevant to this scenario:\nAWS Trusted Advisor provides recommendations to optimize your AWS infrastructure, but it does not track user activity or API calls. The Amazon EC2 instance usage report provides information on usage and costs of EC2 instances, but it does not track user activity or API calls. Amazon CloudWatch is a monitoring service for AWS resources, but it does not track user activity or API calls.\n\n"
}, {
  "id" : 72,
  "question" : "I have a client who is moving their on premise workloads to AWS.\nSince they are very cost conscious, they would like to get first hand information on their expenses they will incur while using AWS services.\nWhich of the following will help them do that?\n",
  "answers" : [ {
    "id" : "e6436746793f427a96cca44709c5e0ed",
    "option" : "AWS Cost Explorer",
    "isCorrect" : "false"
  }, {
    "id" : "35552489a55b41c493ed3b8a46e35802",
    "option" : "AWS Organizations",
    "isCorrect" : "false"
  }, {
    "id" : "2cabb49fa07b449593b87117ccf89226",
    "option" : "AWS Budgets",
    "isCorrect" : "false"
  }, {
    "id" : "08e0449fa339445db0ea1091fdbe8eae",
    "option" : "AWS Pricing Calculator.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nOption A is incorrect since Cost Explorer helps users to view graph displays of cost of your billing data and analyze them &amp; get a forecast for likely spends for the next 12 months.\nThe scenario is more to do with clients getting a cost estimate of different AWS services before they move to AWS cloud.\nOption B is incorrect since AWS Organizations allows clients to consolidate multiple AWS accounts that they may own into an Organization that they can centrally control many parameters like Account billing, IAM permissions etc..AWS Organizations provides a feature called consolidated billing that provides a single bill for multiple accounts.\nOption C is incorrect since AWS Budgets helps clients to plan their service usage, service costs and get informed alerts when the costs reach a certain threshold.\nOption D is CORRECT.\nThrough AWS pricing calculator a client can estimate costs that he will incur for various AWS services that he wishes to use.\nThe pricing calculator guides the user through a set of well defined service parameters eg If S3 is planned to be used for a static website then parameters like â€œStandard storage per monthâ€; â€œPUT, COPY, LIST, POSTâ€ requests to S3 standard could be relevant for determining the cost of using S3 on a monthly basis.\nReferences:\nhttps://docs.aws.amazon.com/pricing-calculator/latest/userguide/what-is-pricing-calculator.html\nhttps://youtu.be/JWz4eCczCkQ\n\nThe answer to the question is (A) AWS Cost Explorer.\nAWS Cost Explorer is a tool that enables AWS customers to monitor, analyze, and optimize their AWS usage and spending. It provides a comprehensive view of costs and usage trends across an entire AWS account, allowing customers to identify cost drivers and cost-saving opportunities.\nWith AWS Cost Explorer, customers can visualize their AWS costs and usage data in a variety of ways, including by service, by account, by region, and by tag. This enables customers to gain insights into how much they are spending on each service and where they can optimize their usage to save money.\nIn addition, AWS Cost Explorer offers forecasting capabilities, enabling customers to project their future costs based on their historical usage patterns. This can be useful for budgeting and planning purposes.\nAWS Organizations and AWS Budgets are also useful tools for managing AWS costs, but they are not specifically designed for providing first-hand information on expenses.\nAWS Organizations is a tool for managing multiple AWS accounts, enabling customers to centralize billing and administration across their organization.\nAWS Budgets is a tool for setting custom cost and usage budgets for AWS services, notifying customers when their spending exceeds their set limits.\nAWS Pricing Calculator is a tool for estimating the cost of running AWS services based on usage and configuration. It can be helpful for getting a rough estimate of expenses, but it is not as comprehensive as AWS Cost Explorer.\nIn summary, AWS Cost Explorer is the best tool for providing first-hand information on expenses while using AWS services, as it offers a comprehensive view of costs and usage trends across an entire AWS account.\n\n"
}, {
  "id" : 73,
  "question" : "What is the value of having AWS Cloud services accessible through an Application Programming Interface (API)?\n",
  "answers" : [ {
    "id" : "091b1c2d3efa42a0be279ea28cdb4d27",
    "option" : "It allows developers to work with AWS resources programmatically",
    "isCorrect" : "true"
  }, {
    "id" : "c1f22bdd228d4b1fbab5d47a611f3424",
    "option" : "AWS resources will always be cost-optimized",
    "isCorrect" : "false"
  }, {
    "id" : "c0de11e65d3d4202989e7ac01699878a",
    "option" : "All application testing can be managed by AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "0da8046d7f544c2a9165ecf994d5596c",
    "option" : "Customerâ€“owned, onâ€“premises infrastructure becomes programmable.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nIt allows developers to easily work with the various AWS resources programmatically.\nFor more information on the various programming tools available for AWS, please refer to https://aws.amazon.com/tools/.\nOption B is incorrect.\nThe AWS API does not reduce cost.\nOption C is incorrect.\nAPI allows the customer's developers to work with resources, not AWS.\nOptions D is incorrect.\nThe AWS API only allows the customer to manage AWS resources, not on-premise.\n\nThe correct answer is A. It allows developers to work with AWS resources programmatically.\nAWS (Amazon Web Services) is a cloud computing platform that provides a wide range of services for developing, deploying, and managing applications and infrastructure in the cloud. One of the key benefits of AWS is the ability to access its services through an Application Programming Interface (API).\nAn API is a set of protocols and tools for building software applications. In the case of AWS, an API allows developers to programmatically access AWS resources such as computing power, storage, and databases, and integrate them into their own applications.\nThis provides a number of benefits for developers, including:\nFlexibility: With access to AWS services through an API, developers can create customized solutions that meet their specific needs. They can choose the services they want to use and integrate them in a way that works best for their application. Automation: APIs allow developers to automate the process of deploying and managing applications in the cloud. This can save time and reduce the risk of errors associated with manual processes. Scalability: AWS services are designed to be highly scalable, and APIs make it easy for developers to scale their applications up or down based on demand. Cost-effectiveness: APIs allow developers to pay only for the services they use, making it easier to manage costs and avoid unnecessary expenses.\nIn summary, having AWS cloud services accessible through an API allows developers to work with AWS resources programmatically, which provides a range of benefits including flexibility, automation, scalability, and cost-effectiveness.\n\n"
}, {
  "id" : 74,
  "question" : "After moving their workload to AWS eu-central-1 region, an administrator would like to configure their email server on an Amazon EC2 instance in a private subnet of the VPC which will use Amazon SES.\nWhat is the most effective setup to implement?\n",
  "answers" : [ {
    "id" : "62000fede90f47d49b544e8d8439568c",
    "option" : "Configure a VPC endpoint powered by AWS PrivateLink.",
    "isCorrect" : "true"
  }, {
    "id" : "b56b7f875bba40a9a31b9a217dd7e348",
    "option" : "Ensure that the private subnet has a route to a NAT gateway in a public subnet.",
    "isCorrect" : "false"
  }, {
    "id" : "e4aa0a6db04a40e3b2a4fb23799eae1e",
    "option" : "Configure the email server with the appropriate Amazon SES endpoint for eu-central region, email-smtp.eu-central-1.amazonaws.com.",
    "isCorrect" : "false"
  }, {
    "id" : "c58dc38b2f324fb58027d0cc54be564e",
    "option" : "Configure the email server to use a service port other than port 25 to avoid Amazon EC2 throttling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nAs of April 29, 2020, AWS announced the addition of Amazon SES as a service available over VPC endpoint powered by AWS PrivateLink.\nThis makes it possible to configure a VPC endpoint which the email server will reach within the VPC without the need for internet access.\nThis is the most effective setup to implement.\nhttps://aws.amazon.com/about-aws/whats-new/2020/04/amazon-ses-now-offers-vpc-endpoint-support-for-smtp-endpoints/\nOption B is INCORRECT because ensuring that the private subnet has a route to a NAT gateway in a public subnet is feasible but would mean that the traffic would traverse the internet to get to the Amazon SES endpoints.\nThis is not the most effective setup to implement.\nOption C is INCORRECT because configuring the email server with the appropriate Amazon SES endpoint would not work since Amazon EC2 instance is in a private subnet and does not have internet access to reach it.\nOption D is INCORRECT because configuring the email server to use a service port other than port 25 is recommended but does not address the requirement of how the email server will reach the Amazon SES endpoint.\n\nWhen an administrator wants to configure an email server on an Amazon EC2 instance in a private subnet of a VPC, there are several considerations to keep in mind.\nOption A: Configure a VPC endpoint powered by AWS PrivateLink This option involves creating a VPC endpoint powered by AWS PrivateLink, which allows the email server in the private subnet to communicate with Amazon SES without the need for an internet gateway or NAT gateway. Instead, the traffic flows over the Amazon network, which can provide improved security and reliability. This can be an effective option for organizations that require a high level of security for their email communications.\nOption B: Ensure that the private subnet has a route to a NAT gateway in a public subnet This option involves creating a NAT gateway in a public subnet and ensuring that the private subnet has a route to that gateway. This allows the email server in the private subnet to communicate with Amazon SES via the internet, using the NAT gateway as a proxy. This option can be effective for organizations that do not require the highest level of security for their email communications but still want to ensure that their traffic is encrypted and travels over a reliable connection.\nOption C: Configure the email server with the appropriate Amazon SES endpoint for the eu-central region, email-smtp.eu-central-1.amazonaws.com This option involves configuring the email server with the appropriate Amazon SES endpoint for the eu-central region. This endpoint allows the email server to communicate directly with Amazon SES over the internet. This option can be effective for organizations that do not require a high level of security for their email communications and want a simple setup.\nOption D: Configure the email server to use a service port other than port 25 to avoid Amazon EC2 throttling This option involves configuring the email server to use a service port other than port 25 to avoid Amazon EC2 throttling. This can be effective for organizations that send a large volume of email from their EC2 instances and want to avoid being throttled by Amazon. However, it may not be necessary for all organizations, and it does not address security concerns.\nIn summary, the most effective setup to implement will depend on the organization's specific needs and requirements. Option A is the most secure and reliable option, but it may not be necessary for all organizations. Option B provides a good balance between security and simplicity. Option C is the simplest option, but it may not be sufficient for organizations with high-security requirements. Option D can be effective for organizations that send a large volume of email but does not address security concerns.\n\n"
}, {
  "id" : 75,
  "question" : "A file-sharing service uses Amazon S3 to store files uploaded by users.\nFiles are accessed with random frequency.\nPopular ones are downloaded every day whilst others not so often and some rarely.\nWhat is the most cost-effective Amazon S3 object storage class to implement?\n",
  "answers" : [ {
    "id" : "1e87d3cf101a426a81c75fb6407011ad",
    "option" : "Amazon S3 Standard",
    "isCorrect" : "false"
  }, {
    "id" : "fb15ba39115040abb50faa429219f4d7",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "1bfc78961c6a4087bd33e61b6f60638a",
    "option" : "Amazon S3 One Zone-Infrequently Accessed",
    "isCorrect" : "false"
  }, {
    "id" : "d1854ac476814a85a23c71f994881cfb",
    "option" : "Amazon S3 Intelligent-Tiering.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nS3 Intelligent-Tiering is a new Amazon S3 storage class designed for customers who want to optimize storage costs automatically when data access patterns change, without performance impact or operational overhead.\nS3 Intelligent-Tiering is the first cloud object storage class that delivers automatic cost savings by moving data between two access tiers - frequent access and infrequent access - when access patterns change, and is ideal for data with unknown or changing access patterns.\nS3 Intelligent-Tiering stores objects in two access tiers: one tier optimized for frequent access and another lower-cost tier optimized for infrequent access.\nFor a small monthly monitoring and automation fee per object, S3 Intelligent-Tiering monitors access patterns and moves objects that have not been accessed for 30 consecutive days to the infrequent access tier.\nThere are no retrieval fees in S3 Intelligent-Tiering.\nIf an object in the infrequent access tier is accessed later, it is automatically moved back to the frequent access tier.\nNo additional tiering fees apply when objects are moved between access tiers within the S3 Intelligent-Tiering storage class.\nS3 Intelligent-Tiering is designed for 99.9% availability and 99.999999999% durability, and offers the same low latency and high throughput performance of S3 Standard.\nhttps://aws.amazon.com/about-aws/whats-new/2018/11/s3-intelligent-tiering/\nOption A is incorrect because Amazon S3 Standard would be an inefficient class for storing those objects that will be accessed rarely.\nOption B is incorrect because storing objects that are frequently accessed in Amazon S3 Glacier would present operational bottlenecks since these objects would not be available instantly.\nhttps://aws.amazon.com/s3/storage-classes/\nOption C is incorrect because storing those objects that are rarely accessed and those that would be accessed frequently in Amazon S3 One Zone-Infrequently Accessed would be inefficient.\n\nThe most cost-effective Amazon S3 object storage class to implement for the given scenario would be C. Amazon S3 One Zone-Infrequently Accessed.\nHere's why:\nOption A: Amazon S3 Standard is designed for frequently accessed data, and is billed based on the amount of data stored, number of requests made to the data, and the amount of data transferred out of Amazon S3. Given that the files are accessed with random frequency, some rarely, it's likely that the cost of storing these files in S3 Standard would be higher than necessary.\nOption B: Amazon S3 Glacier is designed for data archiving and long-term storage. Although it is significantly cheaper than S3 Standard, it is intended for data that is accessed infrequently and is typically retrieved over several hours. Retrieving data from Glacier can be costly and time-consuming, so it may not be the best option for a file-sharing service where files need to be accessed quickly.\nOption C: Amazon S3 One Zone-Infrequently Accessed is designed for infrequently accessed data that can be recreated if lost. It is stored in a single availability zone, which makes it less durable than S3 Standard, but also less expensive. This storage class is ideal for data that is not accessed frequently but needs to be readily available when needed. Given that the files are accessed with random frequency and some rarely, it's likely that this storage class would provide the necessary durability at a lower cost than S3 Standard.\nOption D: Amazon S3 Intelligent-Tiering is designed to automatically move objects between two access tiers based on changing access patterns. This storage class is ideal for data with unknown or changing access patterns, where the frequency of access is difficult to predict. However, given that the files in this scenario are already categorized into popular, less often accessed, and rarely accessed, it may be unnecessary to use Intelligent-Tiering to manage access to these files.\nTherefore, based on the given scenario, the most cost-effective Amazon S3 object storage class to implement would be C. Amazon S3 One Zone-Infrequently Accessed.\n\n"
}, {
  "id" : 76,
  "question" : "Which AWS service automates infrastructure provisioning and administrative tasks for an analytical data warehouse?\n",
  "answers" : [ {
    "id" : "deadc37e253746d4814713a224c7c923",
    "option" : "Amazon Redshift",
    "isCorrect" : "true"
  }, {
    "id" : "765785b32261422dab24de5ae511c0ad",
    "option" : "Amazon DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "0725c3672b55412a870331b0fe23d4cc",
    "option" : "Amazon ElastiCache",
    "isCorrect" : "false"
  }, {
    "id" : "41d7781c6c3e41b08e21dc3b23f038a6",
    "option" : "Amazon Aurora.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS documentation mentions the following:\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud.\nYou can start with just a few hundred gigabytes of data and scale to a petabyte or more.\nThis enables you to use your data to acquire new insights for your business and customers.\nFor more information on AWS Redshift, please refer to the below URL:\nhttp://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html\nChoices B, C and D are incorrect.\nAmazon Redshift is the only data warehousing service out of the choices below.\n\nThe AWS service that automates infrastructure provisioning and administrative tasks for an analytical data warehouse is Amazon Redshift.\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service in the cloud that enables customers to analyze large amounts of data using SQL and BI tools. Amazon Redshift automates much of the undifferentiated heavy lifting involved in setting up, operating, and scaling a data warehouse.\nSome of the key features of Amazon Redshift include:\nAutomatic scaling: Amazon Redshift automatically scales storage and compute resources to meet your needs. Columnar storage: Data is stored in a columnar format, which improves query performance by reducing I/O. Data compression: Amazon Redshift compresses data to reduce storage requirements and improve query performance. Encryption: Amazon Redshift encrypts data in transit and at rest to enhance security. Integration with BI tools: Amazon Redshift integrates with a wide range of BI tools, including Tableau, MicroStrategy, and Power BI.\nOverall, Amazon Redshift is an ideal solution for organizations that need to store and analyze large amounts of data, without having to manage the underlying infrastructure themselves.\n\n"
}, {
  "id" : 77,
  "question" : "An administrator would like to automate the creation of new AWS accounts for the organization's research and development department.\nNew workloads need to be spun-up promptly and categorized into groups.\nHow can this be achieved efficiently?\n",
  "answers" : [ {
    "id" : "a4547b41bdd548f8a2752afd92441fcb",
    "option" : "Use of AWS CloudFormation would be sufficient.",
    "isCorrect" : "false"
  }, {
    "id" : "6df20b35c3c1429286b6d6ee93bb60f6",
    "option" : "Use of AWS Organizations.",
    "isCorrect" : "true"
  }, {
    "id" : "06ec354cb3434bc8ab0b5951bcbb2164",
    "option" : "Using the AWS API to programmatically create each account via command line interface.",
    "isCorrect" : "false"
  }, {
    "id" : "2837c97ed3ec434cb022852a9cbbb703",
    "option" : "AWS Identity Access Management (IAM)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Organizations allows the user to automate the creation of new AWS accounts when they need to launch new workloads quickly.\nThe administrator can add these new accounts to user-defined groups in an organization for easy categorization.\nFor example, you can create separate groups to categorize development and production accounts.\nThen you can apply a Service Control Policy (SCP) to the production group allowing only access to AWS services required by production workloads.\nhttps://aws.amazon.com/organizations/\nOption A is INCORRECT because AWS CloudFormation does not aid in the automated AWS account creation.\nAWS CloudFormation provides a common language for the administrator to describe and provision all the infrastructure resources in their cloud environment.\nCloudFormation allows the administrator to use a simple text file to model and provision all the resources needed for applications across all regions and accounts.\nThis file serves as a single source of truth for your cloud environment.\nhttps://aws.amazon.com/cloudformation/\nOption C is INCORRECT because using AWS API to programmatically create each account via command-line interface is feasible but inefficient.\nThe AWS Command Line Interface (CLI) is a unified tool to manage your AWS services.\nWith just one tool to download and configure, you can control multiple AWS services from the command line and automate them through scripts.\nhttps://aws.amazon.com/cli/\nOption D is INCORRECT because using AWS Identity Access Management (IAM) to fulfill the task is inefficient and tedious.\nAWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely.\nUsing IAM, you can create and manage AWS users and groups.\nYou can use permissions to allow and deny their access to AWS resources.\nIAM is a feature of your AWS account offered at no additional charge.\nYou will be charged only for the use of other AWS services by your users.\nhttps://aws.amazon.com/iam/\n\nTo automate the creation of new AWS accounts for an organization's research and development department, the best solution is to use AWS Organizations. AWS Organizations is a service that enables centralized management of multiple AWS accounts. It allows you to create, manage, and organize multiple AWS accounts with different configurations and permissions under a single master account.\nAWS Organizations provides several benefits such as consolidated billing, cost management, and centralized management of policies across all accounts. It also enables you to create and manage accounts programmatically, which is ideal for automation.\nWith AWS Organizations, you can create accounts in bulk and apply policies to them. You can also organize accounts into groups based on their purpose, such as development, production, or testing. This makes it easier to manage and apply policies to different accounts based on their role in the organization.\nUsing AWS CloudFormation could also be a solution, but it only automates the deployment of resources within a single account. It does not provide a way to create multiple AWS accounts programmatically.\nUsing the AWS API to programmatically create each account via command line interface is also possible, but it would require more effort to manage and organize the accounts.\nAWS Identity Access Management (IAM) is a service that allows you to manage access to AWS resources. While it is important for managing access to resources within accounts, it is not a solution for automating the creation of new accounts.\nIn summary, the best solution for automating the creation of new AWS accounts for an organization's research and development department is to use AWS Organizations. It provides centralized management, organization, and policy application for multiple AWS accounts.\n\n"
}, {
  "id" : 78,
  "question" : "An organization utilizes a software suite that consists of a multitude of underlying microservices hosted on the cloud.\nThe application is frequently giving runtime errors.\nWhich service will help in the troubleshooting process?\n",
  "answers" : [ {
    "id" : "6b646249359e46dcbc611a6581e8872a",
    "option" : "AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "ab3a7a2e8242448eaf860b7aefe04c3a",
    "option" : "AWS CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "6fb86c03f9fb4db7b896d8fee21baa1f",
    "option" : "AWS X-Ray",
    "isCorrect" : "true"
  }, {
    "id" : "4c79c52839af4e09997042d04ed4ce56",
    "option" : "Amazon OpenSearch Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS X-Ray is a service that collects data about requests that your application serves and provides tools that you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.\nAWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.\nhttps://aws.amazon.com/xray/\nOption A is INCORRECT because AWS CloudTrail primarily records user or API activity, â€˜who has done what.' It logs, continuously monitors, and retains account activity related to actions across AWS infrastructure.\nCloudTrail provides event history in the AWS account activity but NOT that of the interaction of software microservices within a suite.\nhttps://aws.amazon.com/cloudtrail/\nOption B is INCORRECT because AWS CloudWatch does the primary function of monitoring and NOT debugging.\nIt collates data and actionable insights to monitor applications.\nIt also responds to system-wide performance changes, optimizes resource utilization, and gets a unified view of operational health.\nHowever, the service does neither debug nor logs errors that occur amongst software microservices within a suite.\nhttps://aws.amazon.com/cloudwatch/\nOption D is INCORRECT because Amazon OpenSearch Service is a managed service that makes it easy to deploy, operate, and scale OpenSearch clusters in the AWS Cloud.\nIt automatically detects and replaces failed OpenSearch Service nodes, reducing the overhead associated with self-managed infrastructures.\nhttps://docs.aws.amazon.com/opensearch-service/latest/developerguide/what-is.html\n\nOut of the given options, AWS X-Ray is the most suitable service for troubleshooting a microservices-based application that frequently gives runtime errors.\nAWS X-Ray is a service that helps developers analyze and debug distributed applications, such as those that are built using microservices architecture. It allows developers to trace requests made to microservices and helps identify the cause of errors or performance issues.\nAWS CloudTrail is a service that provides governance, compliance, and operational auditing of AWS account activity. It logs API calls and other AWS account activity for auditing and compliance purposes. While it may be useful in identifying changes made to the system that could have caused the runtime errors, it is not a direct troubleshooting tool.\nAWS CloudWatch is a monitoring service that provides operational insights for resources and applications running on AWS. It can collect and track metrics, collect and monitor log files, and set alarms. While it can help identify issues with the resources used by the microservices, it is not a direct tool for troubleshooting the application.\nAmazon OpenSearch Service is a managed search and analytics service that provides a distributed search engine based on the open-source Elasticsearch software. While it can be used for monitoring and analyzing logs, it is not specifically designed for troubleshooting microservices-based applications.\nTherefore, the correct answer to the given question is option C - AWS X-Ray.\n\n"
}, {
  "id" : 79,
  "question" : "An administrator would like to automate the replication and deployment of a specific software configuration existent on one EC2 instance onto four hundred others.\nWhich AWS service is BEST suited for this implementation?\n",
  "answers" : [ {
    "id" : "174ba4b3b505491aa3a019af16284a50",
    "option" : "AWS OpsWorks",
    "isCorrect" : "true"
  }, {
    "id" : "91d0d3a356fd42ba9ea7499ad0048426",
    "option" : "AWS Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "c5d3b1005cdf4aa79d79dfa2a0ef3f9b",
    "option" : "AWS Launch Configuration",
    "isCorrect" : "false"
  }, {
    "id" : "97a87a797d7b4db0a21af858a4d2e282",
    "option" : "AWS Auto-scaling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS OpsWorks provides a fully managed configuration automation and management service of Chef and Puppet.\nThese platforms will allow for the use of code to automate the configurations of EC2 instances, including replication, as stated in the scenario.\nWith Chef and Puppet, OpsWorks will allow for the automation of how servers are configured, deployed and managed across Amazon EC2 instances or on-premises compute environments.\nOption B is INCORRECT because AWS Elastic Beanstalk is a service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nIt will not replicate a specific software service configuration onto a multitude of EC2 instances autonomously.\nhttps://aws.amazon.com/elasticbeanstalk/\nOption C is INCORRECT because a Launch Configuration is primarily an instance configuration template that an Auto Scaling group uses to launch EC2 instances.\nIt is the blueprint of the Auto Scaling group.\nIt also determines the configuration output of each instance.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html\nOption D is INCORRECT because Auto-scaling is responsive to preset threshold levels in a deployment environment.\nIt does not offer a fully managed functionality that allows for the mass replication of a specific configuration, as the scenario outlines.\nhttps://aws.amazon.com/autoscaling/\n\nThe BEST AWS service for automating the replication and deployment of software configuration from one EC2 instance to 400 others would be AWS OpsWorks.\nAWS OpsWorks is a configuration management service that automates the deployment of applications and software configurations to EC2 instances. It provides a flexible and scalable way to manage the infrastructure and software on EC2 instances.\nHere are some reasons why AWS OpsWorks is the best fit for this scenario:\nIt provides centralized management: AWS OpsWorks allows the administrator to manage the software configuration of all 400 EC2 instances from a single location. This centralized management reduces the complexity of managing large-scale deployments. It supports automation: AWS OpsWorks supports automation of software configuration management. The administrator can create custom recipes using Chef or Puppet, which can then be used to deploy software configurations to multiple EC2 instances. It supports multiple platforms: AWS OpsWorks supports a variety of platforms, including Windows and Linux, and provides a consistent way to manage software configuration across all platforms. It provides scalability: AWS OpsWorks can scale up or down based on the number of EC2 instances. This means that the administrator can easily manage software configurations on any number of EC2 instances, from a few to thousands.\nAWS Beanstalk, AWS Launch Configuration, and AWS Auto-scaling are also valuable services for managing EC2 instances, but they are not specifically designed for managing software configurations. AWS Beanstalk is a platform for deploying and scaling web applications. AWS Launch Configuration provides a way to configure the launch settings for EC2 instances, while AWS Auto-scaling helps to automatically scale the EC2 instances up or down based on traffic or other factors.\nIn summary, AWS OpsWorks is the best choice for automating the replication and deployment of software configurations on 400 EC2 instances due to its centralized management, automation capabilities, support for multiple platforms, and scalability.\n\n"
}, {
  "id" : 80,
  "question" : "Which AWS service can be deployed to enhance read performance for applications while reading data from NoSQL database?\n",
  "answers" : [ {
    "id" : "59b78dfe6bff473ba4084f2ed8806e87",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "8628e796aa0648d4aaf56a2ee51fcf59",
    "option" : "Amazon DynamoDB Accelerator",
    "isCorrect" : "true"
  }, {
    "id" : "3fe4d5e0167a4232b84cb7b4ece49035",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "5364dd68174e4cd98b3ffd63265092cb",
    "option" : "AWS Greengrass.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon DynamoDB Accelerator (DAX) is a caching service for DynamoDB which can be deployed in VPC in a region where DynamoDB is deployed.\nFor read-heavy applications, DAX can be deployed to increase throughput by providing in-memory caching.\nOption A is incorrect because Amazon Route 53 is an AWS DNS service and cannot improve the performance of DynamoDB.Option C is incorrect because Amazon CloudFront is a global content delivery network that cannot be applied to a DynamoDB table.\nOption D is incorrect because AWS Greengrass is data caching software for connected devices.\nFor more information on caching solutions with AWS, refer to the following URL:\nhttps://aws.amazon.com/caching/aws-caching/\n\nThe AWS service that can be deployed to enhance read performance for applications while reading data from NoSQL database is Amazon DynamoDB Accelerator (DAX). Therefore, the correct answer is B.\nAmazon DynamoDB is a fully-managed NoSQL database service that delivers single-digit millisecond performance at any scale. It is designed to handle large amounts of structured and unstructured data and supports both document and key-value data models. However, even though DynamoDB offers high performance, some applications may still require faster access to frequently accessed data.\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available, and in-memory cache for DynamoDB that improves the performance of read-intensive workloads. It can reduce the response time for read requests from milliseconds to microseconds, even at millions of requests per second.\nDAX is a write-through caching service, meaning that it caches data from DynamoDB in its in-memory cache. When an application requests data from DynamoDB, DAX first checks if the data is available in its cache. If the data is available in the cache, DAX returns it to the application without accessing DynamoDB. If the data is not available in the cache, DAX retrieves it from DynamoDB, caches it in memory, and returns it to the application.\nBy using DAX, applications can significantly improve the read performance of their NoSQL database without requiring any changes to the application logic. DAX is compatible with existing DynamoDB API calls, and applications can use DAX with existing DynamoDB tables and indexes.\nTherefore, Amazon DynamoDB Accelerator (DAX) is the AWS service that can be deployed to enhance read performance for applications while reading data from NoSQL database.\n\n"
}, {
  "id" : 81,
  "question" : "Which AWS service can be used to detect &amp; analyse performance issues related to AWS Lambda applications?\n",
  "answers" : [ {
    "id" : "35a63f47a5ac4b9cb4aa1e59d6ad77bd",
    "option" : "AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "a65d9cc435e949d28cbf0f2b2b989695",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "f10050eed70643b1a3d00eeabed8ceda",
    "option" : "AWS X-Ray",
    "isCorrect" : "true"
  }, {
    "id" : "3a1e2a9455404d0dab2ae0d2893252f3",
    "option" : "AWS Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS X-Ray can be used to detect performance issues for AWS Lambda applications.\nAWS Lambda sends traces to X-Ray which is further analysed to generate a performance report.\nOption A is incorrect because AWS CloudTrail will capture API calls made by AWS Lambda.\nOption B is incorrect because Amazon CloudWatch will track the number of requests, execution time request &amp; error generated, but it won't help in analysing end-to-end application performance.\nOption D is incorrect because AWS Config will not help to detect performance issues with AWS Lambda.\nAWS Config can audit configuration of AWS resource.\nFor more information on debugging AWS Lambda application performance, refer to the following URL:\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-monitoring.html\n\nThe correct answer is C. AWS X-Ray.\nAWS Lambda is a serverless computing service that enables developers to run code without provisioning or managing servers. When an AWS Lambda function is invoked, it starts a new instance of the function to handle the request. However, as the number of function invocations grows, it can become difficult to track and analyze the performance of the application. This is where AWS X-Ray comes into play.\nAWS X-Ray is a service that helps developers analyze and debug distributed applications, including those based on microservices and serverless architectures. It provides a visual representation of the application's architecture, showing how requests flow through the different components of the application. With X-Ray, developers can identify the root cause of performance issues and errors, and improve the overall performance of their applications.\nIn the case of AWS Lambda applications, X-Ray can be used to trace requests as they flow through the different components of the application. It provides detailed information about the duration of each function invocation, as well as any errors or exceptions that occurred during the execution. This allows developers to identify and troubleshoot performance issues, optimize the application's architecture, and improve the end-user experience.\nIn contrast, AWS CloudTrail is a service that provides a record of actions taken by a user, role, or an AWS service in a specific AWS account. It is primarily used for auditing and compliance purposes, rather than performance analysis.\nAmazon CloudWatch is a monitoring and management service that provides data and actionable insights for AWS resources. It can be used to monitor AWS Lambda functions, but it does not provide the detailed performance analysis capabilities of AWS X-Ray.\nAWS Config is a service that provides a detailed inventory of the AWS resources in an account, as well as a history of configuration changes. It is primarily used for compliance and governance purposes, rather than performance analysis.\n\n"
}, {
  "id" : 82,
  "question" : "A cloud solutions architect needs to execute urgent mission-critical tasks on the AWS Management console.\nBut he has left his Windows-based machine at home.\nGiven that only Non-Graphical User Interface (non-GUI), Linux-based machines are currently available, what would be the most secure option to administer these tasks on the cloud infrastructure?\n",
  "answers" : [ {
    "id" : "ce01c9f7202c48eb9ad8b03626273afe",
    "option" : "Share the AWS Management Console credentials with the person at home over the phone to execute the tasks on his behalf.",
    "isCorrect" : "false"
  }, {
    "id" : "b77a11671d0c44a09c6fcd1ad384aa9e",
    "option" : "Use third-party remote desktop software to access the Windows-based machine at home from the non-GUI workstations and administer the required tasks.",
    "isCorrect" : "false"
  }, {
    "id" : "42a6986ca342456e9ad66a13af15d1ae",
    "option" : "Use Secure Shell (SSH) to securely connect to the Windows-based machine from one of the non-GUI Linux-based machines and log onto the AWS Management console.",
    "isCorrect" : "false"
  }, {
    "id" : "14c1311ed885449f8a227990782941e5",
    "option" : "Install and run AWS CLI on one of the non-GUI Linux-based machines, in a shell environment such as bash. The cloud solutions architect will be able to access ALL services just as they can also be accessed from a Windows-based machine.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Command Line Interface (AWS CLI) is an open-source tool that enables access and interaction with AWS services using commands in the command-line shell.\nWith minimal configuration, the cloud solutions architect would start using the functionality equivalent to that provided by the browser-based AWS Management Console from the command prompt in a terminal program such as bash.\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html\nOption A is INCORRECT because sharing AWS Management console credentials is a bad-practice and poses a high-security risk.\nhttps://aws.amazon.com/iam/details/managing-user-credentials/\nOption B is INCORRECT because accessing the AWS Management console via third-party remote desktop software is insecure since the remote machine can be compromised.\nOption C is INCORRECT because it is rather cumbersome in comparison.\nThe secure option is the direct access method of AWS CLI.\n\nThe most secure option for administering urgent mission-critical tasks on the AWS Management console using non-GUI Linux-based machines would be to use Secure Shell (SSH) to securely connect to the Windows-based machine from one of the non-GUI Linux-based machines and log onto the AWS Management console (option C).\nOption A, sharing the AWS Management Console credentials over the phone is not a recommended practice as it could lead to a security breach, especially when it comes to critical tasks. Moreover, it violates the AWS security best practices and policies.\nOption B, using third-party remote desktop software to access the Windows-based machine at home from the non-GUI workstations might not be a secure option because it might involve exposing the Windows machine to the internet, which could lead to potential security risks. Additionally, it also requires a third-party software that might introduce additional vulnerabilities.\nOption D, installing and running AWS CLI on one of the non-GUI Linux-based machines might not provide the same level of access to all AWS services as they can be accessed from a Windows-based machine. AWS CLI provides a command-line interface to access various AWS services but it may not offer the same level of usability as the AWS Management Console.\nIn conclusion, using SSH to securely connect to the Windows-based machine from one of the non-GUI Linux-based machines and logging onto the AWS Management console is the most secure and recommended option for administering urgent mission-critical tasks on the AWS infrastructure.\n\n"
}, {
  "id" : 83,
  "question" : "According to the AWS, what is the benefit of Elasticity?\n",
  "answers" : [ {
    "id" : "cd0fe1cf8bd0406daa8f24e20f11e42d",
    "option" : "Minimize storage requirements by reducing logging and auditing activities",
    "isCorrect" : "false"
  }, {
    "id" : "70d4160e490e42729c0ae00d563e200a",
    "option" : "Create systems that scale to the required capacity based on changes in demand",
    "isCorrect" : "true"
  }, {
    "id" : "e94ec22bf04e4727bb093cd65c7be2c8",
    "option" : "Enable AWS to automatically select the most cost-effective services.",
    "isCorrect" : "false"
  }, {
    "id" : "a5920e20bee14730ba8d4c9a8fb45933",
    "option" : "Accelerate the design process because recovery from failure is automated, reducing the need for testing.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe concept of Elasticity is the means of an application having the ability to scale up and scale down based on demand.\nAn example of such a service is the Autoscaling service.\nFor more information on AWS Autoscaling service, please refer to the below URL:\nhttps://aws.amazon.com/autoscaling/\nA, C and D are incorrect.\nElasticity will not have positive effects on storage, cost or design agility.\n\nThe correct answer is B: Create systems that scale to the required capacity based on changes in demand.\nElasticity is a fundamental characteristic of cloud computing that enables resources to be automatically provisioned and de-provisioned based on changes in demand. The benefit of elasticity is that it allows systems to automatically scale up or down to meet changing workloads, ensuring that there is always enough capacity to handle the workload, while minimizing costs.\nIn practical terms, elasticity means that if an application experiences a sudden surge in traffic, additional resources (such as compute, storage, and network capacity) can be automatically provisioned to handle the increased load. Conversely, when traffic decreases, resources can be automatically de-provisioned, reducing costs.\nElasticity is a key advantage of cloud computing over traditional on-premises infrastructure, where capacity is typically fixed and scaling requires significant manual effort. With cloud computing, customers can provision the exact amount of resources they need, when they need them, and only pay for what they use.\nIn summary, elasticity is a critical feature of cloud computing that allows systems to automatically scale up or down based on changes in demand, ensuring that there is always enough capacity to handle the workload while minimizing costs.\n\n"
}, {
  "id" : 84,
  "question" : "For which of the following scenarios are the Amazon Elastic Compute Cloud (Amazon EC2) Spot instances most appropriate?\n",
  "answers" : [ {
    "id" : "f337ee9f8d664c519f6c858957967059",
    "option" : "Workloads that are only run in the morning and stopped at night",
    "isCorrect" : "false"
  }, {
    "id" : "d5465009ddd44c6b8e1171476b2aae9d",
    "option" : "Workloads where the availability of the Amazon EC2 instances can be flexible",
    "isCorrect" : "true"
  }, {
    "id" : "1ce5f66c94af4d1eaece712b84a84c32",
    "option" : "Workloads that need to run for long periods of time without interruption",
    "isCorrect" : "false"
  }, {
    "id" : "2241d0def8884b46b0f6e78ab5bd3d25",
    "option" : "Workloads that are critical and need Amazon EC2 instances with termination protection.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS documentation mentions the following.\nSpot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.\nFor example, Spot Instances are well-suited for data analysis, batch jobs, background processing, and optional tasks.\nFor more information on AWS Spot Instances, please refer to the below URL:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\nOptions A, C, and D are incorrect.\nSince spot instances can be terminated by Amazon depending on market prices, they cannot be guaranteed to run during a specific period of time.\nIt will impact the workloads especially when they are critical.\n\nAmazon Elastic Compute Cloud (Amazon EC2) Spot instances are suitable for workloads where the availability of instances can be flexible. This makes option B the correct answer. Here's why:\nWhen using Spot instances, customers can bid on unused EC2 instances, which can result in significant savings (up to 90% off the On-Demand prices). However, because these instances are reclaimed by AWS when the spot price exceeds the customer's bid, they are not recommended for workloads that need to run for long periods of time without interruption, such as critical applications or services.\nSpot instances are ideal for workloads with flexible start and end times or that can be interrupted without any significant impact. For example, a batch processing job that can be divided into smaller tasks and run in parallel, or a workload that can automatically stop and start again without losing progress.\nIn contrast, Option A is not a suitable use case for Spot instances because the customer would be charged for the entire hour even if the instance is only used for a few minutes. Option C is also not a suitable use case for Spot instances because, as mentioned above, the instances may be reclaimed by AWS and stop running at any time, which could cause long-running workloads to fail.\nOption D is incorrect because Spot instances do not support termination protection, which means that AWS can reclaim the instances at any time regardless of whether termination protection is enabled.\nTherefore, Amazon EC2 Spot instances are most appropriate for workloads where the availability of instances can be flexible.\n\n"
}, {
  "id" : 85,
  "question" : "A financial company with many resources running on AWS would like a machine learning-driven and proactive security solution that would promptly identify security vulnerabilities, particularly flagging suspicious or abnormal data patterns or activity between AWS services.\nWhich AWS service would best meet this requirement?\n",
  "answers" : [ {
    "id" : "7ee7c19d39954c9fb59b6d9f12502aa4",
    "option" : "AWS Detective",
    "isCorrect" : "true"
  }, {
    "id" : "fdb85b9ddeb146b49a0cf5a0233e5c4c",
    "option" : "AWS Macie",
    "isCorrect" : "false"
  }, {
    "id" : "fdcb2e5450204959ac73e2cc774ba094",
    "option" : "AWS Shield",
    "isCorrect" : "false"
  }, {
    "id" : "d31e00c9b8334147a9b667d80e0890fe",
    "option" : "Amazon CloudWatch Anomaly Detection.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nAWS Detective is a persistent machine learning-driven service that automatically collates log data from all AWS resources.\nThis log data is then applied into machine learning algorithms to derive data patterns between AWS services and resources, graph theory and statistical analysis.\nThis information allows the user to proactively visualize their AWS environment from a security standpoint, thereby allowing them to quickly and efficiently conduct security investigations when they occur.\nhttps://docs.aws.amazon.com/detective/latest/adminguide/what-is-detective.html\nOption B is INCORRECT because AWS Macie primarily matches and discovers sensitive data such as personally identifiable information (PII) but does not have the capability to keep track of data behaviors between AWS services to detect anomalies.\nTherefore the service does not meet the requirement.\nOption C is INCORRECT because AWS Shield is a Distributed Denial of Service (DDoS) protection service that applies to applications running in the AWS environment.\nThe service does not have machine learning capability to keep track of data behaviors between AWS services.\nOption D is INCORRECT because Amazon CloudWatch Anomaly Detection is a machine learning feature limited to Amazon CloudWatch metrics.\nIt does not extend to all the AWS services, so it does not meet the requirement.\n\nThe financial company in this scenario requires a proactive security solution that can identify security vulnerabilities and flag suspicious or abnormal data patterns or activity between AWS services. Among the four options provided, the AWS service that best meets this requirement is AWS Macie.\nAWS Detective is a security service that automatically analyzes and investigates security issues across AWS services. It uses machine learning and statistical analysis to identify and visualize security issues, such as network traffic anomalies and potential data breaches. However, it does not specifically focus on identifying abnormal data patterns or activity between AWS services.\nAWS Shield is a managed DDoS (Distributed Denial of Service) protection service that safeguards web applications running on AWS against DDoS attacks. It does not address the specific requirements of the financial company in this scenario.\nAmazon CloudWatch Anomaly Detection is a machine learning-powered service that detects anomalies in metrics and logs across AWS resources. It automatically identifies unexpected patterns in data and alerts users to potential issues. However, it is not specifically designed to identify abnormal data patterns or activity between AWS services.\nAWS Macie, on the other hand, is a fully-managed data security and privacy service that uses machine learning and pattern matching to discover and classify sensitive data stored within AWS. It can also identify suspicious or abnormal data access activity, such as attempts to access sensitive data from an unusual location or at an unusual time. Additionally, it can help the financial company comply with data privacy regulations, such as GDPR and HIPAA, by automatically identifying sensitive data that requires additional security measures.\nIn summary, AWS Macie is the best option for the financial company's specific requirements as it provides a machine learning-driven and proactive security solution that can identify security vulnerabilities, flag suspicious or abnormal data patterns, and ensure compliance with data privacy regulations.\n\n"
}, {
  "id" : 86,
  "question" : "Which tool can you use to forecast your AWS spending?\n",
  "answers" : [ {
    "id" : "9592388adb54439a9873f957b2096f5d",
    "option" : "AWS Organizations",
    "isCorrect" : "false"
  }, {
    "id" : "ac27a3686ea949bb8ba990bffdf207e0",
    "option" : "Amazon Dev Pay",
    "isCorrect" : "false"
  }, {
    "id" : "bd7081c442ad4dbc81fdd144891d9805",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "b47d5e2e36a2466c83f8c2be423b1855",
    "option" : "AWS Cost Explorer.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following.\nCost Explorer is a free tool that you can use to view your costs.\nYou can view data up to the last 12 months.\nYou can forecast how much you are likely to spend for the next 12 months and get recommendations for what Reserved Instances to purchase.\nYou can use Cost Explorer to see patterns in how much you spend on AWS resources over time, identify areas that need further inquiry, and see trends that you can use to understand your costs.\nYou also can specify time ranges for the data and view time data by day or by month.\nFor more information on the AWS Cost Explorer, please refer to the below URL:\nhttp://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-explorer-what-is.html\nA, B and C are incorrect.\nThese services do not relate to billing and cost.\n\nThe correct answer is D. AWS Cost Explorer.\nAWS Cost Explorer is a tool that allows you to visualize, understand, and manage your AWS costs and usage over time. It provides a range of features and tools that help you optimize your usage and identify potential cost savings. One of these features is the ability to forecast your AWS spending.\nWith AWS Cost Explorer, you can forecast your AWS spending based on historical data and usage patterns. This can help you to better plan your budget and ensure that you are allocating your resources effectively.\nTo use AWS Cost Explorer to forecast your spending, you need to first gather and analyze your historical data. You can do this by using the Cost and Usage Reports (CURs) or by setting up billing alerts and using the AWS Cost and Usage API.\nOnce you have your historical data, you can use the Cost Explorer console to create custom reports and visualizations that show you how your spending has changed over time. You can also use the forecasting tool to project your spending into the future based on your historical usage patterns.\nOverall, AWS Cost Explorer is a powerful tool that can help you to optimize your AWS usage and control your costs. It provides a range of features and tools that make it easy to visualize, understand, and forecast your AWS spending, so that you can make informed decisions and allocate your resources more effectively.\n\n"
}, {
  "id" : 87,
  "question" : "Which of the following is an optional Security layer attached to a subnet within a VPC for controlling traffic in &amp; out of the VPC?\n",
  "answers" : [ {
    "id" : "137dd869213d467f9cf741b47192c1a8",
    "option" : "VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "2ae0d90e631240479f3e82d7915f1b49",
    "option" : "Web Application Firewall",
    "isCorrect" : "false"
  }, {
    "id" : "1f00afc4efbe45138912483b48175983",
    "option" : "Security Group",
    "isCorrect" : "false"
  }, {
    "id" : "bc2e17ff21e04dda88175dfc70c98b09",
    "option" : "Network ACL.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nNetwork ACL can be additionally configured on subnet level to control traffic in &amp; out of the VPC.Option A is incorrect.\nVPC Flow Logs will capture information about IP traffic in &amp; out of VPC.\nThis will not be used for controlling purposes.\nOption B is incorrect.\nWeb Application Firewall (WAF) can be configured to protect web applications from common security threats.\nIt can be deployed on devices such as Amazon CloudFront, Application Load Balancer and Amazon API Gateway.\nOption C is incorrect.\nSecurity Groups are attached at instance level &amp; not at the subnet level.\nFor more information on security within VPC, refer to the following URL:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html#VPC_Security_Comparison\n\nThe optional Security layer attached to a subnet within a VPC for controlling traffic in & out of the VPC is the Network Access Control List (NACL).\nA Security Group is also used to control traffic in and out of the VPC, but it is attached to an instance level, not at the subnet level. A Web Application Firewall is used to protect web applications from common web exploits, such as SQL injection attacks and cross-site scripting attacks. VPC Flow Logs is used for capturing information about the IP traffic going to and from network interfaces in the VPC.\nThe Network ACL, on the other hand, is a stateless firewall that is attached to a subnet within a VPC. It is used to control inbound and outbound traffic at the subnet level. The rules in an NACL can allow or deny traffic based on the source and destination IP address, port number, and protocol. An NACL can be used to create more granular control over traffic flow in and out of a subnet than what is possible with Security Groups.\nIt is important to note that an NACL and a Security Group are two different things and can be used together to provide a multi-layered security approach to the VPC. While Security Groups control traffic at the instance level, an NACL provides a higher level of control over traffic flow at the subnet level.\n\n"
}, {
  "id" : 88,
  "question" : "A radio station compiles a list of the most popular songs each year.\nThe songs are frequently fetched within 180 days.\nAfter that, the users will have a default retrieval time of 12 hours for downloading the files.\nThe files should be stored for over 10 years.\nWhich is the most cost-effective object storage after 180 days?\n",
  "answers" : [ {
    "id" : "27be0ed6f5c94d4f96bdcf0e2f646a5f",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "139400c263e94ca09e1d1d3f105dfaeb",
    "option" : "Amazon S3 One Zone - Infrequently Accessed",
    "isCorrect" : "false"
  }, {
    "id" : "e27f784d83934235ae138152cb89af35",
    "option" : "Amazon S3 Glacier Deep Archive",
    "isCorrect" : "true"
  }, {
    "id" : "503189d3d58843358ac61763e701196b",
    "option" : "Amazon S3 Standard - Infrequently Accessed.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon S3 Glacier Deep Archive is the most cost-effective object storage to implement because the information will be rarely accessed and when it is accessed, its retrieval period will not be instant.\nhttps://aws.amazon.com/s3/faqs/#Amazon_S3_Glacier_Deep_Archive\nOption A is incorrect because the information might not be referred to again after it was created.\nAmazon S3 Glacier is appropriate to a certain degree but not the most cost-effective option.\nOption B is incorrect because Amazon S3 One Zone - Infrequently Accessed is suitable for information that warrants a short retrieval time.\nIn this scenario, a short retrieval time is not critical.\nOption D is incorrect because Amazon S3 Standard - Infrequently Accessed is not a cost-effective option since the list of songs will only be relevant once and then rarely accessed thereafter.\n\nThe most cost-effective object storage for the radio station's files after 180 days would be Amazon S3 Glacier.\nAmazon S3 Glacier is an object storage service that is designed for data archiving and long-term storage. It is ideal for data that is infrequently accessed and has a retrieval time of several hours. The cost of storing data in Amazon S3 Glacier is lower than other storage options, such as Amazon S3 Standard or Amazon S3 One Zone - Infrequently Accessed.\nIn this scenario, the radio station frequently fetches the popular songs within 180 days, which means that the files are being accessed frequently during this time. After 180 days, the users have a default retrieval time of 12 hours, which means that the files are not being accessed frequently anymore. Therefore, it makes sense to store these files in Amazon S3 Glacier, as it is designed for infrequently accessed data.\nAmazon S3 One Zone - Infrequently Accessed is another storage option that is cheaper than Amazon S3 Standard. However, it is stored in a single availability zone, which means that if the availability zone goes down, the data is lost. This is not a desirable option for long-term storage of important files.\nAmazon S3 Glacier Deep Archive is an even cheaper storage option, but it has a retrieval time of up to 12 hours. This would not be suitable for the radio station's needs as they require a default retrieval time of 12 hours or less.\nTherefore, the most cost-effective object storage option for the radio station's files after 180 days would be Amazon S3 Glacier.\n\n"
}, {
  "id" : 89,
  "question" : "Which of the following is a customer responsibility under AWS Shared Responsibility Model?\n",
  "answers" : [ {
    "id" : "8359e48aded74616931e0e697fc06bcd",
    "option" : "Patching of host OS deployed on Amazon S3.",
    "isCorrect" : "false"
  }, {
    "id" : "5c371663134c43218097ba08a5adb0e5",
    "option" : "Logical Access controls for underlying infrastructure.",
    "isCorrect" : "false"
  }, {
    "id" : "84a8fea7ddd641af8c79ba1f777e4f5c",
    "option" : "Physical security of the facilities.",
    "isCorrect" : "false"
  }, {
    "id" : "5a7a254904db4984aa34ee7eab1ba14f",
    "option" : "Patching of guest OS deployed on Amazon EC2 instance.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nUnder the AWS shared responsibility model, AWS takes care of infrastructure configuration &amp; management while customers must take care of the resources they launched within AWS.\nOption A is incorrect.\nAmazon S3 is part of the infrastructure layer &amp; Patching of host OS/Configuration for Amazon S3 is responsibility of AWS.\nOption B is incorrect.\nAWS has the responsibility for the Logical Access controls for the underlying infrastructure.\nOption C is incorrect.\nPhysical Security of the facilities is AWS responsibility.\nFor more information on Shared responsibility model, refer to the following URL:\nhttps://aws.amazon.com/compliance/shared-responsibility-model\n\nThe AWS Shared Responsibility Model defines the division of security and compliance responsibilities between AWS and its customers. AWS is responsible for security \"of\" the cloud, meaning the underlying infrastructure and services, while customers are responsible for security \"in\" the cloud, meaning their applications and data running on top of AWS services.\nOut of the options given, the customer responsibility under the AWS Shared Responsibility Model is to patch the guest OS deployed on an Amazon EC2 instance. Therefore, the correct answer is option D.\nExplanation of each option:\nA. Patching of host OS deployed on Amazon S3: Amazon S3 is a storage service that provides object storage in the cloud. It does not support the deployment of host OS, so patching of host OS is not a customer responsibility for Amazon S3.\nB. Logical Access controls for underlying infrastructure: The underlying infrastructure of AWS, such as network and storage, is AWS's responsibility. AWS provides customers with access controls to secure their data, but managing access controls is a shared responsibility.\nC. Physical security of the facilities: AWS is responsible for the physical security of its facilities, including the data centers and server rooms where its services are hosted.\nD. Patching of guest OS deployed on Amazon EC2 instance: Amazon EC2 is a web service that provides resizable compute capacity in the cloud. Customers are responsible for patching the guest operating system running on an EC2 instance to ensure that it is up to date with the latest security patches.\nIn summary, customers are responsible for managing the security and compliance of their applications and data running on top of AWS services. AWS is responsible for the security and compliance of the underlying infrastructure and services.\n\n"
}, {
  "id" : 90,
  "question" : "Which of the following is a factor when calculating Total Cost of Ownership (TCO) for the AWS Cloud?\n",
  "answers" : [ {
    "id" : "0b1198d968914e94bc1d66439893aaff",
    "option" : "The number of servers migrated to AWS",
    "isCorrect" : "true"
  }, {
    "id" : "0d94cfd9bb224f3ba85642fa1e8e5483",
    "option" : "The number of users migrated to AWS",
    "isCorrect" : "false"
  }, {
    "id" : "6ae0be2db9714342a69383f028c52127",
    "option" : "The number of passwords migrated to AWS",
    "isCorrect" : "false"
  }, {
    "id" : "6c4b736704d143469764f7f08216f448",
    "option" : "The number of keys migrated to AWS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nRunning servers will incur costs.\nThe number of running servers is one factor of Server Costs- a key component of AWS's Total Cost of Ownership (TCO).\nTo estimate the cost for your AWS architecture solution, please refer to the below URL-\nhttps://calculator.aws/#/.\nB, C, and D are incorrect.\nThese are not factors in AWS's Total Cost of Ownership.\n\nWhen calculating the Total Cost of Ownership (TCO) for the AWS Cloud, the factor that is important to consider is the number of servers migrated to AWS.\nA) The number of servers migrated to AWS: This is an important factor in TCO calculation as it determines the size and complexity of the AWS infrastructure needed to support the application or workload. Factors such as server type, storage, networking, and other services required to support the application will all contribute to the cost of running the application in the cloud.\nB) The number of users migrated to AWS: While the number of users can impact the size and complexity of the infrastructure required to support the application, it is not a significant factor in the overall TCO calculation.\nC) The number of passwords migrated to AWS: This factor is not relevant to TCO calculation, as passwords are not a significant cost driver in running an application in the cloud.\nD) The number of keys migrated to AWS: While encryption keys are important for securing data in the cloud, the number of keys migrated is not a significant factor in the overall TCO calculation.\nIn summary, the number of servers migrated to AWS is the factor that is important to consider when calculating TCO for the AWS Cloud.\n\n"
}, {
  "id" : 91,
  "question" : "A group of developers for a startup company store their source code and binary files on a shared open-source repository platform which is publicly accessible over the internet.\nThey have embarked on a new project in which their client requires high confidentiality and security on all development assets.\nWhich AWS service can the developers use to store the source code?\n",
  "answers" : [ {
    "id" : "d5da6b11a99443e69ba11877c2aaa81a",
    "option" : "AWS CodeCommit",
    "isCorrect" : "true"
  }, {
    "id" : "16a31042902e436b93a383bc4b7c0f98",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "false"
  }, {
    "id" : "988a1dd88b1c4a31931da3138dda4df1",
    "option" : "AWS Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "e090744fa89440b286a7eafab40622bd",
    "option" : "AWS CodeStar.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS CodeCommit is a managed source control service.\nIt can be used as a data store to store source code, binaries, scripts, HTML pages and images which are accessible over the internet.\nCodeCommit encrypts files in transit and at rest, which fulfills the additional client requirement (high confidentiality &amp; security) mentioned in the question.\nAlso, CodeCommit works well with Git tools and other existing CI/CD tools.\nhttps://aws.amazon.com/codecommit/\nOption B is INCORRECT because AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\nOption C is INCORRECT because AWS Lambda will allow the developers in the scenario to run code without provisioning or managing servers.\nThe company would pay only for the compute time consumed.\nThere would be no charge when your code is not running.\nhttps://aws.amazon.com/lambda/\nOption D is INCORRECT because AWS CodeStar provides a unified user interface, enabling you to manage your software development activities in one place easily.\nWith AWS CodeStar, you can set up your entire continuous delivery toolchain in minutes, allowing you to start releasing code faster.\nAWS CodeStar makes it easy for your whole team to work together securely, allowing you to manage access and add owners, contributors, and viewers to your projects easily.\nHowever, this question asks for the service to store the source code.\nAWS CodeStar is improper because it is a software development management tool rather than a source control service.\nhttps://aws.amazon.com/codestar/\n\nOption A, AWS CodeCommit, is the correct choice for the developers to store their source code securely.\nAWS CodeCommit is a fully-managed source control service that provides a secure and highly scalable platform for storing and managing the source code. It offers Git-based repositories that enable developers to collaborate and version-control their code, and it integrates with a range of third-party tools such as Jenkins, GitHub, and GitLab.\nThe use of AWS CodeCommit ensures that the developers' source code and binary files are stored securely in a private repository that is not accessible to the public or unauthorized personnel. Additionally, the service provides features like encryption at rest and in transit, access control through AWS Identity and Access Management (IAM), and integration with AWS CloudTrail for auditing and compliance purposes.\nAWS CodeDeploy, option B, is a deployment service that automates code deployments to Amazon EC2 instances, on-premises servers, and Lambda functions, but it does not provide source code storage functionality.\nAWS Lambda, option C, is a serverless computing service that enables developers to run code without the need for provisioning or managing servers. However, it does not provide source code storage functionality.\nAWS CodeStar, option D, is a service that helps developers quickly develop, build, and deploy applications on AWS. It provides templates and pre-configured workflows for popular languages and frameworks, but it does not provide source code storage functionality.\nTherefore, the correct choice for the developers to store their source code securely and confidentially is AWS CodeCommit.\n\n"
}, {
  "id" : 92,
  "question" : "An organization has a persistently high amount of throughput.\nIt requires connectivity with no jitter and very low latency between its on-premise infrastructure and its AWS cloud build to support live streaming and real-time services.\nWhat is the MOST appropriate solution to meet this requirement?\n",
  "answers" : [ {
    "id" : "81b124f115134014aaecf77ba1b3e77d",
    "option" : "AWS Data Streams",
    "isCorrect" : "false"
  }, {
    "id" : "cf5fac56a6374e36b7dcea4f75828f45",
    "option" : "AWS Kinesis",
    "isCorrect" : "false"
  }, {
    "id" : "bdc8fcd17bb142f3a333b17274b589f9",
    "option" : "Kinesis Data Firehose",
    "isCorrect" : "false"
  }, {
    "id" : "b32707ad02874bb9a592cfe13406422b",
    "option" : "AWS Direct Connect.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from the organization's premises to AWS.\nThe service provides a dedicated network connection with one of the AWS Direct Connect locations.\nIt makes it possible to guaranteed high bandwidth and very low latency connectivity.\nhttps://aws.amazon.com/directconnect/\nOption A is INCORRECT because the scenarios require a connectivity option.\nBut Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service.\nIt does not guarantee the quality of connectivity between the organizations on-premise infrastructure and the AWS cloud build.\nThe dataKDS collects is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and more.\nhttps://aws.amazon.com/kinesis/data-streams/\nOption B is INCORRECT because the organization requires a connectivity solution and not an application service.\nAmazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data to get timely insights and react quickly to new information.\nhttps://aws.amazon.com/kinesis/\nOption C is INCORRECT because Amazon Kinesis Data Firehose is used to load streaming data into various destinations like data lakes, data stores and analytics tools.\nHowever, the service does not guarantee link quality between the organization's on-premise infrastructure and the AWS cloud.\nhttps://aws.amazon.com/kinesis/data-firehose/\n\nFor an organization that requires connectivity with no jitter and very low latency between its on-premise infrastructure and its AWS cloud build to support live streaming and real-time services, the most appropriate solution would be D. AWS Direct Connect.\nAWS Direct Connect is a dedicated network connection between an organization's on-premise infrastructure and AWS. This solution provides a reliable, high-performance connection with low latency and no jitter, making it ideal for real-time services and live streaming.\nDirect Connect establishes a dedicated network connection from an organization's on-premises data center to AWS, which is routed over a private connection. The connection is established through a Direct Connect Partner, which could be an internet service provider, a colocation provider, or a network service provider.\nThis solution offers several benefits, including:\nImproved Performance: Direct Connect provides a dedicated, private network connection that offers consistent network performance, low latency, and no jitter, which is essential for real-time services and live streaming. Enhanced Security: Since the connection is private, it is more secure than a public internet connection. An organization can also use Direct Connect to establish a private connection to Amazon VPC, which offers additional security benefits. Cost Savings: Since Direct Connect offers a dedicated connection, an organization can reduce its network costs by avoiding the costs associated with a public internet connection. It also eliminates the need for a VPN and associated hardware. Scalability: Direct Connect provides scalable bandwidth options, which can be increased or decreased based on an organization's needs. This makes it easy to manage network resources and ensures that an organization has the capacity it needs to support its services.\nTherefore, AWS Direct Connect is the most appropriate solution to meet an organization's requirements for connectivity with no jitter and very low latency between its on-premise infrastructure and its AWS cloud build to support live streaming and real-time services.\n\n"
}, {
  "id" : 93,
  "question" : "A Professional Educational Institution maintains a dedicated web server and database cluster that hosts an exam results portal undertaken by its students.\nThe resource is idle for most of the learning cycle and becomes excessively busy when exam results are released.\nHow can this architecture with servers be improved to be cost-efficient?\n",
  "answers" : [ {
    "id" : "325e266f0eee415ba38a670b46be208f",
    "option" : "Configure AWS Elastic load-balancing between the webserver and database cluster.",
    "isCorrect" : "false"
  }, {
    "id" : "aac9d5ee2b2946a897337169b9d9f821",
    "option" : "Configure RDS multi-availability zone for performance optimization.",
    "isCorrect" : "false"
  }, {
    "id" : "8f1b2e29f2174a0a90a90ed522d1103e",
    "option" : "Configure serverless architecture leveraging AWS Lambda functions.",
    "isCorrect" : "true"
  }, {
    "id" : "9db61e9bcaef489fb87cb07692bc13f4",
    "option" : "Migrate the web servers onto Amazon EC2 Spot Instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nLeveraging AWS Lambda functions will remove the need to run a dedicated web server for the organization.\nDuring periods of high requests to the database cluster, AWS lambda back-end infrastructure will automatically scale out resources to meet the demand adequately.\nAWS Lambda provides a platform to run code without provisioning or managing any servers.\nThe organization pays only for the compute time they consume.\nThere is no charge when your code is not running.\nLambda functions can reduce the cost significantly.\nhttps://aws.amazon.com/lambda/\nOption A INCORRECT because the premise of the scenario is about cost-efficiency more than load and server responsiveness.\nThe addition of Elastic load balancing will increase the cost based on the number of instances.\nSo this option is not cheaper.\nhttps://aws.amazon.com/elasticloadbalancing/\nOption B is INCORRECT because RDS Multi-AZ helps with disaster recovery, enhanced availability, and durability.\nHowever, the scenario requires a solution that reduces the cost of maintaining the organization's infrastructure.\nhttps://aws.amazon.com/rds/details/multi-az/\nOption D is INCORRECT because migrating to Amazon EC2 Spot Instances may negatively impact the service during periods of high traffic.\nInstances could be terminated mid-transaction that would have adverse effects on the overall user experience.\nThis would not be a cost-effective solution.\nSpot Instances let you to take advantage of unused EC2 capacity in the AWS cloud.\nSpot Instances are available at up to a 90% discount compared to On-Demand prices.\nSpot Instances can reclaim the capacity back with two minutes of notice.\nhttps://aws.amazon.com/ec2/spot/\nNote: You can test the pricing of different AWS services by using Pricing Calculator.\nhttps://calculator.aws/#/\n\nThe Professional Educational Institution has a web server and database cluster that is used to host an exam results portal for its students. The resource is idle for most of the learning cycle, and it becomes excessively busy when exam results are released. To improve this architecture and make it more cost-efficient, the following options can be considered:\nA. Configure AWS Elastic Load Balancing (ELB) between the web server and database cluster: Elastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. By configuring an ELB between the web server and database cluster, the traffic can be distributed more efficiently, improving availability and fault tolerance while reducing costs.\nB. Configure RDS Multi-Availability Zone (Multi-AZ) for performance optimization: Amazon RDS Multi-AZ provides enhanced availability and durability for database instances within a single region by automatically replicating data between two Availability Zones. Multi-AZ deployment can help minimize downtime during system maintenance, such as patching or upgrades, by automatically failing over to the standby instance in the event of a primary database instance failure.\nC. Configure serverless architecture leveraging AWS Lambda functions: AWS Lambda is a serverless compute service that runs your code in response to events and automatically manages the compute resources for you, so there is no need to provision or manage servers. By leveraging AWS Lambda functions, the Professional Educational Institution can build a serverless architecture for their exam results portal, reducing costs associated with server management, and scaling automatically based on demand.\nD. Migrate the web servers onto Amazon EC2 Spot Instances: Amazon EC2 Spot Instances are spare compute capacity in the AWS Cloud available to customers at a discounted rate. By using EC2 Spot Instances for the web servers, the Professional Educational Institution can save up to 90% on the cost of running those instances. This option requires that the organization can tolerate the possibility of Spot Instances being interrupted or terminated with two minutes' notice, which may not be suitable for all workloads.\nOverall, the best approach depends on the specific needs and requirements of the Professional Educational Institution. However, combining options A and B would improve availability and fault tolerance while minimizing costs, while option C can be considered for a more modern, scalable, and cost-efficient architecture. Option D can also be considered, depending on the workload's tolerance for interruptions and cost savings needs.\n\n"
}, {
  "id" : 94,
  "question" : "A business analyst would like to move away from creating complex database queries and static spreadsheets when generating regular reports for high-level management.\nThey would like to publish insightful, graphically appealing reports with interactive dashboards.\nWhich service can they use to accomplish this?\n",
  "answers" : [ {
    "id" : "c82a5fcbc4e940deb32fe76877ce7bcd",
    "option" : "Amazon QuickSight",
    "isCorrect" : "true"
  }, {
    "id" : "b7886cbe00a54045b1672298fb737333",
    "option" : "Business intelligence on Amazon Redshift",
    "isCorrect" : "false"
  }, {
    "id" : "41b0b19e83714a049085cade68ee6df1",
    "option" : "Amazon CloudWatch dashboards",
    "isCorrect" : "false"
  }, {
    "id" : "2f3566c94ac54dcfbdc2b4de04600a03",
    "option" : "Amazon Athena integrated with Amazon Glue.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon QuickSight is the most appropriate service in the scenario.\nIt is a fully-managed service that allows for insightful business intelligence reporting with creative data delivery methods, including graphical and interactive dashboards.\nQuickSight includes machine learning that allows users to discover inconspicuous trends and patterns on their datasets.\nhttps://aws.amazon.com/quicksight/\nOption B is INCORRECT.\nAmazon Redshift service is a data warehouse and will not meet the requirements of interactive dashboards and dynamic means of delivering reports.\nOption C is INCORRECT.\nAmazon CloudWatch dashboards will not accomplish the requirements of the scenario.\nThey are used to monitor AWS system resources and infrastructure services, though they are customizable and present information graphically.\nOption D is INCORRECT.\nAmazon Athena is a query service that allows for easy data analysis in Amazon S3 by using standard SQL.\nThe service does not meet the requirements of the scenario.\n\nThe service that the business analyst can use to generate insightful, graphically appealing reports with interactive dashboards is Amazon QuickSight (Option A).\nAmazon QuickSight is a fully managed business intelligence service that enables users to create and publish interactive, visually appealing dashboards, reports, and data visualizations. It allows users to connect to various data sources, including AWS services, third-party databases, and SaaS applications, to gather data and create insightful reports.\nWith QuickSight, users can easily build dashboards using drag-and-drop functionality and a range of visualizations, such as bar charts, line charts, scatter plots, and more. QuickSight also includes built-in features for data exploration, such as filters, groupings, and pivots, that allow users to drill down into data to uncover insights and trends.\nAdditionally, QuickSight supports machine learning-powered analytics, enabling users to detect anomalies and forecast trends. It also provides seamless integration with other AWS services, such as Amazon S3, Amazon RDS, and Amazon Redshift, allowing users to quickly access and analyze data stored in those services.\nTherefore, Amazon QuickSight is the most suitable option for the business analyst to move away from creating complex database queries and static spreadsheets and generate insightful, graphically appealing reports with interactive dashboards.\n\n"
}, {
  "id" : 95,
  "question" : "What is the AWS feature that enables fast, easy and secure transfers of files over long distances between your client and your Amazon S3 bucket?\n",
  "answers" : [ {
    "id" : "5a2b65c97a744262a4afeab9fba4de87",
    "option" : "File Transfer",
    "isCorrect" : "false"
  }, {
    "id" : "cf1a8e5f9afd4c92a6bbc53d2ec29226",
    "option" : "HTTP Transfer",
    "isCorrect" : "false"
  }, {
    "id" : "973988395b3e4492aa5d3c6763052151",
    "option" : "Amazon S3 Transfer Acceleration",
    "isCorrect" : "true"
  }, {
    "id" : "f9a6113d2f9a4163a8b8f3af682ca861",
    "option" : "S3 Acceleration.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAmazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket.\nTransfer Acceleration takes advantage of Amazon CloudFront's globally distributed edge locations.\nAs the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.\nFor more information on S3 transfer acceleration, please visit the Link:\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nOptions A, B and D are incorrect.\nThese features deal with transferring data but not between clients and an S3 bucket.\n\nThe correct answer is C. Amazon S3 Transfer Acceleration.\nAmazon S3 Transfer Acceleration is a feature of Amazon S3 that enables fast, easy, and secure transfers of files over long distances between a client and an Amazon S3 bucket. It uses Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet. This makes the transfers faster and more reliable, especially over long distances, and reduces the impact of network latency.\nWhen using S3 Transfer Acceleration, the data is routed through the Amazon CloudFront edge locations closest to the client, which minimizes the distance that the data has to travel. This reduces the impact of network latency, making the transfer faster and more reliable. Additionally, S3 Transfer Acceleration uses Amazon S3's encryption features to ensure that data is transferred securely over the public internet.\nTo use S3 Transfer Acceleration, you can simply enable it on your Amazon S3 bucket. Once enabled, you can use the S3 Transfer Acceleration endpoint to transfer files to and from your bucket. This endpoint is a distinct URL that you can use to upload and download files from your bucket using Amazon S3 Transfer Acceleration.\nIn summary, Amazon S3 Transfer Acceleration is a feature of Amazon S3 that enables fast, easy, and secure transfers of files over long distances between a client and an Amazon S3 bucket. It uses Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet, making them faster and more reliable.\n\n"
}, {
  "id" : 96,
  "question" : "As per the AWS Acceptable Use Policy, how can the penetration testing of EC2 instances be performed?\n",
  "answers" : [ {
    "id" : "25941a274fdb4db38464ce88027a2f03",
    "option" : "May be performed by AWS, and will be performed by AWS upon customer request.",
    "isCorrect" : "false"
  }, {
    "id" : "d8bebedbe70245d2a1b63355d1b11762",
    "option" : "May be performed by AWS, and is periodically performed by AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "2369ab28463647f1a366dae0321334ea",
    "option" : "Are expressly prohibited under all circumstances.",
    "isCorrect" : "false"
  }, {
    "id" : "42c5317fd7724458b76ea3b536459e3c",
    "option" : "Can be performed by the customer, provided they work with the list of services mentioned by AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "c4e7fd9e13784f03b8cf26012d729fa3",
    "option" : "May be performed by the customer on their owninstances, only if performed from EC2 instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer -\nD.You do not need to take prior authorization from AWS before doing a penetration test on EC2 Instances.\nPlease refer to the below URL:\nhttps://aws.amazon.com/security/penetration-testing/\nA B and C are incorrect.\nAWS says as below:\n############\nPermitted Services - You're welcome to conduct security assessments against AWS resources that you own if they make use of the services listed below.\nWe're constantly updating this list; click here to leave us feedback, or request for inclusion of additional services:\no Amazon EC2 instances, NAT Gateways, and Elastic Load Balancers.\no Amazon RDS.\no Amazon CloudFront.\no Amazon Aurora.\no Amazon API Gateways.\no AWS Lambda and Lambda Edge functions.\no Amazon Lightsail resources.\no Amazon Elastic Beanstalk environments.\n###########\n\nAccording to the AWS Acceptable Use Policy, penetration testing of EC2 instances is allowed but subject to certain conditions. The policy states that customers may conduct security assessments or penetration tests on their own instances provided that they comply with AWS's rules and guidelines.\nOption A is incorrect as AWS does not perform penetration testing upon customer request. Option B is incorrect as AWS does not periodically perform penetration testing.\nOption C is incorrect as penetration testing is allowed, subject to certain conditions. Option D is also incorrect as there is no mention of a list of services that customers must work with to conduct penetration testing.\nOption E is the correct answer, as it states that customers may conduct penetration testing on their own instances, but only if it is performed from EC2 instances. This is because AWS has strict rules regarding network security and does not want customers to conduct penetration tests that could harm other customers' instances or AWS's own infrastructure.\nCustomers who wish to perform penetration testing on their EC2 instances must follow AWS's guidelines and obtain prior written consent from AWS. Additionally, customers must use only approved testing methods and tools, and must not conduct tests that could cause damage or disruption to other customers' instances or AWS's infrastructure.\n\n"
}, {
  "id" : 97,
  "question" : "In which five categories does Trusted Advisor service provide insight for an AWS account?\n",
  "answers" : [ {
    "id" : "991ec3f1c9f646f4b9f2da960d68c9db",
    "option" : "Security, fault tolerance, high availability, connectivity and Service Limits",
    "isCorrect" : "false"
  }, {
    "id" : "47e53d69609e4f25beaee38c3f54d1e3",
    "option" : "Security, access control, high availability, performance and Service Limits",
    "isCorrect" : "false"
  }, {
    "id" : "88bec52288114193b8fe19def815cb1e",
    "option" : "Performance, cost optimization, security, fault tolerance and Service Limits",
    "isCorrect" : "true"
  }, {
    "id" : "1807a8d1558c457b817605389bea7009",
    "option" : "Performance, cost optimization, access control, connectivity and Service Limits.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nBelow is the screenshot of what services the Trusted Advisor Dashboard offers.\nCost optimization.\nIt helps to save cost, such as recommending you to delete unused resources or use reserved capacity.\nPerformance.\nIt can improve the performance of the services by ensuring to take advantage of provisioned throughput, and monitoring for overutilized Amazon EC2 instances.\nSecurity.\nIt can improve the security of the application by recommending you to enable AWS security features, and review your permissions.\nFault tolerance.\nIt can increase the availability of the AWS application by recommending to take advantage of auto-scaling, health checks, multi-AZ Regions, and backup capabilities.\nService quotas.\nService quotas also referred to as Service limits, are the maximum number of service resources or operations that apply to an account or a Region.\nTrusted Advisor can notify you if you use more than 80% of a service quota.\nFor more information on the AWS Trusted Advisor, please visit the Link-\nhttps://aws.amazon.com/premiumsupport/trustedadvisor/\nhttps://docs.aws.amazon.com/awssupport/latest/user/trusted-advisor-check-reference.html\n\n\nTrusted Advisor is a service provided by AWS that provides real-time guidance to help customers optimize their AWS infrastructure, improve performance, increase security and reduce costs. The service checks your AWS environment and compares it to AWS best practices in five different categories. The five categories in which Trusted Advisor provides insight for an AWS account are:\nA. Security, fault tolerance, high availability, connectivity and Service Limits\nSecurity - This category provides security-related recommendations to help you secure your AWS environment. It checks for the proper use of IAM, MFA, security groups, encryption, and network access control lists. Fault Tolerance - This category helps you to design a fault-tolerant infrastructure in AWS. It checks for the use of Auto Scaling groups, Elastic Load Balancing, and Multi-AZ deployment. High Availability - This category checks the availability of your AWS environment. It provides recommendations for using multiple availability zones, Elastic Load Balancing, and CloudFront. Connectivity - This category checks for the proper use of AWS Direct Connect, Virtual Private Cloud (VPC), and routing tables. Service Limits - This category checks your usage of AWS services to make sure that you are not reaching any service limits.\nTherefore, option A (Security, fault tolerance, high availability, connectivity and Service Limits) is the correct answer as it includes all the five categories in which Trusted Advisor provides insight for an AWS account. Option B, C and D include some but not all the categories provided by Trusted Advisor.\n\n"
}, {
  "id" : 98,
  "question" : "Which of the following AWS services is suitable to be used as a fully managed data warehouse?\n",
  "answers" : [ {
    "id" : "4aacad32756d450a92e1d14e81fb28d8",
    "option" : "Amazon Athena",
    "isCorrect" : "false"
  }, {
    "id" : "fb643b55c8324d488191b799cb3ae1a1",
    "option" : "Amazon RedShift",
    "isCorrect" : "true"
  }, {
    "id" : "80527c8adffe46988ff5d64cf95ab87e",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "c740be9237bf48648f8687385b905e04",
    "option" : "Amazon Warehouse.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon Redshift is a fully managed, petabyte-scale data warehouse service.\nhttps://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html\nOption A is INCORRECT because Amazon Athena is used to query data and analyze big data in S3.\nOption C is INCORRECT because Amazon CloudWatch is used to monitor AWS resources, collect metrics, configure alarms, etc.\nOption D is INCORRECT because there is no such AWS service.\n\nThe AWS service that is suitable to be used as a fully managed data warehouse is Amazon RedShift.\nAmazon RedShift is a fully managed data warehouse service that is designed to handle petabyte-scale data warehouses. It is a cloud-based data warehousing solution that is built for large-scale analytics workloads. It is optimized for high-performance querying and analysis of structured data using SQL, and it can handle both structured and unstructured data. RedShift is a columnar storage database that uses massively parallel processing (MPP) to distribute workloads across multiple nodes, enabling it to handle large volumes of data with ease.\nRedShift also provides a range of features to optimize performance, including automatic compression, data distribution, and query optimization. Additionally, RedShift supports a variety of standard SQL clients and JDBC/ODBC drivers, making it easy to connect and work with from your existing tools and applications.\nAmazon Athena, on the other hand, is a serverless interactive query service that enables users to analyze data stored in Amazon S3 using standard SQL. It is ideal for ad-hoc querying of data, and it doesn't require users to provision or manage any infrastructure. It is not a data warehouse service, but rather a query service that can be used to analyze data stored in S3.\nAmazon CloudWatch is a monitoring and management service for AWS resources, and it is not a data warehouse service.\nAmazon Warehouse is not a valid AWS service and does not exist.\nTherefore, the correct answer is B. Amazon RedShift.\n\n"
}, {
  "id" : 99,
  "question" : "What best describes the \"Principle of Least Privilege\"? Choose the correct answer from the options given below.\n",
  "answers" : [ {
    "id" : "cf709a49143d4ab4b84adcee69b4ef0b",
    "option" : "All users should have the same baseline permissions granted to them to use basic AWS services.",
    "isCorrect" : "false"
  }, {
    "id" : "fc4a2a8a8b3a49afa4981a2f58a402db",
    "option" : "Users should be granted permission to access only resources they need to do their assigned job.",
    "isCorrect" : "true"
  }, {
    "id" : "5e358f9bd41a4672a517ca13c866a0c5",
    "option" : "Users should submit all access requests in written form so that there is a paper trail of who needs access to different AWS resources.",
    "isCorrect" : "false"
  }, {
    "id" : "017abd181c104e849a2e56d0ee678b3a",
    "option" : "Users should always have a little more permission than they need.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe principle means giving a user account only those privileges which are essential to perform its intended function.\nFor example, a user account for the sole purpose of creating backups does not need to install the software.\nHence, it has rights only to run backup and backup-related applications.\nFor more information on the principle of least privilege, please refer to the following link:\nhttps://en.wikipedia.org/wiki/Principle_of_least_privilege\nOptions A, C, and D are incorrect.\nThese actions would not adhere to the Principle of Least Privilege.\n\nThe \"Principle of Least Privilege\" is a security concept that involves granting users or entities the minimum access required to perform their specific tasks or operations. This principle is important in ensuring the confidentiality, integrity, and availability of data and resources within a system.\nOption A is incorrect because it suggests granting all users the same level of access, regardless of their roles or responsibilities. This approach is not appropriate because it increases the risk of unauthorized access and potentially exposes sensitive information.\nOption B is the correct answer because it aligns with the Principle of Least Privilege. It emphasizes that users should only be granted permission to access resources that are essential for them to perform their assigned job. This approach ensures that users do not have access to resources they do not need, thereby reducing the risk of unauthorized access and potential security breaches.\nOption C is incorrect because it suggests that access requests should be submitted in written form to create a paper trail. While it is essential to have a record of access requests, this approach does not relate to the Principle of Least Privilege.\nOption D is also incorrect because it suggests granting users slightly more permission than they need. This approach violates the Principle of Least Privilege because it grants unnecessary access to users and increases the risk of security breaches.\nIn summary, the Principle of Least Privilege dictates that users should be granted the minimum level of access required to perform their assigned tasks or operations. This approach helps to reduce the risk of unauthorized access and ensure data and resource security.\n\n"
}, {
  "id" : 100,
  "question" : "A developer would like to automate the installation by updating a set of applications on a series of EC2 instances and on-premises servers.\nWhich is the most appropriate service to use to achieve this requirement?\n",
  "answers" : [ {
    "id" : "bc19084cc0eb4b2b9647bed98c6adbb9",
    "option" : "AWS CodeBuild",
    "isCorrect" : "false"
  }, {
    "id" : "2309e3b65ab040828087fc6985fae95d",
    "option" : "AWS CodeCommit",
    "isCorrect" : "false"
  }, {
    "id" : "9316080f72a04999885e18ffee835973",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "true"
  }, {
    "id" : "4424bab48d8840128b7274f14b385a76",
    "option" : "AWS CloudFormation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS CodeDeploy is a deployment service that allows developers to automate the installation of applications to hosts, Amazon EC2 instances, Amazon ECS instances, serverless Lambda functions, or even on-premises servers.\nAWS CodeDeploy can enable the update of those applications.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html\nOption A is INCORRECT.\nAWS CodeBuild is a fully managed service that primarily compiles source code and runs unit tests with the output being artifacts ready for deployment.\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\nOption B is INCORRECT.\nAWS CodeCommit service primarily serves to control software build versions and private storage for software development assets such as binary files, source code and related documentation.\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/welcome.html\nOption D is INCORRECT.\nAWS CloudFormation will not be able to run deployments of applications onto on-premises infrastructure.\nFurthermore, AWS CloudFormation automates the deployment of AWS resources but not applications and code onto hosts.\n\nThe most appropriate service to automate the installation of applications on EC2 instances and on-premises servers is AWS CodeDeploy (option C).\nAWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services, including Amazon EC2 instances and on-premises servers. CodeDeploy makes it easier to rapidly release new features, helps avoid downtime during deployment, and handles the complexity of updating applications.\nAWS CodeBuild (option A) is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. CodeBuild does not deploy the code, but rather is a part of the software development pipeline that occurs before deployment.\nAWS CodeCommit (option B) is a fully managed source control service that makes it easy for companies to host secure and highly scalable private Git repositories. CodeCommit does not directly assist with the deployment of code but can be used as a part of a code pipeline.\nAWS CloudFormation (option D) is a service that provides a common language for describing and provisioning infrastructure resources in the cloud. It is typically used for creating and managing AWS resources, rather than deploying applications to EC2 instances and on-premises servers.\nTherefore, the most appropriate service for automating the installation of applications on EC2 instances and on-premises servers is AWS CodeDeploy.\n\n"
}, {
  "id" : 101,
  "question" : "As per AWS global infrastructure, which of the following components within an AWS Region provides a low latency redundant connectivity?\n",
  "answers" : [ {
    "id" : "abbea25215f74cda9f72c3cca407efe1",
    "option" : "Data Centers",
    "isCorrect" : "false"
  }, {
    "id" : "17af7fd5f80b48c39566f1f37f0af392",
    "option" : "Edge Location",
    "isCorrect" : "false"
  }, {
    "id" : "dc247a1c3cca498498bb370ce584bd04",
    "option" : "Availability Zones",
    "isCorrect" : "true"
  }, {
    "id" : "3bca8438d033467e92425e0b6c171815",
    "option" : "Regional Cache.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nRegions consist of 2 or more Availability Zones within a specific geographical area.\nThese Availability Zones are physically isolated &amp; connected via a low latency redundant link.\nOption A is incorrect because Logical Data Center within each region is called an Availability Zones instead of a Data Center.\nOption B is incorrect because Edge locations are used by CloudFront CDN to deliver content to users with low latency.\nOption D is incorrect because Regional Caches are used by CloudFront which sit between edge locations &amp; origin servers providing additional caching.\nFor more information on AWS Regions &amp; Availability Zones, refer to the following URL:\nhttps://aws.amazon.com/about-aws/global-infrastructure/regions_az/?p=ngi&amp;loc=2\n\nAs per AWS global infrastructure, the component within an AWS Region that provides low latency redundant connectivity is Availability Zones.\nAWS Regions are physical locations around the world where AWS provides infrastructure. Each Region consists of multiple Availability Zones (AZs), which are physically separate data centers within a region. These AZs are interconnected with high-bandwidth, low-latency links to provide redundant and low-latency connectivity within the region.\nEdge Locations are also part of the AWS global infrastructure but are separate from AWS Regions and AZs. They are endpoints for AWS services that are located closer to end-users to reduce latency and improve performance. Edge Locations are used by AWS CloudFront, AWS's content delivery network (CDN), to cache and distribute content to end-users.\nRegional Cache is not a component of the AWS global infrastructure. It is a feature of AWS ElastiCache, which is a web service that makes it easy to deploy and operate an in-memory cache in the cloud.\nIn summary, while Data Centers, Edge Locations, and Regional Cache are all important components of the AWS ecosystem, Availability Zones within an AWS Region provide low latency redundant connectivity.\n\n"
}, {
  "id" : 102,
  "question" : "Which of the following routing policies can be used to provide the best performance to global users accessing a static website deployed on Amazon S3 buckets at multiple regions?\n",
  "answers" : [ {
    "id" : "123f2516e158475ca3ed6904982d0371",
    "option" : "Use Route 53 weighted routing policy.",
    "isCorrect" : "false"
  }, {
    "id" : "47d86dc14a5c485390cc4e8a2a27d607",
    "option" : "Use Route 53 latency routing policy.",
    "isCorrect" : "true"
  }, {
    "id" : "814fbb271e5743f188fdf02c818c363a",
    "option" : "Use Route 53 Geoproximity routing policy.",
    "isCorrect" : "false"
  }, {
    "id" : "c79b38dc55a54fb9a29898cf5922a412",
    "option" : "Use Route 53 Geolocation routing policy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nRoute 53 latency routing policy can be used to provide the least latency when resources are deployed in multiple regions.\nThis policy routes users' requests to the nearest resource based upon latency.\nOption A is incorrect as Route 53 weighted routing policy is used to distribute requests between multiple resources based upon weight of each.\nOption C is incorrect as Route 53 Geoproximity routing policy can be used to route traffic based upon the location of the resource.\nOption D is incorrect as Route 53 Geolocation routing policy can be used to route traffic based upon user location.\nFor more information on Amazon Route 53 routing policy, refer to the following URL:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\nTo provide the best performance to global users accessing a static website deployed on Amazon S3 buckets at multiple regions, the ideal routing policy to use would be Route 53 latency routing policy.\nThe Route 53 latency routing policy directs traffic to the AWS region that provides the lowest network latency for the end user. This means that when a user requests a website, Route 53 checks the latency between the user and each of the available AWS regions that host the website and then directs the user to the region with the lowest latency.\nThis policy is particularly useful for static websites that are hosted on Amazon S3 buckets at multiple regions because it ensures that users can access the website with the lowest possible latency. This helps to reduce the loading time of the website and provides a better user experience.\nThe other routing policies listed are not as ideal for this use case:\nThe Route 53 weighted routing policy is used when traffic needs to be distributed across multiple resources in proportions that you specify. This is not optimal for a static website hosted on Amazon S3 buckets at multiple regions. The Route 53 Geoproximity routing policy is used when you want to route traffic based on the geographic location of the user making the request. This policy is useful when you have resources that are closer to specific geographic locations, but it may not be the best choice for a static website hosted on Amazon S3 buckets at multiple regions. The Route 53 Geolocation routing policy is used to route traffic based on the geographic location of the user making the request. This policy is similar to the geoproximity routing policy, but it provides more granularity in terms of the geographic locations that can be used to route traffic. However, it may not be the best choice for a static website hosted on Amazon S3 buckets at multiple regions.\n\n"
}, {
  "id" : 103,
  "question" : "Which of the following services can be used to optimize performance for global users to transfer large-sized data objects to a centralized Amazon S3 bucket in us-west-1 region?\n",
  "answers" : [ {
    "id" : "9e62b6699bd545fdbee4517e29cff9f2",
    "option" : "Enable S3 Transfer Acceleration on Amazon S3 bucket.",
    "isCorrect" : "true"
  }, {
    "id" : "2087dd69a47a412eb40e6781933a4073",
    "option" : "Use Amazon CloudFront Put/Post commands",
    "isCorrect" : "false"
  }, {
    "id" : "470b1f58d62d441aaa90938af654bec2",
    "option" : "Use Multipart upload",
    "isCorrect" : "false"
  }, {
    "id" : "62a7629c5ae7410a92c011991379431b",
    "option" : "Use Amazon ElastiCache.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nS3 Transfer Acceleration can optimise performance for data transfer between users &amp; objects in Amazon S3 bucket.\nTransfer acceleration uses CloudFront edge location to provide accelerated data transfer to users.\nOption B is incorrect as Amazon CloudFront Put/Post commands can be used for small-sized objects but for large-sized data objects, S3 Transfer Acceleration provides better performance.\nOption C is incorrect as users should use Multipart uploads for all data objects exceeding 100 megabytes.\nBut for better performance, S3 transfer acceleration should be enabled.\nOption D is incorrect as for global users accessing S3 bucket, S3 Transfer Acceleration is a better choice.\nFor more information on Amazon S3 Transfer Acceleration, refer to the following URLs:\nhttps://aws.amazon.com/s3/faqs/#s3ta\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html\n\nThe service that can be used to optimize performance for global users to transfer large-sized data objects to a centralized Amazon S3 bucket in us-west-1 region is option A: Enable S3 Transfer Acceleration on Amazon S3 bucket.\nS3 Transfer Acceleration is a service offered by Amazon S3 that uses Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet. It helps to optimize transfer speed by routing the data over Amazon's network, taking advantage of Amazon's backbone infrastructure, and reducing the impact of network latency.\nWhen S3 Transfer Acceleration is enabled, a unique endpoint is created for the S3 bucket, and data transfers are automatically routed through Amazon CloudFront's network. This helps to reduce the time it takes for data to travel from the user's location to the Amazon S3 bucket.\nOption B: Using Amazon CloudFront Put/Post commands is incorrect as CloudFront is a content delivery network (CDN) service that is primarily used for distributing and caching content to improve the delivery speed to end-users. While CloudFront can be used to distribute content stored in Amazon S3 buckets, it is not the optimal solution for transferring large-sized data objects to a centralized S3 bucket.\nOption C: Using Multipart upload is incorrect as multipart upload is a feature of Amazon S3 that enables users to upload large objects in parts. It is used to increase performance and reliability when uploading large objects. However, it is not designed specifically to optimize performance for global users or for transferring large-sized data objects to a centralized S3 bucket.\nOption D: Using Amazon ElastiCache is incorrect as Amazon ElastiCache is a web service that makes it easy to deploy and operate an in-memory cache in the cloud. It is used to improve the performance of web applications by reducing the load on database servers. It is not designed for optimizing performance for global users or for transferring large-sized data objects to a centralized S3 bucket.\nIn summary, the correct answer is A: Enable S3 Transfer Acceleration on Amazon S3 bucket.\n\n"
}, {
  "id" : 104,
  "question" : "Which of the following is NOT an area of shared controls (Shared between AWS &amp; Customer in different contexts) within the AWS Shared responsibility Model? (Select TWO.)\n",
  "answers" : [ {
    "id" : "8223f72aa34d46baa493dc02931d719e",
    "option" : "Configuration Management",
    "isCorrect" : "false"
  }, {
    "id" : "928de6f5add14ebc9d1c44237a749f88",
    "option" : "Service &amp; communication protection",
    "isCorrect" : "true"
  }, {
    "id" : "811cc11cfc9b43bcbe4f2996b8531ce6",
    "option" : "Patch Management",
    "isCorrect" : "false"
  }, {
    "id" : "c06bdb616f8941f89ae79d977413588c",
    "option" : "IAM User Management",
    "isCorrect" : "true"
  }, {
    "id" : "12fcb501add046abbb4b94902b512f93",
    "option" : "Training &amp; Awareness.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: B and D.\nShared controls are applicable in both the infrastructure &amp; customer layers but in completely separate contexts.\nUnder shared controls, AWS provides requirements for infrastructure while customers must provide their own control implementation for the AWS services that they use.\nOption A is incorrect since configuration management has shared controls.\nAWS is responsible for configuring infrastructure devices while the customer is responsible for configuring their guest OS &amp; applications.\nOption B is CORRECT since Services communication may be subject to data zoning &amp; protection within specific security environments.\nThis is primarily the responsibility of the customer &amp; AWS does not play any role in this.\nThis may take the form of configuring NACL's, Security Groups, Data encryption etcâ€¦\nOption C is incorrect since AWS is responsible for detecting &amp; patching flaws within the infrastructure while the customer is responsible for patching their guest OS &amp; applications.\nOption D is CORRECT since IAM and user management refers to security â€œInâ€ the cloud and are best managed by the customer.\nOption E is incorrect since AWS trains its own employees while customers need to train their own employees.\nReference:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\n\nThe AWS Shared Responsibility Model is a security framework that outlines the division of responsibilities between AWS and its customers. It defines which security controls are the responsibility of AWS and which are the responsibility of the customer.\nAWS is responsible for the security \"of\" the cloud, which includes the underlying infrastructure, network, and hardware. On the other hand, customers are responsible for the security \"in\" the cloud, which includes the security of their data, applications, and systems.\nThe areas of shared controls refer to the security controls that are shared between AWS and the customer. These controls require both parties to work together to ensure security in the cloud.\nNow let's analyze the options:\nA. Configuration Management: This area includes the management of configurations for both AWS and customer-owned resources. It involves the configuration and maintenance of settings and parameters for various resources. This area is a shared control, as both AWS and the customer have responsibilities related to configuration management.\nB. Service & communication protection: This area includes the protection of communication channels between different resources in the cloud, as well as the protection of AWS services. Both AWS and the customer have responsibilities related to service and communication protection. This area is a shared control.\nC. Patch Management: This area includes the management of patches and updates for operating systems and applications on both AWS and customer-owned resources. Both parties have responsibilities related to patch management, making this area a shared control.\nD. IAM User Management: This area includes the management of user identities and permissions for accessing AWS resources. This area is the responsibility of the customer, not AWS. Therefore, IAM User Management is not a shared control.\nE. Training & Awareness: This area includes the training and awareness of personnel regarding security practices and policies. While AWS provides security training to its employees, it is the responsibility of the customer to provide training and awareness to their employees. Therefore, this area is not a shared control.\nTherefore, the two options that are not areas of shared controls within the AWS Shared Responsibility Model are D. IAM User Management and E. Training & Awareness.\n\n"
}, {
  "id" : 105,
  "question" : "Which of the following services can be used to automate software deployments on a large number of Amazon EC2 instance and on-premise servers?\n",
  "answers" : [ {
    "id" : "838f346f526d4c0380118079f11c6afd",
    "option" : "AWS CodePipeline",
    "isCorrect" : "false"
  }, {
    "id" : "82f6aa9ffa644a88b3708877172a7890",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "6b1984c96c524b9fb5f7f6ee5085fb11",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "true"
  }, {
    "id" : "1862895c65124265b94a070be8601838",
    "option" : "AWS Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS CodeDeploy is a managed service that automates software deployment on a large scale to EC2 instances and on-premise servers.\nOption A is incorrect as AWS CodePipeline is a managed service for automation of delivery pipeline for application updates.\nOption B is incorrect as AWS CloudFormation is used to automate infrastructure provisioning &amp; updates.\nOption D is incorrect as AWS Config is used to audit configurations of AWS resources.\nFor more information on AWS CodeDeploy Features, refer to the following URL:\nhttps://aws.amazon.com/codedeploy/features/?nc=sn&amp;loc=2\n\nThe correct answer is C. AWS CodeDeploy.\nAWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS Fargate, AWS Lambda, and on-premises servers. It makes it easy to coordinate the deployment of applications across multiple Amazon EC2 instances or on-premises servers, allowing you to rapidly deploy new features and updates to your applications with minimal downtime.\nAWS CodeDeploy works by using deployment groups, which are collections of Amazon EC2 instances or on-premises servers that are defined by tags, auto-scaling groups, or manually selected instances. Once a deployment group has been defined, you can use AWS CodeDeploy to deploy new versions of your application code to that group with a single click, using either the AWS Management Console, the AWS CLI, or the AWS CodeDeploy API.\nAWS CodeDeploy also provides a number of features that make it easy to manage the deployment process, such as automatic rollback, which allows you to automatically roll back to a previous version of your application if the deployment fails. Additionally, AWS CodeDeploy integrates with other AWS services such as AWS CodePipeline and AWS CloudFormation, allowing you to easily create end-to-end deployment pipelines that automate the entire software delivery process.\nAWS CodePipeline is a continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. It integrates with a number of other AWS services, including AWS CodeDeploy, to automate the build, test, and deployment of your applications.\nAWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so you can spend less time managing those resources and more time focusing on your applications that run in AWS. With AWS CloudFormation, you can create and provision AWS infrastructure deployments predictably and repeatedly.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It helps you to maintain a detailed inventory of your resources, as well as track changes to your configurations over time. While AWS Config can help you identify non-compliant resources, it is not specifically designed for automating software deployments.\n\n"
}, {
  "id" : 106,
  "question" : "Which of the following statements best describe the AWS Personal Health Dashboard? (Select Two)\n",
  "answers" : [ {
    "id" : "58b7f1dfc2b9489a99c406fbe289fbc1",
    "option" : "A concise representation of the general status of AWS services",
    "isCorrect" : "false"
  }, {
    "id" : "de3b2971948a4dc7b2841132533e5973",
    "option" : "User-specific view on the availability and performance of AWS services, underlying their AWS resources.",
    "isCorrect" : "true"
  }, {
    "id" : "dc04148027454f968c098c93fff3d16d",
    "option" : "A service that prompts the user with alerts and notifications on AWS scheduled activities, pending issues, and planned changes.",
    "isCorrect" : "true"
  }, {
    "id" : "f83ffd659b234d9d874aab7e3236c76e",
    "option" : "A minute-by-minute update of system outages and service errors on the AWS global infrastructure",
    "isCorrect" : "false"
  }, {
    "id" : "ac6d3403f3e34f9ebc99db97f14f704f",
    "option" : "A rolling log of all service interruptions across the AWS network and records of incidents persistent for a year.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B and C.\nThe Personal Health Dashboard is a tool that shows the status of AWS services running the user-specific resources.\nIt is a graphical representation that sends alerts, notifications of any personal pending issues, planned changes, and scheduled activities.\nhttps://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/\nOption A is INCORRECT.\nIt describes a general overview of the Service Health Dashboard.\nOption D is INCORRECT.\nIt describes the Service Health Dashboard.\nOption E is INCORRECT.\nIt describes the Status History of the Service Health Dashboard.\n\nThe AWS Personal Health Dashboard is a service provided by AWS that provides users with a personalized view of the status of their AWS resources. It helps users to monitor the health of their AWS services and resources, provides alerts and notifications on scheduled activities, and planned changes.\nTherefore, options B and C are the best descriptions of the AWS Personal Health Dashboard.\nOption A is incorrect because the dashboard is user-specific and provides information only about the user's AWS resources and services.\nOption D is incorrect because the AWS Personal Health Dashboard is not a real-time monitoring tool that provides minute-by-minute updates of system outages and service errors. Instead, it provides notifications on scheduled activities and changes that may impact the user's AWS resources.\nOption E is incorrect because the AWS Personal Health Dashboard does not maintain a rolling log of all service interruptions across the AWS network. However, it does keep a record of past incidents for up to one year, which can be helpful for troubleshooting and auditing purposes.\nIn summary, the AWS Personal Health Dashboard provides users with personalized insights into the availability and performance of their AWS resources and services, as well as alerts and notifications on scheduled activities, planned changes, and incidents.\n\n"
}, {
  "id" : 107,
  "question" : "A startup company that works on social media apps development would like to grant freelance developers temporary access to its Lambda functions setup on AWS.\nThese developers would be signing-in via Facebook authentication.\nWhich service is the most appropriate to grant secure access?\n",
  "answers" : [ {
    "id" : "4b54efa7fde44856834d2c44598edbc6",
    "option" : "Create user credentials using Identity Access Management (IAM).",
    "isCorrect" : "false"
  }, {
    "id" : "9626b829f5114f668c79f9fcc48dda03",
    "option" : "Use Amazon Cognito for web-identity federation.",
    "isCorrect" : "true"
  }, {
    "id" : "ae292ed353be4a679cec49d41b4fa83f",
    "option" : "Create temporary access roles using IAM.",
    "isCorrect" : "false"
  }, {
    "id" : "f45383539f504933b18f3451b04ebfea",
    "option" : "Use a third-party Web ID, federated access provider.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon Cognito web identity federation service acts as a broker that allows authenticated users to access AWS resources.\nAfter successful authentication on platforms such as Facebook, LinkedIn, or Google Mail, users receive temporary authentication code from Amazon Cognito, thereby gain temporary access.\nhttps://aws.amazon.com/cognito/\nOption A is INCORRECT.\nThe access required is temporary and not directly onto the AWS environment.\nIdentity Access Management (IAM) user will be granted access directly using AWS specified credentials.\nOption C is INCORRECT.\nThe IAM user credentials will not authenticate on Facebook.\nThey are confined to logging onto the AWS environment.\nOption D is INCORRECT.\nThere is no need to take a third-party Web ID from federated access providers since Amazon has the Cognito service to perform that function.\n\nThe most appropriate service to grant secure access to the startup company's Lambda functions for freelance developers signing-in via Facebook authentication is Amazon Cognito for web-identity federation (Option B).\nExplanation:\nAmazon Cognito is a user authentication and authorization service that provides secure access to resources in the AWS Cloud. It supports web identity federation, which allows users to authenticate with third-party identity providers like Facebook, Google, and Amazon. With web identity federation, users can sign in using their existing social media accounts without having to create new credentials.\nIn this scenario, the startup company can use Amazon Cognito to create a user pool that integrates with Facebook authentication. This user pool can be used to grant temporary access to the company's Lambda functions to freelance developers who sign-in via Facebook authentication. Amazon Cognito provides secure and scalable authentication and authorization, and it also provides features like multi-factor authentication, password resets, and email and SMS notifications.\nOption A (Create user credentials using Identity Access Management (IAM)) would not be appropriate because IAM is used for creating and managing user identities and permissions within the AWS ecosystem, and not for integrating with third-party identity providers like Facebook.\nOption C (Create temporary access roles using IAM) would also not be appropriate because IAM roles are used for granting permissions to AWS services and resources, and not for integrating with third-party identity providers like Facebook.\nOption D (Use a third-party Web ID, federated access provider) would not be appropriate because it is not clear which provider to use, and it would add unnecessary complexity to the setup. Amazon Cognito provides an easy-to-use and secure solution for web identity federation, and it is a recommended approach for this scenario.\n\n"
}, {
  "id" : 108,
  "question" : "There is a requirement to store objects.\nThe objects must be downloadable via a URL.\nWhich storage option would you choose?\n",
  "answers" : [ {
    "id" : "fdd414a966874c2cb23e5f7baf78aa14",
    "option" : "Amazon S3",
    "isCorrect" : "true"
  }, {
    "id" : "f977f6a0dd2644d89ef136a2be4a6d15",
    "option" : "Amazon Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "861d79c4ca90443f8e285f42364189cd",
    "option" : "Amazon Storage Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "b5d4eb02a04449a08816c3bd9447345b",
    "option" : "Amazon EBS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nAmazon S3 is the perfect storage option.\nIt also provides the facility of assigning a URL to each object which can be used to download the object.\nFor more information on AWS S3, please visit the Link:\nhttps://aws.amazon.com/s3/\nB is incorrect.\nGlacier is for archival and long-term storage.\nThis question is to check the user understanding of AWS S3 service terminology and use cases.\nObjects are stored in S3 and should be downloadable via a URL.\nIt's not possible with EBS.\n\nThe appropriate storage option for the requirement of storing objects that are downloadable via a URL is Amazon S3 (Simple Storage Service).\nAmazon S3 is a highly scalable and durable object storage service that allows users to store and retrieve data from anywhere on the web. S3 enables easy management of objects, including uploading, downloading, and managing access to objects, as well as providing security, durability, and scalability.\nOne of the key features of Amazon S3 is that it provides a unique URL for each object stored in it, allowing easy access to the object from anywhere on the web. This URL can be shared with anyone who needs to access the object, making it easy to share files with others.\nAmazon Glacier is a low-cost storage service designed for data archiving and backup, and is optimized for infrequently accessed data. It is not designed for real-time access to data and does not provide URLs for objects stored in it.\nAmazon Storage Gateway is a hybrid storage service that enables on-premises applications to use cloud storage. It is not designed to store objects for real-time access via a URL.\nAmazon EBS (Elastic Block Store) is a block-level storage service designed to provide persistent storage for Amazon EC2 instances. It is not designed for storing objects for real-time access via a URL.\nTherefore, the most appropriate storage option for the given requirement is Amazon S3.\n\n"
}, {
  "id" : 109,
  "question" : "There is a requirement to host a database server for a minimum period of one year.\nWhich of the following would result in the least cost?\n",
  "answers" : [ {
    "id" : "d87bebfef1d748a99dc7a4b17b3e7512",
    "option" : "Spot Instances",
    "isCorrect" : "false"
  }, {
    "id" : "86953c9ef8c74025844affa339993fa1",
    "option" : "On-Demand",
    "isCorrect" : "false"
  }, {
    "id" : "4ca7ab7a886d4cb8a5d2a59dccf8a6f1",
    "option" : "No Upfront costs Reserved",
    "isCorrect" : "false"
  }, {
    "id" : "d36b58933b7d4ac58e23bc568c3d3629",
    "option" : "Partial Upfront costs Reserved.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nIf the database is going to be used for a minimum of one year at least, it is better to get Reserved Instances.\nYou can save on costs if you use partial upfront options.\nFor more information on AWS Reserved Instances, please visit the Link:\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\nA is incorrect.\nSpot instances can be terminated with fluctuations in market prices.\nUnless the question specifies a use case where high availability is not a requirement, this cannot be assumed.\nB is incorrect.\nOn-Demand is not the most cost-efficient solution.\nC is incorrect.\nNo upfront payment is required.\nHowever, it's a costlier option than Partial/All upfront payment.\nFor more information on the Reserved Instances Payment option, please check below AWS Docs:\nhttps://docs.aws.amazon.com/aws-technical-content/latest/cost-optimization-reservation-models/reserved-instance-payment-options.html\nNote:\nReserved Instances do not renew automatically.\nWhen they expire, you can continue using the EC2 instance without interruption.\nBut you are charged On-Demand rates.\nIn the above example, when the Reserved Instances that cover the T2 and C4 instances expire, you go back to paying the On-Demand rates until you terminate the instances or purchase new Reserved Instances that match the instance attributes.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-reserved-instances.html\n\nTo host a database server for a minimum period of one year, the least cost-effective option would be a Reserved Instance (RI).\nReserved Instances provide a significant discount compared to On-Demand instances. Reserved Instances are a billing option that allows customers to reserve Amazon Elastic Compute Cloud (Amazon EC2) capacity for a specific term, in exchange for a significant discount. RI's have three payment options: No upfront costs, Partial upfront costs, and Full upfront costs.\nNo Upfront costs Reserved Instances offer the lowest upfront payment, but the hourly rate is higher than Partial or Full upfront payment options. Partial Upfront Reserved Instances offer a lower hourly rate than No Upfront payment options and require a portion of the cost to be paid upfront. Full Upfront Reserved Instances offer the lowest hourly rate of all payment options, with the entire cost paid upfront.\nHowever, Reserved Instances are not the most cost-effective option if there is a requirement to host the server for a minimum period of one year, as it may not provide the flexibility to take advantage of newer and cheaper instance types or the ability to scale up or down.\nThe most cost-effective option in this scenario would be to use Spot Instances, as they are the cheapest option but come with the risk of instance termination. Spot Instances are a billing option that allows customers to bid on unused Amazon EC2 capacity, with the price fluctuating based on supply and demand. Spot Instances offer a discount of up to 90% compared to On-Demand instances. Spot Instances are suitable for workloads that are not time-sensitive and can tolerate interruptions.\nTherefore, in this scenario, the least cost-effective option would be Reserved Instances, and the most cost-effective option would be Spot Instances.\n\n"
}, {
  "id" : 110,
  "question" : "During an organization's information systems audit, the administrator is requested to provide a dossier of security and compliance reports and online service agreements between the organization and AWS.\nWhich service can they utilize to acquire this information?\n",
  "answers" : [ {
    "id" : "1530294a8f5b43f2b9a95836130f8349",
    "option" : "AWS Artifact",
    "isCorrect" : "true"
  }, {
    "id" : "30cbf39cc2614c30a662745ffb54ca3e",
    "option" : "AWS Resource Center",
    "isCorrect" : "false"
  }, {
    "id" : "6d152bf37cfa4afd96860c879d046bb7",
    "option" : "AWS Service Catalog",
    "isCorrect" : "false"
  }, {
    "id" : "0e5cacb7f099415a8b2df7befc858d5d",
    "option" : "AWS Directory Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Artifact is a comprehensive resource center to have access to the AWS' auditor-issued reports and security and compliance documentation from several renowned independent standard organizations.\nhttps://aws.amazon.com/artifact/\nOption B is INCORRECT.\nAWS Resource Center is arepository of tutorials, whitepapers, digital training, and project use cases that aid in learning the core concepts of Amazon Web Services.\nhttps://aws.amazon.com/getting-started/\nOption C is INCORRECT.\nAWS Service Catalog allows organizations to create and save their own IT service catalogs for further use.\nBut they have to be approved by AWS.\nIT service catalogs can be multi-tiered application architectures.\nhttps://docs.aws.amazon.com/servicecatalog/latest/adminguide/introduction.html\nOption D is INCORRECT.\nAWS Directory Service is an AWS tool that provides multiple ways to use Amazon Cloud Directory and Microsoft Active Directory with other AWS services.\nhttps://docs.aws.amazon.com/directoryservice/latest/admin-guide/what_is.html\n\nThe service that an administrator can utilize to acquire security and compliance reports and online service agreements between the organization and AWS is AWS Artifact.\nAWS Artifact is a service that provides on-demand access to AWS compliance reports and select online agreements, such as the AWS Customer Agreement, AWS Service Terms, and AWS Business Associate Addendum. AWS Artifact allows users to download these documents in a secure and convenient manner.\nTo access AWS Artifact, users must have an AWS account and must sign in to the AWS Management Console. From there, users can navigate to the AWS Artifact console, where they can browse and download the compliance reports and online agreements that are relevant to their organization.\nAWS Resource Center is a service that provides customers with technical and business resources to help them understand and use AWS services. It includes whitepapers, case studies, videos, and other resources that are designed to help customers optimize their use of AWS. However, it does not provide security and compliance reports or online service agreements.\nAWS Service Catalog is a service that allows organizations to create and manage catalogs of IT services that are approved for use on AWS. It does not provide security and compliance reports or online service agreements.\nAWS Directory Service is a service that allows organizations to connect their AWS resources to an existing on-premises Microsoft Active Directory or to set up and operate their own directory in the AWS Cloud. It does not provide security and compliance reports or online service agreements.\n\n"
}, {
  "id" : 111,
  "question" : "A new department has recently joined the organization and the administrator needs to compose access permissions for the group of users.\nGiven that they have various roles and access needs, what is the best-practice approach when granting access?\n",
  "answers" : [ {
    "id" : "994cb699046b471eba6b3dafaf1a4fb4",
    "option" : "After gathering information on their access needs, the administrator should allow every user to access the most common resources and privileges on the system.",
    "isCorrect" : "false"
  }, {
    "id" : "6ec1b84c2f6943fdba49196905794336",
    "option" : "The administrator should grant all users the same permissions and then grant more upon request.",
    "isCorrect" : "false"
  }, {
    "id" : "ccac4f0982e9419fb28beb550289c17e",
    "option" : "The administrator should grant all users the least privilege and add more privileges to only to those who need it.",
    "isCorrect" : "true"
  }, {
    "id" : "bec7c18bda9744ebbe9b232d4616d3bb",
    "option" : "Users should have no access and be granted temporary access on the occasions that they need to execute a task.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe best-practice for AWS Identity Access Management (IAM) is to grant the least amount of permissions on the system only to execute the required tasks of the user's role.\nAdditional permissions can be granted per user according to the tasks they wish to perform on the system.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege\nOption A is incorrect because granting users access to the most common resources presents security vulnerabilities, especially from those who have access to resources they do not need.\nOption B is incorrect because granting users the same privileges on the system means other users might get access to resources they do not need to carry out their job functions.\nThis presents a security risk.\nOption D is incorrect because the users are part of the organisation; it will be cumbersome for the administrator to create temporal access passes for internal staff constantly.\n\nThe best-practice approach when granting access to a new department with various roles and access needs is to grant all users the least privilege required to perform their job function, and then add more privileges only to those who need it.\nThis approach is commonly known as the principle of least privilege (PoLP) and is a fundamental security concept in which users are granted the minimum level of access required to perform their job functions, and nothing more. This helps to reduce the risk of accidental or intentional misuse, unauthorized access, and other security incidents.\nTo implement the principle of least privilege, the administrator should first gather information on the users' job functions and access needs, and then carefully review the resources and privileges required to perform those tasks. Based on this analysis, the administrator can then grant access to only the resources and privileges that are necessary for each user to perform their job functions.\nIt is important to note that this approach may require more upfront planning and administration to ensure that users have the necessary access to perform their job functions, but it will ultimately reduce the overall risk of security incidents and unauthorized access.\nThe other answer options are not the best practice approach to granting access to a new department with various roles and access needs.\nOption A, which involves granting every user access to the most common resources and privileges on the system, is not an appropriate approach as it may lead to users having more access than they require, increasing the risk of security incidents.\nOption B, which involves granting all users the same permissions and then granting more upon request, is also not recommended as it may lead to unnecessary access and security risks.\nOption D, which involves granting users no access and only granting temporary access when needed, may be appropriate in some situations, but is not the best practice approach as it can be inconvenient and may not provide users with the necessary access to perform their job functions.\n\n"
}, {
  "id" : 112,
  "question" : "Which of the following are advantages of having infrastructure hosted on the AWS Cloud? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "5702075c217a44c1851fa22b85795910",
    "option" : "Having complete control over the physical infrastructure",
    "isCorrect" : "false"
  }, {
    "id" : "2fcd82b8775440b98d707e57d4559a03",
    "option" : "Having the pay as you go model",
    "isCorrect" : "true"
  }, {
    "id" : "bf03867450524a23ac0dcb61c514c1e3",
    "option" : "No Upfront costs",
    "isCorrect" : "true"
  }, {
    "id" : "586d5449d4f144e39e6c8d759739d0cd",
    "option" : "Having no need to worry about security.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe Physical infrastructure is a responsibility of AWS instead of the customer.\nHence it is not an advantage of moving to the AWS Cloud.\nAnd AWS provides security mechanisms, but even the responsibility of security lies with the customer.\n\nThe two advantages of having infrastructure hosted on the AWS Cloud are:\nB. Having the pay as you go model One of the most significant advantages of hosting infrastructure on AWS is the \"pay as you go\" model. This model allows businesses to pay only for the resources they consume, without any upfront costs or long-term commitments. This means businesses can quickly scale their infrastructure up or down based on their needs, without worrying about the cost implications.\nC. No Upfront costs Another advantage of hosting infrastructure on AWS is the absence of upfront costs. In traditional on-premises infrastructure, businesses have to invest significant amounts of money upfront to purchase and install hardware and software. With AWS, businesses can avoid this capital expenditure and start using the infrastructure right away without worrying about the costs.\nA. Having complete control over the physical infrastructure and D. Having no need to worry about security are not correct answers to this question.\nRegarding A, while AWS does provide a high level of control over the infrastructure, it is not complete control over the physical infrastructure. AWS is responsible for maintaining the physical infrastructure and providing access to it, but customers are responsible for configuring, securing, and managing their own resources.\nRegarding D, while AWS provides a high level of security features and services, security is a shared responsibility between AWS and the customer. AWS is responsible for securing the underlying infrastructure, while customers are responsible for securing their own data and applications running on AWS. Therefore, customers still need to worry about security and take appropriate measures to secure their applications and data.\n\n"
}, {
  "id" : 113,
  "question" : "There is an external audit being carried out on your company.\nThe IT auditor needs to have a log of 'who made the requests' to the AWS resources in the company's account.\nWhich of the below services can assist in providing these details?\n",
  "answers" : [ {
    "id" : "e8a8ce2b29ea421d9501dda25d636866",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "b862db0c87ad4ff28f783872a9d8cae9",
    "option" : "AWS CloudTrail",
    "isCorrect" : "true"
  }, {
    "id" : "51d4130a434a4d1281256ff42fdc1b1b",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "46f2753cedaa44499e3141b9dc12bf7f",
    "option" : "AWS SNS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nUsing CloudTrail, one can monitor all the API activity conducted on all AWS services.\nThe AWS Documentation additionally mentions the following.\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nCloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nThis event history simplifies security analysis, resource change tracking, and troubleshooting.\nFor more information on AWS Cloudtrail, please refer to the below URL:\nhttps://aws.amazon.com/cloudtrail/\n\nThe service that can assist in providing the required information is AWS CloudTrail.\nAWS CloudTrail is a web service that records all API calls made in an AWS account, including who made the call, the services that were used, and the time of the call. CloudTrail provides visibility into user activity and resource changes in an AWS account. CloudTrail logs are used to track changes made to resources, to troubleshoot operational issues, and to provide compliance support.\nIn this case, the IT auditor needs to have a log of who made the requests to the AWS resources in the company's account. CloudTrail can provide this information by logging all API calls made in the account, including those made to AWS resources. The logs can be used to identify who made the requests, when the requests were made, and which resources were accessed.\nAWS CloudWatch is a monitoring service for AWS resources and applications, and it can provide metrics and logs for those resources. However, it does not log API calls like CloudTrail does.\nAWS EC2 is a virtual machine service and AWS SNS is a notification service, and neither of these services is designed to log API calls.\nTherefore, the correct answer is B. AWS CloudTrail.\n\n"
}, {
  "id" : 114,
  "question" : "A web administrator maintains several public and private web-based resources for an organisation.\nWhich service can they use to keep track of the expiry dates of SSL/TLS certificates as well as updating and renewal?\n",
  "answers" : [ {
    "id" : "9201339592ef424bb32d2363a5225e59",
    "option" : "AWS Data Lifecycle Manager",
    "isCorrect" : "false"
  }, {
    "id" : "a0487bd499894f428b8bc2d3f5f08657",
    "option" : "AWS License Manager",
    "isCorrect" : "false"
  }, {
    "id" : "44bd53e4530a4311a3cfa8f9c25c2057",
    "option" : "AWS Firewall Manager",
    "isCorrect" : "false"
  }, {
    "id" : "7be8eefa7564414ca8bd599a0ad4c976",
    "option" : "AWS Certificate Manager.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThe AWS Certificate Manager allows the web administrator to maintain one or several SSL/TLS certificates, both private and public certificates including their update and renewal so that the administrator does not worry about the imminent expiry of certificates.\nhttps://aws.amazon.com/certificate-manager/\nOption A is INCORRECT.\nThe AWS Lifecycle Manager creates life cycle policies for specified resources to automate operations.\nhttps://docs.aws.amazon.com/dlm/?id=docs_gateway\nOption B is INCORRECT.\nAWS License Manager serves the purpose of differentiating, maintaining third-party software provisioning vendor licenses.\nIt also decreases the risk of license expirations and the penalties.\nhttps://docs.aws.amazon.com/license-manager/?id=docs_gateway\nOption C is INCORRECT.\nAWS Firewall Manager aids in the administration of Web Application Firewall (WAF), by presenting a centralised point of setting firewall rules across different web resources.\nhttps://docs.aws.amazon.com/firewall-manager/?id=docs_gateway\n\nThe correct answer is D. AWS Certificate Manager.\nAWS Certificate Manager (ACM) is a service that allows you to provision, manage, and deploy SSL/TLS certificates for use with AWS services and your internal resources. It makes it easy to deploy SSL/TLS certificates on AWS resources like Elastic Load Balancers, CloudFront distributions, and API Gateways.\nOne of the features of ACM is the ability to track the expiration dates of SSL/TLS certificates and automatically renew them. ACM manages the entire certificate renewal process for you, so you don't have to worry about manually updating certificates or dealing with expired certificates.\nIn addition to automating certificate renewal, ACM also provides centralized management of your certificates, so you can easily keep track of all the certificates used by your organization. This makes it easier to maintain security and compliance standards.\nTo use ACM, you need to create a certificate request, which can be done from within the ACM console or via the AWS CLI. Once you've created a certificate request, you can use it to provision SSL/TLS certificates for use with your AWS resources.\nIn summary, AWS Certificate Manager is the service that a web administrator can use to keep track of the expiry dates of SSL/TLS certificates as well as updating and renewal. It provides centralized management of certificates and automates the renewal process, making it easy to maintain security and compliance standards.\n\n"
}, {
  "id" : 115,
  "question" : "Which of the following statements regarding billing, cost optimization and cost management in AWS is accurate?\n",
  "answers" : [ {
    "id" : "286116fcd37e4a50948f5d96dac5e51e",
    "option" : "When considering migrating to the cloud, the AWS Total Cost of Ownership (TCO) calculator is guaranteed to save up to 80% of the cost of running on-premise infrastructure.",
    "isCorrect" : "false"
  }, {
    "id" : "40a77087fbac4a3fbd1d30631df84d0f",
    "option" : "In AWS Budgets, utilizing Cost and Usage budgets will optimize and reduce the overall spend by 79%.",
    "isCorrect" : "false"
  }, {
    "id" : "af36fbd67e8045538762dcdf48e99a53",
    "option" : "The AWS Pricing Calculator will workout a revised bill that can reduce the overall spend by 60% if you commit to a long-term usage plan.",
    "isCorrect" : "false"
  }, {
    "id" : "a107ca133e4048009060aa70aedfbd61",
    "option" : "When using Savings Plans, 72% savings can be made on Amazon EC2, AWS Fargate, and AWS Lambda usage.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nSavings Plans are flexible discount pricing models that offer reduced rates if the customer commits to one year or three-year consistent usage.\nThese are confined to Amazon EC2, AWS Fargate, and AWS Lambda usage.\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html\nOption A is INCORRECT because the AWS Total Cost of Ownership (TCO) calculator is an estimation tool.\nIt does not guarantee saving up 80% of the cost of running on-premise infrastructure.\nHowever, the tool allows the customer to estimate and anticipate their total AWS spend according the their use case.\nOption B is INCORRECT because in AWS Budgets, utilizing Cost and Usage budgets will give the customer foresight into how much they would like to use and spend on their AWS services.\nUtilizing this service will not reduce the overall spend by an exact percentage.\nTherefore this statement is inaccurate.\nOption C is INCORRECT because the AWS Pricing Calculator does not revise the customer bill.\nIt allows the customer to derive an estimation of the cost of their AWS resources before the costs are incurred.\nTherefore this statement is inaccurate.\n\nWhen it comes to billing, cost optimization, and cost management in AWS, it is important to understand the various tools and services available to optimize costs and reduce overall spend.\nA. The AWS Total Cost of Ownership (TCO) calculator is a tool that helps customers compare the cost of running their applications on-premises versus in the AWS Cloud. However, it cannot guarantee an 80% cost savings. The actual savings will depend on various factors such as the workload, application requirements, and utilization patterns. Therefore, option A is incorrect.\nB. AWS Budgets is a service that helps customers to set custom cost and usage budgets that alert them when their spend exceeds their set limit. While utilizing cost and usage budgets can help customers optimize their spend, it cannot guarantee a 79% reduction in overall spend. Therefore, option B is also incorrect.\nC. The AWS Pricing Calculator is a service that allows customers to estimate their monthly bill based on their usage and services utilized. It provides the ability to estimate costs for long-term usage plans, such as Reserved Instances, Savings Plans, and Spot Instances. However, it cannot guarantee a 60% reduction in overall spend. The actual savings will depend on various factors such as the usage plan, the term commitment, and the services utilized. Therefore, option C is also incorrect.\nD. Savings Plans are a flexible pricing model that offer significant savings on AWS usage. When using Savings Plans, customers can save up to 72% on Amazon EC2, AWS Fargate, and AWS Lambda usage. This is because Savings Plans provide discounted pricing for a commitment to use a specific amount of compute usage (measured in dollars per hour) over a one or three-year term. Therefore, option D is the correct answer.\nIn conclusion, when it comes to billing, cost optimization, and cost management in AWS, it is important to understand the various tools and services available to optimize costs and reduce overall spend. Utilizing services like AWS Budgets, the AWS Pricing Calculator, and Savings Plans can help customers optimize their spend and reduce costs. However, it is important to note that these services cannot guarantee a specific percentage of savings as the actual savings will depend on various factors.\n\n"
}, {
  "id" : 116,
  "question" : "Which of the following features can be used to preview changes to be made to an AWS resource which will be deployed using the AWS CloudFormation template?\n",
  "answers" : [ {
    "id" : "ea44f52c388f46cb8bb4fbc7f4a8e2d9",
    "option" : "AWS CloudFormation Drift Detection",
    "isCorrect" : "false"
  }, {
    "id" : "fb3bfb312f3f4c3d9f555e6c30857c84",
    "option" : "AWS CloudFormation Change Sets",
    "isCorrect" : "true"
  }, {
    "id" : "6ba8d28315b64d79bb841b8018168b84",
    "option" : "AWS CloudFormation Stack Sets",
    "isCorrect" : "false"
  }, {
    "id" : "574784fef3824150b2c8cde2554a4366",
    "option" : "AWS CloudFormation Intrinsic Functions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS CloudFormation Change Set can be used to preview changes to AWS resources when a stack is executed.\nOption A is incorrect as AWS CloudFormation Drift Detection is used to detect any changes made to resources outside of CloudFormation templates.\nIt would not be able to preview changes that will be made by CloudFormation Templates.\nOption C is incorrect as these are groups of stacks that are managed together.\nOption D is incorrect as these Intrinsic Functions are used for assigning values to properties in CloudFormation templates.\nFor more information on AWS CloudFormation, refer to the following URL:\nhttps://aws.amazon.com/cloudformation/features/\n\nThe correct answer is B. AWS CloudFormation Change Sets.\nAWS CloudFormation is a service that allows you to model and provision AWS infrastructure resources using templates. You can use a template to create, update, and delete a collection of resources together as a single unit, which is called a stack.\nWhen you make changes to a stack, you must update it with the new changes. AWS CloudFormation Change Sets is a feature that helps you preview changes to a stack before you apply them. Change Sets give you a way to see how the changes will impact the resources in the stack and whether the changes will be successful.\nHere's how Change Sets work:\nYou create a Change Set from the stack you want to change. AWS CloudFormation analyzes the changes and generates a report. You review the report and make any necessary changes. You apply the Change Set to update the stack with the new changes.\nChange Sets are useful because they help you avoid unexpected changes or downtime that could occur from updating a stack. Instead, you can preview the changes before making them and ensure that they will work as intended.\nLet's briefly look at the other options provided:\nA. AWS CloudFormation Drift Detection: This feature is used to identify differences between the expected configuration of your resources defined in your CloudFormation template and the actual configuration of your resources. Drift detection does not help you preview changes to a resource; it only identifies differences between the expected and actual state of the resource.\nC. AWS CloudFormation Stack Sets: This feature allows you to deploy CloudFormation stacks across multiple accounts and regions with a single CloudFormation template. Stack Sets do not help you preview changes to a resource; they are used for deploying templates across multiple accounts.\nD. AWS CloudFormation Intrinsic Functions: These functions are used within a CloudFormation template to assign values dynamically, such as referencing other resources within the same stack. They do not help you preview changes to a resource.\n\n"
}, {
  "id" : 117,
  "question" : "Which option best suits the implementation of an Amazon RDS database instance instead of a NoSQL/non-relational database?\n",
  "answers" : [ {
    "id" : "e0ee2e52741e4889a424198d2a83c6bb",
    "option" : "Where datasets are constantly evolving and cannot be confined to a static data schema.",
    "isCorrect" : "false"
  }, {
    "id" : "691f2402517f4fed8053c5a55f223c15",
    "option" : "Where vertical scaling of the databaseâ€™s resources is not permissible and is seldom necessary.",
    "isCorrect" : "false"
  }, {
    "id" : "9a6132747e024ceda9e8aba3c4504db9",
    "option" : "In an organisation whose datasets are dynamic and document-based.",
    "isCorrect" : "false"
  }, {
    "id" : "eb076710f7234e00a88e4dbb7977e45c",
    "option" : "In an organisation where only a finite number of processes query the database in predictable and well-structured Schemas.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAmazon Relational databases service (RDS) is best suited in scenarios where the dataset and forms are consistent such that their data schema is persistently valid.\nIt is best to deploy in an environment where the load can be anticipated and is somewhat finite.\nAmazon RDS engines include Amazon Aurora, MariaDB, PostgreSQL-\nhttps://aws.amazon.com/rds/\nOption A is INCORRECT because Amazon RDS engines are inappropriate in a scenario where datasets are constantly evolving and the data schema is flexible.\nNoSQL/non-relational databases fit this use case.\nOption B is INCORRECT because Amazon Relational Database service engines will scale up with the increase in load.\nIt is often necessary as the traffic patterns to the database increases.\nOption C is INCORRECT because in a scenario where the datasets are dynamic and document-based, the use of JSON and not SQL is appropriate.\nTherefore non-relationals/NoSQL database engines such as Amazon DynamoDB are suitable.\nhttps://aws.amazon.com/nosql/\n\nThe correct answer is D. In an organization where only a finite number of processes query the database in predictable and well-structured Schemas.\nExplanation: Amazon RDS (Relational Database Service) is a managed service that makes it easier to set up, operate, and scale a relational database in the cloud. It supports several popular relational database engines like MySQL, PostgreSQL, Oracle, and Microsoft SQL Server.\nIn contrast, NoSQL/non-relational databases like Amazon DynamoDB are designed for dynamic, non-structured datasets that can be rapidly and easily scaled horizontally, and whose data schema can evolve frequently.\nOption A is incorrect because it describes a use case for a NoSQL/non-relational database like Amazon DynamoDB, which is better suited for constantly evolving datasets.\nOption B is also incorrect because Amazon RDS allows both vertical and horizontal scaling of database resources.\nOption C is incorrect because it describes a use case for a document-based NoSQL database like Amazon DocumentDB or MongoDB.\nOption D is the correct answer because it describes a use case for a traditional relational database like Amazon RDS, which is best suited for organizations that have predictable and well-structured schemas and only a finite number of processes querying the database. Relational databases excel at handling complex queries and data relationships and are often used for transactional workloads.\n\n"
}, {
  "id" : 118,
  "question" : "While making changes to AWS resources e.g.\nadding a new Security Group Ingress rule, I need to capture &amp; record all these changes that will be helpful during an audit.\nWhich of the following AWS service helps me do that?\n",
  "answers" : [ {
    "id" : "dff8d57bc888404b9536ef68e11903c2",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "c2709e3f285a450e9f65c12fd1a5ffc5",
    "option" : "AWS CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "021a16a72f68402880f1e17dc74c2c6b",
    "option" : "AWS Config",
    "isCorrect" : "true"
  }, {
    "id" : "742cfa1799ac44298175efcf4c10b6a7",
    "option" : "AWS CloudFormation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect because AWS Trusted Advisor cannot record the details of configuration changes in the AWS account.\nOption B is incorrect because CloudWatch is a monitoring tool that captures different metrics like CPU utilization, Memory Utilization etc.\nOnce the data is captured, they can then be used for creating dashboards for displaying usage patterns, creating alarms for automating resource creation, e.g.\ncreating a new EC2 instance due to average CPU utilization of an Auto Scaling group going above 70%\nOption C is CORRECT.\nAWS Config records &amp; captures all configuration changes done to AWS resources using the Configuration Recorder.\nConfiguration Items crated by AWS Config can be sent to S3 to be stored as log files.\nThese log files can be retained depending on the S3 lifecycle policies defined &amp; can be referred to during any audit.\nUsing an automated configuration management tool helps an Organization to track compliance of its resources elegantly.\nOption D is incorrect because AWS CloudFormation is used for automating the creation of AWS resources in Organizations that are huge and use a complex infrastructure that may be difficult to create manually.\nReferences:\nhttps://aws.amazon.com/config/\nhttps://youtu.be/kcwy_DWU8ao\n\nThe AWS service that can help capture and record changes made to AWS resources for auditing purposes is AWS Config.\nAWS Config is a service that provides a detailed inventory of the resources in an AWS account and captures a history of configuration changes made to those resources. It also allows you to define rules for monitoring resource configurations and receive alerts if a resource configuration violates those rules.\nWhen you make changes to AWS resources, AWS Config can capture those changes and record them in a configuration history. This allows you to track changes to your resources over time and helps with compliance and auditing requirements.\nIn contrast, AWS Trusted Advisor is a service that provides recommendations for optimizing your AWS resources and improving performance, security, and cost efficiency. It does not provide the ability to capture and record changes made to AWS resources.\nAWS CloudWatch is a monitoring service that can collect and track metrics, collect and monitor log files, and set alarms. It does not provide the ability to capture and record changes made to AWS resources.\nAWS CloudFormation is a service that provides a way to automate the deployment of infrastructure as code. It does not provide the ability to capture and record changes made to AWS resources for auditing purposes. However, it can be used to create AWS Config rules to monitor and enforce compliance with resource configurations.\n\n"
}, {
  "id" : 119,
  "question" : "AWS Organizations help manage multiple accounts effectively in a large enterprise.\nWhich of the following statements related to AWS Organizations are correct? (Select TWO.)\n",
  "answers" : [ {
    "id" : "0975f989f20d4cd0893d3d8a1f8e4cbf",
    "option" : "An Organizational Unit(OU) can have only one parent.",
    "isCorrect" : "true"
  }, {
    "id" : "41c56550d43e42eeaf36e2709b131d9a",
    "option" : "An account can be a member of multiple Organizational Units (OU).",
    "isCorrect" : "false"
  }, {
    "id" : "85b26047dce34ef9a31558b750e7d039",
    "option" : "An SCP policy only impacts a particular AWS account even if it is applied at the root account.",
    "isCorrect" : "false"
  }, {
    "id" : "a06fd3bff1ef47eda1d07bdf3b7a3f55",
    "option" : "Organizational level policies are known as Service Control Policies.",
    "isCorrect" : "true"
  }, {
    "id" : "13053e0ca81c462da3aef6cfed791cd4",
    "option" : "Service Control Policies (SCPs) can only allow actions instead of deny actions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: A, D.\nOption A is CORRECT.An Organizational Unit(OU) can have a single branch going up, e.g.\nIt can either inherit a root or another OU but not both as shown in the figure below.\nOption B is incorrect since an Account can belong to only one OU.\nOption C is incorrect.\nA Policy applied at the Root is applied throughout the Organization i.e.\nto all its OU's and its Accounts.\nA Policy applied to the OU level applies to all OU's and Accounts under those OU's.\nA Policy applied at the Account level is applied to only that Account.\nReferring to the figure above, when a Policy is applied to the OU under the Root, it will also be applied to the OU below it &amp; Accounts B, C,\nD.\nWhen a policy is applied to Account C, it will apply to only that account.\nOption D is CORRECT.\nAWS Organizations automate creation of AWS Accounts, OUs and their hierarchy.\nThey use Service Control Policies (SCP) at OUs.\nSCPs are different from IAM in the sense that they can be applied to the Organization level.\nThey override any IAM policies that are defined at an Account level &amp; may also restrict the IAM policy defined.\nAWS Organizations do not cancel the need for IAM.\nIt compliments what IAM can do by consolidating and centrally managing a lot of things that happen.\nAWS Organizations is not an authority for granting permissions, but it is an authority to approve/disapprove permissions given by IAM.\nOption E is incorrect.\nSCPs can be configured to allow or deny services and actions.\nReferences:\nAWS Organizations user guide.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_introduction.html\nService Control Policies.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\n\n\nAWS Organizations is a service that allows you to manage multiple AWS accounts, and to organize and govern them as a single entity, called an organization. This service helps you to simplify the management of accounts and apply policies across accounts, and it is mainly used by enterprises with multiple AWS accounts.\nNow let's discuss each statement in detail:\nA. An Organizational Unit (OU) can have only one parent. This statement is correct. An organizational unit is a container for accounts, and it can have only one parent. This means that you can organize your accounts hierarchically, and each account can be part of only one OU.\nB. An account can be a member of multiple Organizational Units (OU). This statement is incorrect. An account can only be a member of one OU. However, you can create nested OUs to organize your accounts in a more granular way.\nC. An SCP policy only impacts a particular AWS account even if it is applied at the root account. This statement is incorrect. When you apply a Service Control Policy (SCP) at the root level of your organization, it affects all accounts and OUs within that organization. SCPs are used to restrict the actions that IAM users and roles can perform in AWS services and resources.\nD. Organizational level policies are known as Service Control Policies. This statement is correct. SCPs are the policies that you can use to set controls at the organization level. SCPs allow you to set controls that restrict the actions that IAM users and roles can perform in AWS services and resources.\nE. Service Control Policies (SCPs) can only allow actions instead of deny actions. This statement is incorrect. SCPs can allow or deny actions. By default, an SCP denies all actions, but you can use the \"Allow\" statement to specify the actions that you want to permit.\nIn summary, the correct statements are A and D.\n\n"
}, {
  "id" : 120,
  "question" : "Which of the following is WRONG for NoSQL databases? (Select TWO.)\n",
  "answers" : [ {
    "id" : "2d7b7221385e4423bce7de13f145097a",
    "option" : "They are not relational.",
    "isCorrect" : "false"
  }, {
    "id" : "4f5ba2ad5a6944ee82c16440f7eaf747",
    "option" : "They need to have a well defined schema.",
    "isCorrect" : "true"
  }, {
    "id" : "d6972d9c5ab6476495a41ed7fddf0d77",
    "option" : "DynamoDB Transactions is NOT atomicity, consistency, isolation, and durability (ACID) compliant.",
    "isCorrect" : "true"
  }, {
    "id" : "cfa8328b61c149d0b8b5c0c8429eebbb",
    "option" : "NoSQL databases are horizontally scalable.",
    "isCorrect" : "false"
  }, {
    "id" : "99294d673f434e4b9579e6735bfdb415",
    "option" : "A patientâ€™s record in a hospital system with changing data for every visit is a good candidate to be modelled using a NoSQL database.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: B, C.\nOption A is incorrect since NoSQL databases are not relational.\nThey support data that are semi-structured or unstructured as compared to the structured nature of relational databases like Oracle, MySQL.\nOption B is CORRECT.NoSQL databases do not support a predefined schema like a relational database does (e.g.\nA record of type Book will have a fixed set of attributes defining a schema like ID, Name, Description, Author)\nNot defining a rigid schema allows NoSQL databases the flexibility to support semi-structured &amp; unstructured data.\nOption C is CORRECT.\nDynamoDB transactions provide developers atomicity, consistency, isolation, and durability (ACID) across one or more tables within a single AWS account and region.\nThe details can be found in https://aws.amazon.com/cn/blogs/aws/new-amazon-dynamodb-transactions/.\nOption D is incorrect.\nNoSQL databases are usually run in compute node clusters with data being partitioned across these nodes.\nPartitioning happens automatically with an increase in database size resulting in horizontal scaling.\nOption E is incorrect.\nA Patient's medical record during hospital visits may be updated by multiple people e.g.\nBilling information, Medicines, BP, Height, Weight etc...Defining a person's medical history in a structured format will be impractical &amp; inefficient.\nAnother way to look at a patient's medical record is as a set of documents with a new document being added during every visit with additional information.\nDiagram:\nReferences:\nIntro to AWS Database services.\nhttps://youtu.be/eKyS9rvbj40\n2\nAWS database services documentation.\nhttps://aws.amazon.com/products/databases/\n\n\nThe two options that are incorrect for NoSQL databases are:\nB. They need to have a well-defined schema. C. DynamoDB Transactions is NOT atomicity, consistency, isolation, and durability (ACID) compliant.\nHere is a detailed explanation of why these options are incorrect:\nA. They are not relational. NoSQL databases are non-relational databases, which means they don't use the traditional SQL-based relational database management system (RDBMS) model. Instead, NoSQL databases use a flexible schema model, which allows them to handle unstructured, semi-structured, and structured data. NoSQL databases can handle large volumes of structured and unstructured data and can scale horizontally, making them an ideal choice for handling big data and real-time applications.\nB. They need to have a well-defined schema. NoSQL databases are designed to handle unstructured and semi-structured data, and they don't need a well-defined schema. Unlike RDBMS, NoSQL databases don't enforce a specific data model, which means they can handle data that is constantly changing or has an unknown schema. NoSQL databases provide greater flexibility and scalability compared to traditional RDBMS databases.\nC. DynamoDB Transactions is NOT atomicity, consistency, isolation, and durability (ACID) compliant. DynamoDB is a fully managed NoSQL database service provided by AWS. It is designed to provide high performance, scalability, and availability. DynamoDB supports transactions, which allows users to group multiple write operations into a single, all-or-nothing operation. However, DynamoDB transactions are not ACID-compliant. While DynamoDB provides atomicity and isolation, it doesn't provide strong consistency, which is a critical aspect of ACID compliance.\nD. NoSQL databases are horizontally scalable. One of the key benefits of NoSQL databases is that they are horizontally scalable. This means that as the data grows, users can add more machines or nodes to the database cluster to handle the increased load. Unlike RDBMS, which typically scale vertically, NoSQL databases can scale horizontally without sacrificing performance or availability.\nE. A patient's record in a hospital system with changing data for every visit is a good candidate to be modeled using a NoSQL database. NoSQL databases are an ideal choice for handling unstructured and semi-structured data, making them a good fit for a patient's record in a hospital system with changing data for every visit. NoSQL databases can store data in a flexible schema, which allows them to handle data that is constantly changing or has an unknown schema. Additionally, NoSQL databases can handle large volumes of data and can scale horizontally, making them an ideal choice for handling big data and real-time applications.\n\n"
}, {
  "id" : 121,
  "question" : "What is a valid difference between AWS Global Accelerator and Amazon CloudFront? Choose TWO responses.\n",
  "answers" : [ {
    "id" : "7ce4ee452b9e4a9ab1be96c314828eb9",
    "option" : "AWS Global Accelerator uses the Anycast techniques to accelerate latency-sensitive applications Amazon CloudFront uses Unicast.",
    "isCorrect" : "false"
  }, {
    "id" : "a892aa54828d47d9856034f429b998dd",
    "option" : "Amazon CloudFront makes use of Edge Locations and edge infrastructure, whilst AWS Global Accelerator does not.",
    "isCorrect" : "false"
  }, {
    "id" : "05ddc8f8904f49b89c0cb7a45f68f109",
    "option" : "AWS Global Accelerator does not include the content caching capability that Amazon CloudFront does.",
    "isCorrect" : "true"
  }, {
    "id" : "133a2dea65634665881d63407bc5c549",
    "option" : "AWS Global Accelerator is suitable for applications that are non-HTTP/S such as VoIP, MTTQ and gaming whereas Amazon CloudFront enhances the performance of HTTP-based content such as dynamic web applications, images and videos.",
    "isCorrect" : "true"
  }, {
    "id" : "84ea9f67237c47d4968f54a46eeefe9a",
    "option" : "For the resource endpoint, Amazon CloudFront offers static public IP addresses whilst AWS Global Accelerator does not.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C, D.\nAWS Global Accelerator uses the highly available, high-speed AWS global network and anycast routing techniques to greatly improve the availability and network performance of the customer application.\nBy leveraging Edge Locations and edge infrastructure traffic to and from customer application endpoints ingresses and egresses the AWS global network at geographically closer locations to clients.\nAmazon CloudFront is a content delivery network (CDN) that improves the performance of cacheable web content, like videos, images, using content caches at Edge Locations.\nhttps://aws.amazon.com/global-accelerator/faqs/\nhttps://youtu.be/GAxrPQ3ycsQ\nhttps://youtu.be/AT-nHW3_SVI\nOption A is INCORRECT because Amazon CloudFront does not use Unicast techniques.\nInstead, it uses a content caching mechanism in delivering enhanced web application performance.\nOption B is INCORRECT because both AWS Global Accelerator and Amazon CloudFront service make use of Edge Locations and edge infrastructure on the AWS Global network.\nOption E is INCORRECT because Global Accelerator provides static public IP addresses for the customer resource endpoints, whilst the fully-qualified domain name of the Amazon CloudFront distribution can resolve to dynamic public IP addresses.\n\nAWS Global Accelerator and Amazon CloudFront are both AWS services that improve the performance, availability, and security of applications running on AWS or on-premises. While both services work towards achieving similar goals, they differ in several ways. Let's look at the two valid differences between AWS Global Accelerator and Amazon CloudFront:\nA. AWS Global Accelerator uses the Anycast technique to accelerate latency-sensitive applications, while Amazon CloudFront uses Unicast.\nAWS Global Accelerator uses Anycast, a network addressing and routing methodology, to direct traffic to the optimal AWS endpoint. Anycast allows AWS to advertise the same IP address from multiple locations worldwide, and the traffic is automatically routed to the nearest AWS endpoint with the lowest latency.\nOn the other hand, Amazon CloudFront uses Unicast to distribute content from the edge locations to the users. Unicast is a traditional method that sends packets from one sender to one receiver.\nB. Amazon CloudFront makes use of Edge Locations and edge infrastructure, while AWS Global Accelerator does not.\nAmazon CloudFront is a content delivery network (CDN) service that uses a global network of edge locations to deliver static and dynamic web content to end-users. The edge locations are geographically distributed and cache content closer to the users, reducing the latency and increasing the performance of the applications.\nIn contrast, AWS Global Accelerator uses Anycast to route traffic to the optimal AWS endpoint, without caching the content at the edge locations. AWS Global Accelerator is designed to accelerate the traffic for a wide range of applications, including those that are not HTTP-based, such as gaming, VoIP, and IoT.\nC. AWS Global Accelerator does not include the content caching capability that Amazon CloudFront does.\nThis option is incorrect. AWS Global Accelerator does not cache content at the edge locations, but it uses the Anycast technique to route traffic to the optimal AWS endpoint, improving the performance of the applications.\nD. AWS Global Accelerator is suitable for applications that are non-HTTP/S, such as VoIP, MQTT, and gaming, whereas Amazon CloudFront enhances the performance of HTTP-based content such as dynamic web applications, images, and videos.\nAWS Global Accelerator is designed to accelerate the traffic for a wide range of applications, including those that are not HTTP-based, such as gaming, VoIP, and IoT. AWS Global Accelerator improves the performance of any TCP-based application that requires a stable and reliable network connection, regardless of the protocol or port.\nIn contrast, Amazon CloudFront is a content delivery network (CDN) service that is designed to improve the performance of HTTP-based content, such as dynamic web applications, images, and videos.\nE. For the resource endpoint, Amazon CloudFront offers static public IP addresses, while AWS Global Accelerator does not.\nThis option is incorrect. Both AWS Global Accelerator and Amazon CloudFront support the use of static IP addresses for the resource endpoint. AWS Global Accelerator provides static anycast IP addresses, and Amazon CloudFront provides static IP addresses for the origin server.\n\n"
}, {
  "id" : 122,
  "question" : "Which of the following is the responsibility of the customer to ensure the availability and backup of the EBS volumes?\n",
  "answers" : [ {
    "id" : "f3880f36d6794349a2dfcfa838c98af2",
    "option" : "Delete the data and create a new EBS volume.",
    "isCorrect" : "false"
  }, {
    "id" : "66644336c8994ce791811f3108710538",
    "option" : "Create EBS snapshots.",
    "isCorrect" : "false"
  }, {
    "id" : "dfe237836d624956971f8329c9465a67",
    "option" : "Attach new volumes to EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "533a42133b294fa68e6b7944ceb514e2",
    "option" : "Create copies of EBS Volumes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nThe correct answer is B. Create EBS snapshots.\nAmazon Elastic Block Store (EBS) is a service that provides persistent block-level storage volumes for use with Amazon EC2 instances. EBS volumes are automatically replicated within an Availability Zone (AZ) to protect against component failure. However, it is the responsibility of the customer to ensure the availability and backup of their data stored on EBS volumes.\nHere is an explanation of each answer choice:\nA. Delete the data and create a new EBS volume: This is not a recommended way to ensure availability and backup of EBS volumes. Deleting the data would result in a loss of data, and creating a new EBS volume would require the customer to restore the data from a backup.\nB. Create EBS snapshots: This is the correct answer. EBS snapshots are point-in-time copies of EBS volumes that can be used to back up data, migrate data to a new EBS volume, or create new EBS volumes. Snapshots are stored in Amazon S3, and are incremental backups that only store the data that has changed since the last snapshot.\nC. Attach new volumes to EC2 Instances: While attaching new volumes to EC2 instances is a necessary step to access the data on the volumes, it is not a way to ensure availability and backup of the data.\nD. Create copies of EBS Volumes: Creating copies of EBS volumes is similar to creating snapshots, but it creates a duplicate of the entire volume instead of an incremental backup. This can be useful for creating backups, but it is not the recommended way to ensure availability and backup of EBS volumes.\nIn summary, the customer is responsible for ensuring\n\n"
}, {
  "id" : 123,
  "question" : "Which AWS service gives the user the ability to group AWS resources across different AWS Regions by application and then collectively view their operational data for monitoring purposes?\n",
  "answers" : [ {
    "id" : "9da2a5a6eec0457baf69f08ee61b8e15",
    "option" : "Systems Manager",
    "isCorrect" : "true"
  }, {
    "id" : "6691962335f84aa09c46da06ff1f5b4c",
    "option" : "Management Console",
    "isCorrect" : "false"
  }, {
    "id" : "f8265e9125544479bbe55022aef7c453",
    "option" : "Resource Groups",
    "isCorrect" : "false"
  }, {
    "id" : "ee86f418132c441e9d41f1e60d61a45b",
    "option" : "Resource Access Manager (AWS RAM)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Systems Manager allows users to control their AWS resources by unifying services into a user interface.\nOne in which they can be able to view, automate and monitor operational tasks.\nhttps://aws.amazon.com/systems-manager/\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/what-is-systems-manager.html\nOption B is incorrect because the Manage Console is a web-based graphical user interface that users interact with when administering AWS services and resources.\nhttps://docs.aws.amazon.com/awsconsolehelpdocs/latest/gsg/getting-started.html?id=docs_gateway#learn-whats-new\nOption C is incorrect because Resource Groups are a collection of AWS resources within a single AWS Region.\nIn the scenario, the AWS resources are in different AWS Regions.\nhttps://docs.aws.amazon.com/ARG/latest/userguide/welcome.html\nOption D is incorrect because Resource Access Manager (AWS RAM) allows users to share resources with other AWS accounts or via AWS Organizations.\nhttps://docs.aws.amazon.com/ram/latest/userguide/what-is.html\n\nThe AWS service that allows grouping of AWS resources across different AWS Regions by application and then collectively viewing their operational data for monitoring purposes is Resource Groups.\nResource Groups is a free service provided by AWS that enables the user to create, manage, and view collections of resources across different regions and accounts. It helps in organizing resources based on different criteria like application, environment, cost center, and many more.\nBy creating resource groups, users can get a consolidated view of their resources in a single location without having to navigate between different AWS services. This helps in simplifying resource management and monitoring.\nResource Groups also provides a unified search functionality that allows users to search for resources based on different criteria like resource type, tags, region, and many more. This makes it easier to find specific resources across different regions and accounts.\nTo create a resource group, the user needs to define a group of tags or use existing tags that have already been applied to their resources. The user can then create a group using the Resource Groups console, CLI or API. Once a resource group is created, the user can add or remove resources based on their requirements.\nIn summary, Resource Groups is a powerful service provided by AWS that enables users to organize, monitor and manage their resources across different regions and accounts in a single location.\n\n"
}, {
  "id" : 124,
  "question" : "Which of the following is a situation that would require using both Spot and Reserved EC2 Instances?\n",
  "answers" : [ {
    "id" : "ee9b94f71442430e8a07c190c4a546c0",
    "option" : "A build that has sudden unpredictable workload spikes but for a short time horizon.",
    "isCorrect" : "false"
  }, {
    "id" : "a99a7a765e7a456b9afcaa73255f7ad2",
    "option" : "One in which there is a predictable resource demand over a long time horizon.",
    "isCorrect" : "false"
  }, {
    "id" : "a79acf61f68b48ea89a94ab161974576",
    "option" : "One that has unpredictable spikes for a long time.",
    "isCorrect" : "false"
  }, {
    "id" : "a5ea8adeda3549168f7b1ae6f0de8ac7",
    "option" : "One that has a constantly predictable workload with brief unpredictable spikes.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nIn cases that are characterised by a constantly predictable workload with brief unpredictable spikes, Amazon EC2 Reserved Instances would be the most cost-effective to meet the constantly predictable workload.\nWhilst Spot Instances in an auto-scaling group would suffice to meet the demands of the build.\nhttps://aws.amazon.com/solutions/case-studies/mercadolibre-ec2/\nOption A is INCORRECT because this use case would be cost-effectively serviced by Amazon EC2 Reserved Instances with on-demand instances in an Auto Scaling group to meet the resource demands of the spike.\nOption B is INCORRECT because this use case would be cost-effectively serviced by Amazon EC2 Reserved Instances alone.\nOption C is INCORRECT because this use case would be cost-effectively serviced by Amazon EC2 On-demand Instances in an Auto Scaling group to meet the resource demands of the spike.\n\nAWS EC2 instances can be purchased on-demand, reserved, or via the Spot Market. Each purchase option has its own pros and cons, and depending on the use case, a combination of these purchase options might be ideal.\nThe Spot Market is designed to sell unused EC2 capacity at a discounted price. These unused resources are available for a limited time, and the pricing model for Spot Instances is a bidding system. Spot Instances offer significant cost savings for workloads that are flexible in terms of availability and can be interrupted, as the instances are reclaimed if the spot price goes above the bid price.\nReserved Instances are an ideal option for workloads that have a steady-state, predictable workload. By committing to a specific EC2 instance type for a fixed term, customers can realize a significant cost savings.\nIn the given options, the situation that would require using both Spot and Reserved EC2 instances is option A, which has sudden unpredictable workload spikes but for a short time horizon.\nHere's why:\nThe sudden unpredictable workload spikes suggest that the workload is variable and is not predictable over the long term. This makes it difficult to purchase all instances as Reserved Instances since some of the instances may be underutilized. However, the workload spikes are for a short time horizon. This means that it would not be practical to purchase all the required instances via the Spot Market because it would be challenging to continually manage the Spot bidding process and allocate resources at the precise moment that they are needed. Therefore, a combination of both Reserved Instances and Spot Instances would be ideal for this situation. Reserved instances could be purchased for the baseline workload, while Spot Instances could be used to handle any sudden workload spikes. This would allow the workload to scale up or down as needed, providing both flexibility and cost savings.\nOption B has a predictable resource demand over a long time horizon, making it ideal for Reserved Instances but not Spot Instances. Option C has unpredictable spikes for a long time, making it challenging to use Spot Instances efficiently. Option D has a constantly predictable workload with brief unpredictable spikes, making it ideal for Reserved Instances with On-Demand instances to handle the brief unpredictable spikes.\n\n"
}, {
  "id" : 125,
  "question" : "When designing a highly available architecture, what is the difference between vertical scaling (scaling-up) and horizontal scaling (scaling-out)?\n",
  "answers" : [ {
    "id" : "330ebe0f313e4dc697589f7ea821f39c",
    "option" : "Scaling up provides for high availability whilst scaling out brings fault-tolerance.",
    "isCorrect" : "false"
  }, {
    "id" : "4ef5282268c2441fb39196952ccfb90c",
    "option" : "Scaling out is not cost-effective compared to scaling up.",
    "isCorrect" : "false"
  }, {
    "id" : "693657a1ec4549ee85d7c587bb8f9337",
    "option" : "Scaling up adds more resources to an instance, scaling out adds more instances.",
    "isCorrect" : "true"
  }, {
    "id" : "1cd27f31bc1b4f56b04bc7d4073d240a",
    "option" : "Autoscaling groups require scaling up whilst launch configurations use scaling out.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nIn high availability architectures, Autoscaling is used to give elasticity to the design.\nHorizontal scaling (scaling-out) uses Autoscaling groups to increase processing capacity in response to changes in preset threshold parameters.\nIt could involve adding more EC2 instances of a web server.\nVertical scaling (scaling-up), which can create a single point of failure, involves adding more resources to a particular instance to meet demand.\nhttps://docs.aws.amazon.com/autoscaling/plans/userguide/what-is-aws-auto-scaling.html\nOption A is INCORRECT.\nScaling-up does not provide high availability.\nAdding more resources to one instance is often not a best-practice in architecture design.\nOption B is INCORRECT.\nScaling-out is cost-effective since it involves adding more resources in response to demand and reducing resources (scaling down) when demand is low.\nOption D is INCORRECT.\nAll Autoscaling groups require a launch configuration based on what resources would be provisioned or deprovisioned to meet predefined parameters.\n\nWhen designing a highly available architecture, there are two scaling methods that can be used: vertical scaling (scaling-up) and horizontal scaling (scaling-out).\nVertical Scaling: It involves adding more resources to a single instance to improve its performance. This means increasing the size of the instance by adding more CPU, RAM, or storage. For example, upgrading an instance from a t2.micro to a t2.large is a form of vertical scaling. Vertical scaling can help to improve the performance of an application, but it has limitations in terms of how far it can be scaled due to the hardware limitations of the instance.\nHorizontal Scaling: It involves adding more instances to a system to improve its performance. This means adding more compute nodes, such as adding more servers to a web application. For example, adding more EC2 instances to a load balancer is a form of horizontal scaling. Horizontal scaling allows you to increase the capacity of your application by distributing the load across multiple instances.\nThe main difference between vertical and horizontal scaling is that vertical scaling adds more resources to a single instance, while horizontal scaling adds more instances. Vertical scaling is typically used for applications that require high performance but can be limited by hardware constraints. Horizontal scaling is used for applications that require fault tolerance and high availability by spreading the load across multiple instances.\nAnswer C is the correct answer.\n\n"
}, {
  "id" : 126,
  "question" : "A weather tracking system is designed to track weather conditions of any particular flight route.\nFlight travellers all over the world make use of this information prior to booking their flights.\nTravellers expect quick turnaround time in which the weather display &amp; flight booking will happen which is critical to their business.\nYou have designed this website and are using AWS Route 53 DNS.\nThe routing policy that you will apply to this website is.\n",
  "answers" : [ {
    "id" : "478f202afbc545538693426116e1f2bf",
    "option" : "GeoLocation routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "2daefcb92fc74a8196dab503e9b81236",
    "option" : "Failover routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "aed4c210be1a4c77b8c8f941c3860859",
    "option" : "Multivalueanswer routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "5e375a843f694c8a87e945a9801f8c2f",
    "option" : "Latency based routing policy.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOn reading the scenario carefully, we can see here that the website's performance is of prime importance to its users.\nIt gives them a lot of business value, enabling them to choose their flight paths and make flight bookings on time.\nSo, â€œLatency based routingâ€ is the best answer to this scenario.\nOption A is incorrect because GeoLocation routing is often used to localize content and present the website in the language of its users.\nGeolocation routing lets you choose the resources that serve your traffic based on your users' geographic location, meaning the location that DNS queries originate from.\nFor example, you might want all queries from Europe to be routed to an ELB load balancer in the Frankfurt region irrespective of latency in that region.\nOption B is incorrect because Failover routing is usually used in Disaster Recovery scenarios where an Active-Passive Disaster recovery configuration is present &amp; the Passive resource that was originally the Backup resource has now become the Active resource due to the original Active resource being unhealthy.\nOption C is incorrect sinceMultivalve answer routing provides the ability to return multiple health-checkable IP addresses which is a way to use DNS to improve availability and load balancing.\nOption D is CORRECT since Latency based routing always routes DNS queries to the best performing website (region) irrespective of what happens in the Amazon infrastructure, Internet.\nGoing back to our scenario, if we have ELB load balancers in the US West (Oregon) region and the Asia Pacific(Mumbai) region for the Weather tracking &amp; Airline Ticketing website and if a user from London enters the name of your domain in a browser, the following things will happen:\nDNS routes the query to a Route 53 name server.\nRoute 53 refers to its data on latency between London and the Mumbai region and between London and the Oregon region.\nIf latency is lower between the London and Oregon regions, Route 53 responds to the query with the Oregon load balancer's IP address.\nIf latency is lower between London and the Mumbai region, Route 53 responds with the Mumbai load balancer's IP address.\nReferences:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency\nhttps://youtu.be/BtiS0QyiTK8\n\nBased on the requirements stated in the question, the routing policy that would be most appropriate for the weather tracking system is the Latency-Based Routing Policy.\nThe Latency-Based Routing Policy directs traffic to the Amazon EC2 instances that provide the lowest latency for the end user. In other words, it routes traffic to the instance that can serve the user the fastest. This makes it ideal for services that require quick response times.\nIn the case of the weather tracking system, travellers expect quick turnaround time in which the weather display and flight booking will happen. This means that response time is critical to the success of the business. By using the Latency-Based Routing Policy, traffic can be routed to the instance that can provide the fastest response time to the end user.\nRoute 53's Latency-Based Routing Policy works by measuring the latency between the end user and the AWS resources, such as EC2 instances, that are serving the website. The policy automatically directs traffic to the resource with the lowest latency.\nIn addition to the Latency-Based Routing Policy, other routing policies that can be used with Route 53 include the GeoLocation Routing Policy, which routes traffic based on the geographic location of the user, the Failover Routing Policy, which is used for high availability and failover scenarios, and the Multivalue Answer Routing Policy, which returns multiple values in response to DNS queries.\nHowever, for the weather tracking system, the Latency-Based Routing Policy is the most appropriate routing policy to use because it ensures that users receive the fastest response time possible, which is critical for the success of the business.\n\n"
}, {
  "id" : 127,
  "question" : "Which of the following services can be used as a web application firewall in AWS?\n",
  "answers" : [ {
    "id" : "a517dbfea1e9486b840f55ca5669e3f6",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "6dd9dfe32df74d7b8988c830f40d1f80",
    "option" : "AWS WAF",
    "isCorrect" : "true"
  }, {
    "id" : "566f2346051744d6802b7e2fcce6b864",
    "option" : "AWS Firewall",
    "isCorrect" : "false"
  }, {
    "id" : "febaa68790cc4d2ea9c0ef5f2e11ffe8",
    "option" : "AWS Protection.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAWS WAF is a web application firewall that lets you monitor the HTTP and HTTPS requests that are forwarded to Amazon CloudFront or an Application Load Balancer.\nAWS WAF also lets you control access to your content.\nFor more information on AWS WAF, please refer to the below URL:\nhttps://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html\n\nThe correct answer is B. AWS WAF.\nExplanation:\nAWS WAF (Web Application Firewall) is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. It can be used to control access to web applications based on IP addresses, HTTP headers, HTTP body or URI strings. AWS WAF also allows you to create custom rules that block common attack patterns like SQL injection or cross-site scripting (XSS).\nOption A, AWS EC2 (Elastic Compute Cloud) is a service that provides scalable compute capacity in the cloud. While you can certainly use EC2 instances to run your web application, it is not a web application firewall.\nOption C, AWS Firewall Manager is a service that makes it easy to centrally configure and manage firewall rules across multiple AWS accounts and resources. While it can be used to manage network-based firewalls, it is not a web application firewall.\nOption D, AWS Shield is a managed DDoS (Distributed Denial of Service) protection service. While it can be used to protect your web applications from DDoS attacks, it is not a web application firewall.\nIn conclusion, AWS WAF is the service that can be used as a web application firewall in AWS.\n\n"
}, {
  "id" : 128,
  "question" : "What can be termed as a user-defined label that has a key-value pair of variable character length? It is assigned to AWS resources as metadata for administration and management purposes.\n",
  "answers" : [ {
    "id" : "eb1ca39a2af446b1a93f537d52cb2220",
    "option" : "Resource Tag",
    "isCorrect" : "true"
  }, {
    "id" : "1711f9517ddd416b90bae19a90458a7b",
    "option" : "Resource Group",
    "isCorrect" : "false"
  }, {
    "id" : "ce30d979f64d44abab949987c532738f",
    "option" : "Resource Flag",
    "isCorrect" : "false"
  }, {
    "id" : "8fd25ffe41314af69ce1e18a1af99428",
    "option" : "Tag key.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Resource tags are a critical component when architecting in the cloud.\nThey create an identifying mechanism for the user to group, classify and order all their provisioned resources appropriately.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html\nOption B is INCORRECT.\nAWS Resource groups enable the ordering of AWS resources into logical groupings.\nResources can be ordered by application, environment or software component.\nOption C is INCORRECT.\nFlags are used in AWS CloudFormation.\nThe option is inaccurate.\nOption D is INCORRECT.\nA tag key is only part of what makes up a resource tag.\nEach resource tag will have a key and value string.\n\nThe correct answer is A. Resource Tag.\nResource Tags are user-defined labels that consist of a key-value pair with variable character length. They are used to assign metadata to AWS resources for administration and management purposes.\nTags can be applied to a wide range of AWS resources, including EC2 instances, S3 buckets, and EBS volumes. Resource Tags can help you categorize resources, track their usage, and manage costs.\nFor example, you can use tags to identify resources belonging to a particular project, department, or cost center. You can also use tags to track the cost of resources by creating cost allocation reports based on tag values.\nResource Tags are easy to create and manage using the AWS Management Console, AWS CLI, or AWS SDKs. You can add, edit, or remove tags at any time, and they will be automatically propagated to all associated resources.\n\n"
}, {
  "id" : 129,
  "question" : "A financial Organization has an on-premises Data Center that holds large volumes of customers' financial transaction data on its legacy mainframe systems.\nWhile accessing transaction data, they have implemented a caching solution in the AWS cloud that will hold the customer's financial data due to performance issues.\nThe transaction data is extremely confidential &amp; is heavy in bandwidth while transferring to the cloud.\nWhat connectivity would you recommend for this data transfer? Select the best answer.\n",
  "answers" : [ {
    "id" : "aa53a545d0494bf48ae4256cb9e54fa3",
    "option" : "Direct Connect with a VPN connection",
    "isCorrect" : "true"
  }, {
    "id" : "1016849c2df74b7595cccc4c7251a2ca",
    "option" : "Virtual Private Network (VPN)",
    "isCorrect" : "false"
  }, {
    "id" : "e05713855945454abb21bae506a7fcad",
    "option" : "AWS Storage Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "6360e475c817438886bebe48dc51edf6",
    "option" : "AWS Snowball.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT since Direct Connect provides a dedicated connection to the on-premises data Center bypassing the internet providing a more secure data transfer mechanism.\nIt also allows you to control the bandwidth to transfer massive amounts of data with the Direct Connect partner which is a prime requirement.\nVPN connection ensures that the connection is secure.\nOption B is incorrect.\nBandwidth is important for the connection.\nSo Direct Connect is required.\nOption C is incorrect.\nAWS Storage Gateway is a means that provides a Backup &amp; Recovery option for data to the AWS cloud that is stored within the on-premises Data Center.\nPrimarily used with S3, the transfer still happens through the internet after encryption.\nAlso since the data is backed up asynchronously, the cache may be Eventually Consistent resulting in stale data being retrieved from the cache.\nOption D is incorrect.\nSnowball is an offline data transfer mechanism used when there is a huge amount of data (100TB) that needs to be transferred to the cloud.\nMoving them over a WAN can take years &amp; can be impractical at times.\nA physical appliance is shipped to the on-premise Data Center which can be hooked to a network for transferring data.\nOnce done, it is shipped back to the Cloud Data Center, where it can be copied to storage devices like S3\nSince our scenario requires real-time data availability between the On-Premise Data Center &amp; AWS Cloud, it may not suffice the requirements.\nReferences:\nhttps://docs.aws.amazon.com/directconnect/latest/UserGuide/Welcome.html\n\n\nThe best connectivity option for transferring large volumes of confidential data with heavy bandwidth requirements from an on-premises data center to AWS would be Direct Connect with a VPN connection.\nDirect Connect is a dedicated network connection between an organization's on-premises infrastructure and AWS, which provides a reliable, high-bandwidth, low-latency connection that can be used to transfer large volumes of data securely. Direct Connect provides a private, isolated connection between on-premises infrastructure and AWS and is not routed over the public internet, providing enhanced security for confidential data.\nA VPN connection would add an additional layer of security to the Direct Connect connection by encrypting the traffic between the on-premises infrastructure and AWS. VPN uses industry-standard encryption protocols to create a secure and encrypted tunnel between the two networks, ensuring that data transferred between the networks is protected from interception and tampering.\nAWS Storage Gateway is a hybrid storage service that enables on-premises applications to use AWS storage. While it could be used for data transfer, it is not optimized for heavy bandwidth transfers and may not be the best choice for extremely confidential data.\nAWS Snowball is a physical data transport solution that uses secure, ruggedized devices to transfer large amounts of data into and out of AWS. However, it may not be the best choice for data that needs to be transferred quickly, as it may take several days for the device to physically arrive at the on-premises location, be loaded with data, and then shipped back to AWS for upload.\n\n"
}, {
  "id" : 130,
  "question" : "A start-up organisation would like to deploy a complex web and mobile application development environment instantaneously, complete with the necessary resources and peripheral assets.\nHow can this be achieved efficiently?\n",
  "answers" : [ {
    "id" : "6c02329ce6cf44ddade9da75b159d3ca",
    "option" : "By putting together the necessary components from AWS services, starting with EC2 instances.",
    "isCorrect" : "false"
  }, {
    "id" : "f0281dd42b9b46d4b31c8c54c1f795f0",
    "option" : "Creating AWS Lambda functions that will be triggered by single-button click to call the appropriate API of the respective resources and peripheral assets needed.",
    "isCorrect" : "false"
  }, {
    "id" : "261119fd4719485c8906f9ddbce4c3fe",
    "option" : "Using AWS Quick Starts to identify and provision the appropriate AWS CloudFormation templates",
    "isCorrect" : "true"
  }, {
    "id" : "34a887b39f8c491f99406601d1356501",
    "option" : "Making use of the AWS Serverless Application Repository to identify and deploy the resources needed for a web and mobile application development environment.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS CloudFormation can be used in conjunction with AWS Quick Starts templates, a repository of AWS CloudFormation templates designed by expert architects.\nThese can include third-party resources and peripheral assets tailor-made for a single-button push deployment of specific environments.\nhttps://aws.amazon.com/quickstart/?quickstart-all.sort-by=item.additionalFields.updateDate&amp;quickstart-all.sort-order=desc\nOption A is INCORRECT.\nIt is cumbersome and inefficient to put together the AWS services and resources necessary to deploy a complex web and mobile application development environment.\nOption B is INCORRECT.\nIt is tedious and inefficient to create AWS Lambda function for each of the required components.\nOption D is INCORRECT.\nDevelopers and enterprises primarily use AWS Serverless Application Repository to search, look-up, publish and deploy serverless applications on the cloud.\nhttps://docs.aws.amazon.com/serverlessrepo/latest/devguide/what-is-serverlessrepo.html\n\nWhen it comes to deploying a complex web and mobile application development environment instantaneously, complete with the necessary resources and peripheral assets, there are different approaches to choose from. Here are the explanations for each answer choice:\nA. By putting together the necessary components from AWS services, starting with EC2 instances. This option suggests that you can start by setting up EC2 instances, which are virtual machines that you can configure with the operating system, applications, and storage resources that you need. However, this approach can be time-consuming and may require expertise in managing infrastructure, including security, availability, and scalability. You will also need to set up other resources such as load balancers, databases, and caching solutions, which can add complexity to the deployment process.\nB. Creating AWS Lambda functions that will be triggered by a single-button click to call the appropriate API of the respective resources and peripheral assets needed. This option suggests using AWS Lambda, a serverless compute service that allows you to run code without provisioning or managing servers. With this approach, you can write code that integrates with other AWS services to automate the deployment of resources and peripheral assets needed for your web and mobile application development environment. By creating a single-button click, you can trigger the Lambda functions, which will call the appropriate APIs and provision the necessary resources. This approach can simplify the deployment process, reduce the need for manual intervention, and provide a scalable and cost-efficient solution.\nC. Using AWS Quick Starts to identify and provision the appropriate AWS CloudFormation templates. This option suggests using AWS Quick Starts, which are pre-built reference architectures that provide you with automated deployments for key workloads on the AWS Cloud. AWS Quick Starts are designed to help you get started quickly by providing you with an easy-to-use guide that includes step-by-step instructions and best practices. By selecting the appropriate AWS Quick Start for your web and mobile application development environment, you can quickly identify and provision the necessary resources and peripheral assets using AWS CloudFormation templates, which automate the deployment process and ensure consistency.\nD. Making use of the AWS Serverless Application Repository to identify and deploy the resources needed for a web and mobile application development environment. This option suggests using the AWS Serverless Application Repository, which is a managed repository for serverless applications that allows you to easily discover, configure, and deploy serverless applications and components. By searching for the appropriate serverless application in the repository, you can quickly identify and deploy the necessary resources and peripheral assets for your web and mobile application development environment. This approach can simplify the deployment process, reduce the need for manual intervention, and provide a scalable and cost-efficient solution.\nIn summary, while all of the answer choices provide potential solutions for deploying a complex web and mobile application development environment, the best option would depend on the specific requirements and constraints of the project. Option B, C, and D are all good solutions for instantaneously deploying a web and mobile application development environment efficiently with minimal manual intervention.\n\n"
}, {
  "id" : 131,
  "question" : "Which of the following can be attached to EC2 Instances to store data?\n",
  "answers" : [ {
    "id" : "cf2ea5d7de184717b4ffcf87bf326b78",
    "option" : "Amazon Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "35b002f595024aa5b6167b7b9ae9cba5",
    "option" : "Amazon EBS Volumes",
    "isCorrect" : "true"
  }, {
    "id" : "98723a5279bc4932bb55c5fd8d38f337",
    "option" : "Amazon EBS Snapshots",
    "isCorrect" : "false"
  }, {
    "id" : "06780d75c6bb4394aebe2abf69fddb5c",
    "option" : "Amazon SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following on EBS Volumes:\nAn Amazon EBS volume is a durable, block-level storage device that you can attach to a single EC2 instance.\nYou can use EBS volumes as primary storage for data that requires frequent updates, such as the system drive for an instance or storage for a database application.\nFor more information on EBS Volumes, please refer to the below URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\n\nOf the given options, only Amazon EBS volumes and Amazon EBS snapshots can be attached to EC2 instances to store data.\nAmazon EBS (Elastic Block Store) is a block-level storage service that is used to create persistent block-level storage volumes for Amazon EC2 instances. Amazon EBS volumes can be attached to EC2 instances and used as primary storage for data that requires frequent and fast access.\nAmazon EBS volumes are highly available and durable, and they can be backed up using Amazon EBS snapshots. Amazon EBS snapshots are point-in-time copies of an Amazon EBS volume, stored in Amazon S3. They can be used for data backup, recovery, and migration.\nAmazon Glacier, on the other hand, is a low-cost, long-term storage service that is optimized for infrequently accessed data. It is not recommended for storing data that requires frequent access.\nAmazon SQS (Simple Queue Service) is a fully managed message queuing service that enables decoupling and scaling of distributed systems. It is not used for storing data in the traditional sense.\nTherefore, the correct options are B. Amazon EBS Volumes and C. Amazon EBS Snapshots.\n\n"
}, {
  "id" : 132,
  "question" : "Which of the following networking component can be used to host EC2 resources in the AWS Cloud?\n",
  "answers" : [ {
    "id" : "c11e533114af452b9c3e1cd57108b015",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "1766b0b39b6a4fd7bccdc8e29354558d",
    "option" : "AWS VPC",
    "isCorrect" : "true"
  }, {
    "id" : "7e2ab31e99554184974d943ece24a210",
    "option" : "AWS Elastic Load Balancer",
    "isCorrect" : "false"
  }, {
    "id" : "10575df2c7e641a58cb9a14907474dd1",
    "option" : "AWS Autoscaling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following on Amazon VPC:\nAmazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined.\nThis virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.\nFor more information on AWS VPC, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_Introduction.html\n\nThe correct answer is B. AWS VPC (Virtual Private Cloud).\nExplanation:\nAWS Virtual Private Cloud (VPC) is a service that allows you to launch Amazon Web Services resources into a virtual network that you've defined. With Amazon VPC, you can define a virtual network topology that closely resembles a traditional network that you might operate in your data center.\nUsing VPC, you can create and configure subnets, route tables, and network gateways to build a virtual network in the cloud. Once you have a VPC set up, you can launch Amazon Elastic Compute Cloud (EC2) instances, Amazon Relational Database Service (RDS) instances, and other AWS resources inside it.\nTherefore, AWS VPC is the networking component that can be used to host EC2 resources in the AWS Cloud.\nThe other options listed are:\nA. AWS Trusted Advisor: AWS Trusted Advisor is a service that helps you optimize your AWS infrastructure, security, and cost by providing real-time guidance to improve the performance and reliability of your AWS resources.\nC. AWS Elastic Load Balancer: AWS Elastic Load Balancer is a service that distributes incoming traffic across multiple EC2 instances to increase availability and fault tolerance of applications.\nD. AWS Autoscaling: AWS Autoscaling is a service that automatically adjusts the number of EC2 instances in a fleet to maintain performance and availability of applications based on demand.\nWhile all of these services are important in the context of AWS, they do not directly relate to hosting EC2 instances.\n\n"
}, {
  "id" : 133,
  "question" : "I have a web application that has been deployed to the AWS Mumbai region.\nMy application soon becomes popular.\nNow there are users all over the world who would like to access it.\nIf I use a CloudFront distribution for doing so, which statements are FALSE for CloudFront? (Select TWO.)\n",
  "answers" : [ {
    "id" : "ab24d7e6c8934e6683474bab68968d51",
    "option" : "CloudFront uses the concept of Edge locations for caching and delivering content faster to its users.",
    "isCorrect" : "false"
  }, {
    "id" : "96254ffbd04d403f83983ff8a14ed08e",
    "option" : "CloudFront can help improve performance by using Keep-alive connections between the Edge locations &amp;the origin server.",
    "isCorrect" : "false"
  }, {
    "id" : "5b6663828a5d487d985fc2cb2aea0287",
    "option" : "CloudFront does not cache dynamic content.",
    "isCorrect" : "true"
  }, {
    "id" : "bb150949b29d4d5283af185a32c13235",
    "option" : "CloudFront can use only S3 buckets as their Origin Server from where they can cache content.",
    "isCorrect" : "true"
  }, {
    "id" : "261f3989797744d5b9a60db4c0b8682d",
    "option" : "CloudFront can customize content at the Edge locations before delivering it to users.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C, D.\nOption A is incorrect.\nCloudFront does use the concept of Edge locations for caching content that is requested by the user.\nWhen a user in the US requests for content in a web server hosted in the Mumbai region, CloudFront will initially check whether the content is available at the nearest edge location in the US region.\nIf it is available, the content will be served directly from the Edge location.\nIf not, CloudFront will request the Origin Server, get the content and cache it at the Edge location for serving future requests.\nOption B is incorrect.\nEvery HTTP connection runs on TCP/IP.\nFor every HTTP connection to work, a TCP handshake has to be initially completed.\nLet's consider the following 2 scenarios.\ni)\nTwo users without an Edge proxy - 1 Trip is 100ms.\nWe can see here that the total turnaround time for both users is 400ms.\nii)\nTwo users with a CloudFront Edge proxy.\nWe can see here that for the first user, the total turnaround time is 400ms, while for the second user, the total turnaround time is 240ms(4x20 + 2x80 )\nInstead of re-establishing a second handshake with the Origin, CloudFront leverages the Keep-alive session resulting in reduced latency.\nOption C is CORRECT since we can use the Time ToLive (TTL) value to enable caching of dynamic content.\nOption D is CORRECT.\nCloudFront has been opened to use Origin servers of your choice that can be S3 or a custom origin like EC2, ELB, etc...\nOption E is incorrect.\nCloudFront has the ability to customize content at the Edge location before delivering it to its users.\nFor example, a Lambda function (usually referred to as lambda@Edge) can use the following triggers Viewer Request, Origin Request, Origin Response, Viewer response that can be used for customizing the End User experience.\nReferences:\nhttps://docs.amazonaws.cn/en_us/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\nhttps://youtu.be/sQNONcj0cvc\n\n\nCloudFront is a Content Delivery Network (CDN) provided by Amazon Web Services (AWS). It uses a network of Edge locations to cache and deliver content faster to users. When a user requests content, the request is routed to the nearest Edge location, which serves the content from its cache. If the content is not present in the cache, the request is forwarded to the Origin Server, which can be an Amazon S3 bucket, an EC2 instance, or a Load Balancer.\nThe following are the given statements and their explanations:\nA. CloudFront uses the concept of Edge locations for caching and delivering content faster to its users. This statement is true. Edge locations are the key feature of CloudFront. They are distributed around the world and are located closer to users, which helps reduce latency and improve performance.\nB. CloudFront can help improve performance by using Keep-alive connections between the Edge locations & the origin server. This statement is true. Keep-alive connections allow multiple requests to be sent over a single connection, reducing the overhead of establishing a new connection for each request.\nC. CloudFront does not cache dynamic content. This statement is false. CloudFront can cache dynamic content, but it requires additional configuration to specify the caching behavior for dynamic content.\nD. CloudFront can use only S3 buckets as their Origin Server from where they can cache content. This statement is false. CloudFront can use a variety of Origin Server types, including S3 buckets, EC2 instances, Elastic Load Balancers, and even custom HTTP servers.\nE. CloudFront can customize content at the Edge locations before delivering it to users. This statement is true. CloudFront provides several ways to customize content at the Edge locations, including modifying headers, rewriting URLs, and using Lambda@Edge functions to add custom logic.\nIn summary, the false statements are C and D. CloudFront can cache dynamic content, and it can use a variety of Origin Server types, not just S3 buckets.\n\n"
}, {
  "id" : 134,
  "question" : "Which of the following components of the Cloudfront service can be used to distribute content to users across the globe?\n",
  "answers" : [ {
    "id" : "7c934d3f75c04b1fb96d0bf9b2f7f8d0",
    "option" : "Amazon VPC",
    "isCorrect" : "false"
  }, {
    "id" : "33f292bdbae34492b00de0b12ea11367",
    "option" : "Amazon Regions",
    "isCorrect" : "false"
  }, {
    "id" : "03aa4a4e3db74af189f6b0857766ea1e",
    "option" : "Amazon Availability Zones",
    "isCorrect" : "false"
  }, {
    "id" : "6ebbc1df14a94f14bf2a8f683d45568c",
    "option" : "Amazon Edge locations.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS documentation mentions the following:\nAmazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users.\nCloudFront delivers your content through a worldwide network of data centers called edge locations.\nFor more information on Amazon Cloudfront, please refer to the below URL:\nhttp://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html\n\nThe correct answer is D. Amazon Edge locations.\nAmazon CloudFront is a content delivery network (CDN) service that helps deliver your static and dynamic web content, streaming videos, and APIs to users across the globe. CloudFront is designed to distribute content from origin servers to edge locations, which are located in various geographic regions across the world.\nWhen a user requests content, CloudFront will route the request to the nearest edge location, where the content is cached. This helps reduce latency and improve performance by delivering content to users from the closest edge location.\nAmazon VPC (A) is a virtual private cloud service that allows you to launch AWS resources into a virtual network. It is used for isolating resources and creating a private network for your AWS services.\nAmazon Regions (B) are geographic locations where AWS resources are available. Each region consists of multiple availability zones (C), which are physically separate locations within the same region.\nAlthough CloudFront uses Amazon's global infrastructure, it primarily utilizes edge locations (D) to distribute content to users across the globe.\n\n"
}, {
  "id" : 135,
  "question" : "Your company is planning to move to the AWS Cloud.\nYou need to give a presentation on the cost perspective when moving existing resources to the AWS Cloud.\nConsidering Amazon EC2, which of the following is an advantage from the cost perspective?\n",
  "answers" : [ {
    "id" : "9c1e592fbe7b48db929b3c77d39a0da3",
    "option" : "Having the ability of automated backups of the EC2 instance, so that you donâ€™t need to worry about the maintenance costs.",
    "isCorrect" : "false"
  }, {
    "id" : "9ac75576fc9c43c28ca90d47720dc4ed",
    "option" : "The ability to choose low cost AMIâ€™s to prepare the EC2 Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "2beef2b07b0c4c21962b41646f91996a",
    "option" : "The ability to only pay for what you use.",
    "isCorrect" : "true"
  }, {
    "id" : "e742ab31cde949278454da018d31123d",
    "option" : "Ability to tag instances to reduce the overall cost.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nOne of the advantages of EC2 Instances is the per-second billing concept.\nThis is also given in the AWS documentation.\nWith per-second billing, you pay for only what you use.\nIt takes the cost of unused minutes and seconds in an hour off of the bill.\nSo, you can focus on improving your applications instead of maximizing usage to the hour especially if you manage instances running for irregular periods of time, such as dev/testing, data processing, analytics, batch processing and gaming applications.\nFor more information on EC2 Pricing, please refer to the below URL:\nhttps://aws.amazon.com/ec2/pricing/\n\nWhen moving existing resources to the AWS Cloud, it is important to consider the cost perspective. Amazon EC2 (Elastic Compute Cloud) is a web service that provides resizable compute capacity in the cloud, designed to make web-scale cloud computing easier for developers.\nOut of the given options, option C is the correct answer: \"The ability to only pay for what you use\" is an advantage from the cost perspective. This is because with Amazon EC2, you can launch as many or as few virtual servers as you need, configure security and networking, and manage storage. You only pay for what you use, which means you don't have to make any upfront investments and don't need to worry about capacity planning.\nOption A, \"Having the ability of automated backups of the EC2 instance, so that you don't need to worry about the maintenance costs\" is incorrect because while automated backups can help with the maintenance of the EC2 instance, it does not directly affect the cost perspective.\nOption B, \"The ability to choose low cost AMI's to prepare the EC2 Instances\" is incorrect because while choosing a low cost AMI (Amazon Machine Image) can help reduce costs, it is not an inherent advantage of Amazon EC2 itself.\nOption D, \"Ability to tag instances to reduce the overall cost\" is incorrect because while tagging instances can help you track and manage your resources, it does not directly reduce the overall cost.\nIn conclusion, the advantage from a cost perspective when moving existing resources to the AWS Cloud with Amazon EC2 is the ability to only pay for what you use.\n\n"
}, {
  "id" : 136,
  "question" : "Your company is planning to move to the AWS Cloud.\nOnce it completely moves to the cloud, it wants to ensure that the right security settings are put in place.\nWhich of the following tools are helpful? (Select TWO.)\n",
  "answers" : [ {
    "id" : "b44851e24fd5456cab1f3f24dd2b95ae",
    "option" : "AWS Inspector",
    "isCorrect" : "true"
  }, {
    "id" : "05f158234ef44cb5b1920ec4e97fafa7",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "true"
  }, {
    "id" : "4330381e16f94dd2935386c203b8f839",
    "option" : "AWS Support",
    "isCorrect" : "false"
  }, {
    "id" : "8f011b483b39477ca7a432a7d082d370",
    "option" : "AWS Kinesis.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS documentation mentions the following.\nTrusted Advisor is a service to help you reduce cost, increase performance, and improve security by optimizing your AWS environment.\nTrusted Advisor provides real-time guidance to help you provision your resources following AWS best practices.\nThe AWS Inspector can inspect EC2 Instances against common threats.\nFor more information on the AWS Trusted Advisor, please refer to the below URL:\nhttps://aws.amazon.com/premiumsupport/trustedadvisor/\nhttps://docs.aws.amazon.com/inspector/latest/userguide/inspector_introduction.html\n\nThe two AWS tools that are helpful in ensuring the right security settings are in place are:\nA. AWS Inspector: This tool helps to assess the security and compliance of applications running on AWS infrastructure. AWS Inspector automates the security assessment process by analyzing the behavior of applications and identifying potential security issues. AWS Inspector uses a combination of rules packages and an agent to assess the security of your applications.\nB. AWS Trusted Advisor: This tool provides real-time guidance to help you optimize your AWS infrastructure, improve security, and reduce costs. Trusted Advisor performs checks on your AWS environment and provides recommendations based on industry best practices. The Security category of Trusted Advisor provides recommendations for securing your AWS resources, including IAM, S3, and EC2 instances.\nC. AWS Support: AWS Support provides technical support to AWS customers. While AWS Support can help with security issues, it is not a tool specifically designed for security assessment and compliance.\nD. AWS Kinesis: This is a data streaming service that can help you process and analyze large amounts of data in real-time. While Kinesis can be used for security-related tasks, it is not a tool specifically designed for security assessment and compliance.\nTherefore, the correct answers are A. AWS Inspector and B. AWS Trusted Advisor.\n\n"
}, {
  "id" : 137,
  "question" : "There is a requirement to collect important metrics from AWS RDS and EC2 Instances.\nWhich AWS service would be helpful to fulfill this requirement?\n",
  "answers" : [ {
    "id" : "38095a442c1545c28b7fbb6953f56f11",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "303cf4eee2b844728ffc54f4d9dd74e2",
    "option" : "Amazon CloudSearch",
    "isCorrect" : "false"
  }, {
    "id" : "8f5bfa55319640779ba696c16c19edac",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "true"
  }, {
    "id" : "33c00960a7514f318f99c7b4b2e0593d",
    "option" : "Amazon Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe AWS documentation mentions the following:\nAmazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.\nYou can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, set alarms, and automatically react to changes in your AWS resources.\nFor more information on AWS Cloudwatch, please refer to the URL below:\nhttps://aws.amazon.com/cloudwatch/\n\nThe most appropriate service to collect important metrics from AWS RDS and EC2 instances is Amazon CloudWatch (Option C).\nAmazon CloudWatch is a monitoring service that provides real-time monitoring of AWS resources and applications running on AWS. It provides data and actionable insights for resources such as EC2 instances, RDS databases, Elastic Load Balancers, and more.\nAmazon CloudWatch can collect metrics, logs, and events from various AWS services and applications. With CloudWatch, you can monitor resource utilization, operational performance, and application performance. You can also set alarms to notify you when metrics breach a certain threshold.\nIn the case of RDS and EC2 instances, CloudWatch can collect important metrics such as CPU usage, disk usage, network usage, and more. You can use CloudWatch to monitor the performance of your databases and instances, and identify potential issues before they become critical.\nCloudWatch provides various tools to visualize and analyze collected data, such as dashboards, alarms, and logs. You can create custom dashboards to monitor specific metrics, and set alarms to notify you when certain thresholds are exceeded. You can also use CloudWatch logs to store and analyze logs generated by your instances and applications.\nIn conclusion, Amazon CloudWatch is the most suitable service to fulfill the requirement of collecting important metrics from AWS RDS and EC2 instances.\n\n"
}, {
  "id" : 138,
  "question" : "I need to upload a large number of large-size objects from different Geographic locations to an S3 bucket.\nWhat is the best mechanism to do so in a fast &amp; reliable way?\n",
  "answers" : [ {
    "id" : "7b3f23f4c6ab412195ef58472e67f363",
    "option" : "I can connect to an application running on AWS EC2 that is hosted in multiple regions using Route 53 &amp; use latency based routing to upload files to the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "fad1ae56f4504fbe97dcf9f200dda402",
    "option" : "I can use a Direct Connect link from each of the Geographic location for transferring data quickly.",
    "isCorrect" : "false"
  }, {
    "id" : "da5d2e6590604d52bc9cabe72c51ccb8",
    "option" : "I can use S3 Transfer Acceleration from each Geographic location that will route the data from their respective Edge locations to S3.",
    "isCorrect" : "true"
  }, {
    "id" : "28b03601b8204a1fa539b6565a263fcc",
    "option" : "I can directly access the S3 bucket from the different locations &amp; use a multi-part-upload for transferring huge objects.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect since Route 53 latency routing only calculates latency between different endpoints based on the Internet traffic &amp; location proximity rather than optimizing the network for fast data transfers.\nOption B is incorrect since Direct Connect is used for very specific purposes like extreme security requirements for the data transfer.\nAlso, establishing multiple Direct Connect infrastructures would be expensive from a cost standpoint.\nOption C is CORRECT.\nThe best way to address this scenario is to route the requests to the nearest CloudFront edge location from the different Geographic locations.\nEdge locations provide a fast network infrastructure bypassing much of the internet for delivering content to S3 destinations.\nPerformance gains of nearly 50 - 500% can be observed while using S3 Transfer Acceleration.\nOption D is incorrect.\nIt is possible to use S3 endpoints directly for data transfer.\nBut it will be impractical for situations where the Geographic location is significantly far away from the S3 destination introducing high latency while uploading large objects.\nReferences:\nhttps://medium.com/awesome-cloud/aws-amazon-s3-transfer-acceleration-overview-6baa7b029c27\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\n\n\nThe best mechanism to upload a large number of large-size objects from different geographic locations to an S3 bucket in a fast and reliable way is to use S3 Transfer Acceleration. This service uses Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet, resulting in faster and more reliable transfers than traditional methods.\nOption A, connecting to an EC2 instance hosted in multiple regions using Route 53, may be a viable solution, but it would require the deployment and management of additional infrastructure, potentially increasing complexity and cost.\nOption B, using a Direct Connect link from each geographic location, is a dedicated network connection between the locations and AWS, but it may not be the most cost-effective option, especially for smaller businesses or individuals.\nOption D, accessing the S3 bucket directly and using a multi-part-upload, is a valid option for transferring huge objects, but it may not be the most efficient way to transfer a large number of large-size objects from multiple geographic locations.\nTherefore, S3 Transfer Acceleration is the best mechanism for uploading a large number of large-size objects from different geographic locations to an S3 bucket in a fast and reliable way, as it utilizes Amazon CloudFront's globally distributed edge locations and does not require additional infrastructure or dedicated network connections.\n\n"
}, {
  "id" : 139,
  "question" : "I have developed an application using AWS services that have been deployed to multiple regions.\nHow do I achieve the best Performance and Availability when users from different locations access my application?\n",
  "answers" : [ {
    "id" : "5bae994b8e204e76929b1256d8b8206b",
    "option" : "Use Route 53 latency based routing for improving performance and Availability.",
    "isCorrect" : "false"
  }, {
    "id" : "bdc68338ef184b64b9bc19c35d58428b",
    "option" : "Use a CloudFront distribution for improving performance and Availability.",
    "isCorrect" : "false"
  }, {
    "id" : "f1c7567f0d5b489fa9db75b83d058610",
    "option" : "Use Global Accelerator for improving performance and Availability.",
    "isCorrect" : "true"
  }, {
    "id" : "463783db7181461ebeab484269724e8d",
    "option" : "Use an endpoint of the application directly for accessing it that lies within a userâ€™s Region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect.\nRoute 53 latency-based routing helps select a region that may be relatively faster for the user to send traffic to based on certain factors like internet traffic and proximity to the user's location.\nHowever, the actual route to the destination does not involve providing a fast network path for optimum performance, which is the prime requirement for the scenario.\nLatency based routing does not address Availability.\nOption B is incorrect.\nCloudFront improves performance for both cacheable content (e.g., images, videos) and dynamic content (e.g., API, dynamic site delivery) using edge locations.\nHere we are talking about application performance &amp; Availability with a highly reliable, performant network rather than bringing content closer to the user.\nOption C is CORRECT.\nGlobal Accelerator improves the performance of a wide range of applications over TCP or UDP by proxying packets at Edge locations to applications running in one or more AWS regions.\nGlobal Accelerator provides static IP addresses acting as a fixed entry point to application endpoints (Application Load Balancers, EC2 instances ...) in a single or multiple AZ's offering High Availability.\nIt uses the AWS global network to optimize the path from users to the application, thus improving the resultant traffic performance by as much as 60%\nIt provides very low latency for a great user experience by.\ni)\nRouting traffic to the closest edge location through AnyCast &amp; then routing it to the closest regional endpoint over the AWS global network.\nii)\nGood for Gaming, Media, Mobile applications.\nOption D is incorrect since.\nThe application may not be deployed in the Region that the user is trying to access.\nThere is no way to calculate latency even though there is proximity to the user's region.\nAvailability will be restricted to AZ's rather than Regions if regional endpoints, e.g., ELB's are directly accessed.\nReferences:\nhttps://tutorialsdojo.com/aws-global-accelerator/\nhttps://docs.aws.amazon.com/global-accelerator/latest/dg/what-is-global-accelerator.html\n\n\nWhen deploying an application to multiple regions, it is essential to ensure that users from different locations can access the application with the best possible performance and availability. To achieve this, there are several options available with AWS services:\nA. Use Route 53 latency-based routing for improving performance and availability: This service helps route traffic to the AWS region with the lowest network latency for each user. Route 53 monitors the network latency between the user's location and each available region and routes traffic to the region with the lowest latency. This helps reduce the response time for the user, improving the application's performance. Additionally, Route 53 provides DNS failover capabilities, which can help improve the application's availability in case of a region failure.\nB. Use a CloudFront distribution for improving performance and availability: This service can cache the application's static and dynamic content and distribute it to users from the nearest edge location. This reduces the latency and improves the application's performance. Additionally, CloudFront provides origin failover capabilities, which can help improve the application's availability in case of an origin failure.\nC. Use Global Accelerator for improving performance and availability: This service uses AWS's global network to optimize the application's performance and availability. Global Accelerator can route traffic to the optimal AWS endpoint based on network conditions and health checks, reducing latency and improving the application's performance. Additionally, Global Accelerator provides origin failover capabilities, which can help improve the application's availability in case of an origin failure.\nD. Use an endpoint of the application directly for accessing it that lies within a user's Region: This method requires the application to be deployed to multiple regions, and users can access the application by connecting to the endpoint located in their region. This can improve performance by reducing the network latency between the user and the endpoint. However, this approach requires users to be aware of the endpoint located in their region, and it does not provide failover capabilities.\nOverall, all four options can improve the performance and availability of an application deployed to multiple regions. However, the best approach may depend on the specific requirements of the application and the user base. A combination of these options may also be used to achieve the best possible results.\n\n"
}, {
  "id" : 140,
  "question" : "Which statement is accurate about AWS Budgets and Cost Explorer?\n",
  "answers" : [ {
    "id" : "7792dbebf0ca4e4b8203e1e9b4ca0dea",
    "option" : "AWS Budgets uses the cost visualizations provided by AWS Cost Explorer to show the status of preset budgets and to provide forecasts of estimated costs.",
    "isCorrect" : "true"
  }, {
    "id" : "9e0f259e58db4b2fb60781ddd900457e",
    "option" : "Both AWS Budgets and AWS Cost Explorer can be used to predict usage and to give recommended cost-optimization measures.",
    "isCorrect" : "false"
  }, {
    "id" : "d7862bd515c84815967ad4da450325cf",
    "option" : "AWS Budgets will list the costs incurred over a period of time with a further breakdown by region and linked account.",
    "isCorrect" : "false"
  }, {
    "id" : "726e61da0702496587a7d4fd76d2b3ee",
    "option" : "Due to the sensitivity of billing and cost management information, with the AWS Cost Explorer and AWS Budgets services, it is not possible to view the information for multiple accounts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nUnder the Billing and Cost Management service in the AWS management service, it is possible to use the AWS Budgets and Cost Explorer to show the status of preset budgets and to provide forecasts of estimated costs.\nhttps://aws.amazon.com/aws-cost-management/aws-cost-explorer/\nOption B is incorrect because AWS Budgets does not provide cost-optimization recommendations.\nOption C is incorrect because this feature is not provided by AWS budgets, rather it is provided by Cost Explorer.\nOption D is incorrect because it is possible to view billing information for multiple AWS accounts as long as the administrator has lawful jurisdiction over them.\n\n\nBoth AWS Budgets and AWS Cost Explorer are tools that help manage AWS costs, but they differ in their primary functions.\nAWS Budgets is a free service that enables you to set custom cost and usage budgets that will send alerts via email or SMS when you exceed your thresholds. AWS Budgets allows you to create multiple budgets, track costs and usage trends, and forecast your estimated costs. You can set budgets based on various criteria such as accounts, services, tags, and cost allocation tags. It also allows you to create different types of budgets, including RI Utilization, Custom Cost, and Usage budgets. Additionally, AWS Budgets provides cost visualizations and dashboards to monitor and track your AWS costs.\nOn the other hand, AWS Cost Explorer is a paid service that provides you with a comprehensive view of your AWS costs and usage. It enables you to visualize and analyze your AWS costs and usage trends over time. You can use Cost Explorer to explore your AWS costs and usage by accounts, services, regions, tags, and more. It provides different types of pre-built cost reports that help you analyze your costs, such as daily and monthly spend, cost and usage trends, and reserved instances utilization reports. With Cost Explorer, you can also analyze your cost and usage data and predict future costs.\nTherefore, the statement that is accurate is option A, as AWS Budgets uses the cost visualizations provided by AWS Cost Explorer to show the status of preset budgets and to provide forecasts of estimated costs. Option B is incorrect because while both services can provide cost forecasts, only AWS Cost Explorer provides cost optimization recommendations. Option C is incorrect because while AWS Budgets does provide a breakdown by account, it doesn't provide a breakdown by region. Option D is incorrect because both services allow you to view information for multiple accounts, provided you have the necessary permissions.\n\n"
}, {
  "id" : 141,
  "question" : "When designing a system, you use the principle of â€œdesign for failure and nothing will failâ€\nWhich of the following services/features of AWS can assist in supporting this design principle? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "2f8bf9372876466a80575276b4d7a2bf",
    "option" : "Availability Zones",
    "isCorrect" : "true"
  }, {
    "id" : "d47171e18bc8418282bf989a760f595c",
    "option" : "Regions",
    "isCorrect" : "true"
  }, {
    "id" : "eaaff36b3131483e8aa685d85966e1ef",
    "option" : "Elastic Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "68548fee18b546d3b20d4ed29cab6437",
    "option" : "Pay as you go.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A, B and C.\nEach AZ is a set of one or more data centers.\nBy deploying your AWS resources to multiple Availability zones, you are designing with failure in mind.\nSo if one AZ were to go down, the other AZ's would still be up and running.\nHence your application would be more fault-tolerant.\nFor disaster recovery scenarios, one can move or make resources run in other regions.\nAnd finally, one can use the Elastic Load Balancer to distribute load to multiple backend instances within a particular region.\nFor more information on AWS Regions and AZ's, please refer to the below URL:\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html\n\nThe principle of \"design for failure and nothing will fail\" is based on the idea that system failures are inevitable and should be anticipated in the design phase, with the aim of minimizing their impact on the overall system. AWS provides a range of services and features that can support this design principle, including:\nA. Availability Zones: Availability Zones are physically separate data centers within a region, designed to provide redundancy and high availability. By deploying resources across multiple Availability Zones, you can ensure that your system can continue to operate even if one zone becomes unavailable due to a natural disaster or other unexpected event.\nB. Regions: Regions are separate geographic areas in which AWS provides services. Each region consists of multiple Availability Zones. By deploying resources across multiple regions, you can further increase the availability and resilience of your system, as a failure in one region will not affect resources in another region.\nC. Elastic Load Balancer: Elastic Load Balancer is a service that automatically distributes incoming traffic across multiple instances or Availability Zones. By using Elastic Load Balancer, you can ensure that your system can handle increased traffic loads and that individual instances or zones do not become overloaded. This can help to prevent failures due to resource exhaustion or overload.\nD. Pay as you go: While pay as you go is a pricing model rather than a service or feature, it can support the principle of designing for failure by enabling you to scale resources up or down as needed. By paying only for the resources you use, you can avoid over-provisioning and reduce the risk of failures due to insufficient resources.\nIn conclusion, the three AWS services/features that can assist in supporting the principle of \"design for failure and nothing will fail\" are Availability Zones, Regions, and Elastic Load Balancer.\n\n"
}, {
  "id" : 142,
  "question" : "While proposing AWS Cloud solution to a client as a value proposition, which of the following is not an advantage to use the AWS Cloud?\n",
  "answers" : [ {
    "id" : "826ffcf9d2d64e3fab7ec54b882cacc0",
    "option" : "The AWS Cloud offers a pay-as-you-go model to trade Capital expense for Variable expense.",
    "isCorrect" : "false"
  }, {
    "id" : "840e4deb290f466ca4749217becd2424",
    "option" : "The AWS Cloud offers a Scale-on-demand model to eliminate wasted capacity.",
    "isCorrect" : "false"
  }, {
    "id" : "15a2b1603a5f42debe779c58f534511a",
    "option" : "The AWS Cloud gives complete control of Security to its users so that they can replicate their Data Center Security model on the Cloud.",
    "isCorrect" : "true"
  }, {
    "id" : "07da6b53529a4ae385fcdc734d803fdb",
    "option" : "AWS Cloud freesthe users from spending time &amp; money for maintaining their Data Centers.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect.\nInstead of heavily investing in Data Centers &amp; Servers for hosting their applications, clients can consume resources from the AWS Cloud, e.g., Compute, Storage, and only pay for the resources they have consumed.\nSo Capital Expenditure (CAPEX) gets converted into Operational Expenditure (OPEX) while using a Cloud environment that results in Cost Efficiency for the client.\nOption B is incorrect.\nThe AWS Cloud model helps eliminating to guess infrastructure capacity needs by offering a Scale on Demand model.\nFor example, during Christmas when there is a sudden increase in website traffic, additional resources (like EC2) can be created On-Demand (Scale- Up) to address the surge in traffic.\nSimilarly, when the festival period goes away, the additional resources that were created can be terminated (Scale - Down) so that the user does not need to pay for idle resources.\nOption C is CORRECT.\nAWS Cloud adopts a Shared Responsibility Model where Security &amp; Compliance responsibility is shared between the Client &amp; AWS.\nThis shared responsibility security model helps relieve the client of operational burden as AWS operates, manages &amp; controls the components from the host Operating System and Virtualization layer down to the physical security of its facilities (Data Center's)\nThe client can effectively manage the Security of his applications &amp; the infrastructure in which it resides.\nAWS also helps customers to understand their robust controls in place to maintain security and compliance in the Cloud through their compliance certifications e.g., PCI DSS compliance.\nOption D is incorrect.\nAWS Cloud allows clients to focus on their Projects that differentiate their business rather than maintaining infrastructure.\nAWS performs all the heavy lifting of maintaining facilities (Data Centers).\nReferences:\nhttps://d1.awsstatic.com/executive-insights/en_US/infographic-realizing-business-value-with-aws.pdf\nhttps://youtu.be/Fl57qGYwdK4\n\nAs an AWS Cloud Practitioner, when proposing AWS Cloud solution to a client as a value proposition, it is essential to highlight the advantages of using the AWS Cloud. These advantages include:\nA. The AWS Cloud offers a pay-as-you-go model to trade Capital expense for Variable expense: AWS Cloud's pay-as-you-go model allows clients to pay only for the computing resources they use, without any upfront or long-term commitment. This way, clients can reduce their capital expenditure and convert it into operational expenditure, which can be beneficial for businesses that have fluctuating demand.\nB. The AWS Cloud offers a Scale-on-demand model to eliminate wasted capacity: AWS Cloud's scalability model allows clients to scale their resources up or down depending on the demand. This eliminates the need for clients to provision resources that may go unused, reducing wastage and costs.\nC. The AWS Cloud gives complete control of Security to its users so that they can replicate their Data Center Security model on the Cloud: AWS Cloud provides a secure infrastructure, and clients have complete control over their data security. Clients can replicate their data center security model on the cloud, which can give them peace of mind and ensure data security.\nD. AWS Cloud frees the users from spending time & money for maintaining their Data Centers: AWS Cloud eliminates the need for clients to maintain physical data centers, which can be costly and time-consuming. AWS takes care of the infrastructure and the underlying maintenance, allowing clients to focus on their core business operations.\nTherefore, the answer to the question \"which of the following is not an advantage to use the AWS Cloud?\" is C. The AWS Cloud gives complete control of Security to its users so that they can replicate their Data Center Security model on the Cloud. This is because this option is actually an advantage, and it is one of the reasons why clients choose to use the AWS Cloud.\n\n"
}, {
  "id" : 143,
  "question" : "You have a DevOps team in your current organization structure.\nThey are keen to know if there is any service available in AWS which can be used to manage infrastructure as code.\nWhich of the following can be met with such a requirement?\n",
  "answers" : [ {
    "id" : "79cc123193c0439dbe5ec6d04b77b249",
    "option" : "Using AWS Cloudformation",
    "isCorrect" : "true"
  }, {
    "id" : "27ba058396524254b65ba112e0cad0bf",
    "option" : "Using AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "1b4cbd2e067c4ea7aebde10b0f26bde7",
    "option" : "Using AWS Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "4d97c69b8c9f405a84e54abbcc71d58b",
    "option" : "Using AWS Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS documentation mentions the following.\nAWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications that run in AWS.\nYou create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances)\nAWS CloudFormation takes care of provisioning and configuring those resources for you.\nYou don't need to create and configure AWS resources individually and figure out what's dependent on what.\nAWS CloudFormation handles all of that.\nFor more information on AWS Cloudformation, please refer to the below URL:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html\n\nThe service in AWS that can be used to manage infrastructure as code is AWS CloudFormation, which is a service that allows you to model and provision AWS resources in a declarative way. CloudFormation templates are written in YAML or JSON and can be versioned, reviewed, and edited in a source control system like Git. With CloudFormation, you can create and manage AWS resources like EC2 instances, databases, load balancers, and more, in a repeatable, automated, and consistent way.\nAWS Config is a service that helps you assess, audit, and evaluate the configurations of your AWS resources for compliance purposes. It allows you to set up rules that define the desired state of your resources and alerts you when there are configuration changes that don't comply with those rules. However, it is not specifically designed to manage infrastructure as code.\nAWS Inspector is a service that helps you assess the security and compliance of your EC2 instances and applications by automatically identifying security vulnerabilities and deviations from best practices. It is a security assessment tool and does not manage infrastructure as code.\nAWS Trusted Advisor is a service that provides recommendations to optimize your AWS infrastructure for cost optimization, security, performance, and fault tolerance. It helps you to follow best practices and improve your infrastructure, but it does not manage infrastructure as code.\nTherefore, the correct answer is A. Using AWS CloudFormation.\n\n"
}, {
  "id" : 144,
  "question" : "Which one of the following features does NOT belong to any Well-Architected Framework pillar in AWS?\n",
  "answers" : [ {
    "id" : "74d10e9e7ebd48a6ba54503946bafcfd",
    "option" : "It provides the ability to protect information, systems &amp; assets.",
    "isCorrect" : "false"
  }, {
    "id" : "9010742dedda43cf96fd37769d8b79ce",
    "option" : "It provides the ability to configure servers with much more CPU resources than required so that users do not need to maintain the CPU resources for a long time.",
    "isCorrect" : "true"
  }, {
    "id" : "0c4e130a06c1459daf83bd79b4f0beca",
    "option" : "It provides the ability to avoid or eliminate unneeded costs.",
    "isCorrect" : "false"
  }, {
    "id" : "6d22b5e0d7f140038c6d4bee9e1beb4d",
    "option" : "It provides the ability to recover from infrastructure or system failures.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nThe AWS Well architected framework is based on the five pillars - Security, Reliability, Performance efficiency, Cost Optimization &amp; Operational Excellence.\nOption A is incorrect since it belongs to the Security pillar of the AWS well architected framework.\nOption B is CORRECT because configuring much more CPU resources than required is not appropriate in a cloud environment such as AWS.\nThis method is not cost-efficient as well.\nOption C is incorrect since it belongs to the Cost Optimization pillar of the AWS well architected framework.\nOption D is incorrect since it belongs to the Reliability pillar of the AWS well architected framework.\nReferences:\nhttps://aws.amazon.com/architecture/well-architected/?wa-lens-whitepapers.sort-by=item.additionalFields.sortDate&amp;wa-lens-whitepapers.sort-order=desc\nhttps://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/\n\nThe Well-Architected Framework is a set of best practices and guidelines for designing and operating reliable, secure, efficient, and cost-effective systems in the AWS Cloud. It consists of five pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization.\nAnswer B does not belong to any of the pillars of the Well-Architected Framework in AWS.\nExplanation of each answer:\nA. Security: This pillar focuses on protecting information, systems, and assets. It includes strategies for confidentiality and integrity of data, identifying and managing risks, and implementing controls to detect and mitigate security threats.\nB. Not a Well-Architected Framework pillar: The ability to configure servers with much more CPU resources than required so that users do not need to maintain the CPU resources for a long time is not a characteristic of any of the five pillars of the Well-Architected Framework.\nC. Cost Optimization: This pillar focuses on avoiding or eliminating unneeded costs. It includes strategies for optimizing the use of resources, monitoring usage, and identifying opportunities for cost savings.\nD. Reliability: This pillar focuses on the ability to recover from infrastructure or system failures. It includes strategies for designing fault-tolerant systems, implementing backup and recovery procedures, and testing and validating disaster recovery plans.\nIn summary, the correct answer is B, which is not a feature of any Well-Architected Framework pillar in AWS.\n\n"
}, {
  "id" : 145,
  "question" : "Which of the following is the responsibility of AWS according to the Shared Security Model? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "206f3a762bf74dbdb81977d3bc833af4",
    "option" : "Managing AWS Identity and Access Management (IAM)",
    "isCorrect" : "false"
  }, {
    "id" : "3d06de2df1a5400c852ef797bccead6a",
    "option" : "Securing edge locations",
    "isCorrect" : "true"
  }, {
    "id" : "49e4849831d64808a1107589cf21032d",
    "option" : "Monitoring physical device security",
    "isCorrect" : "true"
  }, {
    "id" : "b2b3e60891374e79aa8f7944fb52ba69",
    "option" : "Implementing service organization Control (SOC) standards.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B, C and D.\nThe responsibility of AWS includes the following.\n1)Securing edge locations.\n2)Monitoring physical device security.\n3)Implementing service organization Control (SOC) standards.\nFor more information on AWS Shared Responsibility Model, please refer to the below URL:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\n\nThe Shared Security Model is a security framework that outlines the responsibilities of both AWS and its customers for securing the resources and data stored in the AWS cloud. The framework clarifies which security responsibilities are AWS's and which are the customers' to help ensure that security risks are properly managed.\nAccording to the Shared Security Model, AWS is responsible for ensuring the security of the cloud infrastructure, while the customer is responsible for securing any data and applications they deploy on the cloud. Specifically, AWS is responsible for the following:\nSecuring the cloud infrastructure: AWS is responsible for securing the hardware, software, and networking infrastructure that supports the cloud. This includes securing edge locations and ensuring the physical security of its data centers, including monitoring access to its facilities. Complying with security standards: AWS is responsible for implementing security controls that comply with various security standards such as SOC 1, SOC 2, and ISO 27001. These standards help ensure that AWS's infrastructure is secure and can be trusted by its customers. Managing the AWS environment: AWS is responsible for managing the cloud infrastructure and the services that run on it. This includes managing AWS Identity and Access Management (IAM) which is a service that enables customers to control access to their AWS resources.\nTherefore, options B, C, and D are the responsibilities of AWS according to the Shared Security Model. AWS is responsible for securing edge locations, monitoring physical device security, and implementing service organization Control (SOC) standards. Option A, managing AWS Identity and Access Management (IAM), is the responsibility of the customer since IAM enables customers to control access to their AWS resources.\n\n"
}, {
  "id" : 146,
  "question" : "Your company has just started using the resources on the AWS Cloud.\nThey want to get an idea of the costs being incurred so far for the resources being used.\nHow can this be achieved?\n",
  "answers" : [ {
    "id" : "3386973db83d4fc6b3030108dc123278",
    "option" : "By going to the Amazon EC2 dashboard. Here you can see the costs of the running EC2 resources.",
    "isCorrect" : "false"
  }, {
    "id" : "3b4b6b84748a48bf9d152c680fe7eec6",
    "option" : "By using the AWS Cost Explorer. Here you can see the running and forecast costs.",
    "isCorrect" : "false"
  }, {
    "id" : "63bd8c6c833e4d4cabe7b0f8110fc69c",
    "option" : "By using the AWS Trusted Advisor dashboard. This dashboard will give you all the costs.",
    "isCorrect" : "false"
  }, {
    "id" : "eb6158098d744c05aef857622960f9b7",
    "option" : "By seeing the AWS Cloud Trail logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "The correct answer is Option.\nB.The AWS documentation mentions the following on AWS Cost Reports:\nCost Explorer is a free tool that you can use to view your costs.\nYou can view data up to the last 12 months and 12 months forecast how much you are likely to spend for the next three months and get recommendations for what Reserved Instances to purchase.\nFor more information on AWS Cost Reports, please refer to the below URL:\nhttp://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-explorer-what-is.html\n\nTo get an idea of the costs being incurred for the resources being used on AWS, you can use the AWS Cost Explorer. The AWS Cost Explorer provides a detailed view of your AWS costs and usage, allowing you to see the running and forecasted costs for your AWS services.\nYou can access the AWS Cost Explorer from the AWS Management Console. After logging in, go to the AWS Cost Explorer dashboard, which provides an overview of your AWS costs and usage. You can then use the various tools and reports within the Cost Explorer to view your costs and usage data by service, account, region, and more.\nSome of the features available in the AWS Cost Explorer include:\nCost and Usage Reports: These reports provide detailed information on your AWS costs and usage data, broken down by service, account, region, and more. You can customize these reports to meet your specific needs and export them to other tools for further analysis. Budgets: You can set up budgets in the AWS Cost Explorer to help you monitor and manage your AWS costs. You can set alerts to notify you when you exceed your budget thresholds, and you can track your progress towards your budget goals over time. Reserved Instances: The AWS Cost Explorer can help you analyze your usage of Reserved Instances (RIs) and determine whether you are using your RIs effectively to save money on your AWS costs. Recommendations: The AWS Cost Explorer provides recommendations on ways to optimize your AWS costs, such as by using RIs, purchasing Savings Plans, or resizing your EC2 instances.\nIn conclusion, the AWS Cost Explorer is a powerful tool for monitoring and managing your AWS costs and usage. It provides detailed insights into your AWS spending, allowing you to make informed decisions about how to optimize your AWS resources and save money on your AWS bill.\n\n"
}, {
  "id" : 147,
  "question" : "By default who has complete administrative control over all resources in the respective AWS account?\n",
  "answers" : [ {
    "id" : "35e274e7a6c742c783cf141759cc6856",
    "option" : "AWS Support Team",
    "isCorrect" : "false"
  }, {
    "id" : "834f2ecf40a940b793610b06fe4b9b2f",
    "option" : "AWS Account Owner",
    "isCorrect" : "true"
  }, {
    "id" : "7c5e43e392c14dd7a72ec4cef65f584a",
    "option" : "AWS Security Team",
    "isCorrect" : "false"
  }, {
    "id" : "39a4f95233f54745b92642a271bfc308",
    "option" : "AWS Technical Account Manager (TAM)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect.\nAWS Support provides a mix of tools and technology, people, and programs designed to proactively help you optimize performance, lower costs, and innovate faster.\nOption B is correct.\nThe entire control of data within an AWS account is with the Account Owner.\nOption C is incorrect.\nThe Security Specialist team is a customer-facing role where we get to talk to customers about the security, identity, and compliance of AWS.\nWe help enable customers to securely and compliantly adopt the AWS Cloud.\nOption D is incorrect.\nA technical account manager provides technical support to customers before and after a sale.\nThe technical account manager will work with the client to build strong relationships and ensure customer satisfaction.\nFor more information on AWS Account identifiers, please refer to the below URL:\nhttps://docs.aws.amazon.com/general/latest/gr/root-vs-iam.html\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html\n\nThe correct answer is B. AWS Account Owner.\nThe AWS account owner has complete administrative control over all resources in the respective AWS account by default. This means that the account owner has full access to create, modify, and delete resources, as well as manage user access and permissions.\nAWS Support Team provides technical assistance and helps to troubleshoot issues with AWS services, but they do not have administrative control over an AWS account.\nThe AWS Security Team is responsible for ensuring the security of AWS infrastructure and services. They do not have direct administrative control over individual AWS accounts.\nThe AWS Technical Account Manager (TAM) is a technical advisor that provides guidance and best practices for AWS services. They do not have administrative control over an AWS account.\nIt's important to note that while the AWS account owner has full administrative control, it's best practice to use the principle of least privilege when granting access to other users. This means giving users only the minimum permissions necessary to perform their job functions, rather than granting them full administrative control.\n\n"
}, {
  "id" : 148,
  "question" : "Your design team is planning to design an application that will be hosted on the AWS Cloud.\nOne of their main non-functional requirements is given below: Reduce inter-dependencies so failures do not impact other components. Which of the following concepts does this requirement relate to?\n",
  "answers" : [ {
    "id" : "1a7157200ab34a0a949e730b76ac026c",
    "option" : "Integration",
    "isCorrect" : "false"
  }, {
    "id" : "209313526a7e45e7a24f32755afb7af2",
    "option" : "Decoupling",
    "isCorrect" : "true"
  }, {
    "id" : "4e7d619ccbdf4cf88544cfe891deb1b1",
    "option" : "Aggregation",
    "isCorrect" : "false"
  }, {
    "id" : "18168537777a42519f8036f4d63b8d94",
    "option" : "Segregation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe entire concept of decoupling components ensures that the different components of applications can be managed and maintained separately.\nIf all components are tightly coupled, the entire application would go down when one component goes down.\nHence it is always a better practice to decouple application components.\nFor more information on a decoupled architecture, please refer to the below URL:\nhttp://whatis.techtarget.com/definition/decoupled-architecture\n\nThe non-functional requirement mentioned in the question is related to the concept of \"decoupling.\"\nDecoupling is a design approach that emphasizes reducing dependencies between components of a system. By minimizing the dependencies, each component can function independently, reducing the risk of failures spreading across the system.\nIn the context of AWS Cloud, decoupling can be achieved through the use of various services such as Amazon S3, Amazon SQS, and AWS Lambda. These services allow different components of an application to interact with each other through well-defined APIs, without having to be tightly coupled.\nFor example, instead of having components directly communicate with each other, they can communicate via Amazon S3 or Amazon SQS, which act as a buffer between them. Similarly, instead of having a monolithic application, AWS Lambda can be used to break down the application into smaller, independent functions that can be executed separately.\nBy decoupling components, the application becomes more resilient to failures since any failures that occur in one component are less likely to affect other components. This approach also makes it easier to scale individual components independently, as well as to replace or upgrade them without affecting the rest of the application.\nTherefore, in the context of the given non-functional requirement, the design team should focus on decoupling components to reduce inter-dependencies and minimize the impact of failures on other components.\n\n"
}, {
  "id" : 149,
  "question" : "Which of the following can be used to increase the fault tolerance of an application?\n",
  "answers" : [ {
    "id" : "dd8a1f7565d44d8ab332a81fa44efbdc",
    "option" : "Deploying resources across multiple edge locations",
    "isCorrect" : "false"
  }, {
    "id" : "82e3e1504bcd4ec097ce298017d0ec74",
    "option" : "Deploying resources across multiple VPCâ€™s",
    "isCorrect" : "false"
  }, {
    "id" : "4fccac49244b440fa0d01b8c05e375c1",
    "option" : "Deploying resources across multiple Availability Zones",
    "isCorrect" : "true"
  }, {
    "id" : "f1f1425c6e4a42fcbf69e72f2a498d9c",
    "option" : "Deploying resources across multiple AWS Accounts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nEach AZ is a set of one or more data centers.\nBy deploying your AWS resources to multiple Availability zones, you are designing with failure in mind.\nSo if one AZ were to go down, the other AZ's would still be up and running.\nHence your application would be more fault-tolerant.\nFor more information on AWS Regions and AZ's, please refer to the below URL:\nhttp://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html\n\nThe correct answer is C. Deploying resources across multiple Availability Zones.\nExplanation: Fault tolerance refers to the ability of an application to continue to function in the event of a failure. AWS provides several services and features to increase the fault tolerance of an application, including deploying resources across multiple Availability Zones.\nAn Availability Zone (AZ) is a distinct location within an AWS Region that is engineered to be isolated from failures in other AZs. Deploying resources across multiple AZs provides a high level of availability and fault tolerance because it ensures that your application will continue to function even if one or more AZs become unavailable due to natural disasters, power outages, or other unforeseen events.\nDeploying resources across multiple edge locations (A) can improve the performance of an application by reducing latency and improving the user experience. Edge locations are part of Amazon CloudFront, AWS's content delivery network (CDN), which delivers content to users from the nearest edge location.\nDeploying resources across multiple VPCs (B) is not directly related to increasing the fault tolerance of an application. VPCs are logical isolated networks within an AWS account, and deploying resources across multiple VPCs can help you organize your infrastructure and improve security, but it does not increase the fault tolerance of an application.\nDeploying resources across multiple AWS accounts (D) can also help you organize your infrastructure and improve security, but it does not directly increase the fault tolerance of an application.\n\n"
}, {
  "id" : 150,
  "question" : "Which of the following security requirements are managed by AWS? Select 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9d855ed8bd5149fcbc203d04fe396d30",
    "option" : "Password Policies",
    "isCorrect" : "false"
  }, {
    "id" : "81a4384dca9b49ee91bd5c14b7afedf7",
    "option" : "User permissions",
    "isCorrect" : "false"
  }, {
    "id" : "67ac5e7c648845aab3d3cf95b7dec2ab",
    "option" : "Physical security",
    "isCorrect" : "true"
  }, {
    "id" : "f39f77cdbd5f4ae8b80e046e9de42db7",
    "option" : "Disk disposal",
    "isCorrect" : "true"
  }, {
    "id" : "43c83e1d536f4294a2da82a513f222c6",
    "option" : "Hardware patching.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C, D and E.\nAs per the Shared Responsibility Model, the Patching of the underlying hardware and physical security of AWS resources is the responsibility of AWS.\nFor more information on AWS Shared Responsibility Model, please refer to the below URL-\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\nDisk disposal-\nStorage Device Decommissioning: When a storage device has reached the end of its useful life, AWS procedures include a decommissioning process designed to prevent customer data from being exposed to unauthorized individuals.\nAWS uses the techniques detailed in DoD 5220.22-M (â€œNational Industrial Security Program Operating Manual â€œ) or NIST 800-88 (â€œGuidelines for Media Sanitizationâ€) to destroy data as part of the decommissioning process.\nAll decommissioned magnetic storage devices are degaussed and physically destroyed in accordance with industry-standard practices.\nFor more information on Disk disposal, please refer to the below URL-\nhttps://d0.awsstatic.com/whitepapers/aws-security-whitepaper.pdf\n\nAWS manages many aspects of security, including physical security, network security, host security, application security, and data security. However, some security requirements are the responsibility of AWS customers, while others are managed by AWS. In this context, we need to identify the security requirements that are managed by AWS.\nThe correct answers are A, C, and E.\nA. Password Policies: AWS manages password policies that enforce password complexity rules, require password rotation, and limit the number of failed login attempts. AWS Identity and Access Management (IAM) allows you to set up password policies and configure multi-factor authentication (MFA) for your IAM users.\nC. Physical security: AWS manages physical security to protect its data centers, network infrastructure, and hardware assets. AWS data centers are equipped with advanced physical security controls, such as biometric access controls, 24/7 monitoring, and perimeter fencing.\nE. Hardware patching: AWS manages hardware patching to ensure that the underlying infrastructure is secure and up to date. AWS patches the underlying hardware, firmware, and software that support the AWS cloud services.\nB. User permissions: User permissions are managed by the AWS customers. AWS IAM provides granular control over user access to AWS resources. IAM allows customers to create and manage users, groups, and roles, and define policies that govern their access to AWS resources.\nD. Disk disposal: Disk disposal is the responsibility of AWS customers. Customers need to ensure that they dispose of their data securely when they terminate their AWS instances or storage volumes.\nTherefore, Password Policies, Physical security, and Hardware patching are security requirements that are managed by AWS.\n\n"
}, {
  "id" : 151,
  "question" : "Which of the following is not the pillars of AWS Well-Architected Framework?\n",
  "answers" : [ {
    "id" : "b64f62acd0ed40fd8c7004b20c0e7fb8",
    "option" : "Automation",
    "isCorrect" : "true"
  }, {
    "id" : "d5142d54a97a40f682da1096ccd91ab3",
    "option" : "Cost Optimization",
    "isCorrect" : "false"
  }, {
    "id" : "7a1fd3b00c344e7ebd7a135e658a2fd6",
    "option" : "Reliability",
    "isCorrect" : "false"
  }, {
    "id" : "76bde05cce7f4ff79ad309f24d056eee",
    "option" : "Performance Efficiency.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAs per AWS Well-Architected Framework, the following are the 5 pillars:\nÂ· Operational Excellence.\nÂ· Security.\nÂ· Reliability.\nÂ· Performance Efficiency.\nÂ· Cost Optimization.\nAutomation is not part of AWS Well-Architected Framework pillars.\nOptions B, C &amp; D are incorrect as these are part of 5 Pillars of AWS Well-Architected Framework.\nReference:\nhttps://docs.aws.amazon.com/wellarchitected/latest/framework/cost-bp.html\n\nThe AWS Well-Architected Framework is a set of best practices and guidelines designed to help customers build and operate reliable, secure, efficient, and cost-effective systems in the cloud. The framework is built around five pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization.\nThe correct answer to this question is A. Automation. Automation is not one of the five pillars of the AWS Well-Architected Framework.\nHere is a brief explanation of each of the five pillars:\nOperational Excellence: This pillar focuses on the ability to run and monitor systems to deliver business value, and to continually improve supporting processes and procedures. Security: This pillar focuses on the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. Reliability: This pillar focuses on the ability to prevent and quickly recover from failures to meet business and customer demand. Performance Efficiency: This pillar focuses on the ability to use computing resources efficiently to meet system requirements and maintain that efficiency as demand changes and technologies evolve. Cost Optimization: This pillar focuses on avoiding unnecessary costs, identifying opportunities to reduce costs, and applying cloud financial management best practices.\nIn summary, while Automation is important in the AWS cloud, it is not one of the five pillars of the AWS Well-Architected Framework.\n\n"
}, {
  "id" : 152,
  "question" : "Your company is planning to offload some of the batch processing workloads on to AWS.\nThese jobs can be interrupted and resumed at any time.\nWhich of the following instance types would be the most cost-effective to use for this purpose?\n",
  "answers" : [ {
    "id" : "9680f8903b524f7d80b0163c687a3fcc",
    "option" : "On-Demand",
    "isCorrect" : "false"
  }, {
    "id" : "89d392103bf842a7bf826e5b0601fc86",
    "option" : "Spot",
    "isCorrect" : "true"
  }, {
    "id" : "7d40fe5a8c1e455e81ec253208d3a99b",
    "option" : "Full Upfront Reserved",
    "isCorrect" : "false"
  }, {
    "id" : "453ec7bcd7244c7784494f8fee438433",
    "option" : "Partial Upfront Reserved.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS documentation mentions the following:\nSpot Instances are a cost-effective choice if you can be flexible about when your applications run and if your applications can be interrupted.\nFor example, Spot Instances are well-suited for data analysis, batch jobs, background processing, and optional tasks.\nFor more information on AWS Spot Instances, please refer to the below URL:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\n\nThe most cost-effective instance type for batch processing workloads that can be interrupted and resumed at any time would be Spot instances.\nSpot instances allow you to bid on unused EC2 instances, and you pay the market price for the instances you use. The price for Spot instances is often significantly lower than the On-Demand or Reserved instances, making them an ideal choice for workloads that can tolerate interruptions and can be resumed later.\nOn-Demand instances are the most expensive option, as you pay for the instances by the hour with no upfront commitment. These instances are best suited for workloads that require consistent, uninterrupted performance.\nReserved instances can be either Full Upfront or Partial Upfront Reserved, which require an upfront commitment for a term of 1 or 3 years. While these instances offer significant cost savings over On-Demand instances, they are not a good fit for workloads that are highly variable and can be interrupted at any time.\nIn summary, Spot instances would be the most cost-effective option for batch processing workloads that can be interrupted and resumed at any time. However, it is important to note that Spot instances may be terminated if the market price for instances rises above your bid price, so you should be prepared to handle interruptions and implement a strategy for handling interrupted workloads.\n\n"
}, {
  "id" : 153,
  "question" : "Which service can be used to create steps required to automate build, test and deployments for a web application?\n",
  "answers" : [ {
    "id" : "1e0faee9ed0c460e88f114fc710caad1",
    "option" : "AWS CodeCommit",
    "isCorrect" : "false"
  }, {
    "id" : "890d816d4502434d9e58ba21bb653729",
    "option" : "AWS CodePipeline",
    "isCorrect" : "true"
  }, {
    "id" : "aed41397f68a4bf981cdd5bc5beb1617",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "false"
  }, {
    "id" : "3d9f416ac6124a11b98bb678d211fb68",
    "option" : "AWS CodeBuild.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS CodePipeline is a fully managed service that automates the release pipeline for application updates.\nFor updates, it uses application code stored in AWS CodeCommit, performs testing using AWS CodeBuild, and uses AWS CodeDeploy for deployment.\nOption A is incorrect as AWS CodeCommit is used to store deployment codes.\nOption C is incorrect as AWS CodeDeploy is used for deployment of codes to resources.\nOption D is incorrect as AWS CodeBuild is used to test and build application code.\nFor more information on AWS CodePipeline, refer to the following URL:\nhttps://aws.amazon.com/codepipeline/faqs/\n\nThe service that can be used to create steps required to automate build, test and deployments for a web application is AWS CodePipeline (option B).\nAWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. With CodePipeline, you can define a series of steps or stages, which represent the different parts of your application delivery process, such as building, testing, and deploying. You can use pre-built actions, or create custom ones, to define the tasks that need to be executed in each stage.\nIn the case of a web application, you can use CodePipeline to automatically build your application code from a source code repository (such as AWS CodeCommit), run automated tests, and then deploy the application to your chosen environment (such as Amazon EC2 instances or AWS Elastic Beanstalk).\nCodePipeline also integrates with other AWS services, such as AWS CodeBuild (option D) for building and packaging your application code, AWS CodeDeploy (option C) for deploying your code to a fleet of EC2 instances or on-premises servers, and AWS CloudFormation for managing your infrastructure as code.\nAWS CodeCommit (option A) is a fully-managed source control service that hosts secure and highly scalable private Git repositories. It can be used to store and version your application code, but it doesn't provide the automated build, test, and deployment capabilities of CodePipeline.\nIn summary, AWS CodePipeline (option B) is the service that can be used to create steps required to automate build, test and deployments for a web application.\n\n"
}, {
  "id" : 154,
  "question" : "Your company is planning to use the AWS Cloud.\nBut there is a management decision that resources need to split department wise.\nAnd the decision is tending towards managing multiple AWS accounts.\nWhich of the following would help in the effective management and also provide an efficient costing model?\n",
  "answers" : [ {
    "id" : "2e81730f909e42e4b6331348be154a05",
    "option" : "AWS Organizations",
    "isCorrect" : "true"
  }, {
    "id" : "b45a4f0479d945e5a897880f88f8946a",
    "option" : "Amazon Dev Pay",
    "isCorrect" : "false"
  }, {
    "id" : "0f0a6b5d589342a684875cfdb4c901a6",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "b0c2723cc7a044efb6343c996f104797",
    "option" : "AWS Cost Explorer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS Organizations offers policy-based management for multiple AWS accounts.\nWith organizations, you can create groups of accounts and then apply policies to those groups.\nOrganizations enable you to centrally manage policies across multiple accounts without requiring custom scripts and manual processes.\nFor more information on the AWS Organizations, please refer to the below URL:\nhttps://aws.amazon.com/organizations/\n\nThe answer is A. AWS Organizations.\nAWS Organizations is a service that helps to manage multiple AWS accounts in an organized manner. It allows you to create groups of AWS accounts, called Organizational Units (OUs), and apply policies to them. This makes it easier to manage and govern multiple accounts.\nBy using AWS Organizations, you can create separate AWS accounts for each department, which allows you to keep resources separated, and also provides more control over security and compliance. Additionally, AWS Organizations allows you to create a hierarchy of OUs, which can be useful for managing complex organizational structures.\nAnother advantage of using AWS Organizations is that it provides a consolidated billing feature. This feature allows you to consolidate the billing for all your AWS accounts, which can make it easier to manage costs and provide an efficient costing model.\nThe other options listed are not as relevant for managing multiple AWS accounts as AWS Organizations.\nAmazon Dev Pay is a payment and billing service that allows developers to monetize their applications and services. It is not related to managing multiple AWS accounts.\nAWS Trusted Advisor is a service that provides recommendations for optimizing your AWS infrastructure. While it can be useful for managing individual accounts, it does not provide the organizational and billing features that AWS Organizations does.\nAWS Cost Explorer is a cost management tool that provides insights into your AWS spending. While it can be useful for managing costs, it does not provide the organizational features that AWS Organizations does, nor does it provide a consolidated billing feature.\n\n"
}, {
  "id" : 155,
  "question" : "Which of the following can be used as an additional security layer for the user name and password when logging into the AWS Console?\n",
  "answers" : [ {
    "id" : "25bae2ce00754f0eb5fc64d6f19f0735",
    "option" : "Multi-Factor Authentication (MFA)",
    "isCorrect" : "true"
  }, {
    "id" : "46af1c550f3a4cf586f887108a6dc647",
    "option" : "Secondary password",
    "isCorrect" : "false"
  }, {
    "id" : "4c666e1e5f7047a59038b31ea87b273a",
    "option" : "Root access privileges",
    "isCorrect" : "false"
  }, {
    "id" : "b936de3a6f474b50a10d6127e31c822c",
    "option" : "Secondary user name.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following:\nAWS Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name and password.\nFor more information on the AWS MFA, please refer to the below URL:\nhttps://aws.amazon.com/iam/details/mfa/\n\nThe additional security layer that can be used for the username and password when logging into the AWS Console is Multi-Factor Authentication (MFA).\nMFA adds an extra layer of security by requiring the user to provide a second form of authentication, such as a unique security code, in addition to the username and password. This helps protect against unauthorized access to the account even if the password is compromised.\nTo use MFA for AWS Console access, the user needs to enable MFA for their IAM user account, and then use an MFA device (such as a hardware token or virtual MFA app) to generate a unique security code that needs to be entered during the login process.\nSecondary password and secondary username are not valid options as they do not provide an extra layer of security but rather just a different set of credentials to access the account.\nRoot access privileges should be limited and not used for everyday tasks, as it provides unrestricted access to all resources in the AWS account, including billing and account management functions. It is not a valid option to add an additional security layer to the username and password when logging into the AWS Console.\n\n"
}, {
  "id" : 156,
  "question" : "Which AWS Cloud service helps in the quick deployment of resources which can use different programming languages such as .Net and Java?\n",
  "answers" : [ {
    "id" : "c57f8f477b2d442c82590bc6c20de76b",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "true"
  }, {
    "id" : "3895788cc8964b6da7b4470aa3d65821",
    "option" : "AWS Elastic Compute Cloud (Amazon EC2)",
    "isCorrect" : "false"
  }, {
    "id" : "a658c6b683004c28808e600e3d626a54",
    "option" : "AWS VPC",
    "isCorrect" : "false"
  }, {
    "id" : "b90ed227f638498ab2f233d33af94487",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following:\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nFor more information on enabling AWS Elastic beanstalk, please refer to the below URL:\nhttps://aws.amazon.com/elasticbeanstalk/?p=tile\n\nThe correct answer is A. AWS Elastic Beanstalk.\nAWS Elastic Beanstalk is a fully-managed service that allows users to quickly deploy and run web applications and services on popular programming languages such as .Net, Java, Node.js, Python, Ruby, Go, and Docker. With Elastic Beanstalk, developers can simply upload their application code and Elastic Beanstalk will automatically handle the deployment, scaling, and monitoring of the application.\nOn the other hand, Amazon EC2 (B) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2 allows users to create virtual machines, known as instances, and run a wide range of computing tasks.\nAWS VPC (C) is a virtual private cloud service that enables users to launch AWS resources into a virtual network that is isolated from other resources in the AWS cloud. VPC provides complete control over the virtual networking environment, including selection of IP address ranges, creation of subnets, and configuration of route tables and network gateways.\nAWS SQS (D) is a fully managed message queuing service that enables users to decouple and scale microservices, distributed systems, and serverless applications. With SQS, messages are stored in a queue until a recipient retrieves them.\nIn summary, Elastic Beanstalk is the correct answer as it is specifically designed for quick deployment of web applications on a variety of programming languages.\n\n"
}, {
  "id" : 157,
  "question" : "Your company handles a crucial e-Commerce application.\nThis application needs to have an uptime of at least 99.5%\nThere is a decision to move the application to the AWS Cloud.\nWhich of the following deployment strategies can help build a robust architecture for such an application?\n",
  "answers" : [ {
    "id" : "0ae764a407cd43d295846b666e2bb191",
    "option" : "Deploying the application across multiple VPCâ€™s",
    "isCorrect" : "false"
  }, {
    "id" : "fa7e062a8fdf4217998e3ea4d2248d29",
    "option" : "Deploying the application across multiple Regions",
    "isCorrect" : "true"
  }, {
    "id" : "aaaf99e49802405ca4f8ce7c43e57db7",
    "option" : "Deploying the application across Edge locations",
    "isCorrect" : "false"
  }, {
    "id" : "d29c59dfa7f04ca6ba338006bac2bd1f",
    "option" : "Deploying the application across multiple subnets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nBusinesses are using the AWS cloud to enable faster disaster recovery of their critical IT systems without incurring the infrastructure expense of a second physical site.\nThe AWS cloud supports many popular disaster recovery (DR) architectures from â€œpilot lightâ€ environments that may be suitable for small customer workload data center failures to â€œhot standbyâ€ environments that enable rapid failover at scale.\nWith data centers in Regions worldwide, AWS provides a set of cloud-based disaster recovery services that enable rapid recovery of your IT infrastructure and data.\nFor more information on enabling AWS Disaster Recovery, please refer to the below URL:\nhttps://aws.amazon.com/disaster-recovery/\n\nTo build a robust architecture for a crucial e-Commerce application that needs to have an uptime of at least 99.5%, one should focus on deploying the application across multiple AWS Regions.\nRegions are separate geographic areas that contain at least two Availability Zones (AZs) that are isolated from each other in terms of infrastructure, power, and network connectivity. Deploying an application across multiple regions ensures that the application remains highly available even if an entire region becomes unavailable due to a natural disaster or other event.\nDeploying the application across multiple VPCs or subnets does not necessarily improve availability as they are within the same region and may be affected by the same event.\nDeploying the application across Edge locations is not relevant as Edge locations are used for content delivery purposes and do not provide the infrastructure required for hosting an application.\nTherefore, the correct answer is B: Deploying the application across multiple Regions.\n\n"
}, {
  "id" : 158,
  "question" : "Your company is moving a large application to AWS using a set of EC2 instances.\nA key requirement is reusing existing server-bound software licensing.\nWhich of the following options is the best for satisfying the requirement?\n",
  "answers" : [ {
    "id" : "09e8710dee0943ed9247c3392fa648dd",
    "option" : "EC2 Dedicated Instances",
    "isCorrect" : "false"
  }, {
    "id" : "f648f8ae9c0c46e584457d88fee7ffd6",
    "option" : "EC2 Reserved Instances",
    "isCorrect" : "false"
  }, {
    "id" : "e3d371aa2a004b53a9a4469c34a848cc",
    "option" : "EC2 Dedicated Hosts",
    "isCorrect" : "true"
  }, {
    "id" : "41f1c33a45c549c3b3cf8d4edce6e9b7",
    "option" : "EC2 Spot Instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT because despite instances run on a single-tenant hardware, AWS does not give visibility to sockets and cores required for reusing server bound licenses.\nAWS highlights this in the comparison table at the following link:\nhttps://aws.amazon.com/ec2/dedicated-hosts/\nOption B is INCORRECT because Reserved Instances are only a purchasing option and there's no way to control the hardware where these instances are running on.\nOption C is CORRECT because instances run on a dedicated hardware where AWS gives visibility of physical characteristics.\nAWS documentation mentions this with the following sentence:â€œ...Dedicated Host gives you additional visibility and control over how instances are placed on a physical server, and you can consistently deploy your instances to the same physical server over time.\nAs a result, Dedicated Hosts enable you to use your existing server-bound software licenses and address corporate compliance and regulatory requirements.â€\nOption D is INCORRECT because Spot Instances are only a purchasing option.\nDiagram: none.\nReferences:\nAWS documentation explains the possibility of reusing server bound license:\nhttps://aws.amazon.com/ec2/dedicated-hosts/\n\nThe best option for reusing existing server-bound software licensing when moving a large application to AWS using a set of EC2 instances is EC2 Dedicated Hosts (option C).\nEC2 Dedicated Hosts provide physical servers fully dedicated to your use, allowing you to use your existing server-bound software licenses. EC2 Dedicated Instances (option A) provide instances running on hardware dedicated to a single AWS account but still share the underlying physical hardware with other EC2 instances from other customers. EC2 Reserved Instances (option B) offer a discounted hourly rate but do not guarantee a dedicated host. EC2 Spot Instances (option D) allow you to bid for unused EC2 capacity and can be interrupted at any time, which is not a reliable option for running production workloads.\nTherefore, EC2 Dedicated Hosts are the best option for satisfying the requirement of reusing existing server-bound software licensing when moving a large application to AWS using a set of EC2 instances.\n\n"
}, {
  "id" : 159,
  "question" : "You are planning on deploying a video-based application onto the AWS Cloud.\nUsers across the world will access these videos.\nWhich of the below services can help efficiently stream the content to the users across the globe?\n",
  "answers" : [ {
    "id" : "4fb6e9632e5444c3b54f0c3fc1e3bf00",
    "option" : "Amazon SES",
    "isCorrect" : "false"
  }, {
    "id" : "f3905eacfb01419f87f1b9ef04c4cf2d",
    "option" : "Amazon Cloudtrail",
    "isCorrect" : "false"
  }, {
    "id" : "84ae09f869ae43fa97a2a02d0f9ef1c8",
    "option" : "Amazon CloudFront",
    "isCorrect" : "true"
  }, {
    "id" : "5a0780958eec4d7c830f6d6e2621705d",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following:\nAmazon CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds.\nLike other AWS services, Amazon CloudFront is a self-service, pay-per-use offering, requiring no long term commitments or minimum fees.\nWith CloudFront, your files are delivered to end-users using a global network of edge locations.\nFor more information on CloudFront, please visit the link:\nhttps://aws.amazon.com/cloudfront/\n\nThe service that can help efficiently stream the content to the users across the globe is Amazon CloudFront, which is a content delivery network (CDN) service provided by AWS.\nAmazon CloudFront works by caching content at edge locations, which are geographically distributed data centers that are closer to the end-users. When a user requests a video, CloudFront will serve it from the edge location that is closest to the user, reducing latency and improving performance.\nCloudFront also supports streaming of on-demand and live video content using the popular HTTP Live Streaming (HLS) and Dynamic Adaptive Streaming over HTTP (DASH) protocols. These protocols allow the video stream to be delivered to the user in small chunks, adapting to the user's network speed and ensuring smooth playback.\nTo use CloudFront for video streaming, the video files need to be stored in an Amazon S3 bucket or an HTTP server that is publicly accessible. CloudFront can then be configured to use the S3 bucket or the HTTP server as the origin, and it will automatically distribute the content to the edge locations.\nIn summary, Amazon CloudFront is the ideal service for efficiently streaming video content to users across the globe, thanks to its content caching at edge locations and support for popular video streaming protocols.\n\n"
}, {
  "id" : 160,
  "question" : "Using Content Delivery Network (CDN), an administrator would like to serve varying types of content based on the viewer's browser cookies.\nWhich is the most appropriate serverless technique that can be used to achieve this?\n",
  "answers" : [ {
    "id" : "9a0481e1d2c5431ba2bde317f6151c0e",
    "option" : "AWS CodeCommit",
    "isCorrect" : "false"
  }, {
    "id" : "3bcd93574b104cd3a89bb4117be2dc8f",
    "option" : "AWS Lambda@Edge",
    "isCorrect" : "true"
  }, {
    "id" : "e00970bbbeb54559a0d9d412e85fbd62",
    "option" : "AWS CodeStar",
    "isCorrect" : "false"
  }, {
    "id" : "d030bca58c35499a92e2725a370af2a4",
    "option" : "AWS Cloud9",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Lambda@Edge is a serverless service that makes it possible to run event-triggered functions on Edge Locations within the AWS Content Delivery Network.\nUsing AWS CloudFront, an administrator can introduce decision-making and compute processing closer to the viewer's location.\nThereby it improves on their browsing experience.\nhttps://aws.amazon.com/lambda/edge/\nOption A is INCORRECT because AWS CodeCommit is inappropriate in addressing the scenario.\nIt is a service that allows for the management of software development versions and software development assets.\nThese include binary files, documents and source code.\nOption C is INCORRECT because AWS CodeStar is a service used to manage software development projects.\nIt is not the appropriate Option for the scenario.\nCodeStar project makes it possible to develop, build and deploy applications.\nOption D is INCORRECT because it is not the best solution though it can be used in the scenario to write, run and deploy code.\nIt is an integrated development environment (IDE) that can accommodate various runtimes.\nSince the Lambda@Edge is best suited to meet the requirements of the scenario, this makes this Option incorrect.\n\nThe most appropriate serverless technique to serve varying types of content based on viewer's browser cookies using Content Delivery Network (CDN) in AWS is AWS Lambda@Edge (Option B).\nAWS Lambda@Edge is a serverless compute service that allows you to run Lambda functions to modify content that's served through CloudFront, AWS's CDN service. With Lambda@Edge, you can modify requests and responses to customize content delivery based on the client's needs. This service allows you to execute custom code without deploying and managing servers, as Lambda@Edge automatically scales and provisions resources as needed.\nUsing Lambda@Edge, an administrator can write a Lambda function to detect the presence of a specific cookie in a viewer's request, and use the cookie to determine what content to serve in response. Lambda@Edge can modify the response header to serve the appropriate content type or redirect the viewer to a different location based on the content they're requesting.\nAWS CodeCommit (Option A) is a source control service that allows teams to collaborate on code changes. It is not used to serve content based on viewer's browser cookies.\nAWS CodeStar (Option C) is a development platform that allows you to quickly develop, build, and deploy applications on AWS. It is not used to serve content based on viewer's browser cookies.\nAWS Cloud9 (Option D) is an integrated development environment (IDE) that allows developers to write, run, and debug code in the cloud. It is not used to serve content based on viewer's browser cookies.\n\n"
}, {
  "id" : 161,
  "question" : "For the AWS Shared Responsibility Model, which of the following responsibilities is NOT a part of shared controls by both customer and AWS?\n",
  "answers" : [ {
    "id" : "b126cd5e92fa46d3b89a1713de68c5b1",
    "option" : "Patch Management",
    "isCorrect" : "false"
  }, {
    "id" : "1a0628c4571c47ba925da2620fb23ea9",
    "option" : "Configuration Management",
    "isCorrect" : "false"
  }, {
    "id" : "3646790dc113410e991cc3eb69c7d888",
    "option" : "Global infrastructure that runs AWS Cloud services.",
    "isCorrect" : "true"
  }, {
    "id" : "c8ba128f4e07495e82dc659cd0befc23",
    "option" : "Training.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nThe global AWS infrastructure including the hardware, software, networking, and facilities is the responsibility of AWS, not the responsibility of the Customer.\nOption A is incorrect.\nAWS is responsible for patching resources within AWS infrastructure, while customers are responsible for patching guest OS and applications.\nOption B is incorrect.\nAWS is responsible for configuring resources within AWS infrastructure, while customers are responsible for configuring their guest OS, databases and applications.\nOption D is incorrect.\nAWS trains for AWS employees while the customer is responsible for training employees within their organizations.\nFor more information on the Shared responsibility model, refer to the following URL:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\n\nThe AWS Shared Responsibility Model outlines the security responsibilities between AWS and the customer. In this model, AWS is responsible for the security \"of\" the cloud, meaning the underlying infrastructure that runs AWS services. On the other hand, customers are responsible for security \"in\" the cloud, meaning the security of the data and applications they run on the AWS infrastructure.\nBoth AWS and the customer share certain security responsibilities, known as \"shared controls.\" These shared controls require a collaborative effort between AWS and the customer to ensure the security of the overall system. The shared controls include:\nA. Patch Management: This involves applying patches and updates to the operating system, applications, and software used by both AWS and the customer. AWS is responsible for patching the underlying infrastructure, while the customer is responsible for patching their own applications and software.\nB. Configuration Management: This involves configuring security settings for both AWS and customer resources. AWS is responsible for securing the infrastructure, while the customer is responsible for configuring their own resources according to AWS best practices.\nD. Training: This involves training personnel to maintain and manage the security of the system. Both AWS and the customer are responsible for training their own personnel on security best practices and procedures.\nAnswer C is not a part of shared controls by both customer and AWS. This is because the global infrastructure that runs AWS Cloud services is solely the responsibility of AWS. AWS designs, builds, and operates its infrastructure, including data centers, networks, and hardware, to ensure the security, availability, and resiliency of its services. Customers do not have access or control over the global infrastructure that runs AWS Cloud services, and are not responsible for it.\nIn summary, the AWS Shared Responsibility Model involves shared controls between AWS and the customer, with each party responsible for specific security aspects. Patch management, configuration management, and training are examples of shared controls, while the global infrastructure that runs AWS Cloud services is solely the responsibility of AWS.\n\n"
}, {
  "id" : 162,
  "question" : "There is a requirement to host EC2 Instances in the AWS Cloud, wherein the utilization is for a duration of more than 3 years.\nWhich of the following would you utilize to minimize the costs?\n",
  "answers" : [ {
    "id" : "ed7f2e2826ac495d9e46fe593cd78b13",
    "option" : "Reserved instances",
    "isCorrect" : "true"
  }, {
    "id" : "d3e980d335e44829ad8bddb482f10462",
    "option" : "On-demand instances",
    "isCorrect" : "false"
  }, {
    "id" : "406ceb837c454a03b4535a1d82060aa7",
    "option" : "Spot instances",
    "isCorrect" : "false"
  }, {
    "id" : "63b52a32c8574151bebdb4fb1acfda56",
    "option" : "Regular instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nWhen you have instances that will be used continuously and throughout the year, the best option is to buy reserved instances.\nBy buying reserved instances, you are actually allocated an instance for the entire year or the duration you specify with a reduced cost.\nFor more information on Reserved Instances, please visit the link:\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\n\nTo minimize the costs for hosting EC2 instances for a duration of more than 3 years in the AWS Cloud, we would utilize Reserved Instances.\nReserved Instances are a billing discount mechanism provided by AWS, allowing customers to reserve capacity for a specific instance type in a specific Availability Zone for either a one-year or three-year term.\nIn exchange for this commitment, AWS offers a significant discount on the hourly charge for running instances. Additionally, there are various types of Reserved Instances, such as Standard Reserved Instances, Convertible Reserved Instances, and Scheduled Reserved Instances, which provide different levels of flexibility in exchange for different pricing discounts.\nOn-demand instances are instances that can be launched at any time and are billed by the hour, with no upfront commitment. These instances are useful when you need instances for a short period or require flexibility in scaling up or down.\nSpot instances are spare compute capacity in AWS data centers that are available for a much lower price than On-demand instances. However, these instances are subject to price fluctuations based on demand, and AWS can terminate these instances with a two-minute warning when the spot price exceeds your bid.\nRegular instances refer to On-demand instances, which are billed by the hour, with no upfront commitment.\nTherefore, the most cost-effective option for hosting EC2 instances for more than three years in the AWS Cloud would be Reserved Instances.\n\n"
}, {
  "id" : 163,
  "question" : "An administrator would like to check if the Amazon CloudFront identity is making access API calls to an S3 bucket where a static website is hosted.\nWhere can this information be obtained?\n",
  "answers" : [ {
    "id" : "8c445829f6a34532a7d498aed3a580a7",
    "option" : "Configuring Amazon Athena to run queries on the Amazon CloudFront distribution.",
    "isCorrect" : "false"
  }, {
    "id" : "19676be4a6c84d029dd4b1e185ac9e4f",
    "option" : "Check AWS CloudWatch logs on the S3 bucket.",
    "isCorrect" : "false"
  }, {
    "id" : "4e6b0a97146645a1a7b523c9a8b23324",
    "option" : "In the webserver, tail for identity access logs from the Amazon CloudFront identity.",
    "isCorrect" : "false"
  }, {
    "id" : "ab137d76638f4f279670639d94560260",
    "option" : "In AWS CloudTrail Event history, look up access calls and filter for the Amazon CloudFront identity.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nBy viewing Event history in Amazon CloudTrail, the administrator can be able to access operational, access and activity logs for the past 90 days, to the S3 bucket that hosts the static website.\nhttps://docs.aws.amazon.com/awscloudtrail/latest/userguide/view-cloudtrail-events-console.html\nOption A is INCORRECT because Amazon Athena will need a specific data repository from which a database and table can be created in order to run queries.\nData repositories can be a folder in an S3 bucket where logs are written to.\nOption B is INCORRECT because AWS CloudWatch does not log access API calls from one resource to another.\nAWS CloudTrail can do this.\nOption C is INCORRECT because it is not possible to access the underlying web server for CloudFront.\nIt is fully managed by AWS.\n\nTo check if the Amazon CloudFront identity is making access API calls to an S3 bucket hosting a static website, an administrator can look up this information in the AWS CloudTrail event history. Therefore, option D is the correct answer.\nAWS CloudTrail is a service that records all API calls made within an AWS account, including the identity of the caller, the time of the call, the API name, and the response generated by the API. This information can be used to monitor and troubleshoot an AWS account's usage and can be used for compliance and auditing purposes.\nTo look up the access calls made by the Amazon CloudFront identity, an administrator can navigate to the AWS CloudTrail Event history in the AWS Management Console. Once there, they can filter the results based on the Amazon CloudFront identity and the S3 bucket in question.\nAlternatively, an administrator could configure Amazon Athena to run queries on the Amazon CloudFront distribution (option A). Athena is a serverless, interactive query service that makes it easy to analyze data in Amazon S3 using SQL. By creating a table that references the CloudFront distribution logs stored in an S3 bucket, an administrator could query the logs to obtain the information they need.\nOption B, checking AWS CloudWatch logs on the S3 bucket, is incorrect because CloudWatch is used for monitoring and logging services within AWS. While it's possible to configure CloudWatch to capture and store logs for an S3 bucket, these logs won't contain information about API calls made by the Amazon CloudFront identity.\nOption C, tailing the identity access logs from the Amazon CloudFront identity on the webserver, is also incorrect because CloudFront is a managed service that doesn't require the use of a web server. Additionally, tailing logs manually can be time-consuming and error-prone, especially for larger environments.\n\n"
}, {
  "id" : 164,
  "question" : "Which of the following AWS services can be used to retrieve configuration changes made to AWS resources causing operational issues?\n",
  "answers" : [ {
    "id" : "87e188ed531445cbb0ec3c9a34cec10b",
    "option" : "Amazon Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "3d20ce12561f43e9b27bf6fc5c060948",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "bc45e09ba0844a8587be408094bcccc2",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "5a606a6dabe8455fb7b6a50da14b4200",
    "option" : "AWS Config.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Config can be used to audit, evaluate configurations of AWS resources.\nIf there are any operational issues, AWS config can be used to retrieve configurational changes made to AWS resources that may have caused these issues.\nOption A is incorrect as Amazon Inspector can be used to analyze potential security threats for an Amazon EC2 instance against an assessment template with predefined rules.\nIt does not provide historical data for configurational changes done to AWS resources.\nOption B is incorrect as AWS CloudFormation provided templates to provision and configure resources in AWS.\nOption C is incorrect as AWS Trusted Advisor can help optimize resources with AWS cloud with respect to cost, security, performance, fault tolerance, and service limits.\nIt does not provide historical data for configurational changes done to AWS resources.\nFor more information on AWS Config, refer to the following URL:\nhttps://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html\n\nThe correct answer is D. AWS Config.\nAWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources continuously. It provides a detailed view of the resources in your account and captures the configuration changes made to them.\nWith AWS Config, you can use AWS Config rules to evaluate the configuration changes against a set of predefined or custom rules, and get notified through Amazon SNS when a resource violates any of the rules. Additionally, AWS Config provides you with an audit trail of the changes made to your resources and can help you to troubleshoot operational issues.\nThe other options are incorrect:\nA. Amazon Inspector is a security assessment service that helps you to improve the security and compliance of your applications by automatically assessing the vulnerabilities in your EC2 instances and applications.\nB. AWS CloudFormation is a service that allows you to create and manage a collection of related AWS resources, including EC2 instances, S3 buckets, and Lambda functions, in a predictable and repeatable way. CloudFormation helps you to automate the provisioning and configuration of your AWS infrastructure.\nC. AWS Trusted Advisor is an online tool that provides you with real-time guidance to help you optimize your AWS resources, increase security and performance, and reduce your overall costs. It provides you with recommendations based on your usage and best practices for the AWS services you are using.\nTherefore, D. AWS Config is the correct answer to this question.\n\n"
}, {
  "id" : 165,
  "question" : "A company is deploying a two-tier, highly available web application to AWS.\nThe application needs a storage layer to store artifacts such as photos and videos.\nWhich of the following services can be used as the underlying storage mechanism?\n",
  "answers" : [ {
    "id" : "e85606e20be84cb1b384d1063b993a37",
    "option" : "Amazon EBS volume",
    "isCorrect" : "false"
  }, {
    "id" : "6ae995a842ed4d11b2613cbed9640f76",
    "option" : "Amazon S3",
    "isCorrect" : "true"
  }, {
    "id" : "1904acd57b7744cb89338cb33f3b8974",
    "option" : "Amazon EC2 instance store",
    "isCorrect" : "false"
  }, {
    "id" : "5546b3fb8c714bca8e56ad695a0f98f1",
    "option" : "Amazon RDS instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAmazon S3 is the default storage service that should be considered for companies.\nIt provides durable storage for all static content.\nFor more information on AWS S3, please visit the Link:\nhttps://aws.amazon.com/s3/\n\nThe most appropriate service for storing artifacts such as photos and videos for a two-tier, highly available web application in AWS is Amazon S3 (Simple Storage Service). Here's why:\nA. Amazon EBS volume - Amazon Elastic Block Store (EBS) volumes provide block-level storage volumes for use with Amazon EC2 instances. EBS volumes are suited for persistent storage of data that requires low-latency access, such as databases. EBS volumes are not well-suited for storing large, unstructured data such as photos and videos, which require object-level storage.\nB. Amazon S3 - Amazon Simple Storage Service (S3) is a highly scalable, durable, and secure object storage service that can be used to store and retrieve any amount of data from anywhere on the web. S3 is designed for large-scale, high-availability applications, and can store and retrieve any type of data, including photos and videos. S3 is also easy to use and has a low cost.\nC. Amazon EC2 instance store - Amazon EC2 instance store provides temporary block-level storage for Amazon EC2 instances. The data on instance store volumes is lost when the instance is stopped, terminated, or fails. Instance store volumes are not suitable for storing artifacts such as photos and videos, which require persistent storage.\nD. Amazon RDS instance - Amazon Relational Database Service (RDS) provides managed database services for several relational database engines, including MySQL, PostgreSQL, and Oracle. RDS is suitable for storing structured data and can be used to store artifacts such as photos and videos, but it may not be the most cost-effective or scalable option for this type of data storage.\nTherefore, the correct answer is B. Amazon S3.\n\n"
}, {
  "id" : 166,
  "question" : "An organization runs several EC2 instances inside a VPC using three subnets, one for Development, one for Test and one for Production.\nThe Security team has some concerns about the VPC configuration.\nIt requires to restrict the communication across the EC2 instances using Security Groups. Which of the following options is true for Security Groups?\n",
  "answers" : [ {
    "id" : "fda2611f7e0241a69cbfc06e56aff933",
    "option" : "You can change a Security Group associated to an instance if the instance state is stopped or running.",
    "isCorrect" : "true"
  }, {
    "id" : "7f3b41261ccc49c6b39ed79290626c64",
    "option" : "You can change a Security Group associated to an instance if the instance state is stopped but not if the instance state is running.",
    "isCorrect" : "false"
  }, {
    "id" : "4cba9ae023684aa3b89af648c5dd7d17",
    "option" : "You can change a Security Group only if there are no instances associated to it.",
    "isCorrect" : "false"
  }, {
    "id" : "efe0f772c3e94c99886b57c4528463df",
    "option" : "The only Security Group you can change is the Default Security Group.",
    "isCorrect" : "false"
  }, {
    "id" : "d530ffb1abc84560b74b5f563ecdb343",
    "option" : "None of the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT because the AWS documentation mentions it in the section calledâ€œChanging an Instance's Security Groupâ€ using the following sentence: â€œAfter you launch an instance into a VPC, you can change the security groups that are associated with the instance.\nYou can change the security groups for an instance when the instance is in the running or stopped state.â€\nOption B, C, D and E are INCORRECT as a consequence of A.Diagram: none.\nReferences:\nhttps://docs.aws.amazon.com/en_pv/vpc/latest/userguide/VPC_SecurityGroups.html\n\nThe correct answer is A. You can change a Security Group associated with an instance if the instance state is stopped or running.\nA Security Group acts as a virtual firewall that controls the inbound and outbound traffic for one or more EC2 instances. By default, all inbound traffic is denied, and all outbound traffic is allowed.\nIn this scenario, the Security team wants to restrict the communication across the EC2 instances using Security Groups. This can be achieved by configuring the Security Group rules to allow or deny specific types of traffic between the instances.\nTo make changes to a Security Group, it is important to understand when it is possible to make changes to the group. The correct answer, A, states that changes can be made to a Security Group associated with an instance if the instance state is stopped or running.\nWhen an EC2 instance is running, it is associated with one or more Security Groups. These Security Groups define the inbound and outbound traffic that is allowed to or from the instance. If a Security Group needs to be changed, the changes can be made to the Security Group rules, and those changes will be applied to the instances associated with the Security Group.\nHowever, if an instance is in the process of starting up or shutting down, the Security Group rules cannot be changed. This is because the Security Group rules are applied to the network interfaces of the instances, and during the startup or shutdown process, the network interfaces are temporarily unavailable.\nOption B is incorrect because it suggests that changes can only be made to a Security Group associated with an instance when the instance state is stopped, which is not true.\nOption C is incorrect because it suggests that changes cannot be made to a Security Group that has instances associated with it, which is not true. Changes can be made to the Security Group rules even if there are instances associated with it.\nOption D is incorrect because it suggests that only the Default Security Group can be changed, which is not true. All Security Groups can be changed.\nTherefore, the correct answer is A. You can change a Security Group associated with an instance if the instance state is stopped or running.\n\n"
}, {
  "id" : 167,
  "question" : "Which of the below-mentioned services is equivalent to hosting virtual servers on an on-premises location?\n",
  "answers" : [ {
    "id" : "ee7d74b9419d400da1e7f702cef4ba65",
    "option" : "AWS IAM",
    "isCorrect" : "false"
  }, {
    "id" : "937b35dc50d34a7f830605f108bda987",
    "option" : "AWS Server",
    "isCorrect" : "false"
  }, {
    "id" : "a6a60cc2328c4b4aa0fc89304b7561f3",
    "option" : "AWS EC2",
    "isCorrect" : "true"
  }, {
    "id" : "01fe27bd9f7e4580be91613d70ef96d0",
    "option" : "AWS Regions.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following:\nAmazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud.\nIt is designed to make web-scale cloud computing easier for developers.\nFor more information on AWS EC2, please refer to the following link:\nhttps://aws.amazon.com/ec2/\n\nThe correct answer is C. AWS EC2.\nExplanation:\nAWS EC2 (Elastic Compute Cloud) is a service that allows you to launch virtual machines (instances) in the cloud. These instances are similar to virtual servers hosted on-premises, as they provide you with complete control over the virtual server's configuration, including the operating system, applications, and networking.\nAWS IAM (Identity and Access Management) is a service that enables you to manage access to AWS services and resources securely. It is not equivalent to hosting virtual servers on an on-premises location.\nAWS Server is not a service provided by AWS. It is a general term that could refer to any server, including on-premises or cloud-based servers.\nAWS Regions are physical locations worldwide where AWS has data centers. Each region consists of multiple Availability Zones. Regions do not provide virtual servers similar to on-premises servers.\nTherefore, the correct answer is AWS EC2, as it provides you with virtual servers similar to those hosted on-premises.\n\n"
}, {
  "id" : 168,
  "question" : "You have a set of EC2 Instances hosted on the AWS Cloud.\nThe EC2 Instances are hosting a web application.\nWhich of the following acts as a firewall to your VPC and the instances in it? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "345a22fb9d37412c8087105c5b16548e",
    "option" : "Usage of Security Groups",
    "isCorrect" : "true"
  }, {
    "id" : "cc2aaf59dfc9458c87d88e63d73a0e33",
    "option" : "Usage of AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "63d0ebf82cb94daa88984d7f11fe0635",
    "option" : "Usage of Network Access Control Lists",
    "isCorrect" : "true"
  }, {
    "id" : "b929639261cd488bb48ac04f772f6db9",
    "option" : "Usage of the Internet gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe AWS Documentation mentions the following.\nA security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nA network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\nFor more information on Security Groups, please refer to the following link:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html\nFor more information on Network Access Control Lists, please refer to the following link:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\n\nThe two answers that act as a firewall to your VPC and the instances in it are:\nA. Usage of Security Groups C. Usage of Network Access Control Lists\nExplanation:\nA Security Group is a virtual firewall that controls inbound and outbound traffic for one or more instances in a VPC. You can think of a security group as a set of firewall rules that control the traffic for your instance. In AWS, each instance must be associated with at least one security group. You can add rules to each security group that allow traffic to or from its associated instances. The rules of a security group are evaluated in the order in which they are created, and the first rule that matches traffic is applied. Security groups are stateful, meaning that any traffic allowed in is automatically allowed back out.\nNetwork Access Control Lists (NACLs) are another security layer for your VPC that act as a stateless packet filter. NACLs are associated with subnets and evaluate inbound and outbound traffic based on rules that you define. Unlike security groups, NACLs evaluate traffic at the subnet level, and each subnet must be associated with one NACL. NACLs provide a more granular control over inbound and outbound traffic, but they are also more complex to configure and manage than security groups.\nD. Usage of the Internet gateway: An Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in a VPC and the internet. It provides a target in your VPC route tables for internet-routable traffic and performs network address translation (NAT) for instances that have been assigned public IP addresses. While an internet gateway allows traffic to flow in and out of your VPC, it does not act as a firewall to control or restrict that traffic.\nB. Usage of AWS Config: AWS Config is a service that enables you to assess, audit, and evaluate the configuration of your AWS resources continuously. It helps you identify and respond to security and compliance risks across your resource inventory. While AWS Config can help you identify security risks and vulnerabilities, it is not a firewall service and does not control traffic flow or restrict access to your resources.\n\n"
}, {
  "id" : 169,
  "question" : "Which of the following can be used to launch EC2 instances on the AWS Cloud?\n",
  "answers" : [ {
    "id" : "0253e33c53ea441e9d6c10670b6e610c",
    "option" : "EBS Volumes",
    "isCorrect" : "false"
  }, {
    "id" : "328b86f834b8497b9e7db4e3c0860290",
    "option" : "EBS Snapshots",
    "isCorrect" : "false"
  }, {
    "id" : "76badcfe63764e339c3925d667ecd776",
    "option" : "Amazon Machine Image",
    "isCorrect" : "true"
  }, {
    "id" : "bd193df89a5d455b984bb5413739d655",
    "option" : "Amazon VMware.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following.\nAn Amazon Machine Image (AMI) provides the information required to launch an instance, which is a virtual server in the cloud.\nYou specify an AMI when you launch an instance and you can launch as many instances from the AMI as you need.\nYou can also launch instances from as many different AMIs as you need.\nFor more information on Amazon Machine Images, please refer to the following Link:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nThe correct answer is C. Amazon Machine Image.\nAn Amazon Machine Image (AMI) is a pre-configured virtual machine image, which is used to launch EC2 instances on the AWS Cloud. An AMI includes an operating system, application server, and any additional software required to run an application.\nWhen launching an EC2 instance, you have the option to select an AMI that meets your requirements. AWS provides a number of pre-configured AMIs, such as Amazon Linux, Windows Server, and Ubuntu, which can be used to launch instances in different regions and availability zones. You can also create your own AMIs or use third-party AMIs that are available in the AWS Marketplace.\nEBS volumes and EBS snapshots are used to store data on EC2 instances, but they are not used to launch instances. EBS volumes are persistent block storage devices that can be attached to an EC2 instance, while EBS snapshots are point-in-time backups of EBS volumes.\nAmazon VMware is not used to launch EC2 instances on the AWS Cloud. It is a service that allows you to migrate your existing VMware workloads to AWS Cloud.\nIn summary, an Amazon Machine Image (AMI) is used to launch EC2 instances on the AWS Cloud.\n\n"
}, {
  "id" : 170,
  "question" : "Which of the below options cannot be used to upload archives to Amazon Glacier?\n",
  "answers" : [ {
    "id" : "e1b459fa9b9a4666b794aed88fcc1c1d",
    "option" : "AWS Glacier API",
    "isCorrect" : "false"
  }, {
    "id" : "9d5ee882c75d4d969d230a68051c36dc",
    "option" : "AWS Console",
    "isCorrect" : "true"
  }, {
    "id" : "1c8177b9448b43fb895422c9b4f0e3e6",
    "option" : "AWS Glacier SDK",
    "isCorrect" : "false"
  }, {
    "id" : "26f0d9d036c24ef69c82155ce0e87e13",
    "option" : "AWS S3 Lifecycle policies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nNote that the AWS Console cannot be used to upload data onto Glacier.\nThe console can only be used to create a Glacier vault which can be used to upload the data.\nFor more information on uploading data onto Glacier, please refer to the following link:\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/uploading-an-archive.html\nOption A - AWS Glacier API.\nAWS Glacier is a storage service optimized for infrequently used data or \"cold data.\" This option is used for programmatically access Glacier and work with it.\nDue to this reason, this option is incorrect.\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/amazon-glacier-api.html\nOption C - AWS Glacier SDK: SDK, i.e., Software Development Kit, is used to develop applications for Amazon S3 Glacier.\nIt provides libraries that map to the underlying REST API and provide objects that you can easily use to construct requests and process responses.\nDue to this reason, it's not a valid answer to the asked question.\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/using-aws-sdk.html.\nOption D - AWS S3 Lifecycle Policies: S3 Lifecycle Policies allow you to automatically review objects within your S3 Buckets and have them moved to Glacier or have the objects deleted from S3.\nhttps://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-lifecycle.html\nhttps://aws.amazon.com/glacier/faqs/\n\nAmazon Glacier is an Amazon Web Services (AWS) storage service that provides long-term storage for data that is rarely accessed. Amazon Glacier enables customers to upload and retrieve large amounts of data in a cost-effective manner. In order to upload archives to Amazon Glacier, there are several options available, including the AWS Glacier API, AWS Console, and AWS Glacier SDK.\nHowever, AWS S3 Lifecycle policies cannot be used to upload archives to Amazon Glacier. S3 Lifecycle policies enable customers to automatically transition objects between different S3 storage classes based on predefined rules. For example, a customer could configure an S3 Lifecycle policy to automatically transition objects from the S3 Standard storage class to the S3 Glacier storage class after a certain period of time.\nWhile S3 Lifecycle policies can be used to transition objects to the S3 Glacier storage class, they cannot be used to upload archives to Amazon Glacier directly. Instead, customers must use the AWS Glacier API, AWS Console, or AWS Glacier SDK to upload archives to Amazon Glacier.\nIn summary, the correct answer is D. AWS S3 Lifecycle policies cannot be used to upload archives to Amazon Glacier.\n\n"
}, {
  "id" : 171,
  "question" : "Your company is planning to pay for an AWS Support plan.\nThey have the following requirements as far as the support plan goes: 24x7 access to Cloud Support Engineers via email, chat &amp; phone Response time of less than 15 minutes for any business-critical system faults Which of the following plans will suffice to keep in mind the above requirement?\n",
  "answers" : [ {
    "id" : "1aa755a8426c421098179b1e5477a1e9",
    "option" : "Basic",
    "isCorrect" : "false"
  }, {
    "id" : "24a66c34c4ba4964bc79f06d5ed0efd5",
    "option" : "Developer",
    "isCorrect" : "false"
  }, {
    "id" : "74be1707832f4629a385114ace40065d",
    "option" : "Business",
    "isCorrect" : "false"
  }, {
    "id" : "245573c413734fb5a2bc61de472621c4",
    "option" : "Enterprise.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nAs per the AWS document, there is no critical support available for Basic, Developer and Business plans.\nEnterprise plan has critical support within 15 minutes.\nThe question mentions less than 15 minutes for critical faults.\nHence the correct answer is Enterprise.\nFor more information on the support plans, please refer to the following Link:\nhttps://aws.amazon.com/premiumsupport/compare-plans/\n\n\nBased on the requirements mentioned in the question, the appropriate AWS Support plan that would meet the requirements is the Enterprise plan.\nLet's take a closer look at the different support plans that AWS offers and the features included in each one:\nBasic Support Plan: This is the default support plan for all AWS customers, and it is available for free. It includes access to AWS Trusted Advisor, which provides recommendations to optimize costs, improve performance, and increase security. It also includes access to AWS Personal Health Dashboard, which provides alerts and notifications about the health of AWS services. Developer Support Plan: This plan is intended for developers and includes access to AWS Trusted Advisor, AWS Personal Health Dashboard, and basic 24x7 support via email. It also includes guidance on using AWS services and tools. Business Support Plan: This plan is intended for businesses and includes access to AWS Trusted Advisor, AWS Personal Health Dashboard, and 24x7 support via email, chat, and phone. It also includes a 1-hour response time for critical issues, as well as support for third-party applications. Enterprise Support Plan: This plan is intended for large enterprises and includes access to AWS Trusted Advisor, AWS Personal Health Dashboard, and 24x7 support via email, chat, and phone. It also includes a 15-minute response time for critical issues, a dedicated Technical Account Manager, and support for custom architectures and applications.\nBased on the requirements mentioned in the question, the Enterprise Support Plan would be the most appropriate choice since it meets all the requirements:\n24x7 access to Cloud Support Engineers via email, chat & phone: This requirement is met by all four plans. Response time of less than 15 minutes for any business-critical system faults: Only the Enterprise Support Plan offers a 15-minute response time for critical issues, which meets this requirement.\nTherefore, the correct answer to the question is D. Enterprise.\n\n"
}, {
  "id" : 172,
  "question" : "Which of the following are features of an edge location? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "0fd26cf7b7744568b7f5c2b3b3c438ec",
    "option" : "Distribute content to users",
    "isCorrect" : "true"
  }, {
    "id" : "a6d68446d208426f8720f5fdfdca0496",
    "option" : "Cache popular contents",
    "isCorrect" : "true"
  }, {
    "id" : "e2cca0813b1044238a33ba524f42c640",
    "option" : "Distribute load across multiple resources",
    "isCorrect" : "false"
  }, {
    "id" : "d912fa9f1a064df3be815e485506a2a5",
    "option" : "Used in conjunction with the Cloudfront service.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - A, B and D.\nThe AWS Documentation mentions the following.\nAmazon CloudFront employs a global network of edge locations and regional edge caches that cache copies of your content close to your viewers.\nAmazon CloudFront ensures that end-user requests are served by the closest edge location.\nRegional edge caches are CloudFront locations that are deployed globally, close to your viewers.\nThey're located between your origin server and the POPs-global edge locations that serve content directly to viewers.\nAs objects become less popular, individual POPs might remove those objects to make room for more popular content.\nFor more information on Cloudfront and Edge locations, please refer to the following link:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/HowCloudFrontWorks.html\n\nAn edge location is a physical location where AWS deploys and maintains a collection of caching servers. These servers are used to provide lower latency and higher throughput to end-users who are accessing your applications or content. Edge locations are used in conjunction with Amazon CloudFront, a content delivery network (CDN) service.\nThe features of an edge location are as follows:\nA. Distribute content to users: One of the primary features of an edge location is the ability to distribute content to end-users. When an end-user requests content from your application, the request is routed to the nearest edge location, where the content is cached. This helps to reduce latency and improve performance for end-users.\nB. Cache popular contents: Another feature of an edge location is the ability to cache popular content. When an end-user requests content that has been previously requested by other users, the content is already cached at the edge location. This reduces the load on your origin servers and improves performance for end-users.\nC. Distribute load across multiple resources: Edge locations can also distribute load across multiple resources. When an end-user requests content, the request is automatically routed to the nearest available edge location. If one edge location is overloaded, the request is automatically routed to another available edge location.\nD. Used in conjunction with the CloudFront service: Edge locations are used in conjunction with the Amazon CloudFront service. CloudFront is a global CDN service that distributes your content to edge locations around the world. When an end-user requests content, the request is automatically routed to the nearest edge location using CloudFront.\n\n"
}, {
  "id" : 173,
  "question" : "Which of the following storage options provides the option of Lifecycle policies that can be used to move objects to archive storage.\n",
  "answers" : [ {
    "id" : "af3b18b4f16745148340d5b6f8c0a3bc",
    "option" : "Amazon S3",
    "isCorrect" : "true"
  }, {
    "id" : "e1277cb5539b42ed8920881b9c65672b",
    "option" : "Amazon Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "5403207f18944a3b94bd67d9c6fb439a",
    "option" : "Amazon Storage Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "0d57c2c10fc44f8c83b375a5a6e0c357",
    "option" : "Amazon EBS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nLifecycle configuration enables you to specify the lifecycle management of objects in a bucket.\nThe configuration is a set of one or more rules, where each rule defines an action for Amazon S3 to apply to a group of objects.\nThese actions can be classified as follows:\nÂ· Transition actions - In which you define when objects transition to another storage class.\nFor example, you may choose to transition objects to the STANDARD_IA (IA, for infrequent access) storage class 30 days after creation, or archive objects to the GLACIER storage class one year after creation.\nÂ· Expiration actions - In which you specify when the objects expire.\nThen Amazon S3 deletes the expired objects on your behalf.\nFor more information on AWS Object Lifecycle management, please visit the Link:\nhttps://aws.amazon.com/s3/\n\nThe correct answer is B. Amazon Glacier.\nAmazon S3 (Simple Storage Service) is a highly scalable and durable object storage service that can store and retrieve any amount of data from anywhere on the web. It is designed to provide 99.999999999% durability and 99.99% availability of objects over a given year. Amazon S3 provides a range of storage classes, including Standard, Infrequent Access (IA), and One Zone-Infrequent Access (One Zone-IA), that customers can use to optimize costs for their specific use cases. However, Amazon S3 does not offer a dedicated archival storage class.\nAmazon Glacier is a dedicated archival storage service that provides secure, durable, and low-cost storage for data archiving and long-term backup. Amazon Glacier is designed to provide 99.999999999% durability and is optimized for infrequently accessed data that can be stored for months, years, or even decades. Amazon Glacier offers a range of storage options, including standard, expedited, and bulk retrievals, and customers can define their own retrieval policies to meet their specific recovery time and cost requirements.\nLifecycle policies are used to automatically transition objects between different storage classes or delete them when they are no longer needed. Amazon S3 provides the option to define lifecycle policies that can be used to transition objects to infrequent access storage classes or delete them after a certain period of time. However, Amazon S3 does not offer the option to move objects to archive storage.\nAmazon Glacier provides the option to define lifecycle policies that can be used to transition objects between different storage classes, including Standard, Expedited, and Bulk, or delete them after a certain period of time. The lifecycle policies in Amazon Glacier can be used to move objects to archive storage after a certain period of time, which helps customers reduce storage costs and optimize the use of their storage resources.\nAmazon Storage Gateway is a hybrid storage service that enables on-premises applications to use cloud storage seamlessly. Amazon Storage Gateway supports file, volume, and tape-based storage solutions, but it does not provide the option to define lifecycle policies that can be used to move objects to archive storage.\nAmazon EBS (Elastic Block Store) is a block-level storage service that provides persistent storage volumes for use with Amazon EC2 instances. Amazon EBS is designed to provide low-latency and consistent performance for mission-critical applications. However, Amazon EBS does not provide the option to define lifecycle policies that can be used to move objects to archive storage.\n\n"
}, {
  "id" : 174,
  "question" : "Which of the following features of Amazon RDS allows for better availability of databases? Choose the answer from the options given below.\n",
  "answers" : [ {
    "id" : "6cc8ded314e2419598ec070f6ecf99af",
    "option" : "VPC Peering",
    "isCorrect" : "false"
  }, {
    "id" : "7bfa928d490d4825ac654275b6db1f86",
    "option" : "Multi-AZ",
    "isCorrect" : "true"
  }, {
    "id" : "e68ab7e687544cd6bbcd7c790d1770c8",
    "option" : "Read Replicas",
    "isCorrect" : "false"
  }, {
    "id" : "230af89290d642fa9156d629e03704bc",
    "option" : "Data encryption.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nIf you are looking to use replication to increase database availability while protecting your latest database updates against unplanned outages, consider running your DB instance as a Multi-AZ deployment.\nFor more information on AWS RDS, please visit the FAQ Link:\nhttps://aws.amazon.com/rds/faqs/\n\nThe correct answer is B. Multi-AZ.\nExplanation: Amazon RDS (Relational Database Service) is a managed database service offered by AWS (Amazon Web Services) that makes it easier to set up, operate, and scale a relational database in the cloud. Multi-AZ is a feature of Amazon RDS that enables automatic failover to a standby replica in a different Availability Zone (AZ) in case of an outage in the primary AZ.\nHere's a brief explanation of the other options given: A. VPC Peering: VPC (Virtual Private Cloud) peering is a networking connection between two VPCs that enables them to communicate with each other as if they are on the same network. While VPC peering is important for network connectivity, it does not directly contribute to the availability of databases.\nC. Read Replicas: Read replicas are copies of a database instance that can be used to offload read traffic from the primary instance. They can also provide additional availability benefits as they can be used to promote to a primary database instance in case of a failure. However, read replicas do not provide automatic failover like Multi-AZ.\nD. Data encryption: Data encryption is an important security feature, but it does not directly contribute to the availability of databases.\nIn summary, Multi-AZ is the feature of Amazon RDS that provides automatic failover to a standby replica in a different AZ, thereby ensuring better availability of databases.\n\n"
}, {
  "id" : 175,
  "question" : "A client who has adopted AWS Cloud would like to ensure that his systems need to deliver continuous business value &amp; improve supporting processes and procedures.\nWhich design pillar will he need to focus for achieving this?\n",
  "answers" : [ {
    "id" : "b2011576d018488c8ab4e782f8af55e7",
    "option" : "Reliability",
    "isCorrect" : "false"
  }, {
    "id" : "92f0f4d841474f87ad02ed944636a2bc",
    "option" : "Scalability",
    "isCorrect" : "false"
  }, {
    "id" : "c53d1933e49b4390a41e6d396f89cda9",
    "option" : "Automation",
    "isCorrect" : "false"
  }, {
    "id" : "67c1769a711e4b29ad4cc03e22dba348",
    "option" : "Operational Excellence.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nContinuous business value is achieved with the ability to monitor existing running systems &amp; improving processes and procedures by managing &amp; automating changes.\nFor eg in response to a saturation in CPU usage of an EC2 instance, a monitoring system like CloudWatch will automatically trigger a creation of a new instance though alarms.\nThis will ensure that the system's capacity meets changing load demands.\nThis is a part of the Operational Excellence pillar of the AWS well architected framework which focuses on running &amp; monitoring systems to deliver business value.\nOption A is incorrect since the Reliability pillar focuses on the ability of the system to recover from infrastructure or service failures.\nOption B is incorrect since scalability is a by-product of monitoring solutions which provide the capability for infrastructure resources to cope with increase or decrease of capacity by adding or terminating resources when not needed.\nOption C is incorrect since automation is the ability to induce certain systemic requirements like scalability, auto recovery using monitoring solutions.\nIt helps in improving system's stability &amp; efficiency of an Organization.\nOption D is CORRECT.\nRefer to the above description for details.\nDiagram:\nReference:\nhttps://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/\n\n\nThe design pillar that the client needs to focus on for achieving continuous business value and improving supporting processes and procedures is \"Operational Excellence.\"\nOperational Excellence is one of the five design pillars of AWS Well-Architected Framework. It focuses on improving and automating processes and procedures, reducing defects, and continuously improving the operational aspects of a system. It is essential to achieve continuous improvement and business value in the cloud environment.\nOperational Excellence includes various best practices, such as defining clear business objectives and continuously measuring and monitoring the performance of systems to identify areas for improvement. It also involves automating operational processes to reduce the risk of human errors and enable faster responses to business needs. This approach helps organizations to optimize their operations, reduce costs, and improve agility.\nIn summary, for the client to achieve continuous business value and improve supporting processes and procedures in the AWS Cloud environment, they need to focus on the Operational Excellence design pillar, which involves defining business objectives, monitoring system performance, and automating operational processes.\n\n"
}, {
  "id" : 176,
  "question" : "Which of the following encryption techniques is used by AWS KMS for integration with other AWS services?\n",
  "answers" : [ {
    "id" : "d4d3c01d6b3a4821b32fe94255b68405",
    "option" : "RSA Encryption",
    "isCorrect" : "false"
  }, {
    "id" : "b85ee4bf19ed4c96b825cbe749ceaca7",
    "option" : "AES Encryption",
    "isCorrect" : "false"
  }, {
    "id" : "27981c934a8748fab35e720fbd675ff7",
    "option" : "Triple DES Encryption",
    "isCorrect" : "false"
  }, {
    "id" : "ccf9d321564f4180af6b560a18230769",
    "option" : "Envelope Encryption.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS KMS uses envelope encryption while integrating with other AWS services.\nIn this encryption technique, data is encrypted by the data encryption key within that service.\nThis data key is further encrypted using the customer master key (CMK) stored in AWS KMS.\nOptions A, B &amp; C are incorrect as AWS KMS does not use these encryption techniques for integration with other AWS services.\nFor more information on AWS KMS, refer to the following URL:\nhttps://aws.amazon.com/kms/features/\n\nAWS KMS (Key Management Service) is a fully managed service that makes it easy to create and control encryption keys that are used to encrypt data. It provides a centralized control point for key management and usage, and enables you to encrypt data in a secure and scalable manner.\nAWS KMS uses envelope encryption to integrate with other AWS services. Envelope encryption is a technique that involves encrypting data with a data key, which is in turn encrypted with a master key. The data key is used to encrypt the actual data, while the master key is used to encrypt the data key.\nBy using envelope encryption, AWS KMS ensures that data is protected with strong encryption, while also providing a flexible and scalable way to manage encryption keys. Envelope encryption also enables you to store data in different AWS services, such as Amazon S3, Amazon RDS, and Amazon Redshift, while still maintaining the same level of encryption security.\nIn summary, the correct answer is D. Envelope Encryption.\n\n"
}, {
  "id" : 177,
  "question" : "You are the architect of a custom application running inside your corporate data center.\nThe application runs with some unresolved bugs that produce a lot of data inside custom log files generating time-consuming activities for the operation team responsible for analyzing them. You want to move the application to AWS using EC2 instances.\nAt the same time, you want to take the opportunity to improve logging and monitoring capabilities, but without touching the application code. What AWS service should you use to satisfy the requirement?\n",
  "answers" : [ {
    "id" : "26e41c7dbbde4455bbd16723d3029987",
    "option" : "AWS Kinesis Data Streams",
    "isCorrect" : "false"
  }, {
    "id" : "a7fac58a61274f5a8cf0d7ad0a785be3",
    "option" : "AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "d8e0bed3a9114d90b6b83800eec7d789",
    "option" : "AWS CloudWatch Logs",
    "isCorrect" : "true"
  }, {
    "id" : "eda558ac075d4be090c9bb248fb2d562",
    "option" : "AWS Application Logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT because in order to feed a Data Streams from custom logs you have to change the application code.\nAWS documentation describes this with the following sentence: â€œTo put data into the stream, you must specify the name of the stream, a partition key, and the data blob to be added to the stream.â€\nOption B is INCORRECT because it is not related to the scenario and custom log files.\nOption C is CORRECT because AWS CloudWatch Logs has the capability to reuse existing application logs increasing efficiency in operation with the ability to generate on them metrics, alerts and analytics with AWS CloudWatch Logs Insight.\nThe application and custom log files are exactly as they were when the application was running on-prem.\nSo you don't need to change any piece of application code that makes them ingestible by AWS CloudWatch Logs.\nAWS official documentation in the FAQ section highlights the reusing capability with the sentence â€œAWS CloudWatch Logs lets you monitor and troubleshoot your systems and applications using your existing system, application and custom log filesâ€¦ so, no code changes are required.â€\nYou can also leverage CloudWatch Metrics, Alarms and Dashboards with Logs to get full operational visibility into your applications.\nThis empowers you to understand your applications, make improvements, and find problems quickly.\nThus you can continue to innovate rapidly.\nOption D is INCORRECT because AWS Application Logs does not exist.\nDiagram: none.\nReferences:\nhttps://aws.amazon.com/cloudwatch/faqs/\n\nThe correct answer is C. AWS CloudWatch Logs.\nAWS CloudWatch Logs is a managed service provided by AWS to monitor, store, and access log files from EC2 instances, AWS CloudTrail, VPC Flow Logs, and other AWS services. It allows users to ingest, aggregate, and analyze log data from different sources in real-time without requiring any custom code.\nIn this scenario, the application generates a lot of data inside custom log files, and the operation team spends a lot of time analyzing them. By moving the application to AWS using EC2 instances and integrating it with AWS CloudWatch Logs, the logs generated by the application can be directed to CloudWatch Logs, where they can be monitored, analyzed, and stored in a central location, without requiring any changes to the application code.\nAWS CloudWatch Logs provides several benefits such as:\nCentralized log management: With CloudWatch Logs, all the logs generated by the application can be stored in a central location, which can be easily accessed and analyzed by the operation team. Real-time log processing: CloudWatch Logs provides real-time processing of logs, enabling the operation team to monitor and respond to events as they happen. Scalability: CloudWatch Logs scales automatically to handle large volumes of logs generated by the application. Easy integration with AWS services: CloudWatch Logs can be integrated easily with other AWS services, such as EC2 instances, AWS CloudTrail, VPC Flow Logs, etc.\nIn summary, by integrating the application running on EC2 instances with AWS CloudWatch Logs, the operation team can monitor, analyze, and store logs generated by the application in a centralized location, without requiring any changes to the application code. This will improve logging and monitoring capabilities, reduce operational overhead, and enable the operation team to respond quickly to any events or issues.\n\n"
}, {
  "id" : 178,
  "question" : "Your company wants to move an existing Oracle database to the AWS Cloud.\nWhich of the following services can help facilitate this move?\n",
  "answers" : [ {
    "id" : "45bd777790e846a19320867885b44a89",
    "option" : "AWS Database Migration Service",
    "isCorrect" : "true"
  }, {
    "id" : "a76d00eb121e40e6928353c44482387d",
    "option" : "AWS VM Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "12b95e8449b84c1b87408c6bf993ae6f",
    "option" : "AWS Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "30fc455c8e3d4248af5b85f9bd587a59",
    "option" : "AWS Trusted Advisor.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAWS Database Migration Service helps you migrate databases to AWS quickly and securely.\nThe source database remains fully operational during the migration, minimizing downtime to applications that rely on the database.\nThe AWS Database Migration Service can migrate your data to and from the most widely used commercial and open-source databases.\nFor more information on AWS Database migration, please refer to the below URL:\nhttps://aws.amazon.com/dms/\n\nThe correct answer is A. AWS Database Migration Service.\nAWS Database Migration Service (DMS) is designed to help migrate databases to AWS quickly and securely. It supports a variety of databases, including Oracle, Microsoft SQL Server, MySQL, PostgreSQL, and others. With DMS, you can migrate your database to AWS with minimal downtime, and it can automatically convert your source database schema and data to the target database format.\nAWS VM Migration Service (B) is not the best choice for migrating an Oracle database to AWS. It's designed for migrating virtual machines to AWS, not databases.\nAWS Inspector (C) is a security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not a database migration tool.\nAWS Trusted Advisor (D) is a service that provides best practice recommendations in areas such as cost optimization, performance, security, and fault tolerance. While it can be useful in many situations, it is not specifically designed for database migration.\nTherefore, the correct answer is A. AWS Database Migration Service.\n\n"
}, {
  "id" : 179,
  "question" : "Which of the following features of AWS RDS allows you to reduce the load on the database while reading data?\n",
  "answers" : [ {
    "id" : "6d01f2926ce840a491d26fa8cafe78a3",
    "option" : "Cross region replication",
    "isCorrect" : "false"
  }, {
    "id" : "93da9630643e45a39c17188c6c81d830",
    "option" : "Creating Read Replicas",
    "isCorrect" : "true"
  }, {
    "id" : "0e98a68a04e54ea497dd4e30fb888b6f",
    "option" : "Using snapshots",
    "isCorrect" : "false"
  }, {
    "id" : "65116c1b8b674e7d9beec3301e3e197e",
    "option" : "Using Multi-AZ feature.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nYou can reduce the load on your source DB Instance by routing read queries from your applications to the read replica.\nRead replicas allow you to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads.\nFor more information on Read Replicas, please refer to the below URL:\nhttps://aws.amazon.com/rds/details/read-replicas/\n\nThe correct answer is B. Creating Read Replicas.\nAmazon Relational Database Service (RDS) is a managed database service that makes it easy to set up, operate, and scale a relational database in the cloud. Read Replicas are a feature of RDS that enables you to create one or more copies of your database that are kept in sync with the primary database. These copies can be used to offload read traffic from the primary database, which can help to reduce the load on the primary database and improve read performance.\nLet's take a closer look at the other options:\nA. Cross region replication: This feature is used to replicate data across multiple AWS regions. While it can improve the availability and durability of your data, it does not help to reduce the load on the database while reading data.\nC. Using snapshots: RDS snapshots allow you to create a point-in-time copy of your database. While you can use these snapshots to create new databases or restore existing ones, they do not help to reduce the load on the database while reading data.\nD. Using Multi-AZ feature: Multi-AZ (availability zone) is a feature that provides high availability and automatic failover for RDS databases. While it can improve the availability of your database, it does not help to reduce the load on the database while reading data.\nIn summary, Read Replicas are the feature of AWS RDS that allows you to reduce the load on the database while reading data. They do this by providing a scalable read capacity that can be used to offload read traffic from the primary database.\n\n"
}, {
  "id" : 180,
  "question" : "Which of the following terms refers to a physical location around the world where data centers are located in AWS?\n",
  "answers" : [ {
    "id" : "440436694d4041bc8477b89ae8b0b7cb",
    "option" : "Sub zone",
    "isCorrect" : "false"
  }, {
    "id" : "215b9a2a9ebb497eba19aedd2b174e5d",
    "option" : "Data center",
    "isCorrect" : "false"
  }, {
    "id" : "59a7947f97614b8fafe81ada6a050c28",
    "option" : "Region",
    "isCorrect" : "true"
  }, {
    "id" : "30ef4b0bd3dc485081e152264abea775",
    "option" : "Edge location.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAWS has the concept of a Region, which is a physical location around the world where we cluster data centers.\nWe call each group of logical data centers an Availability Zone.\nEach AWS Region consists of multiple, isolated, and physically separate AZ's within a geographic area.\nUnlike other cloud providers, who often define a region as a single data center, the multiple AZ design of every AWS Region offers advantages for customers.\nFor more information on Regions and Availability Zones in AWS, please refer to the below URL:\nhttps://aws.amazon.com/about-aws/global-infrastructure/regions_az/\n\nThe correct answer is C. Region.\nExplanation: In AWS, a region is a physical location around the world where data centers are located. Each region is completely independent and contains multiple Availability Zones, which are distinct locations within the region that are engineered to be isolated from failures in other Availability Zones. This allows customers to operate production applications and databases that are more highly available, fault tolerant, and scalable than would be possible from a single data center.\nAWS currently has 25 regions globally, each consisting of multiple availability zones, as of the knowledge cutoff date of September 2021.\nOption A, Sub zone, is not a valid term in AWS terminology and does not refer to any particular feature or concept.\nOption B, Data center, is a physical facility used to house computer systems and related components, including telecommunications and storage systems. While AWS does operate data centers, the term \"data center\" does not specifically refer to a physical location with multiple data centers, as in the case of an AWS region.\nOption D, Edge location, is a term used to refer to endpoints of the AWS network that are used for caching content. These edge locations are used to accelerate the delivery of content to end users, and are typically located in major cities around the world. While edge locations are a critical part of AWS's infrastructure, they are not synonymous with regions.\n\n"
}, {
  "id" : 181,
  "question" : "A company wants to have a database hosted on AWS.\nAs much as possible they want to have control over the database itself.\nWhich of the following would be an ideal option for this?\n",
  "answers" : [ {
    "id" : "1661f2a1743a471f98835df43c5943fd",
    "option" : "Using the AWS DynamoDB service",
    "isCorrect" : "false"
  }, {
    "id" : "73e256011da244a1811c96eef49df9d2",
    "option" : "Using the AWS RDS service",
    "isCorrect" : "false"
  }, {
    "id" : "cbd90896b5e241bab2fc338eec07e311",
    "option" : "Hosting the database on an EC2 Instance",
    "isCorrect" : "true"
  }, {
    "id" : "de7a65d464254bc0a1be7c1093b98caa",
    "option" : "Using the Amazon Aurora service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nIf you want a self-managed database, that means you want complete control over the database engine and the underlying infrastructure.\nIn such a case, you need to host the database on an EC2 Instance.\nFor more information on EC2 Instances, please refer to the below URL:\nhttps://aws.amazon.com/ec2/\n\nThe best option for a company that wants to have control over their database on AWS would be to use the Amazon RDS service (option B).\nAmazon RDS (Relational Database Service) is a managed database service that provides an easy way to create, operate, and scale a relational database in the cloud. It supports several popular database engines such as MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB.\nBy using Amazon RDS, the company can control their database instance, such as configuration, backup, and security. They can also choose the database engine version, instance type, and storage capacity based on their requirements.\nAmazon RDS also provides various automation features such as automated backups, software patching, and scaling, which can reduce the management burden on the company.\nDynamoDB (option A) is a fully managed NoSQL database service, which provides a scalable and high-performance database for web applications. However, it may not be suitable for companies that require more control over their database instance.\nHosting the database on an EC2 instance (option C) would provide the company with complete control over the database instance, but it would also require them to manage the infrastructure and configuration, which may be time-consuming and complex.\nAmazon Aurora (option D) is a managed relational database engine that is compatible with MySQL and PostgreSQL. It provides high performance, scalability, and availability, but it may be more expensive compared to other options.\nIn conclusion, for a company that wants to have control over their database on AWS, using the Amazon RDS service would be the best option, as it provides a balance between control and manageability.\n\n"
}, {
  "id" : 182,
  "question" : "On which of the following resources does Amazon Inspector perform network accessibility checks?\n",
  "answers" : [ {
    "id" : "b3a31da65a414b46bc9458383c116393",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "9bd47284e7394e4c8b2818bbcdadcd34",
    "option" : "Amazon VPN",
    "isCorrect" : "false"
  }, {
    "id" : "314b35d993784591993be835261c8add",
    "option" : "Amazon EC2 instance",
    "isCorrect" : "true"
  }, {
    "id" : "a4039671126c499c89c2d6413591963f",
    "option" : "Amazon VPC.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAmazon Inspector provides two types of packages.\nNetwork reachability rules package checks network accessibility checks on Amazon EC2 instance.\nHost assessment rules package checks vulnerabilities on Amazon EC2 instance.\nOptions A, B &amp; D are incorrect as Amazon Inspector performs network accessibility checks on Amazon EC2 instance, not on Amazon CloudFront, Amazon VPN or Amazon VPC.For more information on Amazon Inspector, refer to the following URL:\nhttps://aws.amazon.com/inspector/faqs/\n\nAmazon Inspector is a security assessment service offered by AWS that enables you to assess the security and compliance of your applications running on Amazon EC2 instances. It checks your applications for security vulnerabilities, deviations from best practices, and any security-related deviations from industry standards.\nWhen you run an Amazon Inspector assessment, it performs several types of security checks on your EC2 instances. One of these checks is network accessibility checks. These checks help you to identify any vulnerabilities in the network configuration of your instances that could allow unauthorized access or data exfiltration.\nThe resources on which Amazon Inspector performs network accessibility checks are EC2 instances. It assesses the security of the instances by testing their network accessibility from the outside world. It checks whether the instances are visible on the network, whether they are accessible from specific ports, and whether they are vulnerable to common attacks such as cross-site scripting (XSS) and SQL injection.\nTherefore, the correct answer to this question is C. Amazon EC2 instance. Amazon CloudFront is a content delivery network service; Amazon VPN is a virtual private network service; and Amazon VPC is a virtual private cloud service. Although these services are related to networking, they are not the resources on which Amazon Inspector performs network accessibility checks.\n\n"
}, {
  "id" : 183,
  "question" : "Which of the following services helps to achieve the computing elasticity in AWS?\n",
  "answers" : [ {
    "id" : "8e19d6eeb9b24d72bce76f5b4fd7ea35",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "edafd17166cd4021ab6cc97e18ea1a21",
    "option" : "VPC Endpoint",
    "isCorrect" : "false"
  }, {
    "id" : "bb2252bd6c864baaa785fec116efa08c",
    "option" : "AWS EC2 Auto Scaling Group",
    "isCorrect" : "true"
  }, {
    "id" : "fbacb0bf58ab426b91e68d5934ea44d3",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAWS EC2 Auto Scaling Group achieves the computing elasticity by scaling up/down the EC2 instances based on demand.\nFor more information on the AWS Autoscaling service, please refer to the below URL:\nhttps://aws.amazon.com/autoscaling/\n\nThe correct answer is C. AWS EC2 Auto Scaling Group.\nAWS EC2 Auto Scaling Group helps to achieve computing elasticity in AWS. Elasticity refers to the ability of a system to scale its resources up or down based on demand. EC2 Auto Scaling Group can automatically scale the number of EC2 instances up or down based on changes in demand for the application.\nFor example, if an application experiences a sudden increase in traffic, EC2 Auto Scaling Group can automatically add additional instances to handle the increased load. Conversely, if the demand decreases, it can automatically terminate instances to reduce costs.\nAWS RDS (A) is a managed relational database service, and it does not provide computing elasticity. VPC Endpoint (B) is a service that allows you to connect to AWS services privately from within your VPC without going over the Internet, but it does not provide computing elasticity. Amazon S3 (D) is a scalable object storage service, but it does not provide computing elasticity.\n\n"
}, {
  "id" : 184,
  "question" : "To receive AWS Trusted Advisor Notifications, what actions are required from the customer end?\n",
  "answers" : [ {
    "id" : "624470bffa7a42bba519270905f738e0",
    "option" : "Open a ticket with AWS Support.",
    "isCorrect" : "false"
  }, {
    "id" : "b16191605cc04e75b180dc934ee955a4",
    "option" : "Set up Notification in Dashboard",
    "isCorrect" : "true"
  }, {
    "id" : "73b45bdc53db4879898532656db9628a",
    "option" : "Set up Amazon Simple Notification Service",
    "isCorrect" : "false"
  }, {
    "id" : "fe574df070414fa3952937630a4abcf2",
    "option" : "No action is required, all Notifications are sent automatically on a weekly basis.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Trusted Advisor Notification is an optional service that needs to be set up from the dashboard providing a list of recipients and selecting resource items for which status is required.\nOption A is incorrect as opening an AWS support ticket is not required to receive AWS Trusted Advisor Notification.\nOption C is incorrect as Amazon SNS is a separate service for push notifications.\nBut it is not required to receive AWS Trusted Advisor Notification.\nOption D is incorrect as you need to set up the notifications in the dashboard.\nFor more information on AWS Trusted Advisor, refer to the following URL:\nhttps://aws.amazon.com/premiumsupport/faqs/?nc=sn&amp;loc=6\n\nAWS Trusted Advisor is a tool that provides recommendations to optimize AWS resources, improve security, and reduce costs. It can be accessed through the AWS Management Console. Trusted Advisor checks are performed automatically and provide immediate feedback to the customer.\nTo receive AWS Trusted Advisor notifications, the customer needs to take action by setting up notification options in the Trusted Advisor dashboard. This involves selecting the notifications they want to receive, such as weekly summary emails or immediate alerts for specific recommendations.\nOption B, \"Set up Notification in Dashboard,\" is the correct answer. Once the notification options are set up, the customer will receive notifications through email or Amazon SNS (Simple Notification Service).\nOption A, \"Open a ticket with AWS Support,\" is not required for Trusted Advisor notifications. It is more appropriate for technical issues or specific support requests.\nOption C, \"Set up Amazon Simple Notification Service,\" is a possible way to receive Trusted Advisor notifications, but it is not necessary. The Trusted Advisor dashboard allows users to select email notifications as well.\nOption D, \"No action is required, all Notifications are sent automatically on a weekly basis,\" is incorrect. Although Trusted Advisor checks are performed automatically on a weekly basis, the customer needs to take action to set up notification options if they want to receive them.\n\n"
}, {
  "id" : 185,
  "question" : "Why is Amazon DynamoDB service best-suited for implementation in mobile, Internet of Things (IoT) and gaming applications?\n",
  "answers" : [ {
    "id" : "eaf881d4cbe44238a6eac99e5c68a121",
    "option" : "DynamoDB is a fully-managed database instance with no infrastructure overheads.",
    "isCorrect" : "false"
  }, {
    "id" : "54bab3141e8a459bb7d581f4cc3bae77",
    "option" : "DynamoDB has a flexible data model and single-digit millisecond latency.",
    "isCorrect" : "true"
  }, {
    "id" : "9cfdaf37e9cb4904abc96e7cf99f96ef",
    "option" : "Whilst in operation, DynamoDB instances are spread across at least three geographically distinct centers, AWS Regions.",
    "isCorrect" : "false"
  }, {
    "id" : "a101a7eaf8b44650af6b0b0e7a43740d",
    "option" : "DynamoDB supports eventual and strongly consistent reads.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nThe use cases mentioned in the scenario have unstructured data in common.\nTherefore, the most appropriate attribute of Amazon DynamoDB is its flexible data model and single-digit millisecond latency.\nhttps://aws.amazon.com/blogs/database/how-to-determine-if-amazon-dynamodb-is-appropriate-for-your-needs-and-then-plan-your-migration/\nOption A is INCORRECT because being fully-managed and having no infrastructure overheads do not distinguish DynamoDB as the best-suited solution for the given use cases.\nOption C is INCORRECT because the aspect of fault-tolerance, disaster recovery and high availability is also present in Amazon Relational Databases (RDS)\nThis feature does not distinguish the service in accordance with the described use cases.\nOption D is INCORRECT because this attribute of DynamoDB does not fully justify its exclusive choice over other instances when considered for implementation in the use cases mentioned in the question.\n\nAmazon DynamoDB is a fully managed, high-performance NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB is best suited for mobile, Internet of Things (IoT), and gaming applications for several reasons, including:\nA. Fully-Managed Database Instance with No Infrastructure Overheads: DynamoDB is a fully managed service, which means that AWS handles the heavy lifting of setting up, scaling, and maintaining the database infrastructure. This frees up application developers from the burden of managing database hardware, software, and configurations, allowing them to focus on building their applications.\nB. Flexible Data Model and Single-Digit Millisecond Latency: DynamoDB offers a flexible data model that can support various data types and data structures. The service also provides single-digit millisecond latency for both read and write operations, which makes it ideal for real-time applications that require fast and responsive data access.\nC. Geographically Diverse Data Centers: When deploying an application on DynamoDB, users can select the region that is closest to their customers to reduce latency and improve performance. DynamoDB instances operate across multiple Availability Zones within an AWS Region to provide high availability and data durability. This means that the data stored in DynamoDB is automatically replicated across multiple geographically diverse data centers.\nD. Support for Eventually and Strongly Consistent Reads: DynamoDB offers two types of reads: eventually consistent reads and strongly consistent reads. Eventually consistent reads provide the most up-to-date data most of the time, while strongly consistent reads provide the most up-to-date data all the time. Developers can choose the type of read that best suits their application's needs. This feature is particularly useful for mobile and IoT applications that may experience intermittent network connectivity and may require the ability to read stale data in some cases.\nIn summary, Amazon DynamoDB is best suited for mobile, Internet of Things (IoT), and gaming applications due to its fully managed database instance with no infrastructure overheads, flexible data model, single-digit millisecond latency, geographically diverse data centers, and support for eventually and strongly consistent reads.\n\n"
}, {
  "id" : 186,
  "question" : "A client who is using AWS cloud services has multiple environments for the EC2 servers like DEV, QA, PROD respectively.\nThe client would like to know billing details for each of these environments to make informed decisions related to saving costs.\nUsing which of the following options the requirements can be achieved?\n",
  "answers" : [ {
    "id" : "952a605f01e24cef8dd01b273aad7183",
    "option" : "Consolidated billing",
    "isCorrect" : "false"
  }, {
    "id" : "e28e539d165e40f48623e325736d0f46",
    "option" : "AWS Budgets",
    "isCorrect" : "false"
  }, {
    "id" : "b1ed3849d2644b509469de13a269eb14",
    "option" : "AWS Cost allocation tags",
    "isCorrect" : "true"
  }, {
    "id" : "2c85d41c7d5347ffb80e2325d63a5e71",
    "option" : "AWS Organizations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nCost allocation tags appear as additional columns in detailed billing reports.\nAs an example, let's say an AWS Region has 50 EC2 instances running out of which 15 are DEV, 15 are QA &amp; the remaining are PROD.\nWhile launching these instances, one can apply a tag named â€œenvâ€ providing value of DEV for 15 instances, QA for 15 instances and PROD for 20 instances.\nThese tags then appear in a detailed billing report as columns which can be filtered based on the environment and the cost of a set of environment servers calculated easily.\nOption A is incorrect since consolidated billing is a feature that applies to scenarios where a client has multiple accounts and would like to receive one bill for all his accounts.\nOption B is incorrect since AWS Budgets are used for setting custom budgets budgeting costs to track costs and usage of resources &amp; get alerted when actual or forecasted cost and usage exceeds the limits set for that budget.\nOption C is CORRECT.\nRefer above description for the same.\nOption D is incorrect.\nAWS Organizations allows you to consolidate multiple AWS accounts into an Organization that can be centrally managed for billing.\nThey can have Service Control Policies applied at the organization level that can override policies set up by IAM for individual accounts.\nReference:\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html\n\nThe correct answer is C. AWS Cost allocation tags.\nAWS Cost allocation tags are a way to label and categorize AWS resources for better cost management and allocation. With cost allocation tags, you can categorize resources in a way that makes sense for your business, such as by environment (DEV, QA, PROD), project, department, or team.\nYou can use cost allocation tags to create reports and budgets that show the cost of each environment separately, allowing you to make informed decisions related to saving costs. You can also use cost allocation tags to track resource usage and cost allocation across multiple AWS accounts.\nConsolidated billing (A) is a feature of AWS Organizations that allows you to consolidate billing for multiple AWS accounts under a single payer account. It simplifies billing and payment, but it doesn't provide the level of cost allocation detail needed to track costs by environment.\nAWS Budgets (B) are a way to set custom cost and usage budgets for your AWS resources and receive alerts when costs or usage exceed the budgeted amount. While budgets can help you monitor costs, they don't provide the level of cost allocation detail needed to track costs by environment.\nAWS Organizations (D) is a service that allows you to centrally manage multiple AWS accounts. It provides features such as consolidated billing, policy management, and access control, but it doesn't provide the level of cost allocation detail needed to track costs by environment.\n\n"
}, {
  "id" : 187,
  "question" : "Which of the following does AWS perform on its behalf for EBS volumes to make it less prone to failure?\n",
  "answers" : [ {
    "id" : "04a14fc56b51418094d88b436ea279b9",
    "option" : "Replication of the volume across Availability Zones",
    "isCorrect" : "false"
  }, {
    "id" : "4a1309f0708d4c909f2fdaf97767142f",
    "option" : "Replication of the volume in the same Availability Zone",
    "isCorrect" : "true"
  }, {
    "id" : "20e6a6342bcb46bf851f77bae7402f8d",
    "option" : "Replication of the volume across Regions",
    "isCorrect" : "false"
  }, {
    "id" : "59170ef2c9a749b9a07706bbf310d4e9",
    "option" : "Replication of the volume across Edge locations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nWhen you create an EBS volume in an Availability Zone, it is automatically replicated within that zone to prevent data loss due to the failure of any single hardware component.\nFor more information on EBS Volumes, please refer to the below URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumes.html\n\nThe correct answer is A. Replication of the volume across Availability Zones.\nAmazon Elastic Block Store (EBS) is a block-level storage service that is used in conjunction with Amazon EC2 instances to provide storage for data. When an EBS volume is created, it is automatically replicated within an Availability Zone to protect against the failure of a single component. This replication is known as EBS volume replication.\nAn Availability Zone (AZ) is a physically separate data center within a region. Each region has multiple Availability Zones that are interconnected with high-speed, low-latency networking. By replicating EBS volumes across Availability Zones, AWS provides durability and high availability for data storage.\nIf a hardware failure or other issue occurs in one Availability Zone, EBS can quickly redirect requests to the replicated EBS volume in another Availability Zone, reducing the risk of data loss or downtime.\nOption B, replicating the volume in the same Availability Zone, does not provide the same level of protection as replicating across Availability Zones. If a single Availability Zone fails, all data stored within that Availability Zone could be lost.\nOption C, replicating the volume across Regions, provides even more protection but can result in higher latency and increased data transfer costs.\nOption D, replicating the volume across Edge locations, is not a feature of EBS. Edge locations are part of the Amazon CloudFront content delivery network and are used to cache frequently accessed content to improve performance.\nTherefore, option A, replication of the volume across Availability Zones, is the correct answer as it provides high availability and durability for EBS volumes by replicating them across different Availability Zones within a region.\n\n"
}, {
  "id" : 188,
  "question" : "You are requested to expose your serverless application implemented with AWS Lambda to HTTP clients.\n( using HTTP Proxy ) Which of the following AWS services can you use to accomplish the task? (Select TWO)\n",
  "answers" : [ {
    "id" : "5f3deb50fe574af9aba3d61ba0936470",
    "option" : "AWS Elastic Load Balancing (ELB)",
    "isCorrect" : "true"
  }, {
    "id" : "1b77011c7b7748a48ed6a1f2dd8161ab",
    "option" : "AWS Route53",
    "isCorrect" : "false"
  }, {
    "id" : "37f1424a7f754e219e68fc60f78eb322",
    "option" : "AWS API Gateway",
    "isCorrect" : "true"
  }, {
    "id" : "a11279cfe384418e86bf7576cc95d067",
    "option" : "AWS Lightsail",
    "isCorrect" : "false"
  }, {
    "id" : "607cfdaba34e433282c88057905a3514",
    "option" : "AWS Elastic Beanstalk.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A and C.\nOption A is CORRECT because AWS documentation mentions that \"Application Load Balancers now support invoking Lambda functions to serve HTTP(S) requests.\" This enables users to access serverless applications from any HTTP client, including web browsers.\nOption B is INCORRECT because Route53 is a Domain Name System and not an HTTP proxy.\nOption C is CORRECT because API Gateway + Lambda is a common pattern for exposing serverless functions via HTTP/HTTPS.\nAWS documentation mentions that \"Creating, deploying, and managing a REST application programming interface (API) to expose backend HTTP endpoints, AWS Lambda functions, or other AWS services.\"\nOption D is INCORRECT because AWS Lightsail has a completely different goal.\nIt is a service to speed up the provisioning of AWS resources.\nOption E is INCORRECT because AWS Elastic Beanstalk has a completely different goal.\nIt is a service that makes it easier for developers to deploy and manage applications in the AWS Cloud quickly.\nDevelopers simply upload their applications, then Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.\nDiagram: none.\nReferences:\nELB:\nhttps://aws.amazon.com/elasticloadbalancing/faqs/?nc=sn&amp;loc=6\nAPI Gateway:\nhttps://aws.amazon.com/api-gateway/faqs/\n\nTo expose a serverless application implemented with AWS Lambda to HTTP clients using HTTP proxy, two AWS services that can be used are:\nAWS API Gateway: AWS API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It allows developers to create RESTful APIs that can be used to expose Lambda functions to HTTP clients. API Gateway also provides features like throttling, caching, and authentication to help manage traffic and ensure security. AWS Elastic Load Balancing (ELB): AWS Elastic Load Balancing (ELB) is a service that automatically distributes incoming traffic across multiple targets, such as EC2 instances, containers, or Lambda functions, in one or more Availability Zones. ELB supports multiple load balancing algorithms and provides features like SSL termination, connection draining, and health checks. By configuring ELB to use Lambda functions as targets, you can expose your serverless application to HTTP clients.\nAWS Route53 and AWS Elastic Beanstalk are not directly related to exposing serverless applications to HTTP clients. AWS Route53 is a domain name system (DNS) service that routes internet traffic to the resources that you specify, while AWS Elastic Beanstalk is a platform as a service (PaaS) that makes it easy to deploy and run web applications. AWS Lightsail is a simplified version of AWS services that provide easy-to-use, preconfigured, and cost-effective virtual private servers (VPS) that can be used to deploy web applications, but it does not provide the ability to expose serverless applications to HTTP clients.\n\n"
}, {
  "id" : 189,
  "question" : "You have an EC2 Instance in development that interacts with the Simple Storage Service.\nThe EC2 Instance is going to be promoted to the production environment.\nWhich of the following features should be used to grant the EC2 instance suitable permissions to access the Simple Storage Service?\n",
  "answers" : [ {
    "id" : "99b0889d07064afcb74ce551f66ae121",
    "option" : "IAM Users",
    "isCorrect" : "false"
  }, {
    "id" : "34e250111ca546ceaa744c1b4e3b659e",
    "option" : "IAM Roles",
    "isCorrect" : "true"
  }, {
    "id" : "de5b0cebe221443cb57f180f99d01d7a",
    "option" : "IAM Groups",
    "isCorrect" : "false"
  }, {
    "id" : "b621152428f549d9850e7da6e075436e",
    "option" : "IAM policies.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAn IAM role is similar to a user, in that it is an AWS identity with permission policies that determine what the identity can and cannot do in AWS.\nHowever, instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it.\nAlso, a role does not have standard long-term credentials (password or access keys) associated with it.\nInstead, if a user assumes a role, temporary security credentials are created dynamically and provided to the user.\nFor more information on IAM Roles, please refer to the below URL:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html\n\nWhen you have an EC2 instance that needs to access other AWS services, such as the Simple Storage Service (S3), it's important to ensure that it has the appropriate permissions to do so. This is where Identity and Access Management (IAM) comes into play.\nIAM is a service that allows you to manage access to AWS services and resources. It provides various features such as users, groups, roles, and policies, which can be used to control who can access which resources and how.\nIn this scenario, since the EC2 instance is going to be promoted to the production environment, you want to make sure that it has the appropriate permissions to access S3. Here are the different options to consider:\nA. IAM Users: IAM users are entities that are associated with a unique set of security credentials (i.e. access key and secret access key) and are used to interact with AWS services programmatically. However, using IAM users for EC2 instances is not recommended as they require the use of long-term credentials that need to be managed, rotated, and secured manually.\nB. IAM Roles: IAM roles are similar to users, but they don't have permanent credentials associated with them. Instead, they are assumed by trusted entities, such as EC2 instances, to grant temporary access to AWS services and resources. This is the recommended approach in this scenario, as it provides a more secure and scalable way to manage permissions for EC2 instances.\nC. IAM Groups: IAM groups are collections of IAM users that share the same permissions. While they can be used to manage permissions for EC2 instances, they are not the best option as they require additional setup and management overhead.\nD. IAM Policies: IAM policies are JSON documents that define permissions for IAM users, groups, and roles. While they are essential in granting permissions, they are not sufficient on their own, as they need to be attached to a user, group, or role to be effective.\nTherefore, the correct answer in this scenario is B. IAM Roles, as they provide a more secure and scalable way to manage permissions for EC2 instances that need to interact with other AWS services, such as S3.\n\n"
}, {
  "id" : 190,
  "question" : "A live online game uses DynamoDB instances in the backend to store real-time scores of the participants as they compete against each other from various parts of the world.\nWhich data consistency option is the most appropriate to implement?\n",
  "answers" : [ {
    "id" : "6e57f1c4b7544d3c8e5e72dc5c98e053",
    "option" : "Strongly consistent",
    "isCorrect" : "true"
  }, {
    "id" : "25f9246d4f2c4fe98156a57dffb2bf6c",
    "option" : "Eventually consistent",
    "isCorrect" : "false"
  }, {
    "id" : "be4d0f415f9c47c5be6635dcbca9153c",
    "option" : "Strong Eventual consistency",
    "isCorrect" : "false"
  }, {
    "id" : "7514dc411e764103896ff9fd9bfc7529",
    "option" : "Optimistic consistency.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nSince the gamers are from geographically distinct locations, the data will need to be immediately readable within a second as soon as it is written.\nTherefore strongly consistency is needed.\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\nOption B is INCORRECT because the scenarios outline that the participants of the game are live.\nIt will not suffice if any of them get updates on scores in less than real-time.\nOption C is INCORRECT because strong eventual consistency is not applicable in DynamoDB.Option D is INCORRECT because only two data consistency models are available with the DynamoDB service.\nOptimistic consistency is not supported.\n\nIn this scenario, we have a live online game that uses DynamoDB instances to store real-time scores of participants. DynamoDB is a NoSQL database that offers multiple data consistency options. The choice of data consistency option depends on the specific use case.\nA. Strongly Consistent: If we choose the strongly consistent option, then every read operation will return the most up-to-date data. It means that DynamoDB will always respond with the latest data, and the reads are not allowed until all previous writes have been completed. This option ensures that the data is consistent across all regions and accounts, but it can cause higher latency.\nB. Eventually Consistent: If we choose the eventually consistent option, then the reads might not reflect the most up-to-date write, but they will eventually converge to a consistent state. It means that the data will be consistent, but it can take some time to propagate across all regions and accounts. This option can improve performance and reduce latency, but it can also cause some inconsistency for a short time.\nC. Strong Eventual Consistency: Strong eventual consistency is a blend of both strongly consistent and eventually consistent options. It guarantees that all replicas will be eventually consistent, but also ensures that the reads are not allowed until all previous writes have been completed. This option is suitable for applications that require high consistency but can tolerate some additional latency.\nD. Optimistic Consistency: Optimistic consistency is a weaker form of consistency that assumes that conflicts are rare, and if they do occur, the application will resolve them. In this option, the database doesn't perform any consistency checks, but the application is responsible for resolving any conflicts. This option can provide better performance, but it can also increase the complexity of the application.\nBased on the use case of a live online game, where real-time scores are updated frequently, it is important to have the most up-to-date data available to all participants. Therefore, the strongly consistent option is the most appropriate to implement, as it ensures that every read operation returns the most up-to-date data, which is critical for the participants to see their scores in real-time. However, this option can cause higher latency, which might affect the overall performance of the game. Therefore, it is important to monitor the latency and optimize the performance of the application accordingly.\n\n"
}, {
  "id" : 191,
  "question" : "Your company is planning to host a large e-commerce application on the AWS Cloud.\nOne of their major concerns is Internet attacks such as DDoS attacks.\nWhich of the following services can help mitigate this concern? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "fd87331afcdc48fb873c2d10f20bd797",
    "option" : "Cloudfront",
    "isCorrect" : "true"
  }, {
    "id" : "4e3d4d0ae547432782d986bdd6846201",
    "option" : "AWS Shield",
    "isCorrect" : "true"
  }, {
    "id" : "29cbb59e61b04246976b54e8fe6085e9",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "948b105ab2014d119e603187d58ea612",
    "option" : "AWS Config.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS Documentation mentions the following on DDoS attacks.\nAWS Services for DDoS Attack Mitigation.\nAWS offers globally distributed, high network bandwidth and resilient services that, when used in conjunction with application-specific strategies, are key to mitigating DDoS attacks.\nFor more information on how to leverage each of these services and details on how their various features help protect against DDoS attacks, see the whitepaper AWS Best Practices for DDoS Resiliency.\nAWS Shield.\nAWS Shield is a managed DDoS protection service that is available in two tiers: Standard and Advanced.\nAWS Shield Standard applies always-on detection and inline mitigation techniques, such as deterministic packet filtering and priority-based traffic shaping, to minimize application downtime and latency.\nAWS Shield Standard is included automatically and transparently to your Elastic Load Balancing load balancers, Amazon CloudFront distributions, and Amazon Route 53 resources at no additional cost.\nWhen you use these services that include AWS Shield Standard, you receive comprehensive availability protection against all known infrastructure layer attacks.\nCustomers who have the technical expertise to manage their own monitoring and mitigation of application layer attacks can use AWS Shield together with AWS WAF rules to create a comprehensive DDoS attack mitigation strategy.\nAWS Shield Advanced provides enhanced DDoS attack detection and monitoring for application-layer traffic to your Elastic Load Balancing load balancers, CloudFront distributions, Amazon Route 53 hosted zones and resources attached to an Elastic IP address, such Amazon EC2 instances.\nAWS Shield Advanced uses additional techniques to provide granular detection of DDoS attacks, such as resource-specific traffic monitoring to detect HTTP floods or DNS query floods.\nAWS Shield Advanced includes 24x7 access to the AWS DDoS Response Team (DRT), support experts who apply manual mitigations for more complex and sophisticated DDoS attacks, directly create or update AWS WAF rules, and can recommend improvements to your AWS architectures.\nAWS WAF is included at no additional cost for resources that you protect with AWS Shield Advanced.\nAWS Shield Advanced includes access to near real-time metrics and reports, for extensive visibility into infrastructure layer and application layer DDoS attacks.\nYou can combine AWS Shield Advanced metrics with additional, fine-tuned AWS WAF metrics for a more comprehensive CloudWatch monitoring and alarming strategy.\nCustomers subscribed to AWS Shield Advanced can also apply for a credit for charges that result from scaling during a DDoS attack on protected Amazon EC2, Amazon CloudFront, Elastic Load Balancing, or Amazon Route 53 resources.\nSee the AWS Shield Developer Guide for a detailed comparison of the two AWS Shield offerings.\nAWS WAF.\nAWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources.\nYou can use AWS WAF to define customizable web security rules that control which traffic accesses your web applications.\nIf you use AWS Shield Advanced, you can use AWS WAF at no extra cost for those protected resources and can engage the DRT to create WAF rules.\nAWS WAF rules use conditions to target specific requests and trigger an action, allowing you to identify and block common DDoS request patterns and effectively mitigate a DDoS attack.\nThese include size constraint conditions to block a web request based on the length of its query string or request body, and geographic match conditions to implement geo restriction (also known as geoblocking) on requests that originate from specific countries.\nFor a complete list of conditions, see the AWS WAF Developer Guide.\nWith AWS WAF, you can also create rate-based rules that automatically block requests from a single IP address if they exceed a customer-defined rate limit.\nOne benefit of rate-based rules is that you can block requests from an IP address while it exceeds the threshold, and then automatically allow requests from that same client once they drop to an acceptable rate.\nThis helps ensure that regular viewers are not held in a persistent block list.\nYou can also combine the rate limit with conditions to trigger different actions for distinct scenarios.\nAmazon Route 53\nOne of the most common targets of DDoS attacks is the Domain Name System (DNS)\nAmazon Route 53 is a highly available and scalable DNS service designed to route end users to infrastructure running inside or outside of AWS.\nRoute 53 makes it possible to manage traffic globally through a variety of routing types, and provides out-of-the-box shuffle sharding and Anycast routing capabilities to protect domain names from DNS-based DDoS attacks.\nAmazon CloudFront.\nAmazon CloudFront distributes traffic across multiple edge locations and filters requests to ensure that only valid HTTP(S) requests will be forwarded to backend hosts.\nCloudFront also supports geoblocking, which you can use to prevent requests from particular geographic locations from being served.\nElastic Load Balancing.\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon Elastic Compute Cloud (Amazon EC2) instances, containers, and IP addresses, and multiple Availability Zones, which minimizes the risk of overloading a single resource.\nElastic Load Balancing, like CloudFront, only supports valid TCP requests, so DDoS attacks such as UDP and SYN floods are not able to reach EC2 instances.\nIt also offers a single point of management and can serve as a line of defense between the internet and your backend, private EC2 instances.\nElastic Load Balancing includes the Application Load Balancer, which is best suited for load balancing of HTTP and HTTPS traffic and also directly supports AWS WAF.VPCs and Security Groups.\nAmazon Virtual Private Cloud (Amazon VPC) allows customers to configure subnet routes, public IP addresses, security groups, and network access control lists in order to minimize application attack surfaces.\nYou can configure load balancers and EC2 instance security groups to allow traffic that originates from specific IP addresses only, such as that from CloudFront or AWS WAF, protecting backend application components from a direct attack.\nFor more information on DDoS attack prevention, please refer to the below URL:\nhttps://aws.amazon.com/answers/networking/aws-ddos-attack-mitigation/\n\nThe two services that can help mitigate DDoS attacks in AWS are CloudFront and AWS Shield.\nCloudFront: CloudFront is a content delivery network (CDN) that distributes content, such as webpages, videos, and images, to servers located worldwide. It has the capability to mitigate DDoS attacks by using a feature called \"Shield Advanced\". This feature uses real-time monitoring and inline mitigation to detect and mitigate attacks against your applications. AWS Shield: AWS Shield is a managed DDoS protection service that safeguards applications running on AWS. It provides protection against common network and transport layer DDoS attacks. There are two types of AWS Shield: AWS Shield Standard and AWS Shield Advanced.\nAWS Shield Standard provides automatic protection for all AWS customers at no additional cost, safeguarding web applications running on Amazon EC2, Elastic Load Balancing (ELB), Amazon CloudFront, and Amazon Route 53. AWS Shield Advanced provides enhanced protections against sophisticated DDoS attacks, with 24/7 access to DDoS response experts, and integration with AWS WAF, a web application firewall that allows you to create custom rules to block or allow traffic to your web applications.\nAWS EC2: Amazon Elastic Compute Cloud (EC2) is a web service that provides resizable compute capacity in the cloud. While EC2 instances themselves do not offer DDoS protection, they can be configured with security groups and network access control lists (ACLs) to provide additional security and prevent attacks. AWS Config: AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. While it does not provide DDoS protection, it can help you identify potential security risks and ensure that your AWS resources are properly configured to prevent attacks.\n\n"
}, {
  "id" : 192,
  "question" : "A research team conducting its work in remote locations of the world, without internet access, wishes to leverage Amazon services for their storage.\nThe team collects petabytes of information at a time.\nWhich service will best meet to transfer the petabytes of information?\n",
  "answers" : [ {
    "id" : "103acb342e854e9fa395cc89b54abbcf",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "99796872ce39420a81a5491b49f26e17",
    "option" : "Amazon Elastic Block Store (EBS)",
    "isCorrect" : "false"
  }, {
    "id" : "b35d2799986447a598cf79f10671b7ba",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "05967b209b2a429ab22857d55bd7dc33",
    "option" : "AWS Snowball.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nThe AWS Snowball service uses physical storage devices to transfer large amounts of data between Amazon Simple Storage Service (Amazon S3) and your onsite data storage location at faster-than-internet speeds.\nBy working with AWS Snowball, you can save time and money.\nSnowball provides powerful interfaces that you can use to create jobs, track data, and track your jobs' status through to completion.\nSnowball devices are physically rugged devices protected by the AWS Key Management Service (AWS KMS)\nThey secure and protect your data in transit.\nhttps://docs.aws.amazon.com/snowball/latest/ug/whatissnowball.html\nOption A is incorrect because Amazon S3 is the most suitable object storage when there is internet connectivity.\nIn this scenario, there is none.\nOption B is incorrect because Amazon EBS provides block level storage volumes suitable for operating systems, database instances and applications.\nFor the research team to store data to detached EBS volumes (not attached to EC2 instances), they would need internet connectivity.\nIn this scenario, the remote location does not have connectivity.\nOption C is incorrect because S3 Glacier is used for infrequent access nor usage.\nIt is unsuitable for this scenario.\n\nThe best service to transfer petabytes of information for a research team working in remote locations without internet access is AWS Snowball.\nAWS Snowball is a physical data transport solution that helps customers migrate large amounts of data into and out of AWS. It is designed to handle petabyte-scale data transfers in a secure and efficient manner.\nIn this case, the research team can use AWS Snowball to transfer their petabytes of information by simply connecting the Snowball device to their storage systems, copying the data onto the device, and then shipping the device to AWS for transfer to Amazon S3.\nAmazon S3 is a highly scalable and durable object storage service that provides industry-leading scalability, data availability, security, and performance. However, transferring petabytes of data over a slow or unreliable network connection can be time-consuming and may result in data loss or corruption.\nAmazon Elastic Block Store (EBS) is a block-level storage service that provides persistent block storage volumes for use with Amazon EC2 instances. It is not ideal for transferring large amounts of data as it is primarily used for attaching volumes to EC2 instances.\nAmazon S3 Glacier is a low-cost storage service that provides secure and durable long-term data retention. It is designed for data that is infrequently accessed and can tolerate longer retrieval times. It is not designed for transferring large amounts of data quickly.\nIn conclusion, the best option for the research team to transfer their petabytes of information without internet access is to use AWS Snowball.\n\n"
}, {
  "id" : 193,
  "question" : "Which of the following AWS services use serverless technology? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "2e7120ce420a4a3da9b4371ac99e580f",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "10681ab6e1f24377b3d66dac710ade94",
    "option" : "EC2",
    "isCorrect" : "false"
  }, {
    "id" : "a67fc0c07e0d4db280c7e7497cf6f9f8",
    "option" : "Simple Storage Service",
    "isCorrect" : "true"
  }, {
    "id" : "c56cc6d79a43477da36c0bd1d3c71ff5",
    "option" : "AWS Autoscaling.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and C.\nThe Simple Storage Service and DynamoDB are services where you don't need to manage the underlying infrastructure.\nFor more information on AWS S3 and DynamoDB, please refer to the below URL:\nhttps://aws.amazon.com/serverless/\nhttps://aws.amazon.com/s3/\nhttps://aws.amazon.com/dynamodb/\n\nThe two AWS services that use serverless technology are DynamoDB and AWS Autoscaling.\nDynamoDB is a managed NoSQL database service that allows users to store and retrieve any amount of data. It is serverless, meaning that AWS manages the infrastructure and automatically scales the database to handle any amount of traffic without any user intervention.\nAWS Autoscaling is a service that automatically scales the number of EC2 instances in a group based on the demand for the application. It is a serverless technology because it automatically scales the infrastructure based on the demand without the need for user intervention.\nEC2 (Elastic Compute Cloud) is a web service that provides resizable compute capacity in the cloud. It is not serverless because it requires users to manage the infrastructure and provision the servers.\nSimple Storage Service (S3) is a highly scalable object storage service that stores and retrieves any amount of data from anywhere on the web. It is not a serverless technology because it requires users to manage the storage and specify the amount of storage required.\nIn summary, DynamoDB and AWS Autoscaling are the two AWS services that use serverless technology. DynamoDB is a serverless NoSQL database service, and AWS Autoscaling is a serverless service that automatically scales EC2 instances based on the demand for the application.\n\n"
}, {
  "id" : 194,
  "question" : "Which of the following disaster recovery deployment mechanisms has the highest downtime?\n",
  "answers" : [ {
    "id" : "0d59c2371242441c9fe5c451e4e54296",
    "option" : "Pilot light",
    "isCorrect" : "false"
  }, {
    "id" : "f39148443cab440f900320ac3b1b6020",
    "option" : "Warm standby",
    "isCorrect" : "false"
  }, {
    "id" : "863c565d7df443aba1488123da622203",
    "option" : "Multi-Site",
    "isCorrect" : "false"
  }, {
    "id" : "3e877b08dbf448dd83122812cf61f637",
    "option" : "Backup and Restore.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe below snapshot from the AWS Documentation shows the spectrum of the Disaster recovery methods.\nIf you go to the further end of the spectrum you have the least time for downtime for the users.\nIn most traditional environments, data is backed up to tape and sent off-site regularly.\nIf you use this method, it can take a long time to restore your system in the event of a disruption or disaster.\nhttps://d1.awsstatic.com/whitepapers/aws-disaster-recovery.pdf\nFor more information on Disaster recovery techniques, please refer to the below URL:\nhttps://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/\n\n\nDisaster recovery (DR) is a crucial aspect of cloud computing as it involves restoring IT systems, infrastructure, and applications to their operational state after an unexpected disruption or disaster. There are different deployment mechanisms for DR, and each has its advantages and disadvantages.\nThe downtime is the period during which a system or service is unavailable, and it is one of the critical factors to consider when evaluating DR deployment mechanisms. The lower the downtime, the better the DR mechanism.\nNow let's take a closer look at the four DR deployment mechanisms listed in the question and see which one has the highest downtime:\nA. Pilot light: The pilot light deployment mechanism is a DR strategy that involves maintaining a minimal version of the production environment. The pilot light contains a subset of critical resources that are necessary to restore the full environment in case of a disaster. When a disaster occurs, the pilot light is used as a starting point to build out the production environment quickly.\nDowntime: The downtime associated with the pilot light mechanism is relatively low because the essential services are already running, and it only requires scaling up resources to restore the full environment. Therefore, the pilot light mechanism has a low downtime.\nB. Warm standby: The warm standby deployment mechanism involves maintaining a partially functional duplicate of the production environment, with fewer resources than the production environment. In case of a disaster, the warm standby environment can be scaled up to meet the production environment's needs quickly.\nDowntime: The downtime associated with the warm standby mechanism is higher than the pilot light because the partially functional environment needs to be scaled up to match the production environment's resources. Therefore, the warm standby mechanism has higher downtime than the pilot light mechanism.\nC. Multi-site: The multi-site deployment mechanism involves replicating the entire production environment to another geographic location. In case of a disaster, the workload is shifted to the replicated environment, which takes over the role of the primary environment.\nDowntime: The downtime associated with the multi-site mechanism is relatively low because the workload is shifted to the replicated environment, which is already functional. However, some data loss might occur, depending on the replication frequency. Therefore, the multi-site mechanism has low downtime.\nD. Backup and Restore: The backup and restore deployment mechanism involves creating backups of the production environment and restoring them in case of a disaster.\nDowntime: The downtime associated with the backup and restore mechanism is the highest among the four mechanisms because it requires the entire environment to be restored from a backup. This process can take hours or even days, depending on the size of the environment and the backup method. Therefore, the backup and restore mechanism has the highest downtime.\nConclusion: Based on the above analysis, the answer to the question is D. Backup and Restore mechanism has the highest downtime compared to the other DR deployment mechanisms listed.\n\n"
}, {
  "id" : 195,
  "question" : "Which of the following services allows you to analyze EC2 Instances against pre-defined security templates to check for vulnerabilities?\n",
  "answers" : [ {
    "id" : "d3cb49cf29e74dad97762e6aa354e7aa",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "420ae23a61004d3fb69708fdd7ea44c8",
    "option" : "AWS Inspector",
    "isCorrect" : "true"
  }, {
    "id" : "87566190991f4d8188d6821a13b8814e",
    "option" : "AWS WAF",
    "isCorrect" : "false"
  }, {
    "id" : "2dbd11fda8394ac1818ea58a755b6ff6",
    "option" : "AWS Shield.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAmazon Inspector enables you to analyze the behavior of your AWS resources and helps you to identify potential security issues.\nUsing Amazon Inspector, you can define a collection of AWS resources that you want to include in an assessment target.\nYou can then create an assessment template and launch a security assessment run of this target.\nFor more information on AWS Inspector, please refer to the below URL:\nhttps://docs.aws.amazon.com/inspector/latest/userguide/inspector_introduction.html\n\nThe correct answer is B. AWS Inspector.\nAWS Inspector is a security assessment service that helps improve the security and compliance of applications deployed on AWS. AWS Inspector provides automated security assessments of EC2 instances and applications to identify vulnerabilities and deviations from best practices. The service is designed to analyze EC2 instances against pre-defined security templates to check for vulnerabilities.\nAWS Inspector allows you to perform security assessments on EC2 instances based on pre-defined rules packages called \"assessment templates\". These templates are based on industry best practices and common security standards, such as CIS AWS Foundations Benchmark and the Payment Card Industry Data Security Standard (PCI DSS).\nAWS Inspector assesses the security posture of EC2 instances by analyzing their configuration, network traffic, and other data points. The service generates a detailed report of any identified vulnerabilities, along with remediation advice and recommendations.\nSome of the benefits of using AWS Inspector include:\nAutomated security assessments: AWS Inspector automates the security assessment process and provides continuous monitoring of your instances and applications. Customizable assessment templates: AWS Inspector provides pre-defined assessment templates, but you can also create custom templates to meet specific security requirements. Remediation advice: AWS Inspector provides detailed remediation advice and recommendations to help you address identified vulnerabilities.\nIn contrast, AWS Trusted Advisor provides recommendations to optimize your AWS infrastructure in terms of cost optimization, performance, security, and fault tolerance. AWS WAF is a web application firewall that helps protect web applications from common web exploits. AWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\n\n"
}, {
  "id" : 196,
  "question" : "Which of the following security services can be used to detect users' personal credit card numbers from data stored in Amazon S3?\n",
  "answers" : [ {
    "id" : "407538f0aab841e49b600bc35e41633f",
    "option" : "Amazon Macie",
    "isCorrect" : "true"
  }, {
    "id" : "95f3e7deabd045d2af2be7172cc00d47",
    "option" : "Amazon GuardDuty",
    "isCorrect" : "false"
  }, {
    "id" : "6cb666b3f98c4380b856fa9bc1b8dc2f",
    "option" : "Amazon Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "22377145b84849df883149e063883e92",
    "option" : "AWS Shield.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon Macie is a managed security service which can be used to detect personally identifiable information (PII) such as names, password, Credit card numbers from large amounts of data stored in Amazon S3 bucket.\nOption B is incorrect as Amazon GuardDuty is used to identify threats by analyzing events from AWS CloudTrail, VPC Flow Logs, and DNS Logs.\nIt cannot be used to detect PII from data stored in the Amazon S3 bucket.\nOption C is incorrect as Amazon Inspector can analyze potential security threats for an Amazon EC2 instance against an assessment template with predefined rules.\nOption D is incorrect as AWS Shield provides protection against DDOS attacks.\nFor more information on Amazon Macie, refer to the following URLs:\nhttps://aws.amazon.com/macie/features/\n\nThe correct answer is A. Amazon Macie.\nAmazon Macie is a security service provided by AWS that helps discover, classify, and protect sensitive data in AWS. It uses machine learning and pattern recognition to detect personally identifiable information (PII) and other sensitive data in Amazon S3, such as credit card numbers, social security numbers, and personally identifiable financial information.\nMacie scans data stored in Amazon S3, creates an inventory of the data and applies machine learning to identify sensitive data patterns. Once it identifies a potential match, it applies additional analytics to determine whether the data is in fact sensitive. When a match is confirmed, it can create alerts, including generating SNS notifications, and it can also generate findings in AWS Security Hub, a service that aggregates and prioritizes security findings from multiple AWS services.\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior in AWS accounts. It is not designed to detect sensitive data like credit card numbers.\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not designed to detect sensitive data like credit card numbers.\nAWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards web applications running on AWS. It is not designed to detect sensitive data like credit card numbers.\nIn conclusion, if you need to detect users' personal credit card numbers from data stored in Amazon S3, Amazon Macie is the AWS service you should use.\n\n"
}, {
  "id" : 197,
  "question" : "You are exploring which AWS service will help you to process a large number of data sets.\nChoose the correct answer from the given list.\n",
  "answers" : [ {
    "id" : "2cda56fce0c1460fb29b5178254671b2",
    "option" : "EMR",
    "isCorrect" : "true"
  }, {
    "id" : "00ef3e570ec24c59a6ccdd15b5c7e7f3",
    "option" : "S3",
    "isCorrect" : "false"
  }, {
    "id" : "52b9fd043d4a4ad3b50438586b183aae",
    "option" : "Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "62379f72b5d440fda440af84c45c1497",
    "option" : "Storage gateway.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following:\nAmazon EMR helps you analyze and process vast amounts of data by distributing the computational work across a cluster of virtual servers running in the AWS Cloud.\nThe cluster is managed using an open-source framework called Hadoop.\nAmazon EMR lets you focus on crunching or analyzing your data without having to worry about the time-consuming setup, management, and tuning of Hadoop clusters or the compute capacity they rely on.\nFor more information on AWS EMR, please refer to the below URL:\nhttp://docs.amazonaws.cn/en_us/aws/latest/userguide/emr.html\n\nThe correct answer is A. EMR (Elastic MapReduce).\nEMR is a managed service that enables the processing of large amounts of data using open-source tools such as Apache Hadoop, Apache Spark, and Presto. It enables you to quickly and easily provision, configure, and manage a cluster of Amazon EC2 instances for big data processing. EMR can also integrate with other AWS services such as Amazon S3, Amazon DynamoDB, and Amazon Redshift, making it easier to ingest data and perform analytics.\nOption B, S3 (Simple Storage Service), is an object storage service that is used to store and retrieve any amount of data from anywhere on the web. While S3 can be used to store large amounts of data, it is not optimized for processing and analyzing large data sets.\nOption C, Glacier, is an archival storage service that is optimized for long-term storage of infrequently accessed data. It is not designed for processing large data sets.\nOption D, Storage Gateway, is a hybrid storage service that enables you to connect your on-premises applications with cloud storage. It is not designed for processing large data sets.\nTherefore, the correct answer is A. EMR.\n\n"
}, {
  "id" : 198,
  "question" : "Which of the following services are used by AWS Service Catalog as a combination to create a portfolio of products?\n",
  "answers" : [ {
    "id" : "790270c8a5314e5bba9a41fe87ecb7e4",
    "option" : "AWS IAM &amp; AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "0add647400c94c72808481786deab595",
    "option" : "AWS Config &amp; AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "e60571f169a44165837ada581d80ef0c",
    "option" : "AWS IAM &amp; AWS CloudFormation",
    "isCorrect" : "true"
  }, {
    "id" : "0f584657115b4c4c806ce6bb5f3c005e",
    "option" : "AWS Config &amp; AWS Organizations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nAWS Service Catalog allows IT organizations to create a portfolio of products that end-users can use to deploy AWS resources as defined in the portfolio.\nFor this, AWS Service Catalog uses AWS IAM &amp; AWS CloudFormation.\nOption A is incorrect as AWS Service Catalog uses AWS IAM &amp; AWS CloudFormation &amp; not AWS Config to create a portfolio.\nOption B is incorrect as AWS IAM is used in combination with AWS CloudFormation by AWS Service Catalog.\nOption D is incorrect as AWS Service Catalog uses neither AWS Config nor AWS Organizations.\nFor more information on AWS Service Catalog, refer to the following URL;\nhttps://aws.amazon.com/servicecatalog/features/\n\nAWS Service Catalog is a service that allows organizations to create and manage catalogs of IT services that are approved for use on AWS. With Service Catalog, administrators can define a portfolio of products (such as EC2 instances or RDS databases), manage product versions and access permissions, and make products available to users through a customizable self-service portal.\nTo create a portfolio of products in AWS Service Catalog, the service uses a combination of AWS Config and AWS CloudFormation. AWS Config is used to monitor and assess resource configurations and compliance with policies, while AWS CloudFormation is used to create and manage AWS resources as code. AWS IAM (Identity and Access Management) can also be used to control access to resources and actions within Service Catalog and other AWS services.\nTherefore, the correct answer to this question is (B) AWS Config & AWS CloudFormation.\n\n"
}, {
  "id" : 199,
  "question" : "You are planning to serve a web application on the AWS Platform by using EC2 Instances.\nWhich of the below principles would you adopt to ensure that even if some of the EC2 Instances crash, you still have a working application?\n",
  "answers" : [ {
    "id" : "28a102e6fbe94cd58ae5e5b4bd42ec3c",
    "option" : "Using a scalable system",
    "isCorrect" : "false"
  }, {
    "id" : "fab743a8621f4afd9534cbec4286e51f",
    "option" : "Using an elastic system",
    "isCorrect" : "false"
  }, {
    "id" : "ca247d96b2294e358d91421564f49bd6",
    "option" : "Using a regional system",
    "isCorrect" : "false"
  }, {
    "id" : "7f3c56271e4244c894317ddbdd1965a8",
    "option" : "Using a fault tolerant system.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nA fault-tolerant system is one that ensures that the entire system works as expected, even there are issues.\nFor more information on designing fault-tolerant applications in AWS, please refer to the below URL:\nhttps://d1.awsstatic.com/whitepapers/aws-building-fault-tolerant-applications.pdf?did=wp_card&amp;trk=wp_card\nhttps://aws.amazon.com/premiumsupport/knowledge-center/autoscaling-fault-tolerance-load-balancer/\nhttps://aws.amazon.com/whitepapers/?whitepapers-main.sort-by=item.additionalFields.sortDate&amp;whitepapers-main.sort-order=desc\n\nTo ensure that the web application remains functional even if some of the EC2 Instances crash, you should use a fault-tolerant system.\nFault tolerance is a characteristic of a system that enables it to continue operating even if some of its components fail. A fault-tolerant system has redundant components, and if one component fails, another takes over its function without interrupting the system's operation.\nIn the case of EC2 Instances, a fault-tolerant system could involve the use of an Auto Scaling group with multiple instances distributed across different Availability Zones. Auto Scaling can automatically launch additional instances when the demand for the web application increases, and it can also replace instances that fail. Distributing instances across multiple Availability Zones ensures that if one Availability Zone becomes unavailable, the web application can continue running in another Availability Zone.\nUsing a scalable or elastic system could help ensure that the web application can handle increases in traffic and workload, but it does not guarantee fault tolerance.\nUsing a regional system may involve replicating the application and the data across multiple regions. While this can help with disaster recovery and business continuity, it may not provide fault tolerance within a single region.\nIn summary, to ensure that the web application remains operational even if some of the EC2 Instances fail, you should use a fault-tolerant system that includes redundant instances distributed across multiple Availability Zones.\n\n"
}, {
  "id" : 200,
  "question" : "Which of the following options would entice a company to use AWS over an on-premises data center? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "f55119bc1ca04f7284e9cfa85e504446",
    "option" : "Having access to Free and Unlimited Storage",
    "isCorrect" : "false"
  }, {
    "id" : "a826fed5eaf544c2bc4ea94c0353a0bc",
    "option" : "Having access to Unlimited Physical servers",
    "isCorrect" : "false"
  }, {
    "id" : "8b8ca69b8dad4c35969cb89ab558bd43",
    "option" : "Having a highly available infrastructure",
    "isCorrect" : "true"
  }, {
    "id" : "6015256baed24489bcc16e7d22a3acf5",
    "option" : "Ability to use resources on demand.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nAdvantages of the AWS Cloud or any cloud system include the ability to have a highly available infrastructure and the usage of resources on demand.\nFor more information on the advantages of using AWS, please refer to the below URL:\nhttps://aws.amazon.com/application-hosting/benefits/\n\nOut of the four options given, the correct answers are C and D.\nC. Having a highly available infrastructure: In an on-premises data center, maintaining high availability can be challenging and costly. Companies must purchase additional hardware and set up redundant infrastructure to ensure that their systems remain available in the event of an outage. In contrast, AWS provides a highly available infrastructure with built-in redundancy and failover capabilities, which reduces the risk of downtime and makes it easier for companies to maintain their applications and services. This is a significant advantage for companies that require high availability for their business-critical applications.\nD. Ability to use resources on demand: One of the most significant advantages of using AWS is the ability to use resources on demand. In an on-premises data center, companies must purchase and provision hardware based on their peak usage requirements, which can result in underutilized resources and wasted capacity. AWS provides a pay-as-you-go model that allows companies to provision resources as needed and only pay for what they use. This provides significant cost savings and enables companies to be more agile and responsive to changing business needs.\nA. Having access to Free and Unlimited Storage: AWS provides scalable storage options, but it is not free or unlimited. Companies must pay for the storage they use, and there are limits to the amount of storage available in each AWS service.\nB. Having access to Unlimited Physical servers: AWS provides access to a large number of virtual servers, but it does not provide unlimited physical servers. Companies can scale their resources up and down as needed, but they are still limited by the underlying physical infrastructure of AWS.\nIn summary, AWS provides a highly available infrastructure with built-in redundancy and failover capabilities and the ability to use resources on demand, which are significant advantages over an on-premises data center.\n\n"
}, {
  "id" : 201,
  "question" : "In serverless services such as AWS Lambda, what are the implications of the Shared Responsibility Model?\n",
  "answers" : [ {
    "id" : "f516c2c23e7245cc85f45fdf294287af",
    "option" : "Amazon has overall responsibility for the infrastructure, including IAM roles and identities that can invoke functions.",
    "isCorrect" : "false"
  }, {
    "id" : "e64d42cc606a4d8d8e378d77255a5b0f",
    "option" : "The user is responsible for the security and access to the instances that handle the compute capacity.",
    "isCorrect" : "false"
  }, {
    "id" : "688dc34f9532429eb8b2948813a1b975",
    "option" : "Amazon is responsible for any malicious code written in the IDE and can terminate any rogue activity.",
    "isCorrect" : "false"
  }, {
    "id" : "4f6d537204fc4af59020a3afa3a7470b",
    "option" : "The user is responsible for IAM roles and identities that can invoke the AWS Lambda functions.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nIn serverless services such as AWS Lambda, Amazon adopts the responsibility of running all the compute, network and storage of the integrated development environment.\nThe user is responsible for the code itself and identity access management.\nhttps://aws.amazon.com/lambda/\nOption A is INCORRECT because Amazon is not responsible for identity access management (IAM) in the Shared Responsibility Model.\nOption B is INCORRECT because the user is not responsible for the security and access to the underlying instances that provide compute capacity in serverless services.\nOption C is INCORRECT because in serverless services such as AWS Lambda, Amazon is not responsible for code written in the IDE.\n\nIn AWS, the Shared Responsibility Model defines the division of responsibilities between AWS and the customer regarding security and management of resources.\nIn the case of serverless services such as AWS Lambda, the infrastructure is fully managed by AWS, which includes the underlying hardware, the operating system, and the application container. Therefore, the customer is not responsible for managing and maintaining the infrastructure.\nHowever, the customer is still responsible for configuring and securing their own code that runs on AWS Lambda. This includes ensuring the security of their code, testing it for vulnerabilities, and authorizing access to their code through IAM roles and policies.\nIn summary, option D is the correct answer: the user is responsible for IAM roles and identities that can invoke the AWS Lambda functions. The other options are incorrect because they either overstate AWS's responsibility or misinterpret the customer's responsibility.\n\n"
}, {
  "id" : 202,
  "question" : "You have 2 accounts in your AWS account- one for the Dev and the other for QA.\nBoth accounts are configured using consolidated billing.\nThe management account has purchased 3 reserved instances.\nThe Dev department is currently using 2 reserved instances.\nThe QA department is planning to use 3 EC2 instances.\nHow many reserved instances (purchased by the management account) can be used by the QA Team?\n",
  "answers" : [ {
    "id" : "0fd8ec0ec1c5401180b59f03dccaee7b",
    "option" : "No Reserved instance",
    "isCorrect" : "false"
  }, {
    "id" : "172195b988c147e389092d37d3fd5c2c",
    "option" : "One Reserved instance",
    "isCorrect" : "true"
  }, {
    "id" : "23100697e03444efb1c3d3cefbaa354b",
    "option" : "Two Reserved instances",
    "isCorrect" : "false"
  }, {
    "id" : "a42a4b7da5354a24a6adc9ed4eb0a9bc",
    "option" : "Three Reserved instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe management account has three reserved instances and two of them are used by Dev department so we have only one reserved instance left that will be used by QA department.\nThe QA team needs three instances so remaining two instances can be added afterwards.\nFor more information on AWS Reserved instances, please refer to the below URL:\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\n\nThe answer is B. One Reserved instance.\nExplanation:\nWhen an AWS account is configured for consolidated billing, it enables the payer account to pay for all accounts associated with it. The payer account can also view a combined view of AWS costs for all accounts in the organization. However, this does not mean that the reserved instances purchased by the payer account can be used by all accounts. Reserved instances are specific to the instance type, availability zone, and region for which they were purchased. Therefore, to use the reserved instance, it must match the instance type, availability zone, and region of the running instance.\nIn this scenario, the management account has purchased three reserved instances. However, the Dev department is currently using two reserved instances, which means there is only one reserved instance left. The QA department is planning to use three EC2 instances, but it does not necessarily mean that all three instances will match the reserved instance purchased by the management account.\nTherefore, it is only possible for the QA team to use one reserved instance that matches their requirements. The other two instances will be billed at the On-Demand rate.\nHence, the answer is B. One Reserved instance.\n\n"
}, {
  "id" : 203,
  "question" : "Which of the following are best practices when designing cloud-based systems? Choose 2 answers from the options below.\n",
  "answers" : [ {
    "id" : "e1867cdd50fc48a4bfdb1a7dd4cfe70e",
    "option" : "Build Tightly-coupled components.",
    "isCorrect" : "false"
  }, {
    "id" : "d9518bf80c7b459d81d2dd5e157ecc8b",
    "option" : "Build loosely-coupled components.",
    "isCorrect" : "true"
  }, {
    "id" : "c23959bac4d6488aaa6f38b039e2655e",
    "option" : "Assume everything will fail.",
    "isCorrect" : "true"
  }, {
    "id" : "9c2273ea57d541a086491247de19fffe",
    "option" : "Use as many services as possible.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nAlways build components that are loosely coupled.\nThis is so that even if one component does fail, the entire system does not fail.\nIf you build with the assumption that everything will fail, you will ensure that the right measures are taken to build a highly available and fault-tolerant system.\nOption D is incorrect because using multiple services increases cost and operational burden, rather use less and efficient services like serverless storage services and serverless compute services.\nFor more information on a well-architected framework, please refer to the below URL:\nhttps://d0.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf\n\nThe two best practices when designing cloud-based systems are building loosely-coupled components and assuming everything will fail.\nBuilding loosely-coupled components means designing the components of the system in a way that they are independent of each other and have minimal interdependencies. This allows for greater flexibility and scalability as changes can be made to one component without affecting the rest of the system. This also makes it easier to troubleshoot and debug the system in case of failures.\nAssuming everything will fail is a key principle of designing systems for the cloud. This means anticipating that any component of the system can fail at any time, and designing the system to be resilient to such failures. This involves using redundancy and fault tolerance mechanisms to ensure that the system continues to operate even in the face of failures.\nBuilding tightly-coupled components, on the other hand, can lead to increased interdependencies between components, making it harder to make changes or troubleshoot issues. It can also make the system less resilient to failures as a failure in one component can have a cascading effect on other components.\nUsing as many services as possible may also not be a best practice as it can lead to increased complexity and cost. It's important to evaluate which services are necessary for the system and choose them carefully, rather than just using as many as possible.\n\n"
}, {
  "id" : 204,
  "question" : "Which of the following services can be used to identify an issue with the Amazon CloudWatch service?\n",
  "answers" : [ {
    "id" : "40555c068c724204876b57363d491cfa",
    "option" : "Amazon CloudWatch Events",
    "isCorrect" : "false"
  }, {
    "id" : "1c4fef8d69d64da098dd24de9e7f4b85",
    "option" : "AWS Personal Health Dashboard",
    "isCorrect" : "true"
  }, {
    "id" : "a0458db5851e4289967d3ee22d55f890",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "3f625efd613d4958b241744dc0937c4f",
    "option" : "Amazon CloudTrail logs.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Personal Health Dashboard provides detailed information about performance &amp; availability of AWS services which may impact customers' resources.\nAWS Personal Health Dashboard works along with Amazon CloudWatch and provides notifications when there are issues with Amazon CloudWatch service.\nOption A is incorrect as Amazon CloudWatch Events cannot be used to identify issues with the Amazon CloudWatch service.\nOption C is incorrect as AWS Trusted Advisor does not send notifications when there are issues in the Amazon CloudWatch service.\nOption D is incorrect as Amazon CloudTrail logs do not notify issues with the Amazon CloudWatch service.\nFor more information on AWS Personal Health Dashboard, refer to the following URL:\nhttps://aws.amazon.com/premiumsupport/faqs/?nc=sn&amp;loc=6\n\nOut of the given options, the service that can be used to identify an issue with the Amazon CloudWatch service is AWS Personal Health Dashboard (B).\nAmazon CloudWatch is a monitoring service provided by AWS that allows you to collect and track metrics, collect and monitor log files, and set alarms for your AWS resources.\nAWS Personal Health Dashboard (PHD) is a personalized view of the health of your AWS resources. It provides alerts and remediation guidance when AWS is experiencing events that might affect your resources. It also provides proactive notification of upcoming changes to your resources that could impact your applications.\nIn the case of Amazon CloudWatch, AWS Personal Health Dashboard can be used to identify any service disruptions or outages that might be affecting the CloudWatch service. If there is a problem with CloudWatch, AWS Personal Health Dashboard will provide you with the relevant information and guidance on how to remediate the issue.\nAmazon CloudWatch Events (A) is a service that enables you to respond to changes in your AWS resources. It can be used to route events to various AWS services and to your own applications.\nAWS Trusted Advisor (C) is a service that provides guidance on how to optimize your AWS infrastructure, improve security and performance, and reduce costs. It is not specifically designed to identify issues with CloudWatch.\nAmazon CloudTrail logs (D) are used to record API calls made to AWS services. They are used for auditing, compliance, and troubleshooting purposes, but are not specifically designed to identify issues with CloudWatch.\n\n"
}, {
  "id" : 205,
  "question" : "Which of the following is the amount of storage that can be stored in the Simple Storage Service?\n",
  "answers" : [ {
    "id" : "1b7213b86fb24be886defed4788a6e1b",
    "option" : "1 TB",
    "isCorrect" : "false"
  }, {
    "id" : "41fab694f21c4cb0a87de966ac395445",
    "option" : "5 TB",
    "isCorrect" : "false"
  }, {
    "id" : "37aa79ffb06b4b2287dc5d2906810411",
    "option" : "1 PB",
    "isCorrect" : "false"
  }, {
    "id" : "14c6673d9e0a460ea8186fd51bbe8691",
    "option" : "Virtually unlimited storage.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following:\nAmazon S3 provides a simple web service interface that you can use to store and retrieve any amount of data, at any time, from anywhere on the web.\nFor more information on AWS S3, please refer to the below URL:\nhttps://aws.amazon.com/s3/faqs/\n\nThe correct answer is D. Virtually unlimited storage.\nAmazon Simple Storage Service (S3) is an object storage service provided by Amazon Web Services (AWS). It provides highly durable and scalable object storage that is designed to store and retrieve any amount of data from anywhere on the web.\nS3 is designed to provide virtually unlimited storage capacity. With S3, you can store an unlimited number of objects, each up to 5 terabytes in size. Furthermore, there is no limit to the total amount of data you can store in S3.\nS3 is highly scalable and can automatically scale up or down depending on the amount of data you store. You only pay for the storage you use, making it cost-effective for businesses of all sizes.\nIn summary, the amount of storage that can be stored in Amazon S3 is virtually unlimited, making it an ideal solution for businesses of all sizes looking for a highly scalable and cost-effective storage solution.\n\n"
}, {
  "id" : 206,
  "question" : "You have been hired as an AWS Architect for a company.\nThere is a requirement to host an application using EC2 Instances.\nThe Infrastructure needs to scale on-demand and also be fault-tolerant.\nWhich of the following would you include in the design? (Select TWO)\n",
  "answers" : [ {
    "id" : "922d0719194d49609551c1250815bf6c",
    "option" : "AWS Auto Scaling",
    "isCorrect" : "true"
  }, {
    "id" : "4a71b6da19c9455387b6dafb305bdce0",
    "option" : "Amazon GuardDuty",
    "isCorrect" : "false"
  }, {
    "id" : "d4db64a5cf4846638d1bb3c1ed9623c8",
    "option" : "Elastic Load Balancing",
    "isCorrect" : "true"
  }, {
    "id" : "1d79c5289b9e49b2ba4f6c52d029432e",
    "option" : "Amazon CloudWatch.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: A and C.\nThe AWS Documentation mentions the following.\nYou can automatically increase the size of your Auto Scaling group when demand goes up and decrease it when demand goes down.\nAs the Auto Scaling group adds and removes EC2 instances, you must ensure that the traffic for your application is distributed across all of your EC2 instances.\nThe Elastic Load Balancing service automatically routes incoming web traffic across such a dynamically changing number of EC2 instances.\nOption B is incorrect since Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behaviour.\nOption D is incorrect since CloudWatch is a monitoring service.\nFor more information on AWS Autoscaling and ELB, please visit the below URL.\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/autoscaling-load-balancer.html\nhttps://aws.amazon.com/guardduty/\n\nAs an AWS Architect, you have to design the infrastructure to host the application using EC2 Instances in a way that it scales on-demand and be fault-tolerant. To meet these requirements, you would include the following two services in your design:\nA. AWS Auto Scaling: AWS Auto Scaling is a service that helps you to automatically scale up or down your application resources based on the demand. It can be used to maintain the desired capacity, availability, and performance of the application. AWS Auto Scaling enables you to scale resources horizontally or vertically. It automatically adds or removes EC2 instances based on the demand, which can help your application handle sudden spikes in traffic. Therefore, including AWS Auto Scaling in your design would help you to achieve scalability.\nC. Elastic Load Balancing: Elastic Load Balancing distributes incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones. Elastic Load Balancing can automatically scale your workload in response to incoming traffic. It also provides high availability by automatically routing traffic to healthy targets. Therefore, including Elastic Load Balancing in your design would help you to achieve fault tolerance and scalability.\nB. Amazon GuardDuty and D. Amazon CloudWatch: Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior in your AWS account. It is not directly related to scaling or fault tolerance of the application hosted on EC2 instances.\nAmazon CloudWatch is a monitoring and observability service that collects and tracks metrics, logs, and events from AWS resources and applications. It can be used to gain operational insight and visibility into your application's performance. While Amazon CloudWatch can be used to monitor and scale resources based on demand, it is not directly related to fault tolerance.\nIn conclusion, including AWS Auto Scaling and Elastic Load Balancing in your design would help you to achieve both scalability and fault tolerance for your application hosted on EC2 instances.\n\n"
}, {
  "id" : 207,
  "question" : "A website for an international sport governing body would like to serve its content to viewers from different parts of the world in their vernacular language.\nWhich is the most suitable service that will allow different language versions of the same website to be served?\n",
  "answers" : [ {
    "id" : "3176c9f6a08e462e88e6ea680c3168b4",
    "option" : "Amazon CloudFront",
    "isCorrect" : "true"
  }, {
    "id" : "336a0bcb12a3417490781e960a1ba385",
    "option" : "Amazon EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "44f616b1671446308a83e8bb4d459abe",
    "option" : "Amazon Lightsail",
    "isCorrect" : "false"
  }, {
    "id" : "5354bfb4f9e54db28b03dd11164f9d3a",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nA better option would be to use CloudFront to detect the user's location and send all traffic to a single origin, which then serves the user back to the correct localization of the site.\nYou can configure CloudFront to add additional geolocation headers that provide more granularity in your caching and origin request policies.\nThe new headers give you more granular control of cache behavior and your origin access to the viewer's country name, region, city, postal code, latitude, and longitude, all based on the viewer's IP address.\nOption B is INCORRECT because EC2 is just a distractor, not suitable for routing and delivery.\nOption C is INCORRECT because Amazon Lightsail will primarily allow for developing, deploying, and hosting websites and web applications.\nThe service will not meet the requirements of the scenario.\nOption D is INCORRECT because the geolocation routing policy of Route53 allows different resources to serve content based on the origin of the request.\nBased on the question it looks like the customer is hosting multiple origins for their app, one for each language.\nThis is not recommended or feasible.\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html#lambda-examples-content-based-routing-examples\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/leverage-amazon-cloudfront-geolocation-headers-for-state-level-geo-targeting/\n\n\nThe most suitable service that will allow different language versions of the same website to be served is Amazon CloudFront.\nAmazon CloudFront is a content delivery network (CDN) service that speeds up the delivery of static and dynamic content such as HTML, CSS, JavaScript, images, and videos to users across the globe. It has edge locations all over the world that cache content and serve it to users from the edge location closest to them, reducing latency and improving performance.\nCloudFront can be used to serve different language versions of the same website by creating multiple origins for each language version of the website, and configuring CloudFront to serve the appropriate version based on the viewer's location or language preference. This can be achieved by setting up CloudFront to use the viewer's Accept-Language header to determine the preferred language and serving the appropriate version of the website.\nIn addition, CloudFront can also be integrated with other AWS services like Amazon S3, AWS Lambda, and Amazon API Gateway to further optimize the delivery of dynamic content and provide additional functionality like serverless computing and API management.\nTherefore, among the given options, Amazon CloudFront is the most suitable service to serve different language versions of the same website to viewers from different parts of the world in their vernacular language.\n\n"
}, {
  "id" : 208,
  "question" : "You have a mission-critical application that must be globally available at all times.\nIf this is the case, which of the below deployment mechanisms would you employ?\n",
  "answers" : [ {
    "id" : "ff13f8500e2740bda0202cb539cd132f",
    "option" : "Deployment to multiple edge locations",
    "isCorrect" : "false"
  }, {
    "id" : "604c7da09e024a9a8b409de653baa688",
    "option" : "Deployment to multiple Availability Zones",
    "isCorrect" : "false"
  }, {
    "id" : "a06b1133296b44dbb82b2a753646d232",
    "option" : "Deployment to multiple Data Centers",
    "isCorrect" : "false"
  }, {
    "id" : "1825b701a23248ab9da5786513d19f66",
    "option" : "Deployment to multiple Regions.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nRegions represent different geographical locations and are suitable for hosting your application across multiple regions for disaster recovery.\nFor more information on AWS Regions, please refer to the below URL:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html\n\nFor a mission-critical application that must be globally available at all times, deploying to multiple regions is the best option.\nExplanation:\nDeployment to multiple edge locations: Edge locations are used for content delivery and caching, and they are not suitable for hosting a mission-critical application that must be globally available at all times. Deploying to edge locations is a good option when your application is content-heavy and you need to improve user experience by caching content closer to the user, but it does not ensure high availability. Deployment to multiple Availability Zones: Availability Zones are physically separate data centers within the same region, and they are designed to be isolated from each other to ensure high availability. Deploying your application to multiple Availability Zones within the same region ensures that your application can withstand failures in one Availability Zone without affecting the availability of your application. However, deploying to multiple Availability Zones in the same region is not enough to ensure global availability, as a failure in the region itself could still affect the availability of your application. Deployment to multiple Data Centers: Deploying your application to multiple data centers is a good option when you need to ensure high availability within a specific region. However, deploying to multiple data centers in different regions is a better option when you need to ensure global availability. Deployment to multiple Regions: Deploying your application to multiple regions ensures global availability and reduces the risk of downtime caused by a disaster that affects a single region. When you deploy your application to multiple regions, you ensure that your application is available to users even if one or more regions become unavailable.\nTherefore, for a mission-critical application that must be globally available at all times, deploying to multiple regions is the best option.\n\n"
}, {
  "id" : 209,
  "question" : "Which of the following can be used to protect against DDoS attacks? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "9fa37ab9d23d4dabbba17babd9da6046",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "311e4bdc14c9420daab849158955c9ee",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "e4c7e93bb80c48ce8c126c9718d42584",
    "option" : "AWS Shield",
    "isCorrect" : "true"
  }, {
    "id" : "a9989c41d7364144a8cedcf5e9d370c3",
    "option" : "AWS Shield Advanced.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - C and D.\nThe AWS Documentation mentions the following:\nAWS Shield - All AWS customers benefit from the automatic protections of AWS Shield Standard, at no additional charge.\nAWS Shield Standard defends against most common, frequently occurring network and transport layer DDoS attacks that target your web site or applications.\nAWS Shield Advanced - For higher levels of protection against attacks targeting your web applications running on Amazon EC2, Elastic Load Balancing (ELB), CloudFront, and Route 53 resources, you can subscribe to AWS Shield Advanced.\nAWS Shield Advanced provides expanded DDoS attack protection for these resources.\nFor more information on AWS Shield, please refer to the below URL:\nhttps://docs.aws.amazon.com/waf/latest/developerguide/ddos-overview.html\n\nThe correct answers are C. AWS Shield and D. AWS Shield Advanced.\nExplanation:\nDDoS (Distributed Denial of Service) is a type of cyber-attack where multiple compromised systems (or bots) are used to flood a network or a server with excessive traffic, thereby disrupting normal service. DDoS attacks are becoming more common, and they can cause significant damage to a business's reputation and financial stability. Therefore, it's essential to have protection against such attacks.\nAWS Shield is a managed DDoS protection service offered by Amazon Web Services (AWS) that safeguards web applications running on AWS. AWS Shield provides always-on detection and automatic inline mitigations that minimize application downtime and latency caused by DDoS attacks. AWS Shield provides protection against several types of DDoS attacks, including network and application-layer attacks.\nAWS Shield Advanced is an additional service that offers more comprehensive DDoS protection than AWS Shield. It provides enhanced detection and mitigation capabilities, including 24/7 access to AWS DDoS Response Team (DRT) for advanced attack mitigation support, real-time visibility and attack analytics, and cost protection against usage spikes caused by DDoS attacks.\nAWS EC2 (Elastic Compute Cloud) is a web service that provides scalable computing capacity in the cloud. Although EC2 instances can be used to host web applications, they do not provide DDoS protection by default.\nAWS RDS (Relational Database Service) is a web service that provides managed database instances in the cloud. Like EC2, RDS instances can be used to host web applications, but they do not provide DDoS protection by default.\nIn conclusion, to protect against DDoS attacks on web applications running on AWS, AWS Shield and AWS Shield Advanced are the recommended services to use.\n\n"
}, {
  "id" : 210,
  "question" : "Which of the following is a serverless compute offering from AWS?\n",
  "answers" : [ {
    "id" : "a55750933fd542b5b9ba20fc1fa165bc",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "402932ccdcde4b608c6f9d782ee6abd7",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "bcc4f27bd6e94363b24d4a2d73af5f46",
    "option" : "AWS SNS",
    "isCorrect" : "false"
  }, {
    "id" : "ce7ddda4184b47f9909f723ff20edcf6",
    "option" : "AWS SQS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers.\nAWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.\nFor more information on AWS Lambda, please refer to the below URL:\nhttps://docs.aws.amazon.com/lambda/latest/dg/welcome.html\n\nThe correct answer is B. AWS Lambda.\nExplanation:\nAWS Lambda is a serverless compute offering from AWS. It is a compute service that lets you run your code without provisioning or managing servers. With Lambda, you can simply upload your code and it will automatically scale to meet the demands of your application, without any additional effort on your part. Lambda supports multiple programming languages, including Python, Node.js, Java, and C#.\nAWS EC2 (A) is a traditional virtual machine (VM) compute offering from AWS. With EC2, you are responsible for provisioning and managing the VMs. EC2 provides you with complete control over your computing resources, including the ability to choose your operating system, configure networking, and install software.\nAWS SNS (C) is a messaging service from AWS that allows you to send and receive messages between different components of your application. SNS is used for messaging and communication between distributed systems, applications, and services.\nAWS SQS (D) is a message queuing service from AWS that enables decoupling of components of a cloud application. SQS allows you to send, store, and receive messages between software components at any volume, without losing messages or requiring other services to be available.\nIn summary, among the given options, AWS Lambda is the only serverless compute offering from AWS, and the other options are different services offered by AWS for messaging, queuing, and virtual machine computing.\n\n"
}, {
  "id" : 211,
  "question" : "Which of the following are the recommended resources to be deployed in theAmazon VPC private subnet?\n",
  "answers" : [ {
    "id" : "3f88ab753c124bcea6f9df8f4db03ef9",
    "option" : "NAT Gateways",
    "isCorrect" : "false"
  }, {
    "id" : "cd7c7e4ae0234ac8943be8ca9eab0451",
    "option" : "Bastion Hosts",
    "isCorrect" : "false"
  }, {
    "id" : "64114bd10a90471dba927ed43411c3e7",
    "option" : "Database Servers",
    "isCorrect" : "true"
  }, {
    "id" : "aab16e0d1d6048f3bdd34507ddc12bc6",
    "option" : "Internet Gateways.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAs Database servers contain confidential information, so for a security perspective, it should be deployed in a Private Subnet.\nAmazon Virtual Private Cloud (Amazon VPC) enables the user to launch AWS resources into a virtual network that a user has defined.\nOption A is incorrect because NAT devices (NAT Gateway, Nat Instance) allow instances in private subnets to connect to the internet, other VPCs, or on-premises networks.\nIt is deployed in a public subnet.\nOption B is incorrect because bastion host is a server whose purpose is to provide access (SSH access) to a private network from an external network, such as the Internet.\nIt is deployed in a public subnet.\nOption D is incorrect because an Internet Gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet.\nFor more information on AWS VPC, please refer to the below URL:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Networking.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat.html\nhttps://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/\n\nThe Amazon Virtual Private Cloud (VPC) is a logically isolated network that enables the user to launch AWS resources into a virtual network that they have defined. Within the VPC, a subnet is a range of IP addresses within the VPC that the user can use to launch Amazon Elastic Compute Cloud (EC2) instances, Relational Database Service (RDS) instances, or other resources.\nIn this context, private subnets are subnets that do not have a direct connection to the internet. The instances in these subnets cannot communicate with the internet unless they are routed through a NAT Gateway, a Bastion Host, or other resources.\nTherefore, the recommended resources to be deployed in the Amazon VPC private subnet are:\nA. NAT Gateways: Network Address Translation (NAT) Gateway allows instances in a private subnet to connect to the internet or other AWS services, without exposing their private IP addresses to the internet. It acts as a bridge between the private subnet and the internet, enabling instances to access the internet for software updates, patching, and other maintenance tasks.\nB. Bastion Hosts: A bastion host is a special-purpose instance that is used to securely administer EC2 instances within a private subnet. It acts as a proxy server, enabling secure access to EC2 instances in a private subnet from a remote network, such as the user's home network. Bastion hosts should be deployed in a public subnet and configured with appropriate security measures.\nC. Database Servers: Database servers should be deployed in a private subnet to ensure that they are not directly accessible from the internet. The database servers can be accessed from other resources within the VPC, such as web servers or application servers, through appropriate routing configurations.\nD. Internet Gateways: Internet Gateways are used to enable communication between instances in a VPC and the internet. However, Internet Gateways are not recommended to be deployed in private subnets, as it would expose the instances to the public internet.\nIn conclusion, the recommended resources to be deployed in the Amazon VPC private subnet are NAT Gateways, Bastion Hosts, and Database Servers. Internet Gateways should not be deployed in private subnets.\n\n"
}, {
  "id" : 212,
  "question" : "A web application co-located in two geographically distinct locations is experiencing degraded service in one of the locations.\nWhat is the most appropriate routing policy to implement in Amazon Route 53?\n",
  "answers" : [ {
    "id" : "51e41da4812146fa9637fcfd63e7cb41",
    "option" : "Geolocation routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "bc153e036875437d8f54dce0ef754617",
    "option" : "Weighted routing policy",
    "isCorrect" : "false"
  }, {
    "id" : "6940953a85f54e7cb13beff8f5bcf450",
    "option" : "Failover routing policy",
    "isCorrect" : "true"
  }, {
    "id" : "270d7ff6a83e45f8a1bef8f507d32355",
    "option" : "Latency-based routing policy.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - C.\nFailover routing policy is the most appropriate routing policy to implement because it will make it possible for traffic to be routed to the resource in good health and not to the one experiencing poor response times.\nSeveral instances can be configured.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-failover\nOption A is INCORRECT because the Geolocation routing policy is essential when traffic to the hosted resources is routed according to its originating source address.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\nOption B is INCORRECT because a weighted routing policy can distribute request traffic amongst resources.\nBut in this scenario, it would be undesirable and cumbersome to manually set weights (preferences) each time any resource is unhealthy.\nThis will not be the most appropriate routing policy.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted\nOption D is INCORRECT because latency-based routing policy allows for queries to be served by the resources with the shortest response times to the user's location.\nThe routing policy does not give the required functionality in the scenario.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-latency\n\nThe most appropriate routing policy to implement in Amazon Route 53 for a web application co-located in two geographically distinct locations that is experiencing degraded service in one of the locations is the failover routing policy.\nThe failover routing policy is designed to route traffic to a specified resource only when it is healthy or available. In this case, the web application in the healthy location would be the primary resource and the one in the degraded location would be the secondary resource.\nWhen using the failover routing policy in Amazon Route 53, you can configure a health check for the primary resource to monitor its health status. If the primary resource becomes unavailable, Amazon Route 53 can automatically switch traffic to the secondary resource.\nThis routing policy is particularly useful for scenarios where you have redundant resources deployed in different locations or availability zones. It helps to ensure that your application is available and responsive to users, even in the event of a localized service disruption.\nIn contrast, the geolocation routing policy is used to route traffic based on the location of the user, while the weighted routing policy is used to distribute traffic between resources based on a specified weight value. The latency-based routing policy routes traffic to the resource with the lowest latency based on the user's location.\nTherefore, the failover routing policy is the most appropriate routing policy to use in this scenario as it ensures that traffic is routed to the healthy resource, while allowing for automatic failover in the event of an outage or degradation of service in one of the locations.\n\n"
}, {
  "id" : 213,
  "question" : "What is the concept of an AWS region?\n",
  "answers" : [ {
    "id" : "83663f78cd5a4934b5acffcfc59552bb",
    "option" : "It is a collection of Edge locations.",
    "isCorrect" : "false"
  }, {
    "id" : "abd2da3636144ab4ac5b490a31bc8b0d",
    "option" : "It is a collection of Compute capacity.",
    "isCorrect" : "false"
  }, {
    "id" : "70a153d0d81b4a5997eeffa3d7c29c4b",
    "option" : "It is a geographical area divided into Availability Zones.",
    "isCorrect" : "true"
  }, {
    "id" : "0f5e23ad8e1149c29ef8c767d1a5c9da",
    "option" : "It is the same as an Availability zone.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nA region is a geographical area divided into Availability Zones.\nEach region contains at least two Availability Zones.\nFor more information on AWS regions and availability zones, please refer to the below URL-\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html\n\nAn AWS region is a physical location in the world where AWS has multiple data centers that are used to host its cloud services. Each region is completely independent and isolated from the other regions, and is designed to provide high availability and fault tolerance by replicating data and services across multiple availability zones.\nAn AWS region consists of at least two availability zones (AZs) that are geographically separated from each other within the same region. AZs are essentially isolated data centers within a region that are designed to be independent of each other in terms of power, cooling, and network connectivity. Each AZ consists of one or more data centers that are physically separated from each other to provide fault tolerance and high availability.\nEach AWS region has its own set of resources and services, including compute, storage, database, networking, and security services, among others. Some AWS services are region-specific, which means they can only be accessed from within a specific region. However, some other services are global, which means they can be accessed from anywhere in the world.\nIn summary, an AWS region is a physical location that consists of multiple availability zones, each with its own independent infrastructure, designed to provide high availability and fault tolerance to AWS customers.\n\n"
}, {
  "id" : 214,
  "question" : "In AWS, which security aspects are the customers' responsibility? Choose 4 answers from the options given below.\n",
  "answers" : [ {
    "id" : "c26eeb3c2a064406824a1a2a183fd7ee",
    "option" : "Security Group and ACL (Access Control List) settings",
    "isCorrect" : "true"
  }, {
    "id" : "b3cc5337a0fc4b8c99d07e819fb11ff5",
    "option" : "Decommissioning storage devices",
    "isCorrect" : "false"
  }, {
    "id" : "5a5d1a7133fc4e69845e3aceb0cd71cb",
    "option" : "Patch management on the EC2 instanceâ€™s operating system",
    "isCorrect" : "true"
  }, {
    "id" : "ecccc26f32c241bdba109bd686b96640",
    "option" : "Life-cycle management of IAM credentials",
    "isCorrect" : "true"
  }, {
    "id" : "87359103d31b42eb93533bd29fb60050",
    "option" : "Controlling physical access to compute resources",
    "isCorrect" : "false"
  }, {
    "id" : "384343698f1441b8810355ceaa79e118",
    "option" : "Encryption of EBS (Elastic Block Storage) volumes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A, C, D and.\nF.Below is the snapshot of the AWS Shared Responsibility Model:\nFor more information on the Shared Responsibility Model, please refer to the below URL:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\n\n\nIn AWS, the shared responsibility model defines the security responsibilities between AWS and its customers. AWS is responsible for the security of the cloud infrastructure that includes hardware, software, and networking. While customers are responsible for securing their own data and applications that they deploy on AWS.\nThe following are the four security aspects that are the customer's responsibility in AWS:\nA. Security Group and ACL (Access Control List) settings: Customers are responsible for managing their security groups and access control lists (ACLs) to ensure that only authorized traffic is allowed to access their instances. AWS provides a network-based firewall that controls traffic to instances, but customers are responsible for configuring security groups and ACLs to enforce additional security policies as required.\nB. Decommissioning storage devices: Customers are responsible for decommissioning storage devices when they are no longer needed. AWS provides secure deletion mechanisms that ensure that data is unrecoverable when a storage device is decommissioned, but customers must ensure that they follow best practices to remove any sensitive data from the storage devices before they are decommissioned.\nC. Patch management on the EC2 instance's operating system: Customers are responsible for ensuring that the operating system of their EC2 instances is patched and up-to-date with the latest security updates. AWS provides managed services such as Amazon EC2 Systems Manager that enable customers to automate patch management, but customers are responsible for configuring and managing these services.\nD. Life-cycle management of IAM credentials: Customers are responsible for managing the life-cycle of their IAM (Identity and Access Management) credentials, including creating and deleting IAM users, and rotating access keys regularly to ensure that only authorized users have access to AWS resources. AWS provides tools such as AWS IAM that enable customers to manage IAM credentials, but customers are responsible for configuring and managing these tools.\nThe following are the security aspects that are AWS's responsibility:\nE. Controlling physical access to compute resources: AWS is responsible for controlling physical access to compute resources such as data centers and servers. AWS employs various physical security measures such as security cameras, biometric authentication, and security guards to ensure that only authorized personnel have access to AWS resources.\nF. Encryption of EBS (Elastic Block Storage) volumes: AWS is responsible for encrypting EBS volumes at rest using industry-standard encryption algorithms. AWS provides various encryption mechanisms such as AWS Key Management Service (KMS) that enable customers to encrypt their data at rest and in transit, but customers are responsible for configuring and managing these mechanisms.\n\n"
}, {
  "id" : 215,
  "question" : "Which of the following can be used to manage identities in AWS?\n",
  "answers" : [ {
    "id" : "7c88ae27f76145ceb59eb65d38a74152",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "aa7aa09eb7d7435e9f2f8b00568fd10d",
    "option" : "AWS IAM",
    "isCorrect" : "true"
  }, {
    "id" : "53f3f84854bc4dac9141701ad14841cc",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "ada5b04a45704aa5beba4ddc7e7db2d6",
    "option" : "AWS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAWS Identity and Access Management (IAM) is a web service that helps you control access to AWS resources securely.\nYou use IAM to control who is authenticated (signed in) and authorized (has permissions) to use resources.\nFor more information on AWS IAM, please refer to the below URL:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html\n\nThe correct answer is B. AWS IAM (Identity and Access Management).\nAWS IAM is a web service that helps you securely control access to AWS resources. You can use IAM to manage users (individuals who interact with AWS), groups (collections of users), and permissions (which determine what actions users and groups can perform on specific resources).\nWith IAM, you can create and manage users and groups, assign and manage permissions, and create and manage roles, which allow temporary access to resources for tasks such as automated deployments or accessing resources across AWS accounts. IAM also integrates with many AWS services, enabling you to control access to resources across your entire AWS infrastructure.\nAWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. It enables you to audit, evaluate, and assess the configurations of your resources to ensure compliance with your organizational policies and industry standards. AWS Trusted Advisor is a service that provides recommendations to help optimize your AWS infrastructure for performance, security, cost, and fault tolerance.\nAWS, on the other hand, refers to Amazon Web Services, which is a cloud computing platform that provides a wide range of services, including computing, storage, databases, analytics, networking, machine learning, and more. AWS is not specifically designed to manage identities, but rather to provide a comprehensive platform for building and running applications and services in the cloud.\n\n"
}, {
  "id" : 216,
  "question" : "You are working on an e-commerce web application running on an EC2 instance.\nBut it is experiencing a bad performance in browsing and searching use cases when heavy load use-cases are running simultaneously.\nThe application monitors highlight a bottleneck in the web tier. You decide to re-engineer the application code by decoupling the web tier components from the order's heavy workloads.\nWhat AWS service can support the application change?\n",
  "answers" : [ {
    "id" : "e5081c7c4ae04d59826e6a6fd956b605",
    "option" : "AWS Auto Scaling",
    "isCorrect" : "false"
  }, {
    "id" : "545721a29cdd4c8ba195ca43991f7e4c",
    "option" : "AWS Elastic Load Balancing",
    "isCorrect" : "false"
  }, {
    "id" : "2bc895c4c78c4b9c9e6ba3895552eda5",
    "option" : "Amazon SQS",
    "isCorrect" : "true"
  }, {
    "id" : "dd9b49da585445f8964dfd2d2e30ce80",
    "option" : "Amazon Kinesis Streams.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT because it is not a service for decoupling use-case.\nAWS Auto Scaling adjusts the capacity to maintain performance leaving unchanged the application design.\nOption B is INCORRECT because AWS ELB is a service to distribute the workload across EC2 instance, not a service to support application refactoring or re-engineering.\nOption C is CORRECT because Amazon SQS implements messaging that is a typical integration pattern to decouple application components.\nAWS documentation mentions \"Amazon SQS offers a reliable, highly-scalable hosted queue for storing messages as they travel between applications or microservices.\nIt moves data between distributed application components and helps you decouple these components\".\nOption D is INCORRECT because it is a streaming service not suitable for the scenario.\nDiagram: none.\nReferences:\nhttps://aws.amazon.com/sqs/faqs/\n\nThe correct answer to the question is C. Amazon SQS.\nExplanation:\nIn the given scenario, the e-commerce web application running on an EC2 instance is experiencing bad performance during heavy load use-cases. The bottleneck in the web tier is identified as the root cause of the problem. To resolve the issue, the application code needs to be re-engineered by decoupling the web tier components from the order's heavy workloads.\nTo decouple the web tier components from the heavy workloads, we can use Amazon Simple Queue Service (SQS), which is a fully managed message queuing service offered by AWS. With SQS, messages can be sent, received, and processed asynchronously between distributed application components and services.\nBy integrating Amazon SQS with the e-commerce web application, we can separate the heavy workloads from the web tier, allowing the web tier to focus on processing the incoming web requests without getting bogged down by long-running and heavy workloads. The heavy workloads can be sent to the SQS queue and processed asynchronously by a separate set of worker instances.\nAWS Auto Scaling and Elastic Load Balancing are useful for scaling web application instances to handle increased traffic, but they do not provide a mechanism for decoupling heavy workloads from the web tier.\nAmazon Kinesis Streams is a managed service for real-time data processing, which is not directly related to decoupling heavy workloads from the web tier.\nTherefore, the correct answer is C. Amazon SQS.\n\n"
}, {
  "id" : 217,
  "question" : "An online streaming company is prohibited from broadcasting its content in certain countries and regions in the world.\nWhich Amazon Route 53 routing policy would be the most suitable in guaranteeing their compliance?\n",
  "answers" : [ {
    "id" : "f38121d279554871b1dda235e0e835a1",
    "option" : "Geoproximity",
    "isCorrect" : "false"
  }, {
    "id" : "9c1d7a03ced4495fa2b6d46b7866de4d",
    "option" : "Geolocation",
    "isCorrect" : "true"
  }, {
    "id" : "e97c19dfea80425b92c74f94b0b50886",
    "option" : "Multi-value answer",
    "isCorrect" : "false"
  }, {
    "id" : "a90b23f8ecfe48c1ad19a02cfe8fc6ff",
    "option" : "Failover.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAmazon Route 53 geolocation routing policy makes it possible for different types of content to be served depending on the browser's geographical location.\nIn this use case, the streaming company can serve a restriction message if Amazon Route 53 detects origin requests from prohibited countries.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geo\nOption A is INCORRECT because geo-proximity allows for DNS traffic to be routed in accordance with a bias or preset preference rule.\nThis allows the user to be served with content from resources closest to their geographical location.\nThis routing manipulates DNS traffic flow only.\nThis routing policy is not the most suitable.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-geoproximity\nOption C is INCORRECT because a multi-value answer primarily addresses the quality of service and resources queried in DNS requests.\nThis routing policy is not the most suitable.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-multivalue\nOption D is INCORRECT because failover allows for the automatic switch to healthy DNS resources if another becomes unavailable.\nIt will not allow for the preferential serving of content based on the geographical location.\nThis routing policy is not the most suitable.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-multivalue\n\nThe Amazon Route 53 service is a highly scalable and available Domain Name System (DNS) web service provided by Amazon Web Services (AWS). Route 53 allows you to route traffic to your resources on AWS or outside of AWS, based on several routing policies. In the context of the given scenario, the company needs to comply with certain restrictions and regulations, which prohibit the broadcasting of their content in specific countries or regions. Therefore, the most suitable Amazon Route 53 routing policy for this requirement would be the Geolocation routing policy.\nGeolocation routing policy allows you to route traffic based on the geographic location of your users. With this policy, you can create DNS records that specify different resources for different geographic locations. For example, you can configure Route 53 to return one IP address for users in a specific country or region, and another IP address for users in another country or region. This makes it ideal for companies that need to comply with certain regulations or restrictions related to specific countries or regions.\nThe Geoproximity routing policy is used to route traffic based on the geographic location of your resources or AWS infrastructure. This policy is useful when you have resources that are located in multiple AWS regions or availability zones, and you want to route traffic to the closest available resource to the user. This policy is not suitable for compliance requirements related to specific countries or regions.\nThe Multi-value answer routing policy allows you to configure multiple values for a single DNS name. When Route 53 receives a DNS query, it returns all the values associated with that name, and the client chooses which value to use. This policy is useful when you have multiple resources that can serve the same function, and you want to distribute traffic evenly among them.\nThe Failover routing policy is used to route traffic to a backup resource when the primary resource is unavailable. This policy is useful when you have critical resources that need to be highly available and reliable. It is not suitable for compliance requirements related to specific countries or regions.\nTherefore, based on the given scenario, the most suitable Amazon Route 53 routing policy would be the Geolocation routing policy.\n\n"
}, {
  "id" : 218,
  "question" : "Which of the following are attributes that determine the cost of On-Demand EC2 Instance? Choose 3 answers from the options given below.\n",
  "answers" : [ {
    "id" : "45e84d064649401e943ea940c1c6a71a",
    "option" : "Instance Type",
    "isCorrect" : "true"
  }, {
    "id" : "099ed9288c35477d9bd6d7d1e7b386bb",
    "option" : "AMI Type",
    "isCorrect" : "true"
  }, {
    "id" : "f52c45c5f3dd4511b910db6a7b654ce2",
    "option" : "Region",
    "isCorrect" : "true"
  }, {
    "id" : "9d123312022a4ff585f0ba77531c7793",
    "option" : "Edge location.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer- A, B and C.\nIf you see the below snapshot from the EC2 on-demand pricing page, you can see the different components that make up the pricing.\nFor more information on AWS EC2 On-Demand pricing, please refer to the below URL:\nhttps://aws.amazon.com/ec2/pricing/on-demand/\n\n\nThe cost of On-Demand EC2 instances in AWS is determined by several factors, including:\nInstance Type: The instance type is the most important factor that affects the cost of an EC2 instance. Each instance type has different hardware configurations and computing capacity, and the cost of each instance type varies based on these factors. Region: The region in which an EC2 instance is launched also affects its cost. The pricing of EC2 instances varies by region because of the differences in the underlying infrastructure costs in each region. Some regions may have higher costs due to higher power or real estate costs, for example. AMI Type: An Amazon Machine Image (AMI) is a pre-configured virtual machine image that is used to create an EC2 instance. The cost of an EC2 instance can be affected by the AMI type selected because some AMIs may include additional software or services that can increase the cost of the instance.\nIn contrast, \"Edge location\" is not an attribute that determines the cost of an On-Demand EC2 instance. Edge locations are locations where Amazon CloudFront content delivery network (CDN) endpoints are located, and are not related to the pricing of EC2 instances.\nIn summary, the three attributes that determine the cost of On-Demand EC2 Instance are Instance Type, Region, and AMI Type.\n\n"
}, {
  "id" : 219,
  "question" : "A company wants to utilize AWS storage.\nFor them, low storage cost is paramount.\nThe data is rarely retrieved and a data retrieval time of 13-14 hours is acceptable for them.\nWhat is the best storage option to use?\n",
  "answers" : [ {
    "id" : "5c7250a9470847b5974fadfc45d92011",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "1f1f2aebf7f24a6390ed4e10627c58ca",
    "option" : "S3 Glacier Deep Archive",
    "isCorrect" : "true"
  }, {
    "id" : "965f7fce10954746bc356abe589c5de2",
    "option" : "Amazon EBS volumes",
    "isCorrect" : "false"
  }, {
    "id" : "e28e4682220c4fa9b995effacbc1dde7",
    "option" : "AWS CloudFront.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nS3 Glacier Deep Archive offers the lowest cost storage in the cloud, at prices lower than storing and maintaining data in on-premises magnetic tape libraries or archiving data offsite.\nIt expands our data archiving offerings, enabling you to select the optimal storage class based on storage and retrieval costs, and retrieval times.\nOption B is correct because S3 Glacier Deep Archive offers low-cost storage and retrieval time doesn't matter for the company.\nIf the question asks for fast retrieval time then S3 Glacier would be correct.\nOption A is incorrect because S3 Glacier is not cheaper than S3 Glacier Deep Archive.\nOptions C and D are incorrect because they are not suitable for data archive and faster retrieval.\nAlso, the CloudFront is not for storage.\nWith S3 Glacier, customers can store their data cost-effectively for months, years, or even decades.\nS3 Glacier enables customers to offload the administrative burdens of operating and scaling storage to AWS, so they don't have to worry about capacity planning, hardware provisioning, data replication, hardware failure detection, and recovery, or time-consuming hardware migrations.\nAmazon S3 Glacier for archiving data that might infrequently need to be restored within a few hours.\nS3 Glacier Deep Archive for archiving long-term backup cycle data that might infrequently need to be restored within 12 hours.\nStorage class Expedited Standard Bulk.\nAmazon S3 Glacier.\n1-5 minutes.\n3-5 hours.\n5-12 hours.\nS3 Glacier Deep Archive.\nNot available.\nWithin 12 hours.\nWithin 48 hours.\nReference:\nhttps://docs.aws.amazon.com/amazonglacier/latest/dev/introduction.html\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/amazon-s3-glacier.html\nhttps://aws.amazon.com/s3/storage-classes/\n\nFor a company that prioritizes low storage cost and has data that is rarely retrieved, the best storage option would be Amazon S3 Glacier Deep Archive (option B).\nAmazon S3 Glacier Deep Archive is designed for long-term data archiving that can be stored for several years. It is the lowest-cost storage option available in AWS, charging $0.00099 per GB per month, with no retrieval fees. The trade-off for this low cost is a long retrieval time, which in this case is acceptable for the company, with a retrieval time of 12-48 hours.\nAmazon S3 Glacier (option A) is also a suitable option for long-term archiving with lower retrieval times of 3-5 hours. However, the cost of S3 Glacier is higher than S3 Glacier Deep Archive, with storage fees of $0.004 per GB per month, and retrieval fees varying based on retrieval speed and frequency.\nAmazon EBS volumes (option C) are block-level storage volumes designed for high-performance and low-latency workloads. They are not suitable for long-term archiving, and the cost of storage is higher than S3 Glacier and S3 Glacier Deep Archive.\nAWS CloudFront (option D) is a content delivery network service used for delivering static and dynamic web content, including videos, images, and applications. It is not a storage option, and it is not designed for long-term data archiving.\nTherefore, option B, Amazon S3 Glacier Deep Archive, is the best storage option for a company that prioritizes low storage cost, with data that is rarely retrieved, and a long retrieval time of 13-14 hours is acceptable.\n\n"
}, {
  "id" : 220,
  "question" : "What are the characteristics of Amazon S3? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "46aba0c8511146e6892222d0b1eb0b52",
    "option" : "S3 allows you to store objects of virtually unlimited size.",
    "isCorrect" : "false"
  }, {
    "id" : "1eec385cf6da4fe387843d8d774bb963",
    "option" : "S3 allows you to store virtually unlimited amounts of data.",
    "isCorrect" : "true"
  }, {
    "id" : "add6bf4d329a46858f458fbc70355344",
    "option" : "S3 should be used to host a relational database.",
    "isCorrect" : "false"
  }, {
    "id" : "12f3d60c164a4ee28097664809ac885e",
    "option" : "Objects are directly accessible via a URL.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - B and D.\nEach object does have a limitation in S3, but you can store virtually unlimited amounts of data.\nAlso, each object gets a directly accessible URL.\nFor more information on AWS S3, please refer to the below URL:\nhttps://aws.amazon.com/s3/faqs/\n\nAmazon S3 (Simple Storage Service) is an object-based storage service provided by Amazon Web Services (AWS) that enables users to store and retrieve data from anywhere on the internet. S3 is designed to provide high durability, availability, and scalability for storing and retrieving any amount of data, at any time, from anywhere on the web.\nThe characteristics of Amazon S3 are:\nA. S3 allows you to store objects of virtually unlimited size: Amazon S3 enables users to store objects up to 5 terabytes in size. This means that S3 allows users to store large files such as videos, images, and backups, which can be accessed quickly and easily from any location.\nB. S3 allows you to store virtually unlimited amounts of data: S3 is designed to scale as the amount of data stored in it grows. Users can store as much data as they need, without worrying about running out of storage space. S3 is also designed to be highly available and durable, so users can access their data whenever they need it.\nC. S3 should not be used to host a relational database: Amazon S3 is designed to store objects, not to host databases. While it is possible to store database files in S3, it is not a recommended practice. Instead, AWS provides other database services like Amazon RDS, Amazon DynamoDB, and Amazon Aurora that are specifically designed for hosting relational databases.\nD. Objects are directly accessible via a URL: Objects stored in Amazon S3 are directly accessible via a URL. This means that users can easily share files stored in S3 with others by providing them with the URL. S3 also supports a variety of access controls, so users can control who has access to their data.\n\n"
}, {
  "id" : 221,
  "question" : "Which AWS service provides a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability?\n",
  "answers" : [ {
    "id" : "40eaeb98c1ba45fe9f856a5cb5e8c84f",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "d0b2219c97894e35a83b62f0b78c05e0",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "f12f29362a9f4419a8306ea01f884c9b",
    "option" : "Oracle RDS",
    "isCorrect" : "false"
  }, {
    "id" : "ee0a964b36f34e7c9962a88226ed4c19",
    "option" : "Elastic Map Reduce.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: - B.\nDynamoDB is a fully managed NoSQL offering provided by AWS.\nIt is now available in most regions for users to consume.\nFor more information on AWS DynamoDB, please refer to the below URL:\nhttp://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html\n\nThe correct answer is B. DynamoDB.\nDynamoDB is a fully managed NoSQL database service provided by AWS. It is designed to provide fast and predictable performance, with seamless scalability. DynamoDB is a highly available, durable, and scalable database service that supports document and key-value store models. It can store and retrieve any amount of data and can handle any level of request traffic. It is ideal for applications that require low-latency data access and high scalability, such as gaming, advertising, and IoT.\nAWS RDS (A) is a managed relational database service that supports several database engines, including MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. It is designed to simplify database administration tasks such as deployment, backups, and software patching. RDS is best suited for applications that require relational databases.\nOracle RDS (C) is not an AWS service. However, Oracle Database can be deployed on AWS using the Amazon EC2 service.\nElastic MapReduce (D) is a fully managed big data processing service that provides a distributed processing framework for processing large amounts of data using Hadoop, Spark, or other open-source tools. It is not a NoSQL database service and is not designed for low-latency data access.\nIn summary, DynamoDB is the correct answer as it is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability, making it ideal for applications that require low-latency data access and high scalability.\n\n"
}, {
  "id" : 222,
  "question" : "In the AWS Billing and Management service, which tool will allow the user to graphically visualize billing and usage over time, particularly monthly running costs?\n",
  "answers" : [ {
    "id" : "1e0f748baaa2405db7f944972a1cba83",
    "option" : "AWS Bills",
    "isCorrect" : "false"
  }, {
    "id" : "4e249d67d7824b72af47cb90a1ba90b6",
    "option" : "AWS Cost Explorer",
    "isCorrect" : "true"
  }, {
    "id" : "53054518f86d4dd6987dfa777be59e8e",
    "option" : "AWS Reports",
    "isCorrect" : "false"
  }, {
    "id" : "3dedeb12777340e6be7a8246bef446e7",
    "option" : "AWS Budgets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - B.\nAWS Cost Explorer allows the user to generate a graphical representation of their billing and usage over time.\nhttps://aws.amazon.com/aws-cost-management/aws-cost-explorer/\nOption A is INCORRECT because AWS Bills will list the historical costs that would have been incurred over the past month with granular options.\nThe tool will not give the graphical visualization as specified in the question.\nOption C is INCORRECT because AWS Reports will give a composite overview of costs and usage.\nThe tool gives a granular perspective of usage and billing but without a graphical output.\nOption D is INCORRECT because AWS Budgets will give the user the status of user-set budgets and provide forecasts of estimated costs.\nThe tool will not give a graphical representation of the data.\n\nThe correct answer is B. AWS Cost Explorer.\nAWS Cost Explorer is a tool in the AWS Billing and Management service that provides graphical visualization of billing and usage over time. It enables users to analyze and monitor their AWS usage and cost trends, which helps them to make informed decisions about cost optimization and budget planning.\nAWS Cost Explorer has several features that allow users to view their costs and usage data in various ways. The tool offers customizable reports and graphs, which can be filtered and sorted by account, service, region, tag, and time period. Users can also create cost and usage budgets and receive alerts when they exceed their budget thresholds.\nAWS Cost Explorer has a user-friendly interface that displays data in a variety of formats, including line charts, stacked area charts, and tables. Users can drill down into the data to get more detailed information about their costs and usage, such as the cost breakdown by resource or the usage trends by instance type.\nIn summary, AWS Cost Explorer is a powerful tool that allows users to monitor, analyze, and optimize their AWS costs and usage. With its intuitive interface and rich features, AWS Cost Explorer provides users with the insights they need to make informed decisions about their AWS spending.\n\n"
}, {
  "id" : 223,
  "question" : "A company is deploying a three-tier, highly available web application to AWS.\nWhich service provides durable storage for static content while utilizing lower Overall CPU resources for the web tier?\n",
  "answers" : [ {
    "id" : "bace4295865646aa94dc7a1c29f42d18",
    "option" : "Amazon EBS volume",
    "isCorrect" : "false"
  }, {
    "id" : "c3e63442468e436099caeb031369a295",
    "option" : "Amazon S3",
    "isCorrect" : "true"
  }, {
    "id" : "b5a7d547364a455a9037e4841ff46987",
    "option" : "Amazon EC2 instance store",
    "isCorrect" : "false"
  }, {
    "id" : "03e78b9f50e74b68ad281dd6d13f543d",
    "option" : "Amazon RDS instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAmazon S3 is the default storage service that should be considered for companies.\nIt provides durable storage for all static content.\nFor more information on AWS S3, please refer to the below URL:\nhttps://aws.amazon.com/s3/faqs/\n\nFor the given scenario, Amazon S3 (Simple Storage Service) is the most appropriate option to store static content for the three-tier, highly available web application.\nAmazon S3 provides a durable and scalable object storage service that can store and retrieve any amount of data from anywhere on the web. S3 is designed to deliver 99.999999999% durability, and data is automatically replicated to multiple AWS Availability Zones (AZs) within a region to ensure high availability.\nSince S3 is a fully managed service, it doesn't require any infrastructure management, and it provides better performance at a lower cost compared to other storage options like EBS volumes or EC2 instance store.\nEBS volumes are block-level storage devices that are attached to EC2 instances, providing high-performance storage with low latency. However, EBS volumes are not suitable for storing static content as they are designed for block storage and require an EC2 instance to be mounted and accessed.\nEC2 instance store provides temporary block-level storage that is directly attached to the EC2 instance. It is a low-latency and high-throughput storage option, but it is not suitable for storing static content as it provides temporary storage, which means that data stored in instance store volumes is lost if the instance is stopped or terminated.\nAmazon RDS is a managed relational database service that provides scalable and reliable database instances. While RDS provides durable storage for database instances, it is not an appropriate option for storing static content.\nIn summary, for durable storage of static content while utilizing lower Overall CPU resources for the web tier, the best option is to use Amazon S3.\n\n"
}, {
  "id" : 224,
  "question" : "In the AWS Billing and Management service, which tool can provide usage-based forecasts of estimated billing costs and usage for the coming months?\n",
  "answers" : [ {
    "id" : "8f582732379341d3907c3ce15777bd08",
    "option" : "AWS Cost Explorer",
    "isCorrect" : "true"
  }, {
    "id" : "ef8a5f6816da4023913c2dd8987ea4d4",
    "option" : "AWS Bills",
    "isCorrect" : "false"
  }, {
    "id" : "fcf84b4698d144f29ab899af5a348546",
    "option" : "AWS Reports",
    "isCorrect" : "false"
  }, {
    "id" : "8038b6d1709e47f9b551ccdb00a952b0",
    "option" : "Cost &amp; Usage Reports.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Cost Explorer can create user-defined custom forecasts for future usage patterns.\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ce-forecast.html\nhttps://aws.amazon.com/about-aws/whats-new/2019/07/usage-based-forecasting-in-aws-cost-explorer/\nOption B is INCORRECT because AWS Bills will list the historical costs that would have been incurred over the past month with granular options.\nThe tool will not give the usage-based forecasts as specified in the question.\nOption C is INCORRECT because AWS Reports will give a composite overview of costs and usage.\nThe tool gives a granular perspective of usage and billing but without usage-based forecasts.\nOption D is INCORRECT because AWS Reports and Cost &amp; Usage Reports are the same tool.\nOption.\nC.\nexplanation outlines why it is inaccurate as a response to the question.\n\nThe tool in the AWS Billing and Management service that provides usage-based forecasts of estimated billing costs and usage for the coming months is AWS Cost Explorer (Option A).\nAWS Cost Explorer is a powerful tool that helps users analyze their AWS spending by providing a visual representation of their usage and costs. With Cost Explorer, users can:\nVisualize their costs and usage data: Users can see their AWS usage and costs in various charts and graphs, making it easier to understand and analyze their spending. Create custom reports: Users can create custom reports that provide insights into their AWS usage and costs. They can filter and group data by service, region, usage type, and more. Forecast their costs: Users can use Cost Explorer to forecast their AWS costs for the upcoming months. This feature helps users plan their budget and optimize their spending.\nAWS Bills (Option B) is a tool that provides a detailed breakdown of a user's AWS charges. It shows the costs for each AWS service and the associated usage. However, it doesn't provide usage-based forecasts of estimated billing costs and usage for the coming months.\nAWS Reports (Option C) is a collection of reports that provide insights into a user's AWS usage and costs. The reports include the AWS Cost and Usage Report, the AWS Budgets Report, and the AWS Reserved Instance Utilization Report. While these reports provide valuable information, they don't provide usage-based forecasts of estimated billing costs and usage for the coming months.\nCost & Usage Reports (Option D) is a feature that provides detailed information about a user's AWS usage and costs. It includes data on hourly usage and costs for each service and resource, making it easier to analyze and optimize spending. However, it doesn't provide usage-based forecasts of estimated billing costs and usage for the coming months.\nIn summary, the correct answer is Option A: AWS Cost Explorer.\n\n"
}, {
  "id" : 225,
  "question" : "Which of the below AWS services allows you to increase the number of resources based on the demand of the application or users?\n",
  "answers" : [ {
    "id" : "6bc3f34c265e497ab9181d3e81dc0e2d",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "e4f4ed3263b54044a2b294b47ebdd074",
    "option" : "AWS Autoscaling",
    "isCorrect" : "true"
  }, {
    "id" : "1bf4bd9f22dd4d66b3b6dfb781d55693",
    "option" : "AWS ELB",
    "isCorrect" : "false"
  }, {
    "id" : "3274ec58cb6f4c90a7a1a73e42774ee0",
    "option" : "AWS Inspector.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAWS Auto Scaling enables you to configure automatic scaling for the scalable AWS resources for your application in a matter of minutes.\nAWS Auto Scaling uses the Auto Scaling and Application Auto Scaling services to configure scaling policies for your scalable AWS resources.\nFor more information on AWS Autoscaling, please refer to the below URL:\nhttps://docs.aws.amazon.com/autoscaling/plans/userguide/what-is-aws-auto-scaling.html\n\nThe AWS service that allows you to increase the number of resources based on the demand of the application or users is AWS Autoscaling, which is option B.\nAWS Autoscaling allows you to automatically adjust the capacity of your EC2 instances based on the demand of your application or users. Autoscaling can help you maintain the availability and performance of your applications by automatically scaling your resources up or down based on the changing needs of your applications.\nWith AWS Autoscaling, you can create scaling policies that define when and how your instances should be scaled. For example, you can create a policy that scales your instances up when CPU utilization reaches a certain threshold or when there is an increase in traffic to your application. Similarly, you can create a policy that scales your instances down when the demand for your application decreases.\nIn addition to scaling EC2 instances, AWS Autoscaling can also be used to scale other AWS resources, such as Elastic Load Balancers and Amazon RDS instances.\nAWS EC2 (option A) is a web service that provides resizable compute capacity in the cloud. While EC2 instances can be manually scaled up or down, Autoscaling provides a way to do this automatically based on demand.\nAWS ELB (option C) is a service that automatically distributes incoming application traffic across multiple targets, such as EC2 instances. While ELB can help improve the availability and performance of your applications, it does not provide the automatic scaling capabilities of Autoscaling.\nAWS Inspector (option D) is a security assessment service that helps you identify potential security issues in your applications and resources. It does not provide any automatic scaling capabilities.\n\n"
}, {
  "id" : 226,
  "question" : "Which of the following AWS managed database service provides processing power that is up to 5X faster than a traditional MySQL database?\n",
  "answers" : [ {
    "id" : "273ea248b606409db56c44adba62d56c",
    "option" : "MariaDB",
    "isCorrect" : "false"
  }, {
    "id" : "0d60b0e531cf4a37bfe856e3c885cf30",
    "option" : "Aurora",
    "isCorrect" : "true"
  }, {
    "id" : "9d52bbf4c166457ca9e55ed21162aecf",
    "option" : "PostgreSQL",
    "isCorrect" : "false"
  }, {
    "id" : "bc89b10ef30f4fad82a89b15b4969bb3",
    "option" : "DynamoDB.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAmazon Aurora (Aurora) is a fully managed, MySQL- and PostgreSQL-compatible, relational database engine.\nIt combines the speed and reliability of high-end commercial databases with the simplicity and cost-effectiveness of open-source databases.\nIt delivers up to five times the throughput of MySQL and up to three times the throughput of PostgreSQL without requiring changes to most of your existing applications.\nFor more information on AWS Aurora, please refer to the below URL:\nhttps://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Aurora.Overview.html\n\nThe correct answer is B. Aurora.\nAmazon Aurora is an AWS-managed relational database that provides the performance and availability of commercial-grade databases at a fraction of the cost. Aurora is fully compatible with MySQL and PostgreSQL, and it is designed to be highly scalable and fault-tolerant.\nAurora uses a distributed, cluster-based architecture that allows it to scale horizontally across multiple Availability Zones while maintaining high availability and durability. It also provides automatic failover, automated backups, and point-in-time recovery, making it an ideal choice for mission-critical applications.\nIn terms of performance, Aurora is up to 5 times faster than MySQL, thanks to its innovative approach to database storage. Aurora stores data in a distributed, log-structured storage system that is optimized for solid-state drives (SSDs). This allows Aurora to achieve high levels of I/O performance and reduces the need for read and write operations on disk.\nIn addition to performance, Aurora also offers a range of other features that make it a popular choice among AWS customers, including:\nMulti-AZ deployment for high availability Automatic scaling to handle increased workloads Point-in-time recovery to restore the database to a specific point in time Integration with AWS CloudFormation and other AWS services Read replicas for scaling read workloads\nIn summary, Aurora is an AWS-managed database service that provides up to 5 times faster processing power than a traditional MySQL database. It is designed to be highly scalable, fault-tolerant, and cost-effective, making it an ideal choice for a wide range of applications.\n\n"
}, {
  "id" : 227,
  "question" : "A company is planning to adopt AWS Cloud services to migrate their on-premises workloads.\nFor that, they require technical support from AWS to design the cloud infrastructure in a better way.\nWhich of the following features related to AWS Support Plans is correct? (Select THREE)\n",
  "answers" : [ {
    "id" : "131c8d64cd074e3fa1d59190d212c863",
    "option" : "Developer Support plan does not support 3rd Party application services like Ruby on Rails.",
    "isCorrect" : "true"
  }, {
    "id" : "677100a939124576b72d7a7bfb6787c4",
    "option" : "AWS provides access to Infrastructure Event Management for all the Support Plans.",
    "isCorrect" : "false"
  }, {
    "id" : "67203ad01a5e46bd8bca54dfc5424152",
    "option" : "Business Support Plan has Technical Account Manager (TAM) support to proactively monitor the environment.",
    "isCorrect" : "false"
  }, {
    "id" : "57802db3b4f346f0a210bee7ba8162e4",
    "option" : "Enterprise Support plan should be used for Business Critical use cases with a response time of less than 15 minutes.",
    "isCorrect" : "true"
  }, {
    "id" : "514032d2dcb84a2bb790cb4cf482e2ad",
    "option" : "Trusted Advisor checks are available for all plans.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers: A, D, and E.\nOption A is CORRECT.\nAWS Support plans for Developer support only services like EC2\nIt does not support the underlying application services or its technologies.\nOption B is incorrect because AWS provides access to Infrastructure Event Management for Business and Enterprise Support Plan.\nOption C is incorrect.\nTechnical Account Manager (TAM) is provided only for the Enterprise Support plan to monitor the environment and assist with optimization and coordinate access to programs and AWS experts.\nOption D is CORRECT.\nBusiness Critical Systems downtime support is provided only for Enterprise Support plans.\nThe response time is &lt; 15 minutes.\nOption E is CORRECT.\nTrusted Advisor checks are available for all support plans.\nBasic &amp; Developer plans have access to 7 Trusted Advisor checks while Business &amp; Enterprise plans have access to all Trusted Advisor checks.\nDiagram:\nReferences:\nhttps://youtu.be/A3522VCDy4A\nhttps://docs.aws.amazon.com/awssupport/latest/user/getting-started.html\n\n\nSure, I'd be happy to help you with this question!\nAWS offers several support plans, each designed to meet different customer needs. These support plans provide technical assistance, resources, and tools to help customers optimize their workloads in the cloud.\nLet's take a look at each statement and determine which ones are correct.\nA. Developer Support plan does not support 3rd Party application services like Ruby on Rails.\nThis statement is true. The Developer Support plan is the most basic support plan, and it provides support for AWS services only. It does not cover third-party applications, including Ruby on Rails.\nB. AWS provides access to Infrastructure Event Management for all the Support Plans.\nThis statement is false. Infrastructure Event Management is a feature of the Business and Enterprise Support plans only. It provides customers with alerts and notifications for AWS events that may affect their infrastructure.\nC. Business Support Plan has Technical Account Manager (TAM) support to proactively monitor the environment.\nThis statement is true. The Business Support plan provides access to a Technical Account Manager (TAM), who is a dedicated AWS support engineer that provides personalized support and proactively monitors the customer's environment.\nD. Enterprise Support plan should be used for Business Critical use cases with a response time of less than 15 minutes.\nThis statement is partially true. The Enterprise Support plan is designed for customers with business-critical workloads and provides a response time of less than 15 minutes for critical cases. However, it is not a requirement to use the Enterprise Support plan for business-critical workloads. Customers can choose any support plan that meets their needs.\nE. Trusted Advisor checks are available for all plans.\nThis statement is true. Trusted Advisor is a tool that provides customers with automated recommendations for optimizing their AWS workloads. It is available for all AWS Support plans.\nTherefore, the correct answers are:\nA. Developer Support plan does not support 3rd Party application services like Ruby on Rails. C. Business Support Plan has Technical Account Manager (TAM) support to proactively monitor the environment. E. Trusted Advisor checks are available for all plans.\n\n"
}, {
  "id" : 228,
  "question" : "Which of the following services helps in governance, compliance, and risk auditing in AWS?\n",
  "answers" : [ {
    "id" : "1c75fbc009134cf8a03da36b9dd6ad92",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "4c96765d6d3848e5abfb113893b44b55",
    "option" : "AWS CloudTrail",
    "isCorrect" : "true"
  }, {
    "id" : "79bfa38027394b098768d8a0a0f70666",
    "option" : "AWS CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "f6e78bd943c943c0a8f9e45bd8148cf8",
    "option" : "AWS SNS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account.\nWith CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure.\nCloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services.\nThis event history simplifies security analysis, resource change tracking, and troubleshooting.\nFor more information on AWS CloudTrail, please refer to the below URL:\nhttps://aws.amazon.com/cloudtrail/\n\nThe correct answer is B. AWS CloudTrail.\nAWS CloudTrail is a service that provides governance, compliance, and risk auditing for AWS accounts. It logs, monitors, and retains account activity related to actions taken within AWS services. This includes API calls made by users, applications, and services.\nAWS CloudTrail captures events such as who made the API call, when it was made, and what resources were affected by the action. It also tracks changes to AWS resources, such as creating, modifying, or deleting EC2 instances, S3 buckets, or security groups.\nBy enabling AWS CloudTrail, customers can ensure compliance with regulatory requirements and company policies by auditing changes made to their AWS environment. It helps to identify unauthorized access or activities, and provides visibility into resource usage and operational activity.\nAWS CloudFormation is a service for provisioning and managing AWS infrastructure resources, such as EC2 instances and S3 buckets. It is not specifically designed for governance, compliance, and risk auditing.\nAWS CloudWatch is a service for monitoring AWS resources, such as EC2 instances, RDS databases, and Lambda functions. While it provides visibility into resource usage and performance, it is not focused on governance, compliance, and risk auditing.\nAWS SNS (Simple Notification Service) is a service for sending notifications or messages to multiple subscribers or endpoints. It is not related to governance, compliance, and risk auditing in AWS.\nTherefore, the correct answer is B. AWS CloudTrail.\n\n"
}, {
  "id" : 229,
  "question" : "When using On-Demand instances in AWS, which of the following is a false statement when it comes to the costing for the Instance?\n",
  "answers" : [ {
    "id" : "a05d66f9ff8044eb862fefa5a431df7d",
    "option" : "You pay no upfront costs for the instance.",
    "isCorrect" : "false"
  }, {
    "id" : "0a28e79e560e4747b3b147644f6f4c50",
    "option" : "You are charged per second based on the hourly rate.",
    "isCorrect" : "false"
  }, {
    "id" : "002bbcd0e5d94055a3f38dcff8c6b29b",
    "option" : "You have to pay the termination fees if you terminate the instance.",
    "isCorrect" : "true"
  }, {
    "id" : "2a2ea76688a24a189339d110bcbd1303",
    "option" : "You pay for as much as you use.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nYou don't need to pay any termination fees when it comes to EC2 Instances.\nFor more information on AWS Ec2 On-demand pricing, please refer to the below URL:\nhttps://aws.amazon.com/ec2/pricing/on-demand/\n\nOn-Demand instances are virtual machines that can be launched on-demand and are charged by the hour, with no upfront costs or long-term commitments. When using On-Demand instances in AWS, the following statements are true:\nA. You pay no upfront costs for the instance. This statement is true. There are no upfront costs associated with On-Demand instances. You only pay for the hours that the instance is running.\nB. You are charged per second based on the hourly rate. This statement is true. On-Demand instances are charged per second based on the hourly rate. This means that you only pay for the actual seconds that the instance is running, and not for any unused time.\nC. You have to pay the termination fees if you terminate the instance. This statement is false. There are no termination fees associated with On-Demand instances. You can terminate the instance at any time without incurring any additional costs.\nD. You pay for as much as you use. This statement is true. With On-Demand instances, you only pay for the hours that the instance is running. This means that you are only charged for the actual usage of the instance and not for any unused time.\nIn summary, when using On-Demand instances in AWS, you do not have to pay any upfront costs, you are charged per second based on the hourly rate, there are no termination fees, and you only pay for the actual usage of the instance.\n\n"
}, {
  "id" : 230,
  "question" : "A website that I have designed On Premise has a consistent increase in traffic during certain occasions like Christmas &amp; Thanksgiving.\nThe first year saw a 20% average increase while the second year saw a 40% average increase.\nIt is observed that every year I need to spend on new Servers which lie idle after usage.\nIf I use AWS Cloud resources instead, the advantage that I will get is.\n(Choose the best answer)\n",
  "answers" : [ {
    "id" : "d9993eb863854032add686515635865a",
    "option" : "I can go Global in minutes",
    "isCorrect" : "false"
  }, {
    "id" : "9116041842ad4152a6833defb31f820c",
    "option" : "I can increase speed &amp; agility of my website",
    "isCorrect" : "false"
  }, {
    "id" : "88288660ccee481f8758ea42c9b2a766",
    "option" : "I do not need to maintain the DNS name",
    "isCorrect" : "false"
  }, {
    "id" : "18bf0fcb71d04072b4440fc15356bd37",
    "option" : "I can save on costs due to the elastic nature of the cloud.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nAs seen from the scenario, every year I need to budget for new servers due to an increase in traffic.\nThis adds to my cost, maintenance of the servers that may not be required at other times in the year.\nDue to the elastic nature of the cloud, I can directly avail cost benefits by using a pay-as-you-go model and dynamically scaling - OUT &amp; scaling - IN my resources when needed.\nOption A is incorrect.\nAlthough going global is an advantage of using cloud due to availability of multiple regions, it does not account for our scenario.\nOption B is incorrect.\nSpeed and agility of a website depends on various factors like application architecture / design, application communication etc..Although a cloud can provide a lot of hosting methodologies like Static web hosting using S3, Using a CDN like CloudFront for performance improvement, it does not account for in our scenario.\nOption C is incorrect.\nThis is not the best advantage that can be achieved in this scenario.\nThis question does not mention anything about DNS names.\nOption D is CORRECT.\nUsing the cloud will have a direct impact on my costs of purchasing new servers each year to support the dynamic increase in load.\nReference:\nhttps://www.testpreptraining.com/tutorial/aws-cloud-practitioner/define-the-aws-cloud-and-its-value-proposition/\n\nThe best answer for this question is D. \"I can save on costs due to the elastic nature of the cloud.\"\nBy using AWS cloud resources, you can take advantage of the elasticity of the cloud. This means that you can easily increase or decrease the amount of computing resources you use to match the demands of your website traffic. In the case of a website that experiences seasonal traffic spikes, you can quickly and easily scale up your infrastructure during these peak periods to handle the increased demand.\nWith on-premise infrastructure, you would need to purchase additional servers to handle the increase in traffic. These servers would be idle for the majority of the year, and you would need to maintain them even when they are not being used. This can be costly in terms of hardware, maintenance, and energy consumption. In contrast, with AWS, you can quickly provision the resources you need during peak periods, and then scale back down when traffic returns to normal levels, only paying for what you use.\nOption A, \"I can go global in minutes,\" is not the best answer because it doesn't address the specific problem of seasonal traffic spikes. While AWS does make it easy to deploy resources in multiple regions, this may not be relevant to the scenario described in the question.\nOption B, \"I can increase speed and agility of my website,\" may be a valid advantage of using AWS, but it doesn't directly address the cost savings associated with the elastic nature of the cloud.\nOption C, \"I do not need to maintain the DNS name,\" is not relevant to the scenario described in the question. DNS (Domain Name System) is a system used to translate domain names into IP addresses, which is not directly related to the scalability of infrastructure during seasonal traffic spikes.\n\n"
}, {
  "id" : 231,
  "question" : "Which of the following statements are FALSE when it comes to AWS DataSync? (Choose TWO.)\n",
  "answers" : [ {
    "id" : "d99ea5748dca4b6495da7d30c195c563",
    "option" : "A fully managed data transfer service with a built-in retry mechanism",
    "isCorrect" : "false"
  }, {
    "id" : "21a12ea7706d48cca4e192a449f1c290",
    "option" : "Can work only over AWS Direct: Connect",
    "isCorrect" : "true"
  }, {
    "id" : "59f7f0b77d3a4a3e866765f9809c3cab",
    "option" : "It is an agentless data transfer service",
    "isCorrect" : "true"
  }, {
    "id" : "a33d9254bdff4099805feb10218f6062",
    "option" : "Can copy data between NFS servers, SMB file shares, Amazon S3 buckets, and Amazon EFS file systems.",
    "isCorrect" : "false"
  }, {
    "id" : "efeb75c90c2a4673924e855f7fb9d564",
    "option" : "It is integrated with AWS CloudWatch.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B and C.\nB and C are FALSE.\nAll the others are TRUEaccording to the AWS Documentation.\nSee reference below.\nDiagram: none.\nReferences:\nhttps://aws.amazon.com/datasync/\nhttps://aws.amazon.com/datasync/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc\n\nThe correct answers are B and C.\nExplanation:\nAWS DataSync is a fully managed service that simplifies and accelerates the online transfer of data between on-premises storage systems and AWS storage services. It can be used to transfer data between on-premises storage and Amazon S3, Amazon EFS, and Amazon FSx for Windows File Server. Here are the explanations of each statement:\nA. A fully managed data transfer service with a built-in retry mechanism This statement is true. AWS DataSync is a fully managed service that automates and accelerates online data transfer between different storage systems. It also has a built-in retry mechanism that ensures the successful delivery of data, even in the event of network disruptions.\nB. Can work only over AWS Direct: Connect This statement is false. AWS DataSync can work over the public internet, VPNs, and AWS Direct Connect. AWS Direct Connect is not a requirement for AWS DataSync to function.\nC. It is an agentless data transfer service This statement is false. AWS DataSync requires a DataSync agent to be installed on the source or destination storage systems. The agent is responsible for coordinating and executing the data transfer tasks.\nD. Can copy data between NFS servers, SMB file shares, Amazon S3 buckets, and Amazon EFS file systems. This statement is true. AWS DataSync supports multiple sources and destinations, including NFS servers, SMB file shares, Amazon S3 buckets, and Amazon EFS file systems.\nE. It is integrated with AWS CloudWatch. This statement is true. AWS DataSync is integrated with AWS CloudWatch, which provides a unified monitoring and logging solution for AWS resources and applications. CloudWatch can be used to monitor DataSync metrics and log files.\n\n"
}, {
  "id" : 232,
  "question" : "In the AWS Well-Architected Framework, which of the following is NOT a Security design principle to design solutions in AWS?\n",
  "answers" : [ {
    "id" : "6b8d9a108e614a3ca4c3532f99ed1321",
    "option" : "Apply Security only at the edge of the network.",
    "isCorrect" : "true"
  }, {
    "id" : "8f9139f7c755408da61f3e7860b612e0",
    "option" : "Protect Data at rest &amp; in transit.",
    "isCorrect" : "false"
  }, {
    "id" : "e18eb21a3bee4818bc28a708e941a7c9",
    "option" : "Implement a strong Identity foundation.",
    "isCorrect" : "false"
  }, {
    "id" : "5b7b4575fb0b4fd99bbc0933638d55cb",
    "option" : "Enable Traceability.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nSecurity needs to be applied at all network layers, like edge of network, VPC, all instances &amp; application with the VPC.\nApplying Security controls at the edge of the network is not an efficient security control &amp; against security design principles.\nAs per AWS Well-Architected Framework, the following are the design principles for security in the cloud:\nÂ· Implement a strong identity foundation.\nÂ· Enable traceability.\nÂ· Apply security at all layers.\nÂ· Automate security best practices.\nÂ· Protect data in transit and at rest.\nÂ· Keep people away from data.\nÂ· Prepare for security events.\nOptions B, C, &amp; D are incorrect as these are part of security design principles that need to be followed while implementing security controls in the cloud.\nFor more information on Security Design Principle with AWS Well-Architected Framework, refer to the following URL:\nhttps://docs.aws.amazon.com/wellarchitected/latest/framework/sec-design.html\n\nThe AWS Well-Architected Framework is a set of best practices and guidelines for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. The framework is organized around five pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization.\nAmong these five pillars, the Security pillar focuses on protecting information and systems. It consists of several design principles that help architects and operators identify and mitigate potential security risks. The four design principles of the Security pillar in the AWS Well-Architected Framework are:\nApply a risk-based approach to security: This principle encourages organizations to identify and prioritize their most critical assets and then apply appropriate security measures based on the level of risk. Protect data in transit and at rest: This principle emphasizes the importance of encrypting data both in transit and at rest to protect it from unauthorized access and ensure its confidentiality. Enable traceability: This principle encourages organizations to implement tools and processes that enable them to track and audit user and system activities, detect and respond to security incidents, and comply with regulations and standards. Implement a strong identity foundation: This principle emphasizes the importance of using a centralized and robust identity and access management (IAM) system to control access to resources and data and enforce least privilege.\nTherefore, the answer to the question is A. Apply Security only at the edge of the network. This statement is incorrect and not a design principle of the Security pillar in the AWS Well-Architected Framework. Security should be applied at multiple layers of the network and infrastructure, including the application layer, data layer, and host layer, to provide defense in depth and mitigate various attack vectors.\n\n"
}, {
  "id" : 233,
  "question" : "What is the key difference between an availability zone and an edge location?\n",
  "answers" : [ {
    "id" : "b3f471480e9f4796ba602de2903e87de",
    "option" : "An availability zone is a grouping of AWS resources in a specific region. An edge location is a specific resource within the AWS region.",
    "isCorrect" : "false"
  }, {
    "id" : "cefe1b5373f54fa6982beb8d99476dbe",
    "option" : "An availability zone is an isolated location within an AWS region, whereas an edge location will deliver cached content to the closest location to reduce latency.",
    "isCorrect" : "true"
  }, {
    "id" : "dfc63d63a802427b821f3fc215aef931",
    "option" : "Edge locations are used as control stations for AWS resources.",
    "isCorrect" : "false"
  }, {
    "id" : "8cf653665215448a8c6c3fe23696e172",
    "option" : "None of the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nIn AWS, there are regions with each region separated in a separate geographic area.\nEach region has multiple, isolated locations known as Availability Zones.\nAn availability zone is used to host resources in a specific region.\nFor more information on Regions and availability zone, please visit the URL:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html\n\nThe correct answer is B.\nExplanation: An Availability Zone (AZ) is an isolated location within an AWS region, designed to be independent from other Availability Zones in terms of power, cooling, and network connectivity. Each availability zone is made up of one or more data centers, and they are located in separate and distinct geographic locations within a region. The primary purpose of Availability Zones is to provide high availability and fault tolerance by ensuring that applications and data are replicated across multiple AZs in the same region.\nAn Edge Location, on the other hand, is a site that is used by Amazon CloudFront to deliver cached content to end-users. Edge locations are located in different geographic locations around the world and are used to reduce latency by delivering content to end-users from the closest location. Each Edge Location is equipped with a caching server that is used to temporarily store copies of frequently accessed content.\nIn summary, Availability Zones are isolated locations designed to provide high availability and fault tolerance, while Edge Locations are used to deliver cached content to end-users from the closest location to reduce latency.\n\n"
}, {
  "id" : 234,
  "question" : "Which of the following security features is associated with a Subnet in a VPC to protect against incoming traffic requests?\n",
  "answers" : [ {
    "id" : "3bb245cc8e2847dab1d1d0879a1d84c0",
    "option" : "AWS Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "c64ff05be1cc410bad08f334e1897b82",
    "option" : "Subnet Groups",
    "isCorrect" : "false"
  }, {
    "id" : "258b92114500438a9e2d46622ac91aef",
    "option" : "Security Groups",
    "isCorrect" : "false"
  }, {
    "id" : "7e486c23c0684c7f8f0c1ddda89a4829",
    "option" : "Network ACL.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer - D.\nThe AWS Documentation mentions the following:\nA network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\nYou might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.\nFor more information on Network ACL, please visit the URL:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\n\nThe security feature associated with a subnet in a VPC (Virtual Private Cloud) that protects against incoming traffic requests is the \"Network Access Control Lists\" (ACLs), which is the answer D.\nNetwork ACLs are stateless, which means that they apply rules to inbound and outbound traffic separately, unlike Security Groups (answer C) that are stateful and allow traffic to return automatically. Network ACLs are associated with subnets and act as a firewall to control traffic flow at the subnet level.\nBy default, a new subnet in a VPC comes with a Network ACL that allows all traffic in and out of the subnet. However, you can create custom Network ACLs and associate them with your subnets to add an additional layer of security by allowing or denying traffic based on the rules that you set.\nAWS Inspector (answer A) is a security service that helps improve the security and compliance of your applications that are deployed on Amazon EC2 instances. It provides a detailed assessment of your application's security posture by analyzing the behavior of your AWS resources and identifying potential security issues.\nSubnet Groups (answer B) is not a security feature, but a logical grouping of subnets within a VPC that share the same route table. They are useful for simplifying the management of multiple subnets that need to use the same set of route rules.\nIn summary, the correct answer to the question is D. Network ACLs.\n\n"
}, {
  "id" : 235,
  "question" : "In AWS billing, what option can be used to ensure costs can be reduced if you have multiple accounts managed by AWS Organizations?\n",
  "answers" : [ {
    "id" : "7e4b9677c5b24a24a68e6a72621ece50",
    "option" : "Combined billing.",
    "isCorrect" : "false"
  }, {
    "id" : "f28d6fed62b045f2adf6095bb3111df0",
    "option" : "Consolidated billing - Volume Discounts.",
    "isCorrect" : "true"
  }, {
    "id" : "bc75409b5d324c09b43c8d7386a20444",
    "option" : "Costs are automatically reduced for multiple accounts by AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "f2cb7bfaa0394530af1e1cb3b254725b",
    "option" : "It is not possible to reduce costs with multiple accounts.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nYou can use the Consolidated Billing feature to consolidate payment for multiple Amazon Web Services (AWS) accounts or multiple Amazon International Services Pvt.\nLtd (AISPL) accounts within your organization by designating one of them to be the payer account.\nFor billing purposes, the consolidated billing feature of AWS Organizations treats all the accounts in the organization as one account.\nThis means that all accounts in the organization can receive the hourly cost-benefit of Reserved Instances that are purchased by any other account.\nOption A is incorrect because combined billing is itself the functionality of Consolidated billing.\nOptions C and D are just the distractors.\nReference:\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/useconsolidatedbilling-discounts.html\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ri-behavior.html\n\nThe correct answer is A. Combined billing.\nAWS Organizations is a service that enables you to consolidate multiple AWS accounts into an organization that you create and centrally manage. With multiple accounts, it can be challenging to manage and monitor costs. However, with combined billing, you can simplify the billing process by consolidating the charges from all of your AWS accounts into a single, aggregated bill. This allows you to see the total cost of your AWS usage across all accounts, making it easier to monitor and control your costs.\nBy using combined billing, you can also take advantage of AWS's volume discounts, which are applied automatically based on your usage across all accounts. AWS volume discounts are applied when you reach certain usage thresholds for specific services, and the more you use, the greater the discount you receive.\nCost optimization is a critical aspect of AWS usage, and combined billing is an essential tool in achieving this. It allows you to see your total costs across all accounts and take advantage of AWS's volume discounts, resulting in significant cost savings over time. It is worth noting that combined billing does not automatically reduce costs, but it is a powerful tool for managing and optimizing costs across multiple accounts.\n\n"
}, {
  "id" : 236,
  "question" : "You have a Web application hosted in an EC2 Instance that needs to send notifications based on events.\nWhich of the below services can assist in sending notifications?\n",
  "answers" : [ {
    "id" : "679a2158048f42fbb8c1354d88349007",
    "option" : "AWS SES",
    "isCorrect" : "false"
  }, {
    "id" : "b7feee63c3ad4307aae8b7adc4dd120d",
    "option" : "AWS SNS",
    "isCorrect" : "true"
  }, {
    "id" : "080ffbaa2c1f4788beddc99bde0b3420",
    "option" : "AWS SQS",
    "isCorrect" : "false"
  }, {
    "id" : "56fea0b8c4e544ec8b1a8e0a7c2f047d",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following:\nAmazon Simple Notification Service (Amazon SNS) is a web service that enables applications, end-users, and devices to send and receive notifications from the cloud instantly.\nFor more information on AWS SNS, please visit the URL:\nhttps://aws.amazon.com/documentation/sns/\n\nThe correct answer is B. AWS SNS.\nAWS SNS (Simple Notification Service) is a fully managed messaging service that enables you to send notifications from the cloud. It is designed to make it easy to send messages to a large number of subscribers in real-time, such as to mobile devices, email addresses, and HTTP endpoints.\nIn this scenario, the EC2 instance hosting the web application can use AWS SNS to send notifications based on events. For example, if the web application detects a critical error or an important event, it can publish a message to an SNS topic. Subscribers who have subscribed to that topic will receive the message, which could be sent via SMS, email, or other channels.\nAWS SES (Simple Email Service) is also a messaging service, but it is designed specifically for sending emails. It can be used to send emails programmatically, such as from an EC2 instance or a Lambda function. However, it is not the best choice for sending notifications based on events since it doesn't support other notification channels like SMS or mobile push notifications.\nAWS SQS (Simple Queue Service) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. It is designed to buffer messages between distributed application components, so they can process them asynchronously. However, it doesn't provide notification capabilities like SNS does.\nFinally, AWS EC2 (Elastic Compute Cloud) is a web service that provides scalable computing capacity in the cloud. It is not a messaging or notification service and doesn't offer any direct assistance for sending notifications.\n\n"
}, {
  "id" : 237,
  "question" : "Which of the following options is TRUE for AWS Database Migration Service (AWS DMS)?\n",
  "answers" : [ {
    "id" : "4ec5b920cbed4d35a2c38e60e2e86a5c",
    "option" : "AWS DMS can migrate databases from on-premise to AWS.",
    "isCorrect" : "false"
  }, {
    "id" : "f8aff28e8df0494c947268130b63c08a",
    "option" : "AWS DMS can migrate databases from AWS to on-premise.",
    "isCorrect" : "false"
  }, {
    "id" : "31136c00b6c642eda36b6fe014908508",
    "option" : "AWS DMS can migrate databases from EC2 to Amazon RDS.",
    "isCorrect" : "false"
  }, {
    "id" : "7464a5022fe845e3ade318e037f9a281",
    "option" : "AWS DMS can have Amazon Redshift and Amazon DynamoDB as target databases.",
    "isCorrect" : "false"
  }, {
    "id" : "1248464431a04a01ac8dc5043b45c517",
    "option" : "All the above.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: E.\nAll the options are CORRECT.\nOptions are clearly described in the AWS DMS documentation at the link below.\nOption A is TRUE and is the â€œmost commonâ€ way to use AWS DMS.\nOption B is TRUE and can be used to create a copy (or migrate) a database from AWS to the on-premise data center.\nOption C is TRUE and can be used to migrate the IaaS solution (e.g., generated from a lift-and-shift wave) to a managed service like Amazon RDS.\nOption D is TRUE, according to AWS documentation.\nDiagram: none.\nReferences:\nhttps://aws.amazon.com/dms/\nhttps://aws.amazon.com/dms/faqs/\n\nThe correct answer is E, All the above.\nAWS Database Migration Service (AWS DMS) is a managed service that enables you to migrate your databases to AWS easily and securely. AWS DMS supports the migration of various types of databases, including on-premises databases, databases hosted on EC2 instances, and databases hosted on other cloud platforms.\nHere is a more detailed explanation of each option:\nA. AWS DMS can migrate databases from on-premise to AWS. AWS DMS supports the migration of databases from on-premises data centers to AWS. This is a common scenario where organizations want to migrate their databases to the cloud to reduce costs, improve scalability, and increase availability.\nB. AWS DMS can migrate databases from AWS to on-premise. AWS DMS also supports the migration of databases from AWS back to on-premises data centers. This is useful when an organization wants to move some or all of their databases back to an on-premises environment due to regulatory or compliance requirements.\nC. AWS DMS can migrate databases from EC2 to Amazon RDS. AWS DMS can migrate databases hosted on EC2 instances to Amazon RDS, which is a managed relational database service provided by AWS. This is useful when an organization wants to move their databases from a self-managed infrastructure to a fully managed service.\nD. AWS DMS can have Amazon Redshift and Amazon DynamoDB as target databases. AWS DMS can migrate data to several different target databases, including Amazon Redshift and Amazon DynamoDB. Amazon Redshift is a fast, fully managed, petabyte-scale data warehouse, while Amazon DynamoDB is a fully managed NoSQL database service.\nIn summary, AWS DMS is a powerful service that enables you to migrate your databases to AWS easily and securely. It supports a wide range of databases and can migrate data to several different target databases, including Amazon Redshift and Amazon DynamoDB.\n\n"
}, {
  "id" : 238,
  "question" : "Which of the following services can be used to automate application code deployment to Amazon EC2 instance?\n",
  "answers" : [ {
    "id" : "eded13a575a04e68815e02a6c632a924",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "87e1359d790d4b03959d100d3a9a4b4b",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "true"
  }, {
    "id" : "5d76c6b6ada14e19b2fff3413835fab7",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "479250b6a3e248d38c2530de2fc48e3d",
    "option" : "AWS CloudFormation.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nAWS Code Deploy can be used to automate deployment to Amazon EC2 instances and on-premise servers.\nOption A is incorrect as AWS Elastic Beanstalk is an application management platform that can be used for managing application deployment handling capacity provisioning, load-balancing, auto-scaling &amp; application health monitoring.\nOption C is incorrect as AWS Config can be used to audit, evaluate configurations of AWS resources.\nIt is not used to automate code deployment to Amazon EC2 instance.\nOption D is incorrect as AWS CloudFormation is used for infrastructure deployment &amp; automating resource creation within AWS Cloud Infrastructure.\nFor more information on AWS CodeDeploy, refer to the following URL:\nhttps://aws.amazon.com/codedeploy/faqs/?nc=sn&amp;loc=6\n\nThe correct answer is B. AWS CodeDeploy.\nAWS CodeDeploy is a fully managed deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, and serverless Lambda functions. It simplifies the process of deploying and updating applications, making it faster and more reliable.\nWith AWS CodeDeploy, you can deploy your code to any number of instances, all at once or in batches, and track the status of each deployment. It supports various deployment strategies, including rolling deployments, blue/green deployments, and canary deployments.\nAWS Elastic Beanstalk, on the other hand, is a fully managed service that makes it easy to deploy and scale web applications and services. It includes a variety of preconfigured platforms and environments for popular programming languages and frameworks. Elastic Beanstalk automatically handles capacity provisioning, load balancing, and scaling of the application.\nAWS Config is a service that provides a detailed inventory of resources and their configurations, as well as configuration change history. It enables you to track compliance with various policies and best practices, and set up automatic remediation actions.\nAWS CloudFormation is a service that allows you to create and manage AWS resources using code. It uses templates to define the infrastructure and configuration of your resources, and automates the provisioning and updating of those resources.\nWhile AWS Elastic Beanstalk, AWS Config, and AWS CloudFormation can all be used to manage and deploy applications on AWS, they are not specifically designed for code deployment automation. In contrast, AWS CodeDeploy is focused specifically on application deployment automation, making it the best choice for this use case.\n\n"
}, {
  "id" : 239,
  "question" : "Which of the following are benefits of the AWS's Relational Database Service (RDS)? Choose the 2 correct answers from the options below.\n",
  "answers" : [ {
    "id" : "d203ac079ce947268a3724119e7937ea",
    "option" : "Automated patches and backups",
    "isCorrect" : "true"
  }, {
    "id" : "b4463a0a5b42462390542f712a72625a",
    "option" : "DB owner can resize the capacity accordingly",
    "isCorrect" : "true"
  }, {
    "id" : "b818d14b2741459fafaf2ae91c68b052",
    "option" : "It allows you to store unstructured data",
    "isCorrect" : "false"
  }, {
    "id" : "b97afa58139540f1a9a1d27a4c122ce4",
    "option" : "It allows you to store NoSQL data.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A and B.\nThe AWS Documentation mentions the following:\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud.\nIt provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups.\nIt frees you to focus on your applications.\nSo you can give them the fast performance, high availability, security and compatibility they need.\nFor more information on AWS RDS, please visit the URL:\nhttps://aws.amazon.com/rds/\n\nThe two correct answers regarding the benefits of AWS's Relational Database Service (RDS) are:\nA. Automated patches and backups B. DB owner can resize the capacity accordingly\nHere's a detailed explanation of each:\nA. Automated patches and backups: AWS RDS automates many of the time-consuming database management tasks, such as applying software patches, performing backups, and monitoring the health of the database. This means that AWS manages the underlying infrastructure, allowing you to focus on your application and its data. With automated backups, you can restore your database to any point in time within the retention period. You can also take manual snapshots at any time and retain them as long as you need.\nB. DB owner can resize the capacity accordingly: AWS RDS allows you to scale your database instance vertically or horizontally based on your needs. Vertical scaling involves increasing or decreasing the instance size, which increases or decreases the amount of CPU, memory, and storage available to the database. Horizontal scaling involves adding or removing replicas, which can improve performance and availability. With AWS RDS, you can scale your database without downtime or data loss, making it easier to meet the demands of your application.\nC. It allows you to store unstructured data: This option is not correct. AWS RDS is designed to store structured data in a relational database, where data is organized into tables with predefined schemas. It does not support storing unstructured data such as documents, images, or videos.\nD. It allows you to store NoSQL data: This option is not correct. AWS RDS is a relational database service and is not designed for storing NoSQL data. AWS provides other services such as Amazon DynamoDB and Amazon DocumentDB for NoSQL databases. These services are optimized for different use cases and offer different features than AWS RDS.\n\n"
}, {
  "id" : 240,
  "question" : "While using IAM within AWS, they recommend a list of best practices to be used while managing Users &amp; their permissions.\nWhich of the IAM practices are best to be chosen? Select TWO.\n",
  "answers" : [ {
    "id" : "91e794c3c9184beb8188c6f9b1ed9de0",
    "option" : "An IAM user should be given default access to all services for being able to develop &amp; deliver applications quickly",
    "isCorrect" : "false"
  }, {
    "id" : "aae7b8ec12b34e36a032a4ea8e836729",
    "option" : "Users should be provided both Passwords &amp; Access keys within the AWS environment",
    "isCorrect" : "false"
  }, {
    "id" : "f8512a818f734f2f95f412459121a799",
    "option" : "Within AWS, services should use roles for accessing other services",
    "isCorrect" : "true"
  }, {
    "id" : "327f2f9f155247fa9b727d547866e76d",
    "option" : "For cross account access it is best to share the Access Keys when one Account needs to access services of another Account",
    "isCorrect" : "false"
  }, {
    "id" : "259163f833b540738aa764e6713441a1",
    "option" : "While creating an AWS Account, its best to create an IAM user &amp; use that Account.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answers: C and E.\nOption A is incorrect.\nThe principle of â€œGrant Least Privilegeâ€ suggests providing least &amp; granular access to any User for accessing AWS services.\nThis will help enhance security of the system and prevent users from performing malicious / accidental tasks within the AWS environment that may compromise the system.\nOption B is incorrect.\nIt's always best to see what kind of access the user requires.\nIf a user does not use the Command Line Interface (CLI), it's best to avoid giving him access to Access Keys.\nSImilarly, if an user only uses the CLI for development, he should be prohibited from accessing the AWS admin console using a password.\nOption C is CORRECT.\nRoles provide temporary access to AWS services &amp; they dont require to share the Access Keys.\nA role that is created may be assigned a definite set of permissions for accessing different services.\nThe service that uses the role actually assumes the role with which it can perform actions on another service.\nThis way, we can induce granular permissions as well as prevent long lived Access Keys from being compromised.\nOption D is incorrect.\nCross account access are scenarios where users in one Account would like to access services of other Accounts.\nEg An user may use AWS Partner solutions to interact with a Customer's Account for monitoring it or do some orchestration.\nAccess keys are long lived credentials for which the AWS Partner must ensure to manage those keys by rotating them frequently to avoid them from being compromised.\nRoles defined with temporary access permissions when given to the user for accessing the Customer's account will be a best practice here without having to provide Access keys.\nOption E is CORRECT.\nRoot accounts that are created should have their Access Keys locked to prevent compromising them.\nRoot Accounts have complete Access Privileges to access different AWS resources.\nThe best practice will be to create an IAM User, Assign required permissions &amp; login as that user for accessing AWS services.\nReference:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html\n\nWhen managing users and their permissions within AWS, it is recommended to follow best practices to ensure security and proper access control. Here are the two best IAM practices to choose:\nC. Within AWS, services should use roles for accessing other services: AWS provides an IAM feature called \"roles,\" which enables services to access other services. When a role is created, permissions can be granted to the role instead of assigning permissions to individual users. Roles can be assumed by users, services, or AWS resources, allowing them to access the resources that the role has been granted permissions for. This practice ensures that users and services only have access to the specific resources they need, and permissions can be easily modified or revoked as necessary.\nE. While creating an AWS Account, it's best to create an IAM user and use that Account: When creating an AWS account, it is recommended to create an IAM user with a strong password and use that user to perform daily tasks, rather than using the root account. The root account has full access to all resources and should only be used for account management tasks. By creating an IAM user, you can assign specific permissions to that user, making it easier to manage access control and security. Additionally, using an IAM user allows you to easily revoke access to a user when they leave the organization, without affecting the root account.\nThe other answer choices are not recommended IAM practices:\nA. An IAM user should be given default access to all services for being able to develop & deliver applications quickly: Giving a user default access to all services is not recommended since it can create security risks, and it is unnecessary for most users. Instead, it is recommended to grant users access to only the services and resources they need to perform their job functions.\nB. Users should be provided both Passwords & Access keys within the AWS environment: Providing both passwords and access keys is not recommended since access keys should be used for programmatic access only, and not shared with users. Instead, users should be provided with a username and password to access the AWS Management Console, and access keys should be restricted to only those users who need them.\nD. For cross-account access, it is best to share the Access Keys when one Account needs to access services of another Account: Sharing access keys between accounts is not recommended since it can create security risks. Instead, cross-account access should be granted using IAM roles or AWS Security Token Service (STS) to ensure that users and services have the appropriate level of access to the resources they need.\n\n"
}, {
  "id" : 241,
  "question" : "A company wants to create standard templates for the deployment of their Infrastructure.\nWhich AWS service can be used in this regard?\n",
  "answers" : [ {
    "id" : "96fe209bc806479c90f7df9b3619e708",
    "option" : "Amazon Simple Workflow Service",
    "isCorrect" : "false"
  }, {
    "id" : "82d0ad14a56a4e3ea8c4a92b32322970",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "c0a922f69c334739a3e22ca69d402150",
    "option" : "AWS CloudFormation",
    "isCorrect" : "true"
  }, {
    "id" : "ece84c7dd0cc42a88d9a014b22fe5c84",
    "option" : "AWS OpsWorks.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAWS CloudFormation gives developers and systems administrators an easy way to create and manage a collection of related AWS resources, provisioning and updating them in an orderly and predictable fashion.\nFor more information on Cloudformation, please visit the URL:\nhttps://aws.amazon.com/cloudformation/\n\nThe AWS service that can be used to create standard templates for the deployment of infrastructure is AWS CloudFormation.\nAWS CloudFormation is a service that enables users to create and manage AWS resources in a repeatable and predictable way. With CloudFormation, users can define a template that describes the resources needed for their application and then deploy that template to create and provision the resources automatically.\nUsing CloudFormation, users can define a template in a JSON or YAML format that describes the resources needed for their application, such as Amazon EC2 instances, Amazon RDS databases, and Amazon S3 buckets. The template can include specifications for the resource types, their properties, and any associated dependencies.\nOnce the CloudFormation template is defined, users can use it to deploy the resources in a consistent and repeatable way across multiple environments. CloudFormation provides a stack-based approach to managing the resources, where each stack represents a collection of AWS resources that are created and managed together as a single unit.\nIn summary, AWS CloudFormation is the service that can be used to create standard templates for the deployment of infrastructure, enabling users to define, manage, and provision AWS resources in a repeatable and predictable way.\n\n"
}, {
  "id" : 242,
  "question" : "You have a distributed application that periodically processes large volumes of data across multiple Amazon EC2 Instances.\nThe application is designed to recover gracefully from Amazon EC2 instance failures.\nYou are required to accomplish this task most cost-effectively. Which of the following will meet your requirements?\n",
  "answers" : [ {
    "id" : "4ab99e02926846559e36731d1253310b",
    "option" : "Spot Instances",
    "isCorrect" : "true"
  }, {
    "id" : "14821c44ba4a446fbd6c10feca85a285",
    "option" : "Reserved instances",
    "isCorrect" : "false"
  }, {
    "id" : "0c2ec711fdb846f786845feb61478b09",
    "option" : "Dedicated instances",
    "isCorrect" : "false"
  }, {
    "id" : "749646ced1fb42ed9472940847831bcb",
    "option" : "On-Demand instances.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nWhen you think of cost effectiveness, you can either have to choose Spot or Reserved instances.\nNow when you have a regular processing job, the best is to use spot instances.\nSince your application is designed to recover gracefully from Amazon EC2 instance failures, there is no issue even if you lose the Spot instances.\nFor more information on spot instances, please visit the URL:\nhttps://aws.amazon.com/ec2/spot/\n\nThe most cost-effective option for running a distributed application that periodically processes large volumes of data across multiple Amazon EC2 Instances and is designed to recover gracefully from Amazon EC2 instance failures is Spot Instances.\nSpot Instances are spare EC2 capacity that is available at a significantly lower price than On-Demand instances. With Spot Instances, you bid on unused EC2 capacity, and your instance runs whenever your bid exceeds the current Spot price. If the Spot price goes above your bid or Amazon EC2 needs the capacity back, your instance is terminated. However, since your application is designed to recover gracefully from Amazon EC2 instance failures, it can simply be relaunched on another Spot instance when needed.\nReserved instances offer a discount on the hourly charge for an instance but require a contract for a specific amount of time. Dedicated instances are instances that run on hardware that's dedicated to a single customer for privacy and compliance reasons, and On-Demand instances are instances that are available for as long as you need them, with no long-term commitments or upfront payments.\nIn summary, Spot Instances offer a cost-effective way to run a distributed application that is designed to recover gracefully from Amazon EC2 instance failures.\n\n"
}, {
  "id" : 243,
  "question" : "You want to create a stream processing solution to process and query real-time streaming data using a SQL-based solution.\nYou are looking for the simplest approach available that AWS provides. What AWS service should you use?\n",
  "answers" : [ {
    "id" : "9afa0156723c4c90b162aa47bbede74d",
    "option" : "Amazon Kinesis Data Streams",
    "isCorrect" : "false"
  }, {
    "id" : "36b1eb6c708a4b50a19b430b579191ca",
    "option" : "Amazon Kinesis Data Analytics",
    "isCorrect" : "true"
  }, {
    "id" : "427bf4ddf69f4915940ed2f627a6e5c8",
    "option" : "Amazon Kinesis Client Library",
    "isCorrect" : "false"
  }, {
    "id" : "6dcb0449268b4005b833f2cbf8e15e37",
    "option" : "Amazon Kinesis Data Firehose.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT because Amazon Kinesis Data Streams is not an AWS service to build analytics applications.\nIt is an ingestion service that provides data streams to consumers (e.g., analytic applications).\nAWS documentation mentions that â€œAmazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming serviceâ€¦\nThe data collected is available in milliseconds to enable real-time analytics use cases such as real-time dashboards, real-time anomaly detection, dynamic pricing, and moreâ€.\nOption B is CORRECT because AWS documentation (FAQ section) mentions that â€œIf you want a fully managed solution and you want to use SQL to process the data from your data stream, you should use Kinesis Data Analyticsâ€.\nOption C is INCORRECT because Amazon Kinesis Client Library (KCL) is a service to process and query streaming data using SQL.\nKCL is more complicated than AWS Kinesis Data Analytics.\nAWS documentation mentions that â€œKinesis Data Analytics uses the KCL to read data from streaming data sources as one part of your underlying application.\nThe service abstracts this from you, as well as many of the more complex concepts associated with using the KCL, such as checkpointingâ€.\nOption D is INCORRECT because Amazon Kinesis Data Firehose is a service for loading data streams and not creating SQL-based applications.\nDiagram: none.\nReferences:\nhttps://aws.amazon.com/kinesis/data-streams/\nhttps://aws.amazon.com/kinesis/data-analytics/faqs/?nc=sn&amp;loc=6\nhttps://aws.amazon.com/kinesis/data-firehose/\n\nThe AWS service that provides the simplest approach for processing and querying real-time streaming data using a SQL-based solution is Amazon Kinesis Data Analytics.\nAmazon Kinesis Data Analytics is a fully-managed service that enables you to easily process and analyze streaming data with SQL without the need for any coding. It integrates with Amazon Kinesis Data Streams and Amazon Kinesis Data Firehose, allowing you to easily process and query real-time streaming data.\nHere's a brief overview of the other AWS services mentioned in the options:\nAmazon Kinesis Data Streams is a service for ingesting, processing, and storing real-time streaming data. It's a low-level service that requires custom coding for data processing and querying. Amazon Kinesis Client Library is a client library that allows you to easily consume and process data from Amazon Kinesis Data Streams using Java or Python. It's not a SQL-based solution and requires custom coding for data processing and querying. Amazon Kinesis Data Firehose is a fully-managed service for delivering real-time streaming data to destinations such as Amazon S3, Amazon Redshift, and Amazon Elasticsearch. It's not a SQL-based solution and requires custom coding for data processing and querying.\nTherefore, the correct answer is B. Amazon Kinesis Data Analytics.\n\n"
}, {
  "id" : 244,
  "question" : "What of the following AWS services allows developers to deploy and manage web applications on the cloud easily?\n",
  "answers" : [ {
    "id" : "61a87605373d4722b2cc71bf1fdd5733",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "b2c44882d86843aab3c475f41dee2562",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "true"
  }, {
    "id" : "b819660969ec437f9b042e5b8e73539a",
    "option" : "Amazon CloudFront",
    "isCorrect" : "false"
  }, {
    "id" : "09ac1f79a0cc416dab9ad87b011b4acd",
    "option" : "Elastic Container Service.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nAWS Elastic Beanstalk makes it even easier for developers to deploy and manage web applications in the AWS Cloud quickly.\nDevelopers simply upload their application and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, load balancing, auto-scaling, and application health monitoring.\nFor more information on Elastic Beanstalk, please visit the URL:\nhttps://aws.amazon.com/elasticbeanstalk/faqs/\n\nThe AWS service that allows developers to deploy and manage web applications on the cloud easily is AWS Elastic Beanstalk, which is option B.\nAWS Elastic Beanstalk is a fully managed service that simplifies the process of deploying and scaling web applications developed in various programming languages such as Java, .NET, PHP, Node.js, Python, Ruby, and Go. Developers only need to upload their code, and Elastic Beanstalk automatically handles the deployment, capacity provisioning, load balancing, and application health monitoring.\nIn addition, Elastic Beanstalk supports several popular web servers, such as Apache, Nginx, Passenger, and IIS, as well as databases such as MySQL, PostgreSQL, Oracle, and SQL Server. Elastic Beanstalk also integrates with other AWS services, such as Amazon RDS, Amazon S3, Amazon CloudWatch, and AWS Identity and Access Management (IAM), for data storage, monitoring, and access control.\nFurthermore, Elastic Beanstalk allows developers to customize the underlying AWS resources, such as EC2 instances, security groups, and scaling policies, based on their specific application requirements. Developers can also deploy updates to their applications automatically or manually, roll back to a previous version, or perform blue-green deployments to reduce downtime and risk.\nTherefore, AWS Elastic Beanstalk provides a simple, flexible, and cost-effective way for developers to deploy and manage web applications on the cloud without having to worry about the underlying infrastructure or operational tasks.\n\n"
}, {
  "id" : 245,
  "question" : "A company is deploying a new two-tier web application in AWS.\nThe company wants to store their most frequently used data to improve the response time for the application.\nWhich AWS service provides the caching solution for the company's requirements?\n",
  "answers" : [ {
    "id" : "951bf680b8f94d35bcad8ed4ba390e82",
    "option" : "MySQL Installed on two Amazon EC2 Instances in a single Availability Zone",
    "isCorrect" : "false"
  }, {
    "id" : "4ed82b3a19874a0c9c71149b9fdbf083",
    "option" : "Amazon RDS for MySQL with Multi-AZ",
    "isCorrect" : "false"
  }, {
    "id" : "f2e28f55f7cb4a0f9d0ac09803081f2b",
    "option" : "Amazon ElastiCache",
    "isCorrect" : "true"
  }, {
    "id" : "b0511f0576f74a9fb21a15d9ebd151c4",
    "option" : "Amazon DynamoDB.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nAmazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory data store or cache in the cloud.\nThe service improves the performance of web applications by allowing you to retrieve information from fast, managed, in-memory data stores, instead of relying entirely on slower disk-based databases.\nFor more information on Elastic cache, please visit the link:\nhttps://aws.amazon.com/elasticache/\n\nThe AWS service that provides the caching solution for the company's requirements is Amazon ElastiCache (Option C).\nAmazon ElastiCache is a fully managed, in-memory caching service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. ElastiCache supports two open-source in-memory caching engines, Redis and Memcached.\nIn this scenario, since the company wants to store their most frequently used data to improve the response time for the application, using a caching service is a good solution. Caching involves storing frequently accessed data in a temporary storage area so that it can be quickly retrieved without the need to access the original data source, which can improve the application's performance.\nOption A and B suggest using MySQL as the data store, but these solutions do not provide a caching mechanism, and as a result, they may not be able to meet the company's requirement for improved response time.\nOption D suggests using Amazon DynamoDB, which is a NoSQL database that can store and retrieve any amount of data and can serve as a primary data store for a web application. However, it does not provide caching capabilities by itself. Although, DynamoDB Accelerator (DAX) is a caching layer for DynamoDB that can provide fast access to frequently accessed data.\nTherefore, the best option for the company's requirement is Amazon ElastiCache, which can improve the response time of the application by caching frequently accessed data in memory, reducing the time it takes to retrieve the data from the original data source.\n\n"
}, {
  "id" : 246,
  "question" : "If you want to take a backup of an EBS Volume, what would you do?\n",
  "answers" : [ {
    "id" : "b1e0721823054bdc8f7d12a394ea72c6",
    "option" : "Store the EBS volume in S3.",
    "isCorrect" : "false"
  }, {
    "id" : "f0e452a5dc9543489f92b7df8414299a",
    "option" : "Store the EBS volume in an RDS database.",
    "isCorrect" : "false"
  }, {
    "id" : "0ede85017b5d4fbc866eb32bb48732a6",
    "option" : "Create an EBS snapshot.",
    "isCorrect" : "true"
  }, {
    "id" : "6f758fbf5736474fbb53e34a1eff966a",
    "option" : "Store the EBS volume in DynamoD.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe AWS Documentation mentions the following:\nYou can back up the data on your Amazon EBS volumes to Amazon S3 by taking point-in-time snapshots.\nFor more information on EBS Snapshots, please visit the link:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\n\nTo take a backup of an Amazon Elastic Block Store (EBS) volume, the recommended approach is to create an EBS snapshot. An EBS snapshot is a point-in-time copy of an EBS volume that is stored in Amazon S3. The snapshot contains all the data on the volume at the time the snapshot was created, and can be used to restore the volume or create a new volume with the same data as the original.\nThe steps to create an EBS snapshot are as follows:\nOpen the Amazon EC2 console. In the navigation pane, choose \"Volumes\". Select the EBS volume that you want to back up. Choose \"Actions\", then choose \"Create Snapshot\". In the \"Create Snapshot\" dialog box, enter a name and description for the snapshot. Choose \"Create Snapshot\".\nNote that creating a snapshot is an asynchronous process, and the snapshot may take several minutes or longer to complete. You can monitor the progress of the snapshot creation in the \"Snapshots\" section of the Amazon EC2 console.\nOnce the snapshot is complete, you can use it to create a new EBS volume with the same data as the original, or restore the original volume to a previous state by creating a new volume from the snapshot and attaching it to an Amazon EC2 instance.\nOption A, storing the EBS volume in S3, is not a recommended approach for taking a backup of an EBS volume. While it is possible to create an S3 bucket and manually upload a copy of the EBS volume to the bucket, this approach does not provide the same level of data consistency and recovery options as creating an EBS snapshot.\nOption B, storing the EBS volume in an RDS database, is not applicable as RDS is a managed relational database service offered by AWS, and does not support storing EBS volumes.\nOption D, storing the EBS volume in DynamoDB, is also not applicable. DynamoDB is a managed NoSQL database service offered by AWS, and does not support storing EBS volumes.\n\n"
}, {
  "id" : 247,
  "question" : "Basant &amp; Josiah are transferring 8 TB &amp; 4 TB of data into S3 in a month respectively.\nBasant's AWS account is the Management account who's consolidated bill includes Basant's own Account &amp; Josiah's Account.\nHow much of the total S3 cost will the Management account be charged for both accounts? (Given below are the charges for S3) Price per TB (first 10 TB) = 0.17 * 1024 = $174.08 Price per TB (Next 40 TB) = 0.13 * 1024 = $133.12\n",
  "answers" : [ {
    "id" : "08a0e984928248a8b0d07b19350b7e98",
    "option" : "174.08 * 8",
    "isCorrect" : "false"
  }, {
    "id" : "7fca258ef09d4748a49cd174e9aa54c5",
    "option" : "174.08 * 12",
    "isCorrect" : "false"
  }, {
    "id" : "818ab6647586445195d6ef549183cce9",
    "option" : "174.08 * 10 + 133.12 * 2",
    "isCorrect" : "true"
  }, {
    "id" : "b891134a567e4761a24c7b72dbafc8a9",
    "option" : "174.08 * 4",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nWhile using AWS consolidated billing, all the Accounts considered for billing are treated as a Single Account.\nThe consolidated billing will appear in the Management Account (Basant's here)\nConsolidated billing provides the advantage of having volume discounts which is provided by combined usage from all Accounts by AWS.\nIt determines which volume pricing tier to apply providing a less overall price whenever possible.\nThe calculations for arriving at the final result are as follows.\nPrice per TB (first 10 TB) = 0.17 * 1024 = $174.08\nPrice per TB (Next 40 TB) = 0.13 * 1024 = $133.12\nBasant's Management account will be charged with 174.08 * 10 + 133.12 * 2 = $2007.04\nIf the billing were to be done separately for both Basant &amp; Josiah, $174.08 will be applied to both accounts and the resulting charges will be $2088.96 (option B).\nOption A is incorrect since this will be only Basant's Account charges.\nOption B is incorrect since this will be the charges without using consolidated billing.\nOption C is CORRECT since volume discounts are applied to the entire 12 TB of data transfer ($0.17 for the first 10TB &amp; $0.13 for the last 2TB) due to use of a single billing Account.\nOption D is incorrect since this will be Josiah's Account charges.\nDiagram:\nReference:\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/consolidated-billing.html\n\n\nTo calculate the total cost for both Basant's and Josiah's accounts, we need to first calculate the total amount of data being transferred to S3.\nBasant is transferring 8 TB of data in a month, and Josiah is transferring 4 TB of data in a month. Therefore, the total amount of data being transferred in a month is 8 TB + 4 TB = 12 TB.\nNow that we know the total amount of data being transferred, we can calculate the total cost based on the pricing tiers for S3.\nFor the first 10 TB, the price is $174.08 per TB. Since we are transferring 12 TB of data, the first 10 TB will be charged at this rate, and the remaining 2 TB will be charged at the next pricing tier.\nSo the cost for the first 10 TB is:\n10 TB * $174.08/TB = $1,740.80\nFor the remaining 2 TB, the price is $133.12 per TB. So the cost for the remaining 2 TB is:\n2 TB * $133.12/TB = $266.24\nAdding the costs for the first 10 TB and the remaining 2 TB, we get:\n$1,740.80 + $266.24 = $2,007.04\nTherefore, the total S3 cost that the Management account will be charged for both Basant's and Josiah's accounts is $2,007.04.\nThe correct answer is C. 174.08 * 10 + 133.12 * 2.\n\n"
}, {
  "id" : 248,
  "question" : "Which of the following options of AWS RDS allows for AWS to failover to a secondary database in case the primary one fails?\n",
  "answers" : [ {
    "id" : "9dfcebe53b78487da4633bbbbd23cf3a",
    "option" : "Amazon RDS Multi-AZ deployments",
    "isCorrect" : "true"
  }, {
    "id" : "2797c03058b842d3bb89e30c12d683fd",
    "option" : "AWS Failover",
    "isCorrect" : "false"
  }, {
    "id" : "bbd4bfbe3f284790ad2472fb239d04bc",
    "option" : "AWS Secondary",
    "isCorrect" : "false"
  }, {
    "id" : "a29b954d64814e689534fbaee9ff2b5f",
    "option" : "AWS Standby.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - A.\nThe AWS Documentation mentions the following.\nAmazon RDS Multi-AZ deployments provide enhanced availability and durability for Database (DB) Instances, making them a natural fit for production database workloads.\nWhen you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary DB Instance and synchronously replicates it to a standby instance in a different Availability Zone (AZ)\nEach AZ runs on its own physically distinct, independent infrastructure and is engineered to be highly reliable.\nIn case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby (or to a read replica in the case of Amazon Aurora)\nSo you can resume database operations as soon as the failover is complete.\nOptions B, C and D are the distractors because there is no AWS service like failover, standby or secondary.\nFailover is the situation when Multi-AZ appears and Standby or Secondary represents other RDS servers appears during Multi-AZ deployments.\nFor more information on AWS RDS Multi-AZ, visit the below link:\nhttps://aws.amazon.com/rds/details/multi-az/\n\nThe correct answer is A. Amazon RDS Multi-AZ deployments.\nAmazon RDS (Relational Database Service) is a managed database service offered by Amazon Web Services (AWS) that allows you to create and operate relational databases in the cloud. Amazon RDS provides easy deployment, scaling, and management of popular relational database engines such as MySQL, PostgreSQL, Oracle, and Microsoft SQL Server.\nA Multi-AZ deployment is a feature of Amazon RDS that provides high availability and fault tolerance for database instances. In a Multi-AZ deployment, AWS automatically provisions and maintains a synchronous standby replica of your primary database instance in a different availability zone (AZ) within the same region. If the primary database instance fails due to hardware failure, software failure, or availability zone outage, AWS automatically switches over to the standby replica, which becomes the new primary database instance.\nThe failover process in Multi-AZ deployments is automatic and does not require any manual intervention. The DNS endpoint of the database remains the same, so your applications can continue to connect to the database without any changes. After the failover, the original primary instance is replaced by a new standby instance, and replication is resumed.\nTo summarize, Amazon RDS Multi-AZ deployments provide a high availability solution for your database by automatically provisioning and maintaining a synchronous standby replica in a different availability zone, and by automatically failing over to the standby replica in case of a primary instance failure.\n\n"
}, {
  "id" : 249,
  "question" : "For which of the following AWS resources, the Customer is responsible for the infrastructure-related security configurations?\n",
  "answers" : [ {
    "id" : "f6814026430e469b9559a673a3b53ca0",
    "option" : "Amazon RDS",
    "isCorrect" : "false"
  }, {
    "id" : "ac8f89a1c09d4e1f9b37501b2aaa0c50",
    "option" : "Amazon DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "cb6f78992db943b9a2ca4101dc3db37e",
    "option" : "Amazon EC2",
    "isCorrect" : "true"
  }, {
    "id" : "dff2a2d841d04e52ac1ef08b5536d89e",
    "option" : "AWS Fargate.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nAmazon EC2 is an Infrastructure as a Service (IaaS) for which customers are responsible for the security and the management of guest operating systems.\nOptions A, B, and D are incorrect as all these resources are part of abstracted services for which AWS is responsible for the security, &amp; infrastructure layer.\nCustomers are responsible for data that is saved on these resources.\nFor more information on the Shared responsibility model, refer to the following URL:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\n\nOut of the given options, the customer is responsible for infrastructure-related security configurations in Amazon EC2.\nAmazon EC2 (Elastic Compute Cloud) is a web service that provides resizable compute capacity in the cloud. It allows customers to rent virtual machines (instances) on which they can run their own applications. The customer has full control over the virtual machine, including the operating system and any applications installed on it.\nThis means that the customer is responsible for managing the security of the operating system, any applications installed on it, and the data stored on it. This includes tasks such as installing security updates, configuring firewalls, managing user access, and monitoring system logs.\nIn contrast, Amazon RDS (Relational Database Service) and Amazon DynamoDB are managed database services. AWS takes care of the infrastructure-related security configurations, such as network isolation, backups, and software patching. However, the customer is still responsible for securing their own data, such as encrypting sensitive data and managing database access.\nAWS Fargate is a serverless computing service that allows customers to run containers without managing the underlying infrastructure. AWS is responsible for the security of the underlying infrastructure, such as securing the network and managing the underlying servers. The customer is responsible for securing their container images and configuring their containers securely.\nIn summary, for Amazon EC2, the customer is responsible for infrastructure-related security configurations such as managing the operating system, applications, and data security. For Amazon RDS, DynamoDB, and AWS Fargate, AWS takes care of the infrastructure-related security configurations, but the customer is still responsible for securing their own data and configuring their applications securely.\n\n"
}, {
  "id" : 250,
  "question" : "Which of the following statements regarding Amazon Athena are accurate? Select TWO.\n",
  "answers" : [ {
    "id" : "162b3f0f39e64e3dae3f12e4e2029d76",
    "option" : "Amazon Athena queries data directly from Amazon S3 and there are no additional data storage commitments beyond the object storage.",
    "isCorrect" : "true"
  }, {
    "id" : "3c8efd8affe1483486f188829011da0b",
    "option" : "Amazon Athena is not suitable for complex analysis such as large joins, window functions and arrays.",
    "isCorrect" : "false"
  }, {
    "id" : "47b71c615bc1472e88b598bb20d76346",
    "option" : "Amazon Athena resources are allocated in accordance to processing and memory requirements prior to deployment.",
    "isCorrect" : "false"
  }, {
    "id" : "81f6ee6b4bbd4f1583fa4b12b7a71583",
    "option" : "Amazon Athena is compatible with data formats such as CSV, JSON, ORC, AVRO and Parquet",
    "isCorrect" : "true"
  }, {
    "id" : "3faacc0c152842f284b7c525a8321e37",
    "option" : "Amazon Athena uses a variety of query languages including SQL, LDAP, JPQL as well as CQL.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A, D.\nAmazon Athena a serverless query service that does not need to build databases on dedicated Elastic Block Store (EBS) volumes.\nInstead, it builds tables from data read directly from Amazon S3 buckets.\nAmazon Athena does not store any of the data.\nThe service is compatible with the regular data formats that include CSV, JSON, ORC, AVRO and Parquet.\nhttps://docs.aws.amazon.com/athena/latest/ug/what-is.html\nOption B is incorrect because Amazon Athena can query Big Data, complex analysis such as large joins, window functions and arrays.\nOption C is incorrect because Amazon Athena is serverless.\nThus the service scales following the resource demands.\nNo prior resource planning is necessary.\nOption E is incorrect because Amazon Athena uses SQL only.\n\nSure, I'd be happy to provide a detailed explanation for you.\nAmazon Athena is a serverless, interactive query service that enables users to analyze data directly in Amazon S3 using SQL. Here are the correct statements regarding Amazon Athena:\nA. Amazon Athena queries data directly from Amazon S3 and there are no additional data storage commitments beyond the object storage.\nThis statement is accurate. Amazon Athena is a serverless service, which means that there are no servers to manage and no infrastructure to provision. Amazon Athena queries data directly from Amazon S3, which means that there are no additional data storage commitments beyond the object storage.\nB. Amazon Athena is not suitable for complex analysis such as large joins, window functions and arrays.\nThis statement is incorrect. Amazon Athena is capable of handling complex analysis, such as large joins, window functions, and arrays. Amazon Athena is based on Presto, which is a distributed SQL query engine. Presto was designed to be scalable and to handle large datasets, so Amazon Athena inherits those capabilities.\nC. Amazon Athena resources are allocated in accordance to processing and memory requirements prior to deployment.\nThis statement is incorrect. Amazon Athena is a serverless service, which means that there are no servers to manage and no infrastructure to provision. Users do not need to allocate resources in advance or worry about scaling their clusters. Amazon Athena automatically scales resources up or down based on the volume of data being processed.\nD. Amazon Athena is compatible with data formats such as CSV, JSON, ORC, AVRO and Parquet.\nThis statement is accurate. Amazon Athena is compatible with a variety of data formats, including CSV, JSON, ORC, AVRO, and Parquet. Users can create tables in Athena that point to data stored in S3 in any of these formats, and then query that data using SQL.\nE. Amazon Athena uses a variety of query languages including SQL, LDAP, JPQL as well as CQL.\nThis statement is incorrect. Amazon Athena uses only SQL as its query language. It does not support LDAP, JPQL, or CQL.\nIn summary, the two accurate statements regarding Amazon Athena are:\nAmazon Athena queries data directly from Amazon S3 and there are no additional data storage commitments beyond the object storage. Amazon Athena is compatible with data formats such as CSV, JSON, ORC, AVRO and Parquet.\n\n"
}, {
  "id" : 251,
  "question" : "An administrator is running a large deployment of AWS resources that are spread across several AWS Regions.\nThey would like to keep track of configuration changes on all the resources and maintain a configuration inventory.\nWhat is the best service they can use?\n",
  "answers" : [ {
    "id" : "f43527c2a9be48879468402078ef919b",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "446db61a6a2f4df486617534c6b0bfb7",
    "option" : "Stacks and Templates",
    "isCorrect" : "false"
  }, {
    "id" : "78d0236922194e82ac5240ec8b22cd0c",
    "option" : "AWS Backup",
    "isCorrect" : "false"
  }, {
    "id" : "35f8362429524e76bea1556282229634",
    "option" : "AWS Config.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer - D.\nAWS Config will meet the scenario requirements.\nThe service allows the administrator to monitor and record configuration changes on AWS resources in their account.\nThe service also allows the administrator to create a resource configuration inventory.\nhttps://aws.amazon.com/config/\nOption A is incorrect because AWS CloudFormation will allow the administrator to create templates of resources such as EC2 instances and RDS instances but not the actual configurations in these resources.\nOption B is incorrect because Templates and Stacks form the basis of AWS CloudFormation.\nThey aid in the automated deployment of whole environments but not the applications that run in them.\nOption C is incorrect because AWS Backup is a fully managed service that allows the administrator to back up data in the cloud and on-premises.\nThe service is not the most appropriate to monitor and record resource configuration changes.\n\nThe best service an administrator can use to keep track of configuration changes on all the resources and maintain a configuration inventory in a large deployment of AWS resources that are spread across several AWS Regions is AWS Config (Option D).\nAWS Config is a fully managed service that provides a detailed view of the resources associated with an AWS account, including how they are configured, how they are related to one another, and how their configurations have changed over time. AWS Config enables continuous monitoring and recording of AWS resource configurations, which can be used for operational troubleshooting, compliance auditing, and security analysis.\nWith AWS Config, an administrator can set up rules to evaluate the configuration changes and track compliance with policies. If any resources are found to be out of compliance with policies, AWS Config can notify administrators through Amazon SNS, Amazon S3, or AWS Lambda.\nAWS CloudFormation (Option A) is a service that allows an administrator to create and manage AWS resources using templates. It is used to provision and deploy a set of resources together in a predictable and repeatable manner.\nAWS Backup (Option C) is a fully managed backup service that makes it easy to centralize and automate the backup of data across AWS services. It is used to automate backup policies and schedules, monitor backup activity, and restore data.\nStacks and Templates (Option B) are concepts used in AWS CloudFormation that allow an administrator to provision and manage a collection of related AWS resources together as a single unit. Stacks are created from templates, which are JSON or YAML formatted text files that describe the resources to be provisioned.\nWhile AWS CloudFormation, AWS Backup, and Stacks and Templates can help an administrator manage and deploy AWS resources, they are not specifically designed to provide a detailed view of the configuration changes over time, which is what AWS Config is designed to do.\n\n"
}, {
  "id" : 252,
  "question" : "Whilst working on a collaborative project, an administrator would like to record the initial configuration and several authorized changes that engineers make to the route table of a VPC.\nWhat is the best method to achieve this?\n",
  "answers" : [ {
    "id" : "7495654846744afa847f4f67e335796f",
    "option" : "Use of AWS Config",
    "isCorrect" : "true"
  }, {
    "id" : "44c4b25f5a8b4de683879c7b30f121b1",
    "option" : "Use of VPC Flow Logs",
    "isCorrect" : "false"
  }, {
    "id" : "d752687b97a745d8997f79233abc28e3",
    "option" : "Use of AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "59d86951eefa4f93b8412d83af3b9e8f",
    "option" : "Use of an AWS Lambda function that is triggered to save a log file to an S3 bucket each time configuration changes are made.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAWS Config can be used to keep track of configuration changes on AWS resources, keeping multiple date-stamped versions in a reviewable history.\nThis makes it the best method to meet the scenario requirements.\nhttps://aws.amazon.com/config/\nOption B is incorrect because VPC flow logs will only capture IP traffic-related information passing through and from network interfaces within the VPC.\nVPC flow logs will not be able to capture configuration changes made to route tables.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/flow-logs.html\nOption C is incorrect because AWS CloudTrail will capture identity access activity, event history into the AWS environment.\nRecording the actions and API calls are not best suited to keep a record of configurations.\nhttps://aws.amazon.com/cloudtrail/\nOption D is incorrect because using a Lambda function to write configuration changes might meet the requirements, but it would not be the best method.\nAWS Config can deliver what is needed with much less administrative input.\n\nThe best method to achieve this is to use AWS CloudTrail.\nAWS CloudTrail is a service that records all API calls and events made in an AWS account. It enables governance, compliance, operational auditing, and risk auditing of the AWS account. CloudTrail can help identify which users and accounts called AWS, the source IP address from which the calls were made, and when the calls occurred.\nBy enabling AWS CloudTrail in the AWS account, administrators can log all API calls made to the VPC and its associated resources, including route tables. CloudTrail records the event details such as who made the change, what action was taken, when the change was made, and the IP address of the user making the change.\nUsing AWS CloudTrail, administrators can create trails that are specific to a VPC or a set of resources within a VPC, and they can specify that only authorized users can make changes to those resources. The recorded events can be stored in Amazon S3 or delivered to Amazon CloudWatch Logs for further analysis.\nUsing AWS Config, an alternative option listed in the exam question, can also record and track changes made to resources in AWS, including VPCs and route tables. However, AWS Config focuses on resource configuration changes over time, not on auditing user activity. AWS Config is designed to continuously monitor and record the configuration changes to resources, not just the initial configuration and authorized changes made by engineers to a specific resource.\nVPC Flow Logs can capture information about the IP traffic going to and from network interfaces in a VPC, including the route table traffic, but they are not designed to track user activity or changes made to the route table configuration.\nUsing an AWS Lambda function that is triggered to save a log file to an S3 bucket each time configuration changes are made is a custom solution that can achieve the goal, but it requires more effort and maintenance than using AWS CloudTrail, which is a fully managed service. Additionally, the Lambda function may not capture all relevant information, such as the IP address of the user making the change, without additional configuration.\n\n"
}, {
  "id" : 253,
  "question" : "A company does not want to manage their database.\nWhich of the following services is a fully managed NoSQL database provided by AWS?\n",
  "answers" : [ {
    "id" : "665b6b1c87ce4f07bb7ed221e55ff1e7",
    "option" : "AWS RDS",
    "isCorrect" : "false"
  }, {
    "id" : "ab0cde5e39f54983987404c51a20c28c",
    "option" : "DynamoDB",
    "isCorrect" : "true"
  }, {
    "id" : "781a6647874d4d539a04c04a9090cb89",
    "option" : "Oracle RDS",
    "isCorrect" : "false"
  }, {
    "id" : "cb97c58c5a344a8385913f7e9defb130",
    "option" : "Elastic Map Reduce.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nDynamoDB is a fully managed NoSQL offering provided by AWS.\nIt is now available in most regions for users to consume.\nFor more information on AWS DynamoDB, visit the below link:\nhttps://aws.amazon.com/dynamodb/\n\nThe correct answer is B. DynamoDB.\nExplanation:\nAWS RDS (A) is a managed relational database service, which provides options for MySQL, Oracle, SQL Server, PostgreSQL, MariaDB, and Amazon Aurora database engines.\nDynamoDB (B) is a fully managed NoSQL database service provided by AWS, which supports key-value and document data models. It is designed to provide low-latency, highly scalable, and fault-tolerant performance for any application that needs consistent, single-digit millisecond latency at any scale. DynamoDB is a good fit for web, mobile, gaming, ad tech, IoT, and many other applications.\nOracle RDS (C) is also a managed relational database service provided by AWS, which provides options for the Oracle database engine.\nElastic MapReduce (D) is a fully managed big data processing service that makes it easy to process large amounts of data using Hadoop, Spark, Presto, Hive, and other popular open-source frameworks. It is not a NoSQL database service.\nTherefore, the correct answer to the question is B. DynamoDB, which is a fully managed NoSQL database provided by AWS.\n\n"
}, {
  "id" : 254,
  "question" : "Which of the following tools can be used to create an estimated cost for a new solution to be deployed on AWS Cloud infrastructure?\n",
  "answers" : [ {
    "id" : "2315c2752c0a44da8a59cfe1eca4e400",
    "option" : "AWS Cost and Usage Report",
    "isCorrect" : "false"
  }, {
    "id" : "86c8060415d9498e8dddafa1b0c07d5e",
    "option" : "AWS Budgets",
    "isCorrect" : "false"
  }, {
    "id" : "7b73f886cad241b9afaae301077dc3f2",
    "option" : "AWS Cost Explorer",
    "isCorrect" : "false"
  }, {
    "id" : "26ca64e6bc964afca85a06a028ff0971",
    "option" : "AWS Pricing Calculator.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer : D.\nAWS Pricing Calculator helps create estimated usage costs for all resources to be deployed on AWS Cloud Infrastructure.\nOption A is incorrect as AWS Cost and Usage Report will provide detailed data for usage summary.\nOption B is incorrect as AWS Budget can set a custom budget to track usage for AWS resources.\nOption C is incorrect as AWS Cost Explorer can be used to analyze &amp; manage AWS usage.\nFor more information on AWS Pricing Calculator, refer to the following URL:\nhttps://docs.aws.amazon.com/pricing-calculator/latest/userguide/what-is-pricing-calculator.html\n\nThe correct answer is D. AWS Pricing Calculator.\nAWS Pricing Calculator is a tool that allows you to estimate the cost of your infrastructure and services before you actually deploy them. It enables you to select the AWS services you want to use, configure them, and get an estimated monthly cost for the services you've chosen.\nThe AWS Pricing Calculator is useful for predicting the cost of a new solution or project that you are planning to deploy on AWS. It can help you to compare the costs of different AWS services, estimate the monthly cost of your infrastructure and services, and optimize your AWS usage to save costs.\nThe other options listed in the question are also AWS tools related to cost management, but they serve different purposes.\nAWS Cost and Usage Report (Option A) is a tool that provides comprehensive cost and usage metrics for all your AWS accounts. It is mainly used for analyzing and optimizing your past usage and costs.\nAWS Budgets (Option B) is a tool that allows you to set custom budgets and receive alerts when your actual costs exceed your budgeted amount. It is mainly used for monitoring your current AWS usage and costs.\nAWS Cost Explorer (Option C) is a tool that provides advanced visualization and analytics of your AWS cost and usage data. It is mainly used for analyzing and optimizing your past and current usage and costs.\nIn summary, if you want to estimate the cost of a new solution or project to be deployed on AWS, the AWS Pricing Calculator is the best tool to use.\n\n"
}, {
  "id" : 255,
  "question" : "Your Security Team has some security concerns about the application data stored on S3\nThe team requires you to introduce two improvements: (i) add â€œencryption at restâ€ and (ii) give them the possibility to monitor who has accessed the data and when the data have been accessed. Which of the following AWS solution would you adopt to satisfy the requirement?\n",
  "answers" : [ {
    "id" : "37185aa76edc4cc794fb799e09a84e00",
    "option" : "AWS Certificate Manager with CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "aa21e6e6b7254477964647a84172d37b",
    "option" : "Server-Side Encryption managed by S3 (SSE-S3) with CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "6ab55e822ae24e59b6a4eebfa44dddae",
    "option" : "Server-Side Encryption managed by customer (SSE-C) with CloudTrail.",
    "isCorrect" : "false"
  }, {
    "id" : "d0060a0fc91f4e2fae53dce755dd935b",
    "option" : "Server-Side Encryption managed by KMS (SSE-KMS) with CloudTrail.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nAmazon S3 is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in Amazon S3\nCloudTrail logs successful operations and attempted calls that failed, such as when the caller is denied access to a resource.\nOperations on KMS keys in other accounts are logged in both the caller account and the KMS key owner account.\nOption A is INCORRECT AWS Certificate Manager is not a solution for encryption at rest.\nIt is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates.\nHence it is a solution for â€œencryption in transitâ€, not an â€œencryption at rest.â€\nOption B is INCORRECT because SSE-S3 does â€œencryption/decryption at restâ€, but it does not offer monitoring capabilities (who/when encrypts/decrypts).\nOption C is INCORRECT because SSE-C does â€œencryption/decryption at restâ€, but it does not offer monitoring capabilities (who/when encrypts/decrypts).\nOption D is CORRECT because SSE-KMS does â€œencryption/decryption at restâ€ and does offer monitoring capabilities.\nCloudTrail captures all API calls to AWS KMS as events, including calls from the AWS KMS console, AWS KMS APIs, the AWS Command Line Interface (AWS CLI), and AWS Tools for PowerShell.\nReferences:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html#sse\nhttps://docs.aws.amazon.com/kms/latest/developerguide/logging-using-cloudtrail.html\n\nTo meet the security team's requirements, you need to enable encryption at rest and provide auditing capabilities to track who accessed the data and when.\nAWS offers several solutions for server-side encryption and monitoring of access to S3 objects.\nOption A, AWS Certificate Manager, is not relevant to this use case. It is a service that manages SSL/TLS certificates for websites and other online services.\nOption B, server-side encryption managed by S3 (SSE-S3), provides encryption at rest for S3 objects using keys managed by AWS. SSE-S3 uses AES-256 encryption and provides automatic key rotation, making it easy to implement and manage. However, it does not provide detailed auditing capabilities to track who accessed the data and when.\nOption C, server-side encryption managed by the customer (SSE-C), allows customers to use their own encryption keys to encrypt data at rest in S3. SSE-C provides greater control over encryption keys and more granular access control but requires more management overhead. Similar to SSE-S3, SSE-C does not provide built-in auditing capabilities.\nOption D, server-side encryption managed by KMS (SSE-KMS), provides encryption at rest for S3 objects using keys managed by the AWS Key Management Service (KMS). SSE-KMS offers the benefits of SSE-S3 and SSE-C, including automatic key rotation and granular access control, while also providing detailed auditing capabilities through AWS CloudTrail.\nTherefore, the best option to satisfy the security team's requirements is D, Server-Side Encryption managed by KMS (SSE-KMS) with CloudTrail. This solution offers encryption at rest with keys managed by KMS and detailed monitoring of access to S3 objects through CloudTrail.\n\n"
}, {
  "id" : 256,
  "question" : "Which of the following services helps provide a connection from on-premises infrastructure to resources hosted in the AWS Cloud? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "158d2be8061e4629a4a7ad8365631589",
    "option" : "AWS VPC",
    "isCorrect" : "false"
  }, {
    "id" : "443f64434acd4ffdb3dcffa5b7608376",
    "option" : "AWS VPN",
    "isCorrect" : "true"
  }, {
    "id" : "87679abf425b4620aeec6acb586e9bf4",
    "option" : "AWS Direct Connect",
    "isCorrect" : "true"
  }, {
    "id" : "94cf23f6d2844d44a79bd80860098f34",
    "option" : "AWS Subnets.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B and C.\nThe AWS Documentation mentions the following.\nAWS Direct Connect makes it easy to establish a dedicated network connection from your premises to AWS.\nUsing AWS Direct Connect, you can establish private connectivity between AWS and your datacenter, office, or colocation environment, which in many cases can reduce your network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.\nYou can connect your VPC to remote networks by using a VPN connection.\nFor more information on AWS Direct Connect, please visit the link:\nhttps://aws.amazon.com/directconnect/?p=tile\nFor more information on AWS VPN, please visit the link:\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/vpn-connections.html\n\nThe two AWS services that help provide a connection from on-premises infrastructure to resources hosted in the AWS Cloud are:\nB. AWS VPN: AWS VPN (Virtual Private Network) is a service that enables users to establish a secure and encrypted connection between an on-premises network and a VPC (Virtual Private Cloud) in AWS. With AWS VPN, users can securely access their resources in the cloud as if they were connected to their local network.\nC. AWS Direct Connect: AWS Direct Connect is a service that provides a dedicated network connection between an on-premises network and AWS. It enables users to establish a private, high-bandwidth connection to AWS services, which can help reduce network costs, increase bandwidth throughput, and provide a more consistent network experience than Internet-based connections.\nA. AWS VPC: AWS VPC (Virtual Private Cloud) is a service that allows users to create an isolated virtual network within AWS. While it doesn't directly provide a connection from on-premises infrastructure to resources hosted in the AWS Cloud, it can be used in conjunction with AWS VPN or AWS Direct Connect to provide a secure and isolated environment for resources in the cloud.\nD. AWS Subnets: AWS Subnets are subdivisions of a VPC that allow users to segment their network into smaller, more manageable networks. While they don't directly provide a connection from on-premises infrastructure to resources hosted in the AWS Cloud, they can be used in conjunction with AWS VPN or AWS Direct Connect to create a more granular network topology in the cloud.\nIn summary, AWS VPN and AWS Direct Connect are the two services that directly help provide a connection from on-premises infrastructure to resources hosted in the AWS Cloud. AWS VPC and AWS Subnets can be used in conjunction with these services to provide a secure and isolated environment for resources in the cloud, as well as to create a more granular network topology.\n\n"
}, {
  "id" : 257,
  "question" : "Which of the following is a template that contains the software configuration to launch an ec2 instance?\n",
  "answers" : [ {
    "id" : "73505f76c53a4ae2b3a29929902d6cdc",
    "option" : "EBS Volumes",
    "isCorrect" : "false"
  }, {
    "id" : "688db17a53234a8882b2cd6a01760d41",
    "option" : "AMI",
    "isCorrect" : "true"
  }, {
    "id" : "e2c18c260d944ecc98b672bd09cae4d7",
    "option" : "EC2 Snapshot",
    "isCorrect" : "false"
  }, {
    "id" : "cb545174b1d54e0ab1d74779ad9eff09",
    "option" : "EBS Snapshot.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - B.\nThe AWS Documentation mentions the following.\nAn Amazon Machine Image (AMI) provides the information required to launch an instance, which is a virtual server in the cloud.\nYou specify an AMI when you launch an instance, and you can launch as many instances from the AMI as you need.\nYou can also launch instances from as many different AMIs as you need.\nFor more information on Amazon Machine Images, please refer to the following link:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html\n\nThe correct answer is B. AMI.\nAmazon Machine Image (AMI) is a pre-configured virtual machine image that contains the necessary information to launch an EC2 instance, including the operating system, application server, and application code. AMIs are used as templates to create new EC2 instances that have the same configuration as the original instance from which the AMI was created.\nWhen an EC2 instance is launched using an AMI, the instance is created with the same configuration as the original instance from which the AMI was created. This can include the operating system, software packages, libraries, and any other configuration settings that were applied to the original instance.\nEBS volumes, EC2 snapshots, and EBS snapshots are related to storage in AWS, but they do not contain the necessary software configuration to launch an EC2 instance.\nEBS volumes are block-level storage devices that can be attached to EC2 instances to provide additional storage capacity.\nEC2 snapshots are point-in-time backups of an EC2 instance's data, including the operating system, applications, and data files. These snapshots can be used to restore an instance to a previous state or to create new instances with the same data.\nEBS snapshots are similar to EC2 snapshots, but they are used to back up EBS volumes rather than EC2 instances.\nIn summary, an AMI is a template that contains the software configuration to launch an EC2 instance. It includes the operating system, application server, and application code, and is used to create new instances that have the same configuration as the original instance from which the AMI was created.\n\n"
}, {
  "id" : 258,
  "question" : "There is a requirement to host a set of servers in the Cloud for a short period of 3 months.\nThe instances should be highly available and cost-effective.\nThe instances can not be interrupted during processing.\nWhich of the following types of instances should be chosen?\n",
  "answers" : [ {
    "id" : "09677709266c42a69e81eb2228212313",
    "option" : "Spot Instances",
    "isCorrect" : "false"
  }, {
    "id" : "e3349d8c1c574d81a44349198e7f05fe",
    "option" : "On-Demand",
    "isCorrect" : "false"
  }, {
    "id" : "b97fcc649fc2409fa4ddb9c44fd22157",
    "option" : "No Upfront costs Reserved",
    "isCorrect" : "false"
  }, {
    "id" : "7c52081a5509498fa6b418e1693a5bfb",
    "option" : "Partial Upfront costs Reserved.",
    "isCorrect" : "false"
  } ],
  "explanations" : "\nOption B: On-Demand instances are instances that can be launched and terminated at any time, and you pay only for the hours or minutes that you use them. They are cost-effective for short-term usage but do not provide any guarantee of availability beyond the underlying infrastructure's general availability. On-Demand instances do not require any upfront payment or long-term commitment, making them a flexible option for short-term usage.\nOption C and D: Reserved instances are instances that you can reserve for a one-year or three-year term, providing you with a significant discount on the hourly rate. There are two types of Reserved instances, No Upfront costs Reserved and Partial Upfront costs Reserved, which require different payment structures upfront. They are more cost-effective than On-Demand instances for long-term usage, but they require upfront payment, and you commit to using them for a fixed term. Reserved instances offer availability guarantees, but they are not designed for short-term usage.\nOption A: Spot instances allow you to bid for unused EC2 capacity and can offer significant cost savings compared to On-Demand and Reserved instances. However, the availability of spot instances is not guaranteed, and they can be terminated at any time with only two minutes' notice. Spot instances are an excellent option for workloads that can handle interruptions and have the ability to be restarted if the instance is terminated. Spot instances are not recommended for applications that require continuous availability and cannot tolerate interruptions.\nGiven the requirement of hosting a set of servers in the cloud for a short period of 3 months, the instances should be highly available and cost-effective, and the instances cannot be interrupted during processing, the best option would be On-Demand instances. On-Demand instances offer high availability without any upfront payment or long-term commitment, making them the most flexible and cost-effective option for short-term usage. Reserved instances are designed for long-term usage, and Spot instances cannot guarantee continuous availability, making On-Demand instances the best choice for this use case.\n\n"
}, {
  "id" : 259,
  "question" : "Which three actions can Amazon Macie perform on the users' data with artificial intelligence (AI) and machine learning (ML)?\n",
  "answers" : [ {
    "id" : "c350070f83a6412cbdd21b510e95917a",
    "option" : "Discover, Monitor, Protect",
    "isCorrect" : "true"
  }, {
    "id" : "b35440533804432e973fd5fc7bcdb5a3",
    "option" : "Observe, Design, Alert",
    "isCorrect" : "false"
  }, {
    "id" : "b22c520795ee4a309b534a5452f92629",
    "option" : "Optimize, Alert, Secure",
    "isCorrect" : "false"
  }, {
    "id" : "e6b49d4454ec4bed89c9d6723d2c8d9b",
    "option" : "Detect, Discover, Alert.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer - A.\nAmazon Macie is a fully managed data security and data privacy service that uses machine learning and pattern matching to help you discover, monitor, and protect sensitive data in your AWS environment.\nAmazon Macie Classic is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS.\nMacie Classic is supported in the following AWS Regions:\nUS East (N.\nVirginia) (us-east-1)\nUS West (Oregon) (us-west-2)\nhttps://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html\nhttps://docs.aws.amazon.com/macie/latest/user/what-is-macie.html\nOption B is incorrect because observe, design, alert is not the main functional components of Amazon Macie.\nOption C is incorrect because optimize, alert, secure is not the main functional components of Amazon Macie.\nOption D is incorrect because detect, discover, alert are not the main functional components of Amazon Macie.\n\nAmazon Macie is a security service offered by AWS that uses AI and ML techniques to discover, classify, and protect sensitive data stored in S3 buckets. The service is capable of performing various actions to ensure the security of user data. Among the four options provided, the correct answer is A. Discover, Monitor, Protect.\nHere's an explanation of each action:\nDiscover - This action involves using AI and ML algorithms to automatically discover and classify sensitive data stored in S3 buckets. Amazon Macie scans the data in S3 buckets and identifies personally identifiable information (PII), financial data, healthcare information, and other sensitive data types based on predefined data patterns and machine learning models. Monitor - After discovering sensitive data, Amazon Macie continuously monitors the S3 buckets for any changes or potential threats. The service generates alerts and notifications whenever it detects any suspicious activity, such as unauthorized access or data exfiltration. Protect - Amazon Macie provides various protection mechanisms to secure the sensitive data stored in S3 buckets. For example, the service can automatically encrypt the data at rest and in transit, set access controls and permissions, and provide continuous monitoring and auditing of S3 bucket activity.\nOption B, Observe, Design, Alert, is incorrect as it doesn't accurately describe Amazon Macie's capabilities. While Amazon Macie does provide alerting and monitoring capabilities, it doesn't offer any design or observation features.\nOption C, Optimize, Alert, Secure, is also incorrect. While Amazon Macie does provide alerts and security features, it doesn't provide any optimization capabilities.\nOption D, Detect, Discover, Alert, is partially correct as Amazon Macie does provide discovery and alerting capabilities, but it doesn't provide any detect action.\n\n"
}, {
  "id" : 260,
  "question" : "Which of the following is not a disaster recovery deployment technique?\n",
  "answers" : [ {
    "id" : "da6ca64455d84677897321019ee66633",
    "option" : "Pilot light",
    "isCorrect" : "false"
  }, {
    "id" : "8649be8eb0054264b27c84d5c00b21cf",
    "option" : "Warm standby",
    "isCorrect" : "false"
  }, {
    "id" : "601429dd6dfb4c34ab780dbd8b495151",
    "option" : "Single Site",
    "isCorrect" : "true"
  }, {
    "id" : "cea051437afc40cdb44c24f7255ed5ed",
    "option" : "Multi-Site.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer - C.\nThe below snapshot from the AWS documentation shows the different disaster recovery techniques.\nFor more information on Disaster recovery techniques, please refer to the below URL:\nhttps://aws.amazon.com/blogs/aws/new-whitepaper-use-aws-for-disaster-recovery/\n\n\nDisaster recovery deployment techniques are used to ensure business continuity in the event of a disaster or outage. These techniques involve deploying backup resources and systems to restore business operations as quickly as possible.\nLet's go through each of the options listed and determine if they are a valid disaster recovery deployment technique or not:\nA. Pilot light: This is a disaster recovery deployment technique in which a minimal version of an application or system is always running in the cloud. The full application can be quickly deployed if a disaster occurs. This technique allows for a quick and cost-effective recovery in the event of a disaster.\nB. Warm standby: This is another disaster recovery deployment technique in which a duplicate of the primary system is running in the cloud. Data is replicated from the primary system to the standby system on a regular basis. If a disaster occurs, the standby system can be quickly activated to restore business operations.\nC. Single Site: This is not a disaster recovery deployment technique. Single site refers to a configuration in which all resources are located in a single data center or location. In the event of a disaster or outage, there is no backup or redundancy available to restore business operations.\nD. Multi-Site: This is a disaster recovery deployment technique in which resources are deployed across multiple data centers or locations. If one location becomes unavailable due to a disaster or outage, business operations can be restored from another location.\nTherefore, the correct answer is C. Single Site, which is not a valid disaster recovery deployment technique. The other options, Pilot Light, Warm Standby, and Multi-Site, are all valid disaster recovery deployment techniques.\n\n"
}, {
  "id" : 261,
  "question" : "A start-up organization is using the cost explorer tool to view and analyze its costs and usage.\nWhich of the below statements are correct with regards to the cost explorer tool? (Select TWO)\n",
  "answers" : [ {
    "id" : "c5d1f29a823b40c2b022ee1f64fda738",
    "option" : "Data is available for up to the last 24 months.",
    "isCorrect" : "false"
  }, {
    "id" : "20c8f2b7252b42b983e94cdd7106579f",
    "option" : "Provides a forecast of the likely spend for the next six months",
    "isCorrect" : "false"
  }, {
    "id" : "563fe5c4e158430087e1e228d52391a2",
    "option" : "Spot Instances to purchase are recommended",
    "isCorrect" : "false"
  }, {
    "id" : "73594db29e8a424388e8c2ed3d5b5c75",
    "option" : "Provides Usage-Based Forecasting",
    "isCorrect" : "true"
  }, {
    "id" : "44d03dec511c46e586b36f5b5e5ff5d8",
    "option" : "Provides trends that you can use to understand your costs.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D and E.\nOption A is INCORRECT.\nData is available for up to the last 12 months.\nOption B is INCORRECT.\nCost explorer provides Usage-Based Forecasting not a forecast for the next 12 months.\nOption C is INCORRECT.\nA recommendation is provided to purchase the Reserved instances.\nOption D is CORRECT as Cost explorer provides custom usage forecasts to gain a line of sight into your expected future costs.\nOption E is CORRECT.\nReference:\nhttps://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/ce-what-is.html\n\nThe cost explorer tool is a feature of AWS that helps customers visualize, understand and manage their AWS usage and costs. It provides a range of cost and usage reports, including trend data, to help identify cost drivers, track usage patterns and optimize costs.\nA. Data is available for up to the last 24 months. This statement is correct. The cost explorer tool provides data for up to the last 24 months, allowing customers to view their usage and cost trends over a longer period and make informed decisions about their AWS usage.\nB. Provides a forecast of the likely spend for the next six months This statement is incorrect. While the cost explorer tool does provide historical data and usage trends, it does not provide a forecast of the likely spend for the next six months.\nC. Spot Instances to purchase are recommended This statement is incorrect. The cost explorer tool does not provide recommendations on purchasing Spot Instances. Spot Instances are a type of EC2 instance that allows customers to bid on unused Amazon EC2 capacity, often resulting in significant cost savings.\nD. Provides Usage-Based Forecasting This statement is incorrect. While the cost explorer tool provides usage and cost trends, it does not provide usage-based forecasting.\nE. Provides trends that you can use to understand your costs. This statement is correct. The cost explorer tool provides a range of usage and cost trends that customers can use to understand their costs and usage patterns. By identifying cost drivers and optimizing usage, customers can reduce their AWS costs and improve their overall cost efficiency.\n\n"
}, {
  "id" : 262,
  "question" : "The project team requires an AWS service that provides a filesystem simultaneously mounted from different instances of EC2\nWhich AWS service will satisfy this requirement?\n",
  "answers" : [ {
    "id" : "00b556f72d714dea836e8d1e07276b3f",
    "option" : "Amazon EFS",
    "isCorrect" : "true"
  }, {
    "id" : "940c04b0e69e4c7bac82ecf6dc691064",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "4a3a329e9cb443de8c1fd858f0bd33c8",
    "option" : "Amazon EBS",
    "isCorrect" : "false"
  }, {
    "id" : "c72e168cbbfb4445a482e0291994fa90",
    "option" : "Amazon FSx for Windows File Server.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAmazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) for high availability and durability.\nYou can access your file systems across AZs and regions.\nYou can also share files between thousands of Amazon EC2 instances and on-premises servers via AWS Direct Connect or AWS VPN.\nOption B is INCORRECT.\nAmazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance.\nOption C is INCORRECT.\nAmazon Elastic Block Store (Amazon EBS) provides persistent block storage volumes for Amazon EC2 instances in the AWS Cloud.\nOption D is INCORRECT.\nAmazon FSx for Windows File Server provides a fully managed native Microsoft Windows file system so you can easily move your Windows-based applications that require file storage to AWS.\nReference:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-overview/storage-services.html\n\nThe AWS service that would satisfy the requirement of a filesystem that can be simultaneously mounted from different instances of EC2 is Amazon Elastic File System (EFS), option A.\nAmazon EFS is a scalable and fully managed cloud-based file storage service that provides a simple and scalable file system for use with Amazon EC2 instances. With EFS, multiple EC2 instances can access the same file system concurrently, providing a shared file storage solution that can be used for a variety of use cases.\nOption B, Amazon S3 (Simple Storage Service), is a highly scalable object storage service that is designed for storing and retrieving large amounts of data, but it is not a file system and is not designed for simultaneous access from multiple EC2 instances.\nOption C, Amazon Elastic Block Store (EBS), provides block-level storage volumes that can be attached to EC2 instances, but it is not a file system and cannot be simultaneously mounted from multiple EC2 instances.\nOption D, Amazon FSx for Windows File Server, is a fully managed Windows file system that is designed to provide shared file storage for Windows-based applications and workloads, but it is not designed for simultaneous access from multiple EC2 instances.\nTherefore, the correct answer is A, Amazon Elastic File System (EFS).\n\n"
}, {
  "id" : 263,
  "question" : "Which of the below statements is incorrect with regards to the advantages of moving to cloud?\n",
  "answers" : [ {
    "id" : "1a437c7925d34fd9a6d66117e2f9309b",
    "option" : "Trade variable expense for capital expense",
    "isCorrect" : "true"
  }, {
    "id" : "37e676db32b24c42a6ede59c5f038711",
    "option" : "Stop spending money running and maintaining data centres",
    "isCorrect" : "false"
  }, {
    "id" : "a2fbe4394e92422dbc58fdb79e95b11e",
    "option" : "Benefit from massive economies of scale",
    "isCorrect" : "false"
  }, {
    "id" : "2f2c562a0ece47caadb642162c1fb617",
    "option" : "Go global in minutes.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nThe statement is incorrect.\nThe correct statement is, â€œTrade capital expense for variable expenseâ€.\nOption B is INCORRECT.\nThe statement is correct.\nOption C is INCORRECT.\nThe statement is correct.\nOption D is INCORRECT.\nThe statement is correct.\nReference:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-overview/six-advantages-of-cloud-computing.html\n\nThe correct answer is (D) \"Go global in minutes\" is incorrect with regards to the advantages of moving to the cloud.\nExplanation: Moving to the cloud has several advantages that can benefit businesses of all sizes. Here's a detailed explanation of the three correct statements and why the incorrect statement is incorrect:\nA. Trade variable expense for capital expense: One of the biggest advantages of moving to the cloud is that it allows businesses to trade variable expenses (such as paying for hardware upgrades and maintenance) for capital expenses (such as paying for cloud services on a fixed monthly or annual basis). This allows businesses to plan their expenses more effectively and allocate resources more efficiently.\nB. Stop spending money running and maintaining data centres: Running and maintaining data centres can be expensive and time-consuming for businesses. By moving to the cloud, businesses can reduce or eliminate the need for on-premises hardware, which can save them money on maintenance and upgrades.\nC. Benefit from massive economies of scale: Cloud providers like Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) operate at massive scales, serving millions of customers worldwide. By leveraging this scale, businesses can benefit from lower costs, better performance, and greater scalability than they could achieve on their own.\nD. Go global in minutes: This statement is incorrect because while cloud providers do offer global infrastructure and services, it still takes time to deploy and configure resources in different regions. It is not a matter of simply pressing a button and being able to operate in any location. Additionally, some countries have regulations and restrictions on data residency and privacy that must be taken into consideration.\nIn summary, while moving to the cloud can provide many advantages for businesses, it is important to understand the limitations and requirements associated with cloud infrastructure and services.\n\n"
}, {
  "id" : 264,
  "question" : "Project team enhancing the security features of a banking application, requires implementing a threat detection service that continuously monitors malicious activities and unauthorized behaviors to protect AWS accounts, workloads, and data stored in Amazon S3\nWhich AWS services should the project team select?\n",
  "answers" : [ {
    "id" : "deec0107510c44938c95319078fae991",
    "option" : "AWS Shield",
    "isCorrect" : "false"
  }, {
    "id" : "3741ad800c94473fa57dd00a2ff3c26f",
    "option" : "AWS Firewall Manager",
    "isCorrect" : "false"
  }, {
    "id" : "a85738fd62384e9c93e6552f0ea1206c",
    "option" : "Amazon GuardDuty",
    "isCorrect" : "true"
  }, {
    "id" : "503ea5539585430796b51a5b0b06c58a",
    "option" : "Amazon Inspector.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT.\nAWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\nOption B is INCORRECT.\nAWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall rules across your accounts and applications in AWS Organization.\nOption C is CORRECT.\nAmazon GuardDuty is a threat detection service that continuously monitors malicious activities and unauthorized behaviors to protect your AWS accounts, workloads, and data stored in Amazon S3.\nOption D is INCORRECT.\nAmazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS.\nReference:\nhttps://aws.amazon.com/guardduty/\nhttps://aws.amazon.com/firewall-manager/\nhttps://aws.amazon.com/shield/\nhttps://aws.amazon.com/inspector/\n\nThe project team is enhancing the security features of a banking application and wants to implement a threat detection service that continuously monitors malicious activities and unauthorized behaviors to protect AWS accounts, workloads, and data stored in Amazon S3. To achieve this, the team needs to select the right AWS services that can help them monitor and protect their infrastructure effectively.\nLet's take a look at the four possible options provided in the answer choices:\nA. AWS Shield - AWS Shield is a managed service that provides protection against Distributed Denial of Service (DDoS) attacks. While it's a useful service for protecting against DDoS attacks, it doesn't provide the continuous monitoring and threat detection capabilities that the project team is looking for. Hence, it is not the right choice in this scenario.\nB. AWS Firewall Manager - AWS Firewall Manager is a service that makes it easy to centrally configure and manage AWS WAF (Web Application Firewall) rules across your accounts and applications. While it's a useful service for managing firewall rules, it doesn't provide the continuous monitoring and threat detection capabilities that the project team is looking for. Hence, it is not the right choice in this scenario.\nC. Amazon GuardDuty - Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect AWS accounts, workloads, and data stored in Amazon S3. It uses machine learning, anomaly detection, and integrated threat intelligence to identify and prioritize security findings. It also provides detailed findings and integrates with AWS security services for automated remediation. This service aligns perfectly with the project team's requirements and is the right choice in this scenario.\nD. Amazon Inspector - Amazon Inspector is an automated security assessment service that helps improve the security and compliance of applications deployed on AWS. It analyzes the behavior of applications to identify potential security issues, vulnerabilities, and deviations from best practices. While it's a useful service for automated security assessments, it doesn't provide the continuous monitoring and threat detection capabilities that the project team is looking for. Hence, it is not the right choice in this scenario.\nTherefore, the project team should select Amazon GuardDuty as the AWS service to implement the continuous monitoring and threat detection capabilities required to protect their AWS accounts, workloads, and data stored in Amazon S3.\n\n"
}, {
  "id" : 265,
  "question" : "Which of the following support plans offer 24*7 technical support via phone, email, and chat access to Cloud Support Engineers? (Select TWO.)\n",
  "answers" : [ {
    "id" : "739f7253f4bf4fae978e5296eaf515f4",
    "option" : "Basic",
    "isCorrect" : "false"
  }, {
    "id" : "c57841a1b5204033acc3ee13f0e9a54a",
    "option" : "Developer",
    "isCorrect" : "false"
  }, {
    "id" : "e9362eaa827b4f52aebe8aff736de6a8",
    "option" : "Business",
    "isCorrect" : "true"
  }, {
    "id" : "34899b352437436cbdee5f5e1005f947",
    "option" : "Premium",
    "isCorrect" : "false"
  }, {
    "id" : "a1155c1be01c4988938aed5f1270e24c",
    "option" : "Enterprise.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: C and E.\nOption A is INCORRECT because 24*7 technical support via phone, email, and chat access to Cloud Support Engineers is not available in Basic Plan.\nOption B is INCORRECT because 24*7 technical support via phone, email, and chat access to Cloud Support Engineers is not available in the Developer Plan.\nOption C is CORRECT.24*7 technical support via phone, email, and chat access to Cloud Support Engineers is available in Business Plan.\nOption D is INCORRECT.\nThere is no Premium plan.\nOption E is CORRECT.\n24*7 technical support via phone, email, and chat access to Cloud Support Engineers is available in Enterprise Plan.\nReference:\nhttps://aws.amazon.com/premiumsupport/plans/\n\nThe two AWS Support plans that offer 24x7 technical support via phone, email, and chat access to Cloud Support Engineers are:\nD. Premium E. Enterprise.\nHere's a detailed explanation of each AWS Support plan:\nA. Basic: The Basic support plan is included with all AWS accounts for free. It provides 24/7 access to customer service, documentation, whitepapers, and support forums. However, it does not include technical support, and response times for non-technical inquiries can be up to 12 hours.\nB. Developer: The Developer support plan is designed for developers and startups that are building and testing new applications. It includes technical support during business hours only (8am-6pm), with a response time of up to 12 hours. It also includes basic AWS Trusted Advisor checks.\nC. Business: The Business support plan is designed for small and medium-sized businesses that are running production workloads on AWS. It includes 24/7 technical support with a response time of up to 1 hour for critical issues. It also includes AWS Trusted Advisor checks, and a dedicated Technical Account Manager (TAM).\nD. Premium: The Premium support plan is designed for large enterprises that are running mission-critical workloads on AWS. It includes 24/7 technical support with a response time of up to 15 minutes for critical issues. It also includes AWS Trusted Advisor checks, a dedicated TAM, and infrastructure event management.\nE. Enterprise: The Enterprise support plan is designed for large enterprises with complex IT environments that require a high level of technical support. It includes 24/7 technical support with a response time of up to 15 minutes for critical issues. It also includes AWS Trusted Advisor checks, a dedicated TAM, infrastructure event management, and support for custom applications.\nIn summary, the Premium and Enterprise support plans offer 24/7 technical support via phone, email, and chat access to Cloud Support Engineers, with a response time of up to 15 minutes for critical issues.\n\n"
}, {
  "id" : 266,
  "question" : "Which AWS product provides a unified user interface, enabling easy management of software development activities in one place, along with, quick development, build, and deployment of applications on AWS?\n",
  "answers" : [ {
    "id" : "4e053b3da8f246e39db0cf04155f0f61",
    "option" : "Amazon CodeGuru.",
    "isCorrect" : "false"
  }, {
    "id" : "ff7dc53d093d4a9a89dad17f494429f6",
    "option" : "AWS CodeBuild",
    "isCorrect" : "false"
  }, {
    "id" : "67e4df93990749e983880e925cf5c514",
    "option" : "AWS CodeArtifact",
    "isCorrect" : "false"
  }, {
    "id" : "7a8ac50be7cb4995b26577f9302e4ba0",
    "option" : "AWS CodeStar.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nAmazon CodeGuru is a developer tool powered by machine learning that provides intelligent recommendations for improving code quality and identifying an application's most expensive lines of code.\nOption B is INCORRECT.\nAWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy.\nOption C is INCORRECT.\nAWS CodeArtifact is a fully managed artifact repository service that makes it easy for organizations of any size to securely store, publish, and share software packages used in their software development process.\nOption D is CORRECT.\nAWS CodeStar enables you to develop, build, and deploy applications on AWS quickly.\nAWS CodeStar provides a unified user interface, enabling you to manage your software development activities in one place easily.\nReference:\nhttps://aws.amazon.com/codeguru/\nhttps://aws.amazon.com/codeartifact/\nhttps://aws.amazon.com/codebuild/\nhttps://aws.amazon.com/codestar/\n\nThe correct answer is D. AWS CodeStar.\nAWS CodeStar is a fully managed service that provides a unified user interface for developing, building, and deploying applications on AWS. With AWS CodeStar, developers can quickly create, manage, and deploy applications on AWS using a variety of popular programming languages, including Java, Python, Ruby, and Node.js.\nAWS CodeStar offers an integrated development environment (IDE) for creating and editing code, as well as tools for building and deploying applications. It supports multiple AWS services, such as AWS Lambda, Amazon EC2, Amazon S3, Amazon DynamoDB, and Amazon RDS, enabling developers to easily create and deploy applications that use these services.\nAWS CodeStar provides pre-configured project templates, which can be customized to meet specific application requirements. These templates include sample code, configuration files, and deployment scripts, allowing developers to quickly get started with application development.\nAWS CodeStar also provides project management tools that allow developers to easily collaborate on projects and manage project resources. This includes features such as issue tracking, project dashboards, and team management tools.\nIn summary, AWS CodeStar provides a comprehensive set of tools and services that enable developers to easily develop, build, and deploy applications on AWS, all from a single, unified user interface.\n\n"
}, {
  "id" : 267,
  "question" : "__________________ automates the discovery of sensitive data at scale and lowers the cost of protecting your data using machine learning and pattern matching techniques.\n",
  "answers" : [ {
    "id" : "cebd0a3c7f1140b1972d4925e5404d89",
    "option" : "Amazon Macie",
    "isCorrect" : "true"
  }, {
    "id" : "cda115e43da44355abaf951f0c65e012",
    "option" : "AWS Shield",
    "isCorrect" : "false"
  }, {
    "id" : "7a60006768d640bab4d36780e0332cd2",
    "option" : "Amazon GuardDuty",
    "isCorrect" : "false"
  }, {
    "id" : "8b1e113841a646a6852a101de2416938",
    "option" : "AWS Security Hub.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAmazon Macie automates the discovery of sensitive data at scale and lowers the cost of protecting your data.\nOption B is INCORRECT.\nAWS Shield is a managed Distributed Denial of Service (DDoS) protection service that safeguards applications running on AWS.\nOption C is INCORRECT.\nAmazon GuardDuty is a threat detection service that continuously monitors the malicious activities and unauthorized behaviors to protect your AWS accounts, workloads, and data stored in Amazon S3.\nOption D is INCORRECT.\nAWS Security Hub gives you a comprehensive view of your high-priority security alerts and security posture across your AWS accounts.\nReferences:\nhttps://aws.amazon.com/macie/\nhttps://aws.amazon.com/shield/\nhttps://aws.amazon.com/guardduty/\nhttps://aws.amazon.com/security-hub/\n\nThe correct answer is A. Amazon Macie.\nAmazon Macie is a fully managed data security and privacy service that uses machine learning and pattern matching techniques to automate the discovery of sensitive data at scale, and help lower the cost of protecting your data. Amazon Macie works by continuously monitoring your data in Amazon S3 and other supported AWS data stores for sensitive data, such as personally identifiable information (PII), credit card numbers, and intellectual property. When Amazon Macie detects sensitive data, it alerts you with a comprehensive report that includes the location of the sensitive data, the type of sensitive data, and the risk level associated with the data.\nAmazon Macie can also help you to comply with various data privacy regulations, such as GDPR, by identifying where sensitive data resides within your AWS environment. Additionally, Amazon Macie provides visualizations and reports that can help you to understand the access patterns and usage of your data, so you can identify potential security risks or compliance issues.\nIn summary, Amazon Macie helps to automate the discovery of sensitive data, lowers the cost of protecting your data using machine learning and pattern matching techniques, and provides comprehensive reports and visualizations to help you maintain compliance and identify potential security risks.\n\n"
}, {
  "id" : 268,
  "question" : "Security and Compliance is a shared responsibility between AWS and the customer.\nWhich amongst the below-listed options are AWS responsibilities?(Select TWO.)\n",
  "answers" : [ {
    "id" : "7a168bf220a54e8982426abcd497d59e",
    "option" : "Perform all the necessary security configuration and management tasks for Amazon Elastic Compute Cloud (Amazon EC2).",
    "isCorrect" : "false"
  }, {
    "id" : "36e745234c5542749d2fec72b92c4bfd",
    "option" : "Patch management of the guest OS and applications",
    "isCorrect" : "false"
  }, {
    "id" : "ee4e246f40d44ccb810865a6c7731a71",
    "option" : "Security of the data in the AWS cloud",
    "isCorrect" : "false"
  }, {
    "id" : "f665fa0881d44820b1db120f9256956c",
    "option" : "Security of the AWS cloud",
    "isCorrect" : "true"
  }, {
    "id" : "9b364b65ede64431b3b4b46f74f797b5",
    "option" : "Patch management within the AWS infrastructure.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D and E.\nOption A is INCORRECT because Amazon Elastic Compute Cloud (Amazon EC2) is categorized as Infrastructure as a Service (IaaS)\nHence this is the customer's responsibility.\nOption B is INCORRECT because AWS is responsible for patching and fixing flaws within the infrastructure.\nBut customers are responsible for patching their guest OS and applications.\nOption C is INCORRECT as Security of the data in the cloud is the customer's responsibility.\nOption D is CORRECT as security of the cloud is AWS's responsibility.\nOption E is CORRECT.\nAWS is responsible for patching and fixing flaws within the infrastructure.\nReference:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/\n\nAccording to the shared responsibility model of AWS, security and compliance are shared between AWS and the customer. AWS is responsible for the security of the cloud infrastructure, which includes physical security, network security, and hypervisor security. On the other hand, the customer is responsible for the security of their applications, data, and operating systems running in the cloud.\nOut of the given options, the two AWS responsibilities are:\nD. Security of the AWS cloud: This responsibility includes the security of the cloud infrastructure, which includes physical security of AWS data centers, network security to protect against network-based attacks, and hypervisor security to ensure the virtualization layer is secure.\nE. Patch management within the AWS infrastructure: AWS is responsible for maintaining and patching the underlying infrastructure components such as compute, storage, and networking. This responsibility includes regular patching and updates of the AWS infrastructure to ensure that it remains secure and compliant.\nOption A is not an AWS responsibility as the customer is responsible for the security configuration and management tasks for their Amazon EC2 instances. Option B is also not an AWS responsibility as the customer is responsible for the patch management of their guest OS and applications. Option C is not entirely an AWS responsibility as data security is a shared responsibility, where the customer is responsible for protecting their data while AWS provides the necessary tools and services to secure the data within their infrastructure.\nIn summary, AWS is responsible for the security of the cloud infrastructure, while the customer is responsible for securing their applications, data, and operating systems running in the cloud.\n\n"
}, {
  "id" : 269,
  "question" : "Most up-to-the-minute information on AWS service availability could be determined from?\n",
  "answers" : [ {
    "id" : "cd94fe729fa74120b01f4c6be68d49b2",
    "option" : "AWS Personal Health Dashboard",
    "isCorrect" : "false"
  }, {
    "id" : "bcc857265c0b48f280a64f640674ea3a",
    "option" : "AWS Service health dashboard",
    "isCorrect" : "true"
  }, {
    "id" : "3531dc5136d44f508dd107ed75df32c2",
    "option" : "Amazon Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "02cf2e968c954d5a8e11854d3bf8c321",
    "option" : "AWS Control Tower.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT because AWS Personal Health Dashboard provides alerts and remediation guidance when AWS experiences events that may impact you.\nOption B is CORRECT, as AWS publishes most up-to-the-minute information on AWS service availability here.\nOption C is INCORRECT because Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers.\nOption D is INCORRECT as AWS Control Tower is the easiest way to set up and govern a new, secure multi-account AWS environment.\nReference:\nhttps://aws.amazon.com/premiumsupport/technology/personal-health-dashboard/\nhttps://status.aws.amazon.com/\nhttps://aws.amazon.com/controltower/\n\nThe most up-to-the-minute information on AWS service availability can be determined from the AWS Service Health Dashboard, which provides a real-time view of the health of AWS services.\nThe AWS Service Health Dashboard provides an overview of the current status of AWS services and regions, including any ongoing or resolved issues, scheduled maintenance, and upcoming events that may affect service availability. This dashboard is updated in real-time to reflect the current state of AWS services and regions.\nIn addition to the Service Health Dashboard, AWS also provides the AWS Personal Health Dashboard, which provides personalized alerts and insights into events that may affect your AWS resources. This dashboard can help you proactively manage your resources and minimize downtime or disruption.\nAmazon CloudWatch is a monitoring and management service that provides data and insights into the performance of AWS resources and applications. It can be used to monitor AWS services and resources in real-time, and can trigger alerts and notifications based on predefined thresholds or conditions.\nAWS Control Tower is a service that helps organizations set up and govern a secure, multi-account AWS environment. While it provides visibility into the status of resources and accounts within an organization, it is not specifically designed to provide real-time information on AWS service availability.\nOverall, while all of these tools and services can be useful for managing and monitoring AWS resources, the AWS Service Health Dashboard is the most up-to-date and comprehensive source for real-time information on AWS service availability.\n\n"
}, {
  "id" : 270,
  "question" : "Applications are installed on Amazon EC2 instances in which an IAM role is configured.\nWhich of the following services provides temporary security credentials for the applications to access to other AWS resources?\n",
  "answers" : [ {
    "id" : "eb5eb9f022004bd6aa6112f19c3b7be8",
    "option" : "AWS IAM user",
    "isCorrect" : "false"
  }, {
    "id" : "b2e474b5c7e4434088f24f9e66f8ad9e",
    "option" : "Amazon Cognito",
    "isCorrect" : "false"
  }, {
    "id" : "9c7877eb67f3487086dad4ad3a5ab2ca",
    "option" : "AWS IAM groups",
    "isCorrect" : "false"
  }, {
    "id" : "66a50548b23a4e0fb42fc5bcde5d6feb",
    "option" : "AWS STS.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer : D.\nApplications deployed on Amazon EC2 instance can be provided security credentials using AWS STS, allowing short-term limited period credentials.\nWith this, there is no need to save credentials in the Amazon EC2 instance.\nOptions A &amp; C are incorrect as the IAM users &amp; IAM group can create authentication &amp; manage access for users accessing AWS services.\nSaving these credentials on EC2 for applications while accessing other AWS resources is against security guidelines.\nOption B is incorrect as Amazon Cognito helps to provide user access control for mobile &amp; web apps.\nFor more information on use cases for AWS STS, refer to the following URL:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html\n\nThe correct answer is D. AWS STS.\nAWS STS (Security Token Service) is a web service that enables you to request temporary, limited-privilege credentials for AWS services. These temporary credentials can be used to access AWS resources from applications running on Amazon EC2 instances with an IAM role configured.\nWhen a request for temporary credentials is made, STS verifies the identity of the requestor and returns a set of temporary security credentials, including an access key ID, a secret access key, and a security token. The temporary security credentials have limited permissions and a short lifespan, typically one hour, after which they expire and can no longer be used.\nThe other options are not correct because:\nA. AWS IAM user: An IAM user is a permanent identity that you create in AWS to represent a person or application that interacts with AWS services. IAM users have long-term credentials that consist of an access key ID and a secret access key, which do not provide temporary access.\nB. Amazon Cognito: Amazon Cognito is a user authentication and authorization service. It does not provide temporary credentials for accessing other AWS resources.\nC. AWS IAM groups: An IAM group is a collection of IAM users. Like IAM users, IAM groups have long-term credentials that do not provide temporary access.\n\n"
}, {
  "id" : 271,
  "question" : "An industry regulatory body requires a healthcare insurance company to fully control cryptographic key management locally to ensure the safeguarding of sensitive patient data.\nHow can the organization achieve this, given that all their workloads are in the cloud?\n",
  "answers" : [ {
    "id" : "b20311e0da62445887743605081e68d7",
    "option" : "AWS Key Management Service (KMS)",
    "isCorrect" : "false"
  }, {
    "id" : "f4a8db2f979f46fb886eb9aa69c97655",
    "option" : "AWS Certificate Manager (ACM)",
    "isCorrect" : "false"
  }, {
    "id" : "9b9e6bb7313d4ab9800eb086ce3b3862",
    "option" : "AWS CloudHSM",
    "isCorrect" : "true"
  }, {
    "id" : "d807257733494ba194614da8d4a69d20",
    "option" : "Server-Side Encryption (SSE)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer:C.\nAWS CloudHSM allows the administrator to have full and exclusive control over the generation and management of cryptographic keys on actual hardware security modules that are physically stored in AWS data centers.\nhttps://aws.amazon.com/cloudhsm/\nOption A is INCORRECT because AWS Key Management Service (KMS) is a fully-managed AWS service.\nThis makes the service an ineligible option since the prerequisite of the use case is for the company to retain full control and administrator of the keys.\nOption B is INCORRECT because AWS Certificate Manager (ACM) is primarily for issuing verified security certificates for SSL/HTTPS on AWS resources such as Elastic Load Balancer or Web Application Firewall (WAF).\nOption D is INCORRECT because server-side encryption (SSE) is typically encryption of objects or data within the bucket in Amazon S3\nThe data is encrypted at the object level as it is saved on AWS storage infrastructure and then decrypted when it is accessed.\n\nThe best option for the healthcare insurance company to fully control cryptographic key management locally and safeguard sensitive patient data in the cloud is to use AWS CloudHSM (C).\nAWS CloudHSM is a service that allows organizations to generate and manage their own encryption keys for use with AWS services and applications. It provides dedicated hardware security modules (HSMs) that are designed to meet the highest regulatory and compliance requirements for cryptographic key storage and management.\nWith CloudHSM, the healthcare insurance company can deploy a cluster of HSMs in their own Amazon Virtual Private Cloud (VPC) and retain full control over their encryption keys. This ensures that the keys never leave the organization's control and can only be accessed by authorized personnel.\nAdditionally, AWS CloudHSM supports industry standard cryptographic protocols such as Advanced Encryption Standard (AES) and Secure Hash Algorithm (SHA), providing a high level of security for patient data.\nWhile AWS Key Management Service (KMS) (A) and Server-Side Encryption (SSE) (D) also offer encryption key management services, they do not provide the level of control and assurance that CloudHSM offers, especially in highly regulated industries like healthcare.\nAWS Certificate Manager (ACM) (B) is not directly related to cryptographic key management, but rather it provides a way to manage SSL/TLS certificates for secure communication between clients and servers.\n\n"
}, {
  "id" : 272,
  "question" : "For heterogeneous database migrations from on-premise to AWS, which of the following services is used by AWS Database Migration Service?\n",
  "answers" : [ {
    "id" : "dbe4a35fc5664343933bf48a9a993936",
    "option" : "AWS Application Discovery Service",
    "isCorrect" : "false"
  }, {
    "id" : "d346017574a348b9a88129664ff790e4",
    "option" : "AWS Server Migration Service",
    "isCorrect" : "false"
  }, {
    "id" : "1b2eb50bcba54e039c81f4852923a1f3",
    "option" : "AWS Schema Conversion Tool",
    "isCorrect" : "true"
  }, {
    "id" : "d90969df65014037be7d29ddf302b7b0",
    "option" : "AWS Migration Hub.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nAWS Database Migration Service uses AWS Schema Conversion tool for heterogeneous database migrations where source database &amp; destination database is different.\nThis tool converts source database format compatible with that of destination database format.\nOption A is incorrect as AWS Application Discovery service enables customers to collect data, including configuration, usage &amp; behavior of servers from existing IT environments.\nThis data can be used while migrating these services to AWS.\nOption B is incorrect as the AWS Server migration service helps automate the process for migrating large numbers of servers from on-premise to AWS.\nOption D is incorrect as the AWS migration hub is a tool to monitor application migration from on-premise to different AWS Regions.\nFor more information on AWS Database Migration Service, refer to the following URL:\nhttps://aws.amazon.com/dms/faqs/?nc=sn&amp;loc=6\n\nThe correct answer is C. AWS Schema Conversion Tool.\nAWS Database Migration Service (AWS DMS) is a fully-managed service that makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. AWS DMS helps you migrate your data to and from most widely used commercial and open-source databases such as Oracle, MySQL, PostgreSQL, Microsoft SQL Server, MariaDB, Amazon Aurora, and more.\nAWS Schema Conversion Tool (AWS SCT) is a standalone application that helps you migrate your database schema from one database engine to another. It supports homogeneous migrations (migrations between the same type of database engines) and heterogeneous migrations (migrations between different types of database engines).\nWhen performing a heterogeneous migration with AWS DMS, you use AWS SCT to convert your database schema and code to a format that's compatible with the target database engine. AWS SCT analyzes your source schema and code, and provides recommendations for converting it to the target database engine. You can use AWS SCT to convert your schema and code for many popular database engines, including Oracle, Microsoft SQL Server, PostgreSQL, and MySQL.\nAWS Application Discovery Service is used to help you plan your migration to AWS by identifying your on-premises applications, their dependencies, and their performance characteristics. AWS Server Migration Service is used to automate the migration of your on-premises virtual machines to AWS. AWS Migration Hub provides a central place to track and manage your application migrations to AWS.\n\n"
}, {
  "id" : 273,
  "question" : "Based on the AWS Well-Architected Framework, how should a start-up company with a dynamic AWS environment manage its users and resources securely without affecting the cost? Select (TWO)\n",
  "answers" : [ {
    "id" : "a025da6fcb7b4a648591b8147425c469",
    "option" : "Create multiple unique IAM users with administrator access for each functional group of the company.",
    "isCorrect" : "false"
  }, {
    "id" : "6091a74c5cee404d9a703d8fcc5244d2",
    "option" : "Use of AWS CloudFront template versions and revision controls to keep track of the dynamic configuration changes.",
    "isCorrect" : "false"
  }, {
    "id" : "baed8d68fc034c29b9c96d36d9fb94ab",
    "option" : "Use of AWS Organizations with respective OUs that differentiate billing across the companyâ€™s functions.",
    "isCorrect" : "true"
  }, {
    "id" : "f159546546f54d9ab1b7d6ae5c403f25",
    "option" : "Implement the most stringent security measures on the VPC-edge rather than on the resource hosts.",
    "isCorrect" : "true"
  }, {
    "id" : "2c2afa60536c439b8e395e4f4497d897",
    "option" : "Provisioning of resources and compute capacity that accommodates future growth.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C and D.\nOption A is INCORRECT because creating multiple IAM users with administrator privileges is not a best practice.\nRights and privileges should be assigned on a least privileged basis.\nOption B is INCORRECT because AWS CloudFront service is a content distribution service that does not have configuration templates, neither does it perform this function.\nThis is the function of AWS CloudFormation.\nOption C is CORRECT Based on the Cost and Optimization pillar, a focal area is the analysis of cost within the AWS environment and how they are distributed within functional groups or departments of the company.\nAnd AWS Organization provides composite billing into a single account.\nOption D is CORRECT Based on the Security pillar, it is advantageous to filter unwanted traffic at VPC-edge rather than on the hosts.\nIt is a best practice to drop undesirable packets before they enter the AWS.\nOption E is INCORRECT because provisioning of resources and compute capacity that accommodates future growth means that there is a capacity that is idle in anticipation of long-term growth.\nAccording to the AWS Well-Architected Framework reliability pillar, this is not cost-effective; the company should only pay for what is being used.\nReferences.\nhttps://wa.aws.amazon.com/wat.pillar.costOptimization.en.html\nhttps://docs.aws.amazon.com/wellarchitected/latest/security-pillar/wellarchitected-security-pillar.pdf\n\nBased on the AWS Well-Architected Framework, managing users and resources securely while minimizing costs requires a comprehensive approach. The framework outlines several best practices that companies can follow to achieve this goal. However, for the given scenario of a start-up company with a dynamic AWS environment, the two most relevant solutions are:\nC. Use of AWS Organizations with respective OUs that differentiate billing across the company's functions: AWS Organizations is a service that helps to consolidate multiple AWS accounts into an organization that can be centrally managed. Organizations provide a set of policy-based controls for managing billing, security, and compliance across multiple accounts. A start-up company with a dynamic AWS environment can use AWS Organizations to group its resources into organizational units (OUs) based on business functions, projects, or teams. This way, the company can easily track and allocate its AWS spending by function or team without compromising security.\nE. Provisioning of resources and compute capacity that accommodates future growth: A start-up company with a dynamic AWS environment needs to provision its resources and compute capacity based on expected growth. To minimize costs, it is important to avoid over-provisioning or under-provisioning of resources. Companies can use services like AWS Auto Scaling to automatically scale their resources up or down based on demand. This way, the company can ensure that it has enough resources to handle expected growth without wasting money on idle resources.\nA, B, and D are not the most relevant solutions in this scenario because:\nA. Creating multiple unique IAM users with administrator access for each functional group of the company is not a good practice because it can lead to increased administrative overhead and higher security risks. It is better to follow the principle of least privilege and give users only the permissions they need to perform their jobs.\nB. Using AWS CloudFront template versions and revision controls to keep track of the dynamic configuration changes is more relevant for managing web content delivery than for managing users and resources securely.\nD. Implementing the most stringent security measures on the VPC-edge rather than on the resource hosts is not the best practice because it can lead to increased complexity and lower performance. Instead, companies should implement security measures at multiple layers, including the VPC-edge, resource hosts, and applications, to achieve defense in depth.\n\n"
}, {
  "id" : 274,
  "question" : "Which pillar of the AWS Well-Architected Framework focuses on maintaining computing efficiency using serverless architectures?\n",
  "answers" : [ {
    "id" : "fd85baca4b994540a02f7eeaba9e1c53",
    "option" : "Performance Efficiency pillar",
    "isCorrect" : "true"
  }, {
    "id" : "1fcfe6b21f9e4effac201c7999ca7616",
    "option" : "BigData Management and Operations pillar",
    "isCorrect" : "false"
  }, {
    "id" : "1343f8446c7f459792dbe825e4765c74",
    "option" : "Information and Reliability pillar",
    "isCorrect" : "false"
  }, {
    "id" : "55e2636b55c74eb4a40cea2e8877c2dd",
    "option" : "Operational excellence pillar.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nThe Performance Efficiency pillar is one of the five AWS Well-Architected Framework pillars.\nUnder this pillar, the focus is on the use of collected data, computing resources and performance metrics to accurately meet the system's requirements.\nThis extends to making informed decisions that improve the efficiency of the organization's implemented system within the AWS environment and meet the demands of technological evolution and changes.\nOption B is INCORRECT because there is no BigData Management and Operations pillar within the five pillars of the Well-Architected Framework.\nOption C is INCORRECT because Information and Reliability are not one of the pillars in the AWS Well-Architected Framework.\nOption D is INCORRECT because the Operational excellence pillar describes how operational processes within the AWS environment can be enhanced to deliver business value.\nUnder this pillar, fundamental areas are responsiveness, enactment of operational standards, and automated processes to champion daily operations.\nhttps://d1.awsstatic.com/whitepapers/architecture/AWS-Performance-Efficiency-Pillar.pdf?ref=wellarchitected-ws\n\nThe correct answer is A. Performance Efficiency pillar.\nThe AWS Well-Architected Framework is a set of best practices and guidelines that help architects and developers design and operate reliable, secure, efficient, and cost-effective systems in the cloud. The framework consists of five pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization.\nThe Performance Efficiency pillar focuses on using computing resources efficiently to meet system requirements and to maintain performance as demand changes. Serverless architectures are a key component of this pillar because they allow you to run your code without provisioning or managing servers. This eliminates the need to maintain and scale infrastructure, and reduces operational costs.\nServerless architectures are based on event-driven computing, where code is executed in response to an event. AWS offers several serverless services, such as AWS Lambda, AWS Step Functions, and Amazon API Gateway, that enable you to build serverless applications and workflows that are scalable, reliable, and cost-effective.\nBy leveraging serverless architectures, you can optimize the performance of your systems, reduce operational overhead, and achieve higher levels of agility and scalability. In addition, serverless architectures are inherently fault-tolerant, as they distribute workloads across multiple availability zones and automatically handle scaling and capacity management.\nTherefore, the Performance Efficiency pillar of the AWS Well-Architected Framework is the one that focuses on maintaining computing efficiency using serverless architectures.\n\n"
}, {
  "id" : 275,
  "question" : "Which of the following tools can be used to check service limits for resources launched within AWS Cloud Infrastructure?\n",
  "answers" : [ {
    "id" : "6757ed2d68a4427aae91b45956313105",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "76963c8b83ef4eb1a05d9cab3606b581",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "true"
  }, {
    "id" : "663c09d640c542f6af41e1ce7d98467e",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "1da038da6b53451ea9f71c24b9fb2de9",
    "option" : "AWS CloudTrail.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nAWS Trusted Advisor checks for service usage for all the resources within AWS Cloud and provides notifications.\nOption A is incorrect as AWS Config can be used to audit, evaluate configurations of AWS resources.\nBut it does not check service limits for resources.\nOption C is incorrect as Amazon CloudWatch monitors AWS resources and applications on these resources.\nBut it does not check service limits for resources.\nOption D is incorrect as AWS CloudTrail is a logging service, recording activity made to AWS resources.\nBut it does not check service limits for resources.\nFor more information on AWS Trusted Advisor, refer to the following URL:\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/\n\nThe correct answer is B. AWS Trusted Advisor.\nAWS Trusted Advisor is a tool that provides recommendations to optimize AWS resources, improve security, and increase performance and reliability of AWS services. It also checks the usage and limits of AWS resources and services, and provides suggestions to stay within the limits and avoid unnecessary costs.\nWhen an AWS account is created, there are limits set for the number of resources that can be launched within a particular AWS service. These limits are known as service quotas or service limits. For example, there are limits for the number of Amazon Elastic Compute Cloud (EC2) instances that can be launched, the number of Amazon Relational Database Service (RDS) instances that can be launched, etc.\nAWS Trusted Advisor can check the usage and limits of AWS resources and services, and provide recommendations to stay within the limits. It can also provide suggestions to increase the limits if necessary. This helps to avoid service disruptions and unexpected charges due to exceeding the limits.\nAWS Config is a service that provides a detailed inventory of AWS resources, their configuration history, and configuration changes. It does not provide information about service limits.\nAmazon CloudWatch is a monitoring and management service for AWS resources. It collects and monitors metrics, logs, and events from AWS resources and applications. It does not provide information about service limits.\nAWS CloudTrail is a service that provides a record of actions taken by a user, role, or AWS service in an AWS account. It does not provide information about service limits.\n\n"
}, {
  "id" : 276,
  "question" : "The developers in the operations department want to use an IDE to run, test and debug code for Lambda functions.\nWhich AWS service is the most appropriate?\n",
  "answers" : [ {
    "id" : "65fcc4bef60c4802a9462eac31cc0503",
    "option" : "AWS Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "62dce49f7ff24c3da19f363e468da011",
    "option" : "AWS CodeDeploy",
    "isCorrect" : "false"
  }, {
    "id" : "3a1b3421c692414ebe98882a8fc6627e",
    "option" : "AWS CodeCommit",
    "isCorrect" : "false"
  }, {
    "id" : "4fde421faae542f5917dc9c3f7c75aaf",
    "option" : "AWS Cloud9",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nThe AWS Cloud9 service is the most appropriate service to utilize on the given options to write code, typically for the components of microservices, as well as run, test and debug the code.\nAWS Cloud9 IDE is accessed via the web browser and has a customizable interface to suit the developer's preferred runtime and other preferences.\nhttps://docs.aws.amazon.com/cloud9/latest/user-guide/welcome.html\nOption A is INCORRECT because AWS Lambda is ideal for functions, purpose-specific logic that performs repetitive processes.\nGiven the dynamic nature of the microservice, Lambda functions would not be the most appropriate option.\nOption B is INCORRECT because AWS CodeDeploy is ideal for deploying software and not building and testing it.\nOption C is INCORRECT because AWS CodeCommit is ideal for securely storing and privately managing software development assets such as binary files and source code.\nIt is not the most appropriate AWS service to meet the requirements of the use case.\n\nThe most appropriate AWS service for developers in the operations department who want to use an IDE to run, test and debug code for Lambda functions is AWS Cloud9 (option D).\nAWS Cloud9 is a cloud-based integrated development environment (IDE) that allows developers to write, run, and debug code in the cloud. It supports a wide range of programming languages and provides a complete development environment with features such as code highlighting, debugging, and version control.\nAWS Lambda is a serverless computing service that allows developers to run code without provisioning or managing servers. While Lambda supports multiple programming languages, it does not provide an IDE for developers to write, test, and debug their code.\nAWS CodeDeploy is a service that automates code deployments to Amazon EC2 instances or on-premises servers. It does not provide an IDE for writing code.\nAWS CodeCommit is a fully-managed source control service that allows developers to store and manage their code in Git repositories. It does not provide an IDE for writing or testing code.\nTherefore, the most appropriate AWS service for the developers in the operations department to use an IDE to run, test and debug code for Lambda functions is AWS Cloud9.\n\n"
}, {
  "id" : 277,
  "question" : "AWS well architected framework recommends central management of all AWS Accounts from a security standpoint.\nWhat helps me to configure services and resources centrally?\n",
  "answers" : [ {
    "id" : "6b8194eb2ca644ac8f705104d33b93ba",
    "option" : "AWS Config",
    "isCorrect" : "false"
  }, {
    "id" : "3d57df76e9b9429c8a77ef18e6107a98",
    "option" : "AWS Organizations",
    "isCorrect" : "true"
  }, {
    "id" : "b17f16d013f642a7922cc4ee65305cf4",
    "option" : "AWS Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "6ef10a64d00248e080760fe33d70ffc6",
    "option" : "All the above.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect.\nAWS Config works with rules.\nAn Organization may have defined various compliance rules for various AWS services.\nThese rules can be defined in AWS Config and data aggregated centrally for measuring compliance.\nOption B is CORRECT.\nAWS Organizations helps configure policies related to different services centrally.\nAlso known as Service Control Policies (SCP), they can be defined for your entire Organization.\nAs an example if you have configured a central logging of all API calls using CloudTrail, member accounts cannot override that policy using IAM policies.\nOption C is incorrect.\nAWS Inspector automates DevSecOps in the cloud by detecting security vulnerabilities in EC2 workloads.\nOption D is incorrect.\nOnly AWS Organizations have the ability to provide a central management of all my AWS accounts.\nReference:\nhttps://aws.amazon.com/architecture/well-architected/?wa-lens-whitepapers.sort-by=item.additionalFields.sortDate&amp;wa-lens-whitepapers.sort-order=desc\n\nThe AWS Well-Architected Framework is a methodology designed to help architects build secure, high-performance, resilient, and efficient infrastructure for their applications. One of the best practices recommended by the framework is to use AWS Organizations for central management of all AWS accounts from a security standpoint.\nAWS Organizations is a service that allows you to manage multiple AWS accounts centrally. It enables you to create and manage groups of accounts, automate account creation, and apply policies to accounts or groups of accounts. By using AWS Organizations, you can simplify billing and cost management, apply security policies, and share resources across accounts.\nOne of the benefits of using AWS Organizations is that it allows you to configure services and resources centrally. This means that you can apply security policies, compliance requirements, and other configurations across multiple accounts or groups of accounts.\nAWS Config is another service that helps you manage and monitor your AWS resources. It provides a detailed inventory of your AWS resources, tracks changes to your resources over time, and enables you to audit resource configurations against best practices and compliance standards.\nAWS Inspector is a security assessment service that helps you improve the security and compliance of your applications deployed on AWS. It provides a comprehensive view of your security posture and identifies security vulnerabilities in your resources.\nTherefore, the correct answer is (B) AWS Organizations. However, it's important to note that while AWS Config and AWS Inspector can help you manage and monitor your AWS resources, they do not enable central management of multiple AWS accounts like AWS Organizations does.\n\n"
}, {
  "id" : 278,
  "question" : "An administrator needs to manage many AWS accounts within a big organization.\nWhich of the following is the most appropriate AWS service for an administrator to utilize?\n",
  "answers" : [ {
    "id" : "7e1f532fa3ec47a8ab7f8da28c88d481",
    "option" : "AWS CloudTrail",
    "isCorrect" : "false"
  }, {
    "id" : "8722d0b74b62428d90c11d58fd6862fc",
    "option" : "IAM Roles",
    "isCorrect" : "false"
  }, {
    "id" : "d820cbeabdf64e73aeb72361515bc9c9",
    "option" : "IAM Policies",
    "isCorrect" : "false"
  }, {
    "id" : "24740299033b4be3a16948a5e2fdfa9a",
    "option" : "AWS Organisations.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer:D.\nOption A is INCORRECT because CloudTrail is a tool to track AWS API activities.\nOption B is INCORRECT because IAM Roles are typically utilized when one AWS service grants access to another.\nThe use case requires access management to several services.\nOption C is INCORRECT because implementation of IAM Policies cannot manage access to specific sets of AWS services.\nThus this option does not meet the requirements of the use case.\nOption D is CORRECT because AWS Organisations orders AWS accounts into logical groups called organization units and is a suitable tool to manage many AWS accounts.\n\nThe most appropriate AWS service for an administrator to utilize when managing many AWS accounts within a big organization is AWS Organizations (Option D).\nAWS Organizations is a service that allows an administrator to manage multiple AWS accounts in a centralized way. With AWS Organizations, an administrator can create groups of AWS accounts, apply policies across these groups, and automate the account creation process.\nBy using AWS Organizations, an administrator can simplify the management of AWS accounts by centralizing billing, applying security policies, and simplifying resource sharing across accounts. AWS Organizations provides a hierarchical structure to organize AWS accounts into organizational units (OUs) and apply policies to those OUs. This hierarchical structure also allows for more granular control of access and permissions.\nCloudTrail (Option A) is a service that records all API calls made in an AWS account. While it provides a comprehensive audit trail for an AWS account, it does not provide centralized management of multiple AWS accounts.\nIAM Roles (Option B) are a way to delegate permissions to resources in an AWS account. While IAM roles can be used to manage permissions within a single AWS account, they are not a suitable tool for managing multiple AWS accounts.\nIAM Policies (Option C) are used to define permissions for AWS users, groups, and roles within a single AWS account. While they can be used to manage permissions within an AWS account, they are not a suitable tool for managing multiple AWS accounts.\nIn summary, AWS Organizations is the most appropriate AWS service for an administrator to utilize when managing many AWS accounts within a big organization because it provides centralized management of multiple AWS accounts, allows for the application of policies across accounts, and provides a hierarchical structure for organizing accounts.\n\n"
}, {
  "id" : 279,
  "question" : "A group of non-tech savvy friends are looking to set up a website for an upcoming event at a cost-effective price, with a novice-friendly interface.\nWhich AWS service is the most appropriate to use?\n",
  "answers" : [ {
    "id" : "a5ccfa9fe637400f81460f6b9529c4ba",
    "option" : "Use AWS Marketplace to install a ready-made WordPress AMI.",
    "isCorrect" : "false"
  }, {
    "id" : "3333e45c066c416aae4ec175a6ced1a5",
    "option" : "Use AWS Lightsail.",
    "isCorrect" : "true"
  }, {
    "id" : "b77781cb14104e4180997e8f9f7ec036",
    "option" : "Use a pre-configured customizable Apache web server on an Amazon EC2 instance.",
    "isCorrect" : "false"
  }, {
    "id" : "2df23528bf6742619a44983689800ef4",
    "option" : "Download a pre-configured website on an EC2 instance from a third-party website generator.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer:B.\nAWS Lightsail is an inexpensive, easy-to-use, novice-friendly and interactive platform to configure and launch web applications or websites quickly.\nAWS Lightsail is best to utilize for simple workloads and has fast start-to-end deployment.\nIt is built to assist users who have little or no experience with web application design.\nIt also allows users to implement small deployments that are easily disposable or easily scale with the use case.\nhttps://aws.amazon.com/lightsail/features/\nOption A is INCORRECT because using AWS Marketplace to install a ready-made WordPress AMI onto an EC2 instance is feasible but not the most novice-friendly method.\nOption C is INCORRECT because using a pre-configured customizable Apache web server on an Amazon EC2 instance is feasible but requires the user to have a fair knowledge of Apache.\nThe question requires a novice-friendly option.\nOption D is INCORRECT because downloading a pre-configured website on an EC2 instance from a third-party website generator is feasible but not the most novice-friendly method.\n\nFor a group of non-tech savvy friends looking to set up a website for an upcoming event at a cost-effective price, with a novice-friendly interface, the most appropriate AWS service to use would be AWS Lightsail (Option B).\nAWS Lightsail is a simple and easy-to-use service that provides all the necessary tools for launching and managing a website or application quickly and cost-effectively. It is designed to be user-friendly, with a simplified web interface that makes it easy for non-technical users to create and manage their website.\nAWS Lightsail provides various pre-configured templates, which includes popular web applications like WordPress, Joomla, Drupal, and more, to enable novice users to launch a website with minimal effort. These templates have been preconfigured with all the necessary software components, such as web server, database, and PHP, to make the website creation process easier.\nFurthermore, AWS Lightsail also provides a wide range of tools and resources that help users to manage their website and scale it up as needed. For example, users can use the integrated DNS management tools to easily create custom domains, set up SSL certificates, and more. AWS Lightsail also provides automatic backups, monitoring, and alerts to ensure that the website is always up and running.\nIn contrast, the other options provided in the question are less suitable for novice users with limited technical skills. Using AWS Marketplace to install a ready-made WordPress AMI (Option A) requires more technical knowledge than using AWS Lightsail, as it involves configuring the server environment and installing the software manually.\nUsing a pre-configured customizable Apache web server on an Amazon EC2 instance (Option C) requires a more advanced level of technical knowledge, as it involves configuring and managing an EC2 instance and the web server software manually.\nFinally, downloading a pre-configured website on an EC2 instance from a third-party website generator (Option D) is also less suitable for novice users, as it requires technical skills to set up and manage the website on an EC2 instance.\nIn conclusion, AWS Lightsail is the most appropriate option for non-tech savvy users who want to create a website at a cost-effective price with a novice-friendly interface.\n\n"
}, {
  "id" : 280,
  "question" : "Which of the following accurately describes a typical use case in which the AWS CodePipeline service can be utilized?\n",
  "answers" : [ {
    "id" : "96081faff0d1454e8f10af456c1261ca",
    "option" : "To compose code in an integrated development environment that enables developers to run, test and debug components of a dynamic microservice.",
    "isCorrect" : "false"
  }, {
    "id" : "176061a217094238970a761739d2d869",
    "option" : "To compile and deploy a microservice onto Amazon EC2 instances or AWS Lambda functions.",
    "isCorrect" : "false"
  }, {
    "id" : "e465f8da799c4a719c7b60d009f2415b",
    "option" : "To securely share code, collaborate on source code, version control and store binaries on an AWS fully-managed platform that scales seamlessly.",
    "isCorrect" : "false"
  }, {
    "id" : "78671e18095a48faaf9484f4c8f18b27",
    "option" : "To orchestrate and automate the various phases involved in the release of application updates in-line with a predefined release model.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer:D.\nThe question is looking for a typical use case for AWS CodePipeline.\nOption D is the most appropriate because AWS CodePipeline is typically utilized when orchestrating and automating the various phases involved in the release of application updates in-line with a release model that the developer defines.\nhttps://aws.amazon.com/codepipeline/\nOption A is INCORRECT because composing code in an integrated development environment that enables developers to run, test, and debug components of a dynamic microservice is the typical AWS Cloud9 IDE function.\nOption B is INCORRECT because compiling and deploying microservices on Amazon EC2 instances or AWS Lambda functions are the typical functions of AWS CodeDeploy.\nOption C is INCORRECT because securely sharing code, collaborating on source code, version control and storing binaries on an AWS fully-managed platform describe the functions of CodeCommit.\n\nThe correct answer is D. To orchestrate and automate the various phases involved in the release of application updates in-line with a predefined release model.\nAWS CodePipeline is a continuous integration and continuous delivery service that helps users to automate the build, test, and deploy phases of their application releases. It is primarily used to streamline the application delivery process and to enable faster and more reliable application updates.\nOption A is incorrect because AWS CodePipeline is not an integrated development environment (IDE). Rather, it is a tool that helps automate the delivery process of applications.\nOption B is incorrect because AWS CodePipeline is not primarily used for deploying code onto specific compute instances or serverless computing functions. Rather, it can be used to automate the delivery process of applications across various compute services on AWS.\nOption C is incorrect because AWS CodePipeline is not a source code management tool, such as AWS CodeCommit or GitHub. Rather, it can integrate with these tools to automate the delivery process of applications.\nIn summary, AWS CodePipeline is a tool that can help automate the delivery process of applications and is primarily used to orchestrate and automate the various phases involved in the release of application updates in-line with a predefined release model.\n\n"
}, {
  "id" : 281,
  "question" : "To access data stored in Amazon S3 bucket from on-premise locations using AWS Direct Connect, which of the following interface &amp; routing protocols require to be configured?\n",
  "answers" : [ {
    "id" : "20d98ce244c7488bbf7ee2ff1377fdb5",
    "option" : "Public VIF with BGP Routing Protocol",
    "isCorrect" : "true"
  }, {
    "id" : "fb25c3e9941c475c9a920dade29dc16c",
    "option" : "Private VIF with BGP Routing Protocol",
    "isCorrect" : "false"
  }, {
    "id" : "d65550146597479193ff1741543058c4",
    "option" : "Public VIF with Static Routing Protocol",
    "isCorrect" : "false"
  }, {
    "id" : "1c56c8d1f66a41d7a9d25e1a5dab6e1d",
    "option" : "Private VIF with Static Routing Protocol.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nAWS Direct Connect provides a dedicated network connectivity from on-premise to AWS.\nAWS Direct Connect supports only the BGP routing protocol for this connectivity.\nIt supports multiple virtual connections on a single physical link.\nTo access public resources on AWS, Public Virtual Interface needs to be created.\nTo access resources within VPC, a Private Virtual Interface is required.\nOption B is incorrect as Public VIF is required (not Private VIF) for accessing public resources within AWS cloud like Amazon S3 bucket.\nPrivate VIF is used to access resources within Amazon VPC.Options C &amp; D are incorrect as AWS Direct Connect only supports BGP routing Protocol, not static routing protocol.\nFor more information on AWS Direct Connect, refer to the following URL:\nhttps://aws.amazon.com/directconnect/?nc=sn&amp;loc=0\n\nTo access data stored in an Amazon S3 bucket from on-premise locations using AWS Direct Connect, you will need to establish a connection between the on-premises data center and the VPC hosting the S3 bucket.\nAWS Direct Connect enables you to create a dedicated network connection between your on-premises data center and AWS, bypassing the internet. With Direct Connect, you can establish a private, high-bandwidth network connection that reduces network costs, increases bandwidth throughput, and provides a more consistent network experience than internet-based connections.\nTo configure the interface and routing protocols required for accessing data stored in an Amazon S3 bucket, you can choose between two types of Virtual Interfaces (VIF) - Public VIF or Private VIF.\nPublic VIF: This type of VIF allows you to connect to public AWS services, such as Amazon S3, Amazon EC2, and Amazon DynamoDB. A Public VIF must be used with BGP (Border Gateway Protocol) routing protocol, which is a standardized protocol for exchanging routing information between network devices.\nPrivate VIF: This type of VIF allows you to establish a private connection between your on-premises data center and your VPC in the AWS Cloud. A Private VIF can be used with either BGP or Static routing protocol.\nTherefore, to access data stored in an Amazon S3 bucket from on-premise locations using AWS Direct Connect, you will need to configure a Private VIF with either BGP or Static Routing Protocol. So, the correct answer is either B or D.\nIt's important to note that if you want to use a Public VIF to access Amazon S3, you will also need to configure an S3 VPC Endpoint, which allows you to access S3 using a private network connection.\n\n"
}, {
  "id" : 282,
  "question" : "An architect is designing a solution.\nAppropriate data classification is being implemented by classifying the data sensitivity levels.\nThe solution needs to consider the encryption of data and tokenization.\nWhich of the design principles for security in the cloud is the architect applying?\n",
  "answers" : [ {
    "id" : "bcf9a8a111be45798e51bbf45eb5d9a4",
    "option" : "Protect data in transit and at rest.",
    "isCorrect" : "true"
  }, {
    "id" : "e665b97d4dd4429b8b7892e46635305e",
    "option" : "Apply security at all layers.",
    "isCorrect" : "false"
  }, {
    "id" : "f631d2a636e04689a8a43300db3bac81",
    "option" : "Implement a strong identity foundation.",
    "isCorrect" : "false"
  }, {
    "id" : "b98fb482a0bf446f9cbe428ac127d68e",
    "option" : "Prepare for security events.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is correct.\nâ€œProtecting data in transit and at restâ€ includes implementation of techniques to ensure data protection.\nIn this scenario, the architect is implementing data classification techniques, applying sensitivity level, encryption etc.\nThat helps in data protection while at rest and in transit.\nOption B is incorrect, because, â€œapplying security at all layersâ€ principle refers to implementing the security controls at various levels of solutions' architecture like application, code, VPC, etc.\nOption C is incorrect.\nâ€œImplement a strong identity foundationâ€ principle enforces the philosophy of implementing the principle of least privilege and other IAM principles like authorizing and delegating privileges strictly based on the duties to be performed.\nOption D is incorrect.\nâ€œPrepare for security eventsâ€ ensures preparedness for security events aligned to organizational requirements by performing risk assessment, creation of necessary checkpoints, and implementing proper incident management process and tools.\nReferences:\nhttps://wa.aws.amazon.com/wat.pillar.security.en.html\n\nThe architect is applying the design principle of \"Protect data in transit and at rest\" by implementing appropriate data classification and encryption of data and tokenization.\nData classification is the process of categorizing data into different levels of sensitivity based on its value, confidentiality, and criticality. This helps organizations to understand the level of protection that each type of data requires and to apply the appropriate security controls to protect it.\nEncryption is the process of converting plain text into ciphertext to protect the confidentiality of data. Encryption can be applied to data in transit, such as data transmitted over a network, and data at rest, such as data stored in a database or on a storage device.\nTokenization is the process of replacing sensitive data with non-sensitive data called tokens. Tokens are randomly generated and cannot be used to derive the original data. Tokenization can be used to protect sensitive data such as credit card numbers or personally identifiable information.\nThe design principle of \"Protect data in transit and at rest\" emphasizes the importance of implementing appropriate security controls to protect data throughout its lifecycle, including when it is being transmitted over a network or stored on a device.\nBy applying appropriate data classification, encryption, and tokenization, the architect is taking steps to protect data both in transit and at rest, which aligns with this design principle. Therefore, the correct answer is A. Protect data in transit and at rest.\n\n"
}, {
  "id" : 283,
  "question" : "With a focus on the Well-Architected Framework's security pillar, you want to define standards and best practices for your EC2 instances and validate adherence to these standards so as the workload security is enhanced.\nWhich of the below would you choose?\n",
  "answers" : [ {
    "id" : "2e704fb8b86d4b0c92fcde9d87d73c9b",
    "option" : "Amazon Detective",
    "isCorrect" : "false"
  }, {
    "id" : "890fc693d6034785a83fd275f0a3a3f9",
    "option" : "AWS Inspector",
    "isCorrect" : "true"
  }, {
    "id" : "3d8571874e534eb3acd117e5ec8f86ca",
    "option" : "AWS Security Hub",
    "isCorrect" : "false"
  }, {
    "id" : "927be534082a48ebaf3001f0cce5fc1f",
    "option" : "AWS Resource Access Manager (RAM)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is Incorrect.\nAmazon Detective is a security service that makes use of AI making it easy for the users to identify, analyse and investigate security issues or suspicious activities.\nAmazon Detective offers advantages in terms of swifter investigations by providing visualizations that are easy to use.\nOption B is Correct.\nAmazon Inspector is a tool to perform security assessment.\nInspector gives us ability to define standards and best practices and assesses adherence to these standards.\nOption C is Incorrect.\nAWS Security Hub is an AWS service that gives a comprehensive view of security alerts and other security related information from across different AWS accounts, services and other configured 3rd party applications.\nOption D is Incorrect.\nAWS Resource Access Manager (RAM) is a service that enables users to share AWS resources easily and securely.\nThe resources could be shared with any AWS account or within your AWS Organization.\nReferences:\nhttps://aws.amazon.com/detective/\nhttps://aws.amazon.com/inspector/\nhttps://aws.amazon.com/security-hub/\nhttps://aws.amazon.com/ram/\n\nIf you want to define standards and best practices for your EC2 instances and validate adherence to these standards, the best choice would be AWS Inspector.\nAWS Inspector is a security assessment service that helps improve the security and compliance of your applications running on Amazon EC2 instances. It can identify security issues by analyzing your EC2 instances against predefined rules for common security vulnerabilities.\nWith AWS Inspector, you can define rules packages that include best practices and security standards. You can then run assessments on your EC2 instances to validate adherence to these standards. Inspector provides a detailed report that includes recommendations to remediate identified security issues.\nAmazon Detective is a security service that makes it easy to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities. It's not specifically designed to define standards and best practices for your EC2 instances.\nAWS Security Hub is a comprehensive security service that provides central visibility into your security and compliance posture across your AWS accounts. While it provides insights and recommendations on potential security issues, it's not designed to define standards and best practices for your EC2 instances.\nAWS Resource Access Manager (RAM) is a service that allows you to share AWS resources across AWS accounts. It's not designed to improve the security of your EC2 instances.\nTherefore, the best answer for the given question would be B. AWS Inspector.\n\n"
}, {
  "id" : 284,
  "question" : "A customer wants to opt for a support plan so that he can receive the below benefits. - 24x7 phone, email, and chat access to Cloud Support Engineers - Application-specific consultative review and guidance with regards to architecture Which of the below support plan would you recommend?\n",
  "answers" : [ {
    "id" : "51b326361501467fa166098afd0eb97e",
    "option" : "Basic",
    "isCorrect" : "false"
  }, {
    "id" : "d8f83b06dd0046199026caeb79958acf",
    "option" : "Developer",
    "isCorrect" : "false"
  }, {
    "id" : "f9393cb4ac434ed8a2681b02dbaf7fdd",
    "option" : "Business",
    "isCorrect" : "false"
  }, {
    "id" : "08da5b5d10b14995a48db07fc5eb66d8",
    "option" : "Enterprise.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect.\nThe basic plan does not provide any of the required benefits.\nOption B is incorrect.\nThe developer plan does not provide any of the required benefits.\nOption C is incorrect.\nAlthough \"24x7 phone, email, and chat access to Cloud Support Engineers\" is available in the business plan, \"Application-specific consultative review and guidance with regards to architecture\" is not available in this support plan.\nOption D is Correct.\nEnterprise plan should be recommended to the customer as both the required benefits are available in Enterprise support plan.\nReferences:\nhttps://aws.amazon.com/premiumsupport/plans/\n\nBased on the customer's requirements, the support plan that would be recommended is the Enterprise support plan.\nThe Enterprise support plan provides 24x7 access to AWS Technical Account Managers (TAMs) who are experts in AWS architecture and can provide guidance on how to optimize your applications and infrastructure for performance, security, and cost-effectiveness. The TAMs also provide assistance in troubleshooting and resolving technical issues.\nAdditionally, the Enterprise plan provides access to AWS Infrastructure Event Management, which proactively monitors your AWS infrastructure for events that may impact availability, performance, or security. The plan also provides access to AWS Trusted Advisor, which provides automated checks and recommendations on best practices for cost optimization, security, fault tolerance, and performance improvement.\nIn summary, the Enterprise support plan provides access to a wide range of services and resources that can help customers optimize their AWS environment and maximize the benefits of using the cloud. It is the best option for customers who require personalized and proactive support for their mission-critical applications and infrastructure.\n\n"
}, {
  "id" : 285,
  "question" : "Which of the below is NOT a benefit of moving to the cloud?\n",
  "answers" : [ {
    "id" : "ccbb0c7f75c0427f96abbe0f7baa24c5",
    "option" : "Scalability",
    "isCorrect" : "false"
  }, {
    "id" : "c22d7809b7c941b5adea7f34788e0afa",
    "option" : "Decreased TCO",
    "isCorrect" : "false"
  }, {
    "id" : "7a14a6114606458abb72e2e55e248eb8",
    "option" : "Redundancy",
    "isCorrect" : "false"
  }, {
    "id" : "b4cc88d991e94b578bc1a5e339682803",
    "option" : "Increased time to market.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect because scalability and elasticity are two of the main benefits of AWS.\nOption B is incorrect because TCO (Total Cost of Ownership) reduces upon migrating to the cloud.\nOption C is incorrect because AWS cloud provides redundancy at multiple levels.\nOption D is Correct because the time to market is \"Reduced\" and not \"Increased\".\nReferences:\nhttps://d1.awsstatic.com/whitepapers/cloud-migration-main.pdf\n\nMoving to the cloud provides numerous benefits, such as scalability, decreased total cost of ownership (TCO), redundancy, and increased time to market. However, the question asks which of the options is NOT a benefit of moving to the cloud.\nA. Scalability - Moving to the cloud enables organizations to scale their resources up or down based on their needs. Organizations can add or remove computing resources (such as servers, storage, and networking) as needed, without the need for capital expenditures. This can help organizations optimize their costs and increase their agility.\nB. Decreased TCO - Moving to the cloud eliminates the need for organizations to purchase, maintain, and upgrade their own hardware and software. This can result in a significant reduction in total cost of ownership (TCO) for organizations, as they only pay for what they use on a pay-as-you-go basis.\nC. Redundancy - Cloud providers typically offer multiple availability zones and regions, which enables organizations to replicate their data and applications across multiple locations. This can improve reliability and availability, as it reduces the risk of a single point of failure.\nD. Increased time to market - Moving to the cloud can enable organizations to develop and deploy applications more quickly, as they can take advantage of pre-built services and infrastructure provided by cloud providers. This can also help organizations reduce the amount of time and resources required to manage their own infrastructure, allowing them to focus more on innovation and delivering value to their customers.\nTherefore, the answer to the question is None of the above, as all of the options are benefits of moving to the cloud.\n\n"
}, {
  "id" : 286,
  "question" : "Important functions of your production application are degraded.\nThe support engineer has an AWS account with a Business-level AWS Support plan.\nWhile opening a support case with AWS, what severity should the support engineer choose?\n",
  "answers" : [ {
    "id" : "aca71425e26d473cb76b3a0e6dda118e",
    "option" : "System impaired",
    "isCorrect" : "false"
  }, {
    "id" : "f5d8b24c19d74cb09945098914036100",
    "option" : "Production system impaired",
    "isCorrect" : "true"
  }, {
    "id" : "38c48050d88644d385a4204566e8cd9e",
    "option" : "Production system down",
    "isCorrect" : "false"
  }, {
    "id" : "281a28e14b5d49478815eeb84a6f88d7",
    "option" : "Business-critical.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect.\nChoose System Impaired when Non-critical functions of your application are behaving abnormally or you have a time-sensitive development question.\nIn the given scenario, the application is a production application.\nRefer to the table â€œChoosing a severityâ€ in the link below for more details.\nOption B is correct.\nImportant functions of production application are impaired/degraded.\nSince the support engineer has an AWS account with a Business plan, he/she can open a case with AWS with this severity selected.\nRefer to the table â€œChoosing a severityâ€ in the link below for more details.\nOption C is incorrect.\nIn this scenario, the business is not significantly impacted because important functions of the application are available.\nHowever, the performance is degraded.\nRefer to the table â€œChoosing a severityâ€ in the link below for more details.\nOption D is incorrect.\nThis option should be selected when the business is at risk and the application's critical functions aren't available.\nRefer to the table â€œChoosing a severityâ€ in the link below for more details.\nReferences:\nhttps://docs.aws.amazon.com/awssupport/latest/user/case-management.html#choosing-severity\n\nWhen opening a support case with AWS, the severity level chosen will determine the priority of the case and the expected response time from AWS support.\nIn this scenario, the important functions of the production application are degraded, which means that the production system is still functioning, but not at optimal levels.\nHere are the definitions of each severity level and their corresponding response times:\nA. System impaired - This severity level should be chosen when the issue is causing a partial loss of functionality or performance degradation in a non-production environment. The expected initial response time is within 12 hours.\nB. Production system impaired - This severity level should be chosen when the issue is causing a partial loss of functionality or performance degradation in a production environment. The expected initial response time is within 4 hours.\nC. Production system down - This severity level should be chosen when the issue is causing a complete loss of functionality in a production environment. The expected initial response time is within 1 hour.\nD. Business-critical - This severity level should be chosen when the issue is causing a significant impact on critical business operations in a production environment. The expected initial response time is within 15 minutes.\nBased on the scenario given, it is not clear whether the important functions of the production application being degraded are causing a complete loss of functionality or just a partial loss of functionality. However, since the support engineer has a Business-level AWS Support plan, it is reasonable to assume that the production application is critical to their business. Therefore, the support engineer should choose severity level D, Business-critical, to ensure the fastest possible response time from AWS support.\n\n"
}, {
  "id" : 287,
  "question" : "In the shared responsibility model for infrastructure services, such as Amazon Elastic Compute Cloud, which of the below two are customers responsibility?\n",
  "answers" : [ {
    "id" : "ba6eac449672478db27a4898e43e0100",
    "option" : "Network infrastructure",
    "isCorrect" : "false"
  }, {
    "id" : "ce3ad727f1c84d38915597b832bdd7ca",
    "option" : "Amazon Machine Images (AMIs)",
    "isCorrect" : "true"
  }, {
    "id" : "9b2617ade6ee4366976ec7af5b872540",
    "option" : "Virtualization infrastructure",
    "isCorrect" : "false"
  }, {
    "id" : "9273b005cb2342c996981b27501a75c0",
    "option" : "Physical security of hardware",
    "isCorrect" : "false"
  }, {
    "id" : "93f8f32c6854452380982f1478ce914a",
    "option" : "Policies and configuration.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: B, E.\nIn the shared responsibility model, AWS is primarily responsible for â€œSecurity of the Cloud.â€ The customer is responsible for â€œSecurity in the Cloud.â€ In this scenario, the mentioned AWS product is IAAS (Amazon EC2) and AWS manages the security of the following assets:\n- Facilities.\n- Physical security of hardware.\n- Network infrastructure.\n- Virtualization infrastructure.\nCustomers are responsible for the security of the following assets:\n- Amazon Machine Images (AMIs)\n- Operating systems.\n- Applications.\n- Data in transit.\n- Data at rest.\n- Data stores.\n- Credentials.\n- Policies and configuration.\nOption A is incorrect.\nRefer to the explanation above and link in the references for more details.\nOption B is Correct.\nRefer to the explanation above and link in the references for more details.\nOption C is incorrect.\nRefer to the explanation above and link in the references for more details.\nOption D is incorrect.\nRefer to the explanation above and link in the references for more details.\nOption E is correct.\nRefer to the explanation above and link in the references for more details.\nReferences:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-security-best-practices/know-the-aws-shared-responsibility-model.html\n\nThe shared responsibility model is a key concept in cloud computing that helps to define the roles and responsibilities of both the cloud provider and the customer. In this model, the provider is responsible for the security of the cloud infrastructure, while the customer is responsible for the security of the data and applications they deploy on the cloud.\nWhen it comes to infrastructure services such as Amazon Elastic Compute Cloud (EC2), the shared responsibility model applies to both the underlying infrastructure and the virtual machines that run on it. Specifically, Amazon is responsible for securing the physical infrastructure of its data centers, as well as the hypervisor and the operating system that manages the virtual machines. On the other hand, the customer is responsible for securing the applications and data they deploy on the virtual machines.\nOut of the options given, the two responsibilities that fall under the customer's responsibility in the shared responsibility model for EC2 are:\nE. Policies and configuration: The customer is responsible for configuring and maintaining the security settings of their virtual machines, such as setting up firewalls, applying security patches, and implementing access controls. They are also responsible for defining policies that govern how their applications and data are accessed and used.\nB. Amazon Machine Images (AMIs): An Amazon Machine Image (AMI) is a pre-configured virtual machine that customers can use to create new instances on EC2. While Amazon is responsible for securing the underlying infrastructure that runs AMIs, the customer is responsible for ensuring that the AMIs they use are secure and up-to-date. This includes verifying that the AMIs are free of vulnerabilities, have the latest security patches, and adhere to the customer's security policies.\nThe other options are responsibilities that fall under Amazon's responsibility in the shared responsibility model:\nA. Network infrastructure: Amazon is responsible for securing the network infrastructure that connects EC2 instances to the internet and to other AWS services.\nC. Virtualization infrastructure: Amazon is responsible for securing the hypervisor and the virtualization infrastructure that manages EC2 instances.\nD. Physical security of hardware: Amazon is responsible for securing the physical infrastructure of its data centers, including the servers, storage devices, and networking equipment that run EC2.\n\n"
}, {
  "id" : 288,
  "question" : "An organization is planning to implement a solution to ease the administrative effort of managing access permissions of AWS accounts and applications.\nBecause of this initiative, the organization plans to manage the access to AWS accounts and applications centrally.\nWhich of the below would you suggest?\n",
  "answers" : [ {
    "id" : "99e0e91437d2442cb7a04b3f3d109d02",
    "option" : "AWS SSO",
    "isCorrect" : "true"
  }, {
    "id" : "ee05105522734efd9bfe9f18a914fcbe",
    "option" : "AWS Identity and Access Management (IAM)",
    "isCorrect" : "false"
  }, {
    "id" : "399f17a52752435e822b40c09a1dd4f6",
    "option" : "AWS Account Manager",
    "isCorrect" : "false"
  }, {
    "id" : "f5f0942caadf432db5fb795466ea45d0",
    "option" : "AWS Resource Access Manager (RAM)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is correct.\nAWS Single Sign-On (SSO) simplifies access management and manages access to multiple AWS accounts and business applications centrally.\nOption B is incorrect.\nAWS Identity and Access Management (IAM) deals with access management, providing the ability to manage access to AWS services and resources securely.\nUsing IAM, AWS users and groups could be created and managed.\nYou can also add permissions to allow or deny access to AWS resources.\nOption C is incorrect.\nAWS Account Manager is not a valid offering from AWS.\nOption D is incorrect.\nAWS Resource Access Manager (RAM) is a service that enables users to share AWS resources easily and securely.\nThe resources could be shared with any AWS account or within your AWS Organization.\nReferences:\nhttps://aws.amazon.com/single-sign-on/\nhttps://aws.amazon.com/iam/\nhttps://aws.amazon.com/ram/\n\nBased on the information provided, the best option for the organization to manage access permissions centrally for AWS accounts and applications would be AWS Single Sign-On (SSO).\nAWS SSO is a service that simplifies the management of access to AWS resources and business applications by enabling the organization to centrally manage access to multiple AWS accounts and applications using Single Sign-On. This means that users can sign in to all their assigned accounts and applications using a single set of credentials.\nAWS SSO provides the following benefits:\nCentralized management of access: AWS SSO provides a centralized location to manage access permissions to AWS accounts and applications. This reduces the administrative effort required to manage access and provides greater visibility and control over access permissions. Easy user onboarding: AWS SSO provides a simple process for adding new users and groups, which makes it easy to onboard new employees and contractors. Enhanced security: AWS SSO provides integration with AWS Multi-Factor Authentication (MFA) to add an extra layer of security for user sign-in. Easy integration with third-party applications: AWS SSO provides built-in integration with many popular business applications, such as Salesforce, Box, and Office 365.\nAWS Identity and Access Management (IAM) is another service that allows organizations to manage access to AWS resources, but it is more focused on managing access to specific AWS resources within a single AWS account. IAM is not designed to manage access across multiple AWS accounts or applications.\nAWS Account Manager is not a service provided by AWS.\nAWS Resource Access Manager (RAM) allows organizations to share resources across multiple AWS accounts. However, it is not designed to manage access permissions for AWS accounts and applications centrally.\nIn conclusion, based on the information provided, AWS SSO is the best option for the organization to manage access permissions centrally for AWS accounts and applications.\n\n"
}, {
  "id" : 289,
  "question" : "An e-learning start-up providing school students with practice tests needs to run an application during the holiday season (2 months)\nThe application will be deployed on an EC2 instance; however, the application downtime will have an adverse effect on the company's reputation.\nWhich option below would be the best fit for this scenario?\n",
  "answers" : [ {
    "id" : "27df3f3827cf4774b0ae8a5eec43cbeb",
    "option" : "Reserved",
    "isCorrect" : "false"
  }, {
    "id" : "c7097ecf33d04b23b0993ec3fb7c97a1",
    "option" : "Dedicated",
    "isCorrect" : "false"
  }, {
    "id" : "6c197cd8f16046cdac0a9c33d9e7323a",
    "option" : "Spot",
    "isCorrect" : "false"
  }, {
    "id" : "7d64b1ca0de74853a6246d6c7c4c26fb",
    "option" : "On-Demand.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect.\nThe required duration of 2 months is not long enough to reserve an instance.\nOption B is incorrect.\nDedicated instances are not cost-efficient and will not be suggested in this scenario.\nOption C is incorrect.\nSpot instances would not be an apt choice, as there is a probability of disruption.\nOption D is Correct.\nOn-Demand instances would be a correct choice, as RIs could not be opted.\nBecause of the terms mentioned, dedicated instances would be costly and with spot instances, there are chances of disruption.\nReferences:\nhttps://aws.amazon.com/ec2/pricing/reserved-instances/\nhttps://aws.amazon.com/ec2/pricing/dedicated-instances/\nhttps://aws.amazon.com/ec2/spot/\nhttps://aws.amazon.com/ec2/pricing/on-demand/\n\nIn this scenario, an e-learning startup needs to run an application during the holiday season, and downtime of the application can have a negative impact on the company's reputation. The application will be deployed on an EC2 instance, and the company wants to choose the most suitable EC2 instance pricing option.\nA. Reserved instances provide the option to reserve EC2 capacity in advance for a 1- or 3-year term, which can provide cost savings compared to on-demand pricing. However, reserved instances require an upfront payment, and the capacity is not always available when needed. Therefore, it may not be the best fit for this scenario where the startup needs to run the application only for two months.\nB. Dedicated instances are instances that run on hardware that is dedicated to a single customer. While this option provides greater control and security, it is not cost-effective for a startup with a limited budget.\nC. Spot instances allow customers to bid for unused EC2 instances' capacity, which can provide significant cost savings. However, the bid price can fluctuate based on supply and demand, and the instance can be terminated when the market price exceeds the bid price. Therefore, it may not be the best fit for this scenario where the startup needs to run the application without any interruption.\nD. On-Demand instances provide the option to pay for EC2 capacity per hour without any upfront payment or long-term commitment. This pricing option allows the startup to scale up or down as needed, and it does not require any upfront payment. Therefore, it may be the best fit for this scenario where the startup needs to run the application for a short period without any interruption.\nTherefore, the best fit for this scenario would be the On-Demand pricing option.\n\n"
}, {
  "id" : 290,
  "question" : "Which of the AWS service helps you to set reservation utilization and trigger alerts when utilization drops below the defined threshold?\n",
  "answers" : [ {
    "id" : "502c4fb5f4224771831dfc188e8e3836",
    "option" : "AWS Budgets",
    "isCorrect" : "true"
  }, {
    "id" : "8ba04efce1dd4f0092b411ef02da2d5f",
    "option" : "Savings Plans",
    "isCorrect" : "false"
  }, {
    "id" : "31a705113bb945fb9a49866a1520180d",
    "option" : "Reserved Instance (RI) Reporting",
    "isCorrect" : "false"
  }, {
    "id" : "37444ff46f99400e9d7b00bfe8256244",
    "option" : "Reservation and Utilization dashboard.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nIn AWS Budgets, you can set RI utilization budgets that define a utilization threshold and receive alerts when your RI usage falls below that threshold.\nOption B is incorrect.\nSavings Plans is a pricing model providing significant savings (up to 72%) upon committing to a consistent usage (measured in $/hour) for a 1- or 3-year term.\nAWS offers two types of Savings Plans, Compute Savings Plans and EC2 Instance Savings Plans.\nOption C is incorrect.\nReserved Instance (RI) Reporting is a cost management tool that helps customers to manage and monitor their instance reservations.\nOption D is incorrect.\nReservation and Utilization dashboard is incorrect.\nReferences:\nhttps://aws.amazon.com/aws-cost-management/aws-budgets/\nhttps://aws.amazon.com/savingsplans/\nhttps://aws.amazon.com/aws-cost-management/reserved-instance-reporting/\n\nThe correct answer is A. AWS Budgets.\nAWS Budgets is an AWS service that allows you to define custom cost and usage budgets for your AWS resources and services. This service enables you to track and monitor your AWS resource costs and usage, and receive alerts when your costs or usage exceed or drop below your specified budget thresholds.\nUsing AWS Budgets, you can set reservation utilization budgets and receive alerts when your reservation utilization drops below the defined threshold. This is useful when you have purchased reserved instances and want to ensure that you are fully utilizing them to optimize cost savings.\nSavings Plans, on the other hand, are a flexible pricing model that offers savings on your AWS usage in exchange for a commitment to use a specific amount of compute power or a specific amount of money per hour for a term of one or three years. Savings Plans do not provide the same level of budgeting and alerting capabilities as AWS Budgets.\nReserved Instance (RI) Reporting is a feature within AWS Cost Explorer that provides detailed reports on your RI utilization and savings. This feature does not allow you to set utilization budgets or trigger alerts.\nThe Reservation and Utilization dashboard is a feature within the AWS Management Console that allows you to view and manage your reservations and usage, but it does not provide budgeting or alerting capabilities.\n\n"
}, {
  "id" : 291,
  "question" : "Which of the below 2 options can NOT be used to enable â€œS3 Block Public Accessâ€?(Select TWO.)\n",
  "answers" : [ {
    "id" : "f5deab1837ac4252b7998335e0f0ce6b",
    "option" : "Rest APIs",
    "isCorrect" : "false"
  }, {
    "id" : "8c1a1316fad247569da3a443ad6be8ca",
    "option" : "S3 Console",
    "isCorrect" : "false"
  }, {
    "id" : "653cc256b646403aa56f9545a8cd4303",
    "option" : "SDKs",
    "isCorrect" : "false"
  }, {
    "id" : "a9315169468f4c99be1e7317fc502209",
    "option" : "AWS CLI â€œS3 mbâ€ command",
    "isCorrect" : "true"
  }, {
    "id" : "d398ec0ff3464b4ca8f42e27a260e48f",
    "option" : "S3 Object Lock.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D and E.\nOption A is INCORRECT.\nWe can use Rest APIs to enable â€œS3 Block Public Accessâ€.\nOption B is INCORRECT.\nS3 console can be used to enable â€œS3 Block Public Accessâ€.\nOption C is INCORRECT.\nSDKs can be used to enable â€œS3 Block Public Accessâ€.\nOption D is CORRECT.\nAWS CLI S3 mb command is used to â€œmake a bucketâ€ and cannot be used to enable â€œS3 Block Public Accessâ€.\nOption E is CORRECT.\nS3 Object Lock helps us to associate retention date to S3 objects.\nDeletion of the S3 objects is prevented until the specified retention date.\nReference:\nhttps://aws.amazon.com/blogs/storage/amazon-s3-consistently-raises-the-bar-in-data-security/\nhttps://docs.aws.amazon.com/cli/latest/reference/s3/mb.html\n\nThe S3 Block Public Access feature is a security feature in Amazon S3 that helps customers ensure that the S3 buckets and objects are not publicly accessible. This feature provides four settings that you can use to restrict public access to your S3 resources.\nThe four settings are:\nBlock Public Access settings for an individual bucket Block Public Access settings for an Amazon S3 account Block Public Access settings for Amazon S3 access points Block Public Access settings for S3 bucket policies\nNow, let's review the given options and see which of them can NOT be used to enable S3 Block Public Access.\nOption A: Rest APIs - INCORRECT\nAmazon S3 provides REST APIs that allow you to manage your S3 resources programmatically. The REST APIs allow you to set Block Public Access settings for your S3 buckets and objects, and therefore, can be used to enable S3 Block Public Access.\nOption B: S3 Console - INCORRECT\nThe Amazon S3 console provides a web-based interface that allows you to manage your S3 resources. The console allows you to set Block Public Access settings for your S3 buckets and objects, and therefore, can be used to enable S3 Block Public Access.\nOption C: SDKs - INCORRECT\nAmazon S3 provides SDKs for different programming languages that allow you to manage your S3 resources programmatically. The SDKs allow you to set Block Public Access settings for your S3 buckets and objects, and therefore, can be used to enable S3 Block Public Access.\nOption D: AWS CLI â€œS3 mbâ€ command - INCORRECT\nThe AWS Command Line Interface (CLI) provides a command-line interface that allows you to manage your AWS resources from the terminal. The S3 CLI command \"s3 mb\" can be used to create an S3 bucket, but it does not provide an option to set Block Public Access settings. However, you can use the \"s3api\" command to set Block Public Access settings for your S3 buckets and objects, and therefore, this option is incorrect.\nOption E: S3 Object Lock - CORRECT\nS3 Object Lock is a feature that allows you to store objects using a write-once-read-many (WORM) model. S3 Object Lock can be used to protect objects from deletion or modification, but it does not provide any settings to restrict public access to your S3 resources. Therefore, this option is correct.\nIn conclusion, the options that cannot be used to enable S3 Block Public Access are Option D (AWS CLI \"S3 mb\" command) and Option E (S3 Object Lock).\n\n"
}, {
  "id" : 292,
  "question" : "Your organization has planned to decommission its data centers.\nYou have a task to migrate hundreds of TB of archived data and 22 TB of active data to S3 and EFS, as per the designed solution.\nWhich of the below service will be best suited for this task?\n",
  "answers" : [ {
    "id" : "49bf2b6f338541a0ba2ee5b3e614e03c",
    "option" : "AWS Data Sync",
    "isCorrect" : "true"
  }, {
    "id" : "68ea2173fea84a6f8ccf7a4f6fa7f632",
    "option" : "AWS Direct Connect",
    "isCorrect" : "false"
  }, {
    "id" : "1b06b66764a24d2db8b36c86c13a4315",
    "option" : "AWS Data Pipeline",
    "isCorrect" : "false"
  }, {
    "id" : "765f95aaa0e843e4916958d0b903bd89",
    "option" : "AWS Migration Hub.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAWS Data Sync is a simple and fast way to move huge amounts of data (hundreds of terabytes) between on-prem storage to S3, EFS, FSx.\nOption B is INCORRECT.\nAWS Direct Connect is an offering that helps run workloads that are heavy on bandwidth in AWS.\nAWS Direct Connect enables private and dedicated connections between the on-premises network and AWS.\nAWS Data Sync could be used over the internet or Direct Connect.\nOption C is INCORRECT.\nAWS Data Pipeline is a web service that facilitates data processing and movement between various AWS services (like compute and storage)\nData pipeline also works well with data sources that are on-premise.\nIn the given data migration scenario, data sync is a more apt choice.\nOption D is INCORRECT.\nAWS Migration Hub is a service that facilitates discovery of the existing applications and IT assets and provides a view to better plan and track application migrations.\nReference:\nhttps://aws.amazon.com/datasync/\nhttps://aws.amazon.com/directconnect/\nhttps://aws.amazon.com/datapipeline/\nhttps://aws.amazon.com/migration-hub/\n\nThe best service for migrating hundreds of TB of archived data and 22 TB of active data to S3 and EFS is AWS DataSync.\nAWS DataSync is a fully managed service that simplifies and accelerates the online transfer of data between on-premises storage systems and AWS services. With DataSync, you can automate and accelerate the migration of large amounts of data from on-premises storage systems to AWS, as well as between AWS storage services.\nDataSync provides an optimized network connection and data transfer protocol that can handle large-scale data transfers securely and efficiently. DataSync supports file and object-based transfers, as well as data migration for a wide range of storage systems and protocols, including NFS, SMB, and object storage.\nAWS Direct Connect is a networking service that provides dedicated, private network connections between on-premises data centers and AWS. Direct Connect can be used to improve performance and reduce network costs, but it is not the best service for migrating large amounts of data.\nAWS Data Pipeline is a data processing service that helps you move data between different AWS services and on-premises data sources. Data Pipeline is designed to process and transform data, but it is not the best service for migrating large amounts of data.\nAWS Migration Hub is a service that provides a central location to track and manage application migrations to AWS. Migration Hub can help you plan and execute your migration, but it is not a data migration service.\n\n"
}, {
  "id" : 293,
  "question" : "Which AWS service provides a POSIX compliant, fully managed and cost-effective NFS file storage solution?\n",
  "answers" : [ {
    "id" : "6528a254a01e49a488fed24758b1dda4",
    "option" : "Amazon FSx for Windows File Server",
    "isCorrect" : "false"
  }, {
    "id" : "e3bbb15f0f5c48cc9accf1eb5cc244b3",
    "option" : "AWS Elastic File System",
    "isCorrect" : "true"
  }, {
    "id" : "350260fcc6e844da9496769cbb2c53d8",
    "option" : "Amazon FSx for Lustre",
    "isCorrect" : "false"
  }, {
    "id" : "e07f1a0dcecb491c8a889e2a64cef23a",
    "option" : "Amazon Elastic Block Store.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nAmazon FSx for Windows File Server is built for windows server and is accessible over SMB protocol.\nOption B is CORRECT.\nEFS is a POSIX compliant, fully managed and cost-effective NFS file storage solution that could be used with both on-prem and AWS resources.\nOption C is INCORRECT.\nAmazon FSx for Lustre provides storage for compute workloads.\nAmazon FSx for lustre is a fully managed high-performance and cost-effective storage.\nOption D is INCORRECT.\nAmazon Elastic Block Store is a block storage solution and not an NFS file storage solution.\nReference:\nhttps://aws.amazon.com/fsx/windows/\nhttps://aws.amazon.com/efs/\nhttps://aws.amazon.com/fsx/lustre/\nhttps://aws.amazon.com/ebs/\n\nThe AWS service that provides a POSIX compliant, fully managed and cost-effective NFS file storage solution is the AWS Elastic File System (EFS).\nEFS is a fully managed, scalable, and highly available file storage service for use with Amazon EC2 instances. It provides a POSIX-compliant file system that allows you to store and access files from multiple instances simultaneously. EFS is designed to provide a highly available, scalable, and durable file system that can scale to petabyte-scale storage capacity.\nEFS supports the NFSv4 protocol, which is a widely used file access protocol that is compatible with most operating systems. It also supports a range of performance modes that allow you to choose the appropriate balance between performance and cost for your workload.\nIn contrast, Amazon FSx for Windows File Server is a fully managed Windows file storage service that provides file storage for Windows-based applications. Amazon FSx for Lustre is a high-performance file system designed for compute-intensive workloads. Amazon Elastic Block Store (EBS) is a block-level storage service that is used to store data for EC2 instances.\nTherefore, the correct answer to the question is option B, AWS Elastic File System.\n\n"
}, {
  "id" : 294,
  "question" : "Which of the below-listed protocols are NOT supported by â€œAWS Transfer Familyâ€? (Select TWO)\n",
  "answers" : [ {
    "id" : "f9fba404cbbe4cee995666b6ed3aa762",
    "option" : "SFTP",
    "isCorrect" : "false"
  }, {
    "id" : "59dd843534234f46b24884135088ca8a",
    "option" : "FTPS",
    "isCorrect" : "false"
  }, {
    "id" : "d35068ea052a4009a73bebe901190008",
    "option" : "FTP",
    "isCorrect" : "false"
  }, {
    "id" : "0615e1b1201242f6893de19d4b2b4cbc",
    "option" : "HTTPS",
    "isCorrect" : "true"
  }, {
    "id" : "1a33add433404f319676e10e545b1f92",
    "option" : "SCP.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D and E.\nOption A is INCORRECT.\nSFTP is a protocol supported by â€œAWS Transfer Familyâ€.\nOption B is INCORRECT.\nFTPS is a protocol supported by â€œAWS Transfer Familyâ€.\nOption C is INCORRECT.\nFTP is a protocol supported by â€œAWS Transfer Familyâ€.\nOption D is CORRECT.\nOption E is CORRECT.\nSCP protocol is NOT supported by â€œAWS Transfer Familyâ€.\nhttps://aws.amazon.com/aws-transfer-family/faqs/\n\nAWS Transfer Family is a fully managed service that enables the transfer of files over the Internet using a variety of protocols. It simplifies the process of securely transferring files to and from Amazon S3, Amazon EFS, and Amazon SFTP.\nThe protocols supported by AWS Transfer Family are:\nA. SFTP (Secure File Transfer Protocol): This is a secure file transfer protocol that provides secure file transfer between computers.\nB. FTPS (File Transfer Protocol Secure): This is a secure version of FTP that uses SSL/TLS encryption.\nC. FTP (File Transfer Protocol): This is a standard network protocol used to transfer files from one host to another over a TCP-based network, such as the internet.\nD. HTTPS (Hypertext Transfer Protocol Secure): This is a secure version of HTTP that uses SSL/TLS encryption.\nE. SCP (Secure Copy): This is a protocol for securely copying files between hosts on a network.\nBased on the above descriptions, the protocols that are NOT supported by AWS Transfer Family are FTP and SCP. Therefore, options C and E are the correct answers to this question.\n\n"
}, {
  "id" : 295,
  "question" : "Which fully managed AWS database offering is suitable for analytical workloads and can utilize standard SQL queries and existing BI Tools?\n",
  "answers" : [ {
    "id" : "a7cfcbb3c14b432f903463fd20cc60d9",
    "option" : "Amazon DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "08ea9df860ce435b8783d49f0cb9d54b",
    "option" : "Amazon Redshift",
    "isCorrect" : "true"
  }, {
    "id" : "ae5e040b160b4e63bb33f18dd7b0433c",
    "option" : "Amazon Aurora",
    "isCorrect" : "false"
  }, {
    "id" : "51d407ba17124a03aa4127d500d236b9",
    "option" : "Amazon RDS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nDynamoDB is a NoSQL Key-Value database.\nDynamodb is a document database.\nOption B is CORRECT.\nRedshift is a fully managed data warehouse offering from AWS that allows usage of standard SQL queries and existing BI tools.\nRedshift is suitable for OLAP.\nOption C is INCORRECT.\nAurora is a MySQL and PostgreSQL-compatible relational database service offering from AWS.\nAurora is suitable for OLTP.\nOption D is INCORRECT.\nRDS is a distributed relational database service.\nAurora is an RDS service.\nhttps://aws.amazon.com/dynamodb/\nhttps://aws.amazon.com/redshift/faqs/\nhttps://aws.amazon.com/rds/aurora/\nhttps://aws.amazon.com/rds/\n\nThe fully managed AWS database offering that is suitable for analytical workloads and can utilize standard SQL queries and existing BI Tools is Amazon Redshift.\nAmazon Redshift is a fast, scalable, and fully managed cloud data warehouse service that makes it simple and cost-effective to analyze all your data using SQL and your existing business intelligence (BI) tools. It is designed for big data analytics, providing petabyte-scale data warehousing with columnar storage and massively parallel processing.\nRedshift allows you to use standard SQL queries to analyze your data and supports popular BI tools such as Tableau, Power BI, and QuickSight. It also integrates with Amazon S3, Amazon EMR, and other AWS services, making it easy to move data into and out of Redshift for processing and analysis.\nAmazon DynamoDB is a fully managed NoSQL database service that is optimized for performance and scalability, and is not suitable for analytical workloads that require complex SQL queries. DynamoDB is designed to handle high-traffic web and mobile applications and is ideal for use cases that require low latency and high throughput.\nAmazon Aurora is a MySQL and PostgreSQL-compatible relational database engine that is designed for performance and availability. While Aurora is a suitable option for transactional workloads, it is not optimized for analytical workloads and does not provide the same level of performance and scalability as Redshift.\nAmazon RDS is a managed relational database service that supports multiple database engines, including MySQL, PostgreSQL, Oracle, and SQL Server. While RDS is a suitable option for transactional workloads, it is not optimized for analytical workloads and does not provide the same level of performance and scalability as Redshift.\nIn summary, Amazon Redshift is the best option for analytical workloads that require complex SQL queries and support for existing BI tools.\n\n"
}, {
  "id" : 296,
  "question" : "Amazon Aurora is a relational database compatible with MySQL and PostgreSQL.\nWhich of the below 2 listed are NOT the features of Amazon Aurora? (Select TWO.)\n",
  "answers" : [ {
    "id" : "70497ecff86848e08597b031111d58b4",
    "option" : "Amazon Aurora is slower than standard MySQL databases.",
    "isCorrect" : "true"
  }, {
    "id" : "5ad256990b68428daaa51634231b77fa",
    "option" : "Amazon Aurora provides replication across 3 AZs.",
    "isCorrect" : "false"
  }, {
    "id" : "7bfb590a40a344218f1544b1ce097f22",
    "option" : "Amazon Aurora features self-healing storage system.",
    "isCorrect" : "false"
  }, {
    "id" : "1ee4d0140d55427990ce11311624a2f4",
    "option" : "Amazon Aurora can auto scale upto 32 T.",
    "isCorrect" : "true"
  }, {
    "id" : "e8bf6eb6a2be462eb3b325b9502d7cc2",
    "option" : "Amazon Aurora is fully managed.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, D.\nOption A is CORRECT.\nThe statement is incorrect.\nCorrect statement: Amazon Aurora is faster than standard MySQL (up to five times faster).\nOption B is INCORRECT.\nThis is a feature of Amazon Aurora.\nOption C is INCORRECT.\nThis is a feature of Amazon Aurora.\nOption D is CORRECT.\nThe statement is incorrect.\nCorrect statement: Amazon Aurora can auto-scale up to 128 TB.Option E is INCORRECT.\nThis is a feature of Amazon Aurora.\nhttps://aws.amazon.com/rds/aurora/\n\nSure, I'd be happy to provide a detailed explanation of the features of Amazon Aurora and which two of the listed options are not among its features.\nAmazon Aurora is a cloud-based relational database service that is fully managed by AWS. It is designed to be highly available, scalable, and compatible with MySQL and PostgreSQL. Here are some of the key features of Amazon Aurora:\nA. Amazon Aurora is slower than standard MySQL databases. This statement is not true. Amazon Aurora is actually designed to be faster than standard MySQL databases. It uses a distributed, fault-tolerant architecture that allows it to deliver high performance and availability. In addition, it has an optimized storage subsystem that is designed to deliver high I/O performance, and it can automatically scale its compute and storage resources based on demand.\nB. Amazon Aurora provides replication across 3 AZs. This statement is true. Amazon Aurora is designed to be highly available and fault-tolerant, and it achieves this through its ability to replicate data across multiple availability zones (AZs). When you create an Aurora database, you can specify that you want it to be replicated across three AZs, which means that if one AZ fails, your database will still be available in the other two AZs.\nC. Amazon Aurora features self-healing storage system. This statement is true. One of the key features of Amazon Aurora is its self-healing storage system. This system is designed to automatically detect and repair any issues with the underlying storage infrastructure, without any intervention required from the user. This helps to ensure that your data is always available and that you don't experience any downtime due to storage issues.\nD. Amazon Aurora can auto scale up to 32 T. This statement is true. Amazon Aurora is designed to be highly scalable, and it can automatically scale its compute and storage resources based on demand. It can scale its storage capacity up to 64 TB, and it can also scale its compute capacity up to 32 vCPUs and 244 GB of memory.\nE. Amazon Aurora is fully managed. This statement is true. Amazon Aurora is a fully managed database service, which means that AWS handles all of the administrative tasks involved in running a database, such as provisioning, patching, and backups. This allows you to focus on developing your applications rather than worrying about database maintenance.\nTherefore, the two statements that are not features of Amazon Aurora are A. Amazon Aurora is slower than standard MySQL databases and D. Amazon Aurora can auto scale up to 32 T.\n\n"
}, {
  "id" : 297,
  "question" : "Data on Amazon EBS volumes could be backed up by performing point-in-time snapshots.\nSnapshots are incremental backups.\nOnly the changed blocks after the most recent snapshot are saved. Where are these EBS snapshots stored?\n",
  "answers" : [ {
    "id" : "cb4ca80e1cac4db3ad902402e9565656",
    "option" : "Amazon EBS Volume",
    "isCorrect" : "false"
  }, {
    "id" : "52ba5a4f0e6e4589a7a33248e586b228",
    "option" : "Amazon S3",
    "isCorrect" : "true"
  }, {
    "id" : "1e65b6e93cb84c1bb20a03ccd1c709ca",
    "option" : "Amazon EFS",
    "isCorrect" : "false"
  }, {
    "id" : "1d5bf9d7c1fd436c8cba13c05a3411d8",
    "option" : "Amazon Aurora.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nSnapshots are point-in-time backups of EBS volumes and are stored in Amazon S3.\nOption B is CORRECT.\nRefer to the link below for details.\nOption C is INCORRECT.\nSnapshots are stored in Amazon S3.\nOption D is INCORRECT.\nSnapshots are not stored in Aurora.\nThe snapshots are stored in Amazon S3.\nReference:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSSnapshots.html\n\nEBS (Elastic Block Store) volumes are virtual hard drives that are attached to EC2 instances. EBS volumes are designed for durability and availability, but they are not immune to failure, data loss, or accidental deletion. Therefore, it is important to create backups of EBS volumes to prevent data loss and to recover from failures.\nOne way to back up EBS volumes is by taking snapshots, which are point-in-time copies of the volume's data. A snapshot captures the current state of the EBS volume, including all its blocks, whether they contain data or not. Snapshots are incremental backups, which means that only the changed blocks after the most recent snapshot are saved, reducing the time and cost of backup operations.\nWhen you create an EBS snapshot, AWS stores it in Amazon S3 (Simple Storage Service), which is a highly scalable and durable object storage service that can store and retrieve any amount of data from anywhere on the internet. Amazon S3 uses a pay-as-you-go pricing model, which means that you only pay for the storage and data transfer that you use, without any upfront costs or minimum fees.\nAmazon S3 provides several features that make it suitable for storing EBS snapshots, including:\nHigh durability: Amazon S3 stores data across multiple availability zones (AZs) within a region, which ensures that your data is protected against hardware failures, software errors, and natural disasters. Versioning: Amazon S3 can keep multiple versions of the same object, which allows you to restore a previous version of an EBS snapshot if needed. Encryption: Amazon S3 can encrypt your data at rest using server-side encryption (SSE) or client-side encryption (CSE), which helps to protect your data from unauthorized access.\nIn summary, EBS snapshots are stored in Amazon S3, which provides a highly durable, scalable, and cost-effective storage solution for backup and recovery operations.\n\n"
}, {
  "id" : 298,
  "question" : "Which of the below listed is NOT a requirement for setting up (enabling) S3 replication?\n",
  "answers" : [ {
    "id" : "cfe9c716c2aa423b8f75bb271c4359cd",
    "option" : "Bucket owner of the source bucket should have both source and destination Regions enabled for their account.",
    "isCorrect" : "false"
  }, {
    "id" : "e52956d1502d41eba35656060cf4cadc",
    "option" : "Versioning should be enabled for buckets in both source and destination.",
    "isCorrect" : "false"
  }, {
    "id" : "5d8f7b46dfe94693bacc66ef9b857f66",
    "option" : "Amazon S3 must have been granted object replication permissions from the source bucket to the destination bucket on behalf of the owner.",
    "isCorrect" : "false"
  }, {
    "id" : "5a3ded75faf44cf3850d7c6a77c2dd3b",
    "option" : "Bucket owner of the destination should have both source and destination Region enabled for their account.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nThe given condition is a requirement for enabling S3 replication.\nOption B is INCORRECT.\nThe given condition is a requirement for enabling S3 replication.\nOption C is INCORRECT.\nThe given condition is a requirement for enabling S3 replication.\nOption D is CORRECT.\nThe given condition is incorrect.\nBucket owner of the destination should have destination Region enabled for their account.\nReference:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\n\nS3 replication is a service provided by Amazon S3 that automatically replicates data from a source S3 bucket to a destination S3 bucket in different Regions. This helps to ensure that data is available for access even if one of the Regions becomes unavailable.\nTo set up (enable) S3 replication, the following requirements must be met:\nA. Bucket owner of the source bucket should have both source and destination Regions enabled for their account.\nThis means that the owner of the source bucket must have enabled the Regions where the source and destination buckets are located for their account. The owner can do this by logging in to the AWS Management Console, navigating to the S3 service, and selecting the desired Regions.\nB. Versioning should be enabled for buckets in both source and destination.\nVersioning is a feature in Amazon S3 that keeps multiple versions of an object in the same bucket. Enabling versioning in both the source and destination buckets ensures that all versions of the objects are replicated.\nC. Amazon S3 must have been granted object replication permissions from the source bucket to the destination bucket on behalf of the owner.\nTo enable S3 replication, the owner of the source bucket must grant Amazon S3 permission to replicate objects from the source bucket to the destination bucket on their behalf. This is done by creating a replication configuration in the source bucket that specifies the destination bucket and the replication rules.\nD. Bucket owner of the destination should have both source and destination Region enabled for their account.\nThe owner of the destination bucket must also have enabled the Regions where the source and destination buckets are located for their account.\nTherefore, the answer to the question is B. Versioning should be enabled for buckets in both source and destination. It is not a requirement for setting up S3 replication, but it is recommended to enable versioning to ensure that all versions of the objects are replicated.\n\n"
}, {
  "id" : 299,
  "question" : "A start-up organization is planning to use the AWS Budget so that it can plan its instance reservations, along with service usage and costs.\nIt is agreed that 10 budgets would be created. What would be the cost accured for these 10 AWS budgets by the organization?\n",
  "answers" : [ {
    "id" : "97e70ed4dbef41d5b58eac144f10ae6c",
    "option" : "(9 * rate of budget) per day.",
    "isCorrect" : "false"
  }, {
    "id" : "1ab8a960673f4cbd9479700a996a5afb",
    "option" : "(8 * rate of budget) per day.",
    "isCorrect" : "true"
  }, {
    "id" : "eb16663f925d4e589ffcd9ed1cb83e2a",
    "option" : "(7 * rate of budget) per day.",
    "isCorrect" : "false"
  }, {
    "id" : "81420023849b4e81848bcd954f138610",
    "option" : "(10 * rate of budget) per day.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nYou can add actions to your budgets to control IAM and Service Control Policy permissions as well as AWS resources when thresholds are exceeded (or forecasted to exceed)\nYour first two action-enabled budgets are free (regardless of the number of actions you configure per budget) per month.\nAfterward, each subsequent action-enabled active budget will incur a $0.10 daily cost.\nhttps://aws.amazon.com/aws-cost-management/pricing/\n\nTo understand the cost accrued by the organization for 10 AWS budgets, let's first understand what AWS Budgets are.\nAWS Budgets is a free service provided by Amazon Web Services that allows customers to set custom cost and usage budgets for their AWS services. AWS Budgets enables customers to plan their service usage and costs, and get notified when their usage or costs exceed a certain threshold. Customers can create up to 10 AWS Budgets per account.\nNow, let's consider the options provided in the question.\nA. (9 * rate of budget) per day. B. (8 * rate of budget) per day. C. (7 * rate of budget) per day. D. (10 * rate of budget) per day.\nAs mentioned earlier, customers can create up to 10 AWS Budgets per account. Therefore, the correct answer would be D, which states that the cost accrued for these 10 AWS budgets by the organization would be (10 * rate of budget) per day.\nIt is important to note that the rate of budget would depend on the specific AWS service being used, and the pricing model for that service. Therefore, the organization would need to determine the appropriate rate of budget for each of the 10 AWS Budgets they create based on their usage and cost requirements.\n\n"
}, {
  "id" : 300,
  "question" : "Which of the below-given service could be used in the AWS cloud for the management of encryption keys?\n",
  "answers" : [ {
    "id" : "2287fc0df81b4cc4867cb495cf92eaff",
    "option" : "Amazon Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "799bfe2c605c4a808f9ee12179e134ca",
    "option" : "Amazon Cognito",
    "isCorrect" : "false"
  }, {
    "id" : "8a4ee572398a402d8dd7a7e0c60ddb96",
    "option" : "CloudHSM",
    "isCorrect" : "true"
  }, {
    "id" : "0eab1983130d4f9ab9dc02c214539762",
    "option" : "Amazon GuardDuty.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nCloudHSM (HSM stands for Hardware Security Module) helps in the management of encryption keys.\nStandards compliant CloudHSM is a fully-managed service.\nOption A is INCORRECT.\nAmazon Inspector helps improve the security and compliance adherence of the applications deployed on the AWS cloud.\nOption B is INCORRECT.\nAmazon Cognito is an AWS service for developers that simplifies user sign-up and sign-in to their developed mobile apps and web apps.\nOption C is CORRECT.\nRefer to the explanation above.\nOption D is INCORRECT.\nAmazon GuardDuty performs continuous monitoring to protect AWS account, S3 data and workloads from any malicious, unauthorized activities.\nhttps://aws.amazon.com/inspector/\nhttps://aws.amazon.com/cognito/\nhttps://aws.amazon.com/cloudhsm/\nhttps://aws.amazon.com/guardduty/\n\nThe correct answer for this question is C. CloudHSM.\nCloudHSM (Hardware Security Module) is a service provided by AWS that helps in the management of encryption keys. It offers secure key storage and cryptographic operations within a dedicated hardware device. Customers can use CloudHSM to generate, store, and manage their own encryption keys for various AWS services and other applications.\nCloudHSM uses FIPS 140-2 Level 3 validated HSMs and provides highly secure key storage, tamper-evident auditing, and access controls to customers. Customers can choose to have a single tenant or multi-tenant HSM in their environment, depending on their security requirements.\nAmazon Inspector is a security assessment service that helps in identifying security issues and vulnerabilities in applications running on AWS infrastructure. It does not provide any key management functionality.\nAmazon Cognito is an identity management service that provides user authentication, authorization, and user management for web and mobile applications. It does not provide any key management functionality.\nAmazon GuardDuty is a threat detection service that continuously monitors AWS infrastructure for malicious activity and unauthorized behavior. It does not provide any key management functionality.\nIn summary, CloudHSM is the AWS service that can be used for the management of encryption keys in the cloud.\n\n"
}, {
  "id" : 301,
  "question" : "To maximize user satisfaction, you are asked to improve the performance of the application for local and global users.\nAs part of this initiative, you must monitor the application endpoint health and route traffic to the most appropriate application endpoint.\nWhich service will you prefer to use?\n",
  "answers" : [ {
    "id" : "7fe366cab21b41dba2a6ae0ca6860bbe",
    "option" : "Amazon Global Accelerator",
    "isCorrect" : "true"
  }, {
    "id" : "c7bdc059c7174424938847415655da02",
    "option" : "Amazon DAX accelerator",
    "isCorrect" : "false"
  }, {
    "id" : "08256a71179345f08b85acfc2deaa1ca",
    "option" : "Amazon S3 transfer acceleration",
    "isCorrect" : "false"
  }, {
    "id" : "7f1a819b5b7e4d66a0663484254a7a86",
    "option" : "AWS Direct Connect.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nGlobal accelerator is a networking service that utilizes AWS global network to optimize the â€œuser to applicationâ€ path.\nThe performance benefits realized by the use of the Global accelerator can be tested using a speed comparison tool provided by AWS.\nGlobal accelerator differs from S3 transfer acceleration and DynamoDB accelerator.\nS3 transfer acceleration, accelerates the transfers of files to the S3 bucket by utilizing edge locations.\nFully managed DynamoDB Accelerator (DAX) is a highly available in-memory cache for Dynamodb.\nOption A is CORRECT.\nRefer to the explanation above.\nOption B is INCORRECT.\nRefer to the explanation above.\nOption C is INCORRECT.\nRefer to the explanation above.\nOption D is INCORRECT.\nAWS Direct Connect is an AWS offering that simplifies setting up dedicated network connectivity between AWS and on-premises infrastructure.\nhttps://aws.amazon.com/global-accelerator/\nhttps://aws.amazon.com/dynamodb/dax/\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/transfer-acceleration.html\nhttps://aws.amazon.com/directconnect/\n\nThe service that should be used to monitor the application endpoint health and route traffic to the most appropriate endpoint is Amazon Global Accelerator.\nAmazon Global Accelerator is a networking service that improves the availability and performance of applications by providing local or global users with a static anycast IP address. This anycast IP address is associated with multiple application endpoints in one or more AWS regions.\nBy using Amazon Global Accelerator, you can monitor the health of your application endpoints and automatically route traffic to the most available and responsive endpoint based on proximity, health, and routing policies that you can configure. This results in improved application performance and availability for users.\nAmazon DAX accelerator is a caching service that helps to improve the performance of applications by caching frequently accessed data in memory. However, it is not relevant to the requirement of monitoring application endpoint health and routing traffic to the most appropriate endpoint.\nAmazon S3 transfer acceleration is a feature that helps to accelerate data transfers to and from Amazon S3 by optimizing the network path between the client and the S3 bucket. It is not relevant to the requirement of monitoring application endpoint health and routing traffic to the most appropriate endpoint.\nAWS Direct Connect is a service that provides a dedicated network connection from your on-premises environment to AWS. It is used for scenarios where you need a dedicated, private connection rather than using the public internet. It is not relevant to the requirement of monitoring application endpoint health and routing traffic to the most appropriate endpoint.\nTherefore, the best option to use in this scenario is Amazon Global Accelerator.\n\n"
}, {
  "id" : 302,
  "question" : "Which of the following resources can AWS WAF integrate with to protect web applications against common web exploits? (Select TWO.)\n",
  "answers" : [ {
    "id" : "91194701601149dcb62c38d594faedd0",
    "option" : "Amazon CloudFront",
    "isCorrect" : "true"
  }, {
    "id" : "c84a1f2a17f64e42a15462bf2e5ac4b9",
    "option" : "Internet Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "5b4f1200823c4b72a3c07fcf019349bb",
    "option" : "Web server hosted on Amazon EC2 instance.",
    "isCorrect" : "false"
  }, {
    "id" : "788e7c230f47426dbc3ec8d0bb02efb1",
    "option" : "Application Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "52e51d22dbe14d6890a5f3ee91219b70",
    "option" : "Static Website hosted on Amazon S3 bucket.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: A and D.\nAWS WAF can be deployed with the following resources to protect web applications &amp; APIs from common web exploits.\na.\nApplication Load Balancer.\nb.\nAmazon CloudFront.\nc.\nAmazon API Gateway.\nd.\nAWS AppSync.\nOptions B, C &amp; E are incorrect as AWS WAF is not integrated with Internet Gateway, Amazon EC2 instance or Amazon S3 bucket.\nFor more information on AWS WAF, refer to the following URL:\nhttps://aws.amazon.com/waf/\n\nAWS WAF (Web Application Firewall) is a managed service that helps protect web applications against a wide range of web exploits that could affect the application's availability, compromise the security of the application or the data stored in it, or violate compliance requirements.\nAWS WAF can integrate with several AWS services to provide protection for web applications. The correct answer to the question is A and D, as AWS WAF can integrate with Amazon CloudFront and Application Load Balancer (ALB).\nA. Amazon CloudFront Amazon CloudFront is a content delivery network (CDN) that can distribute content to users around the world with low latency and high transfer speeds. When AWS WAF integrates with Amazon CloudFront, it can inspect and filter traffic that is sent to the web application through the CDN. This integration allows AWS WAF to protect against common web exploits such as SQL injection, cross-site scripting (XSS), and distributed denial of service (DDoS) attacks.\nD. Application Load Balancer Application Load Balancer (ALB) is a load balancing service that can distribute incoming traffic across multiple targets, such as EC2 instances, containerized applications, and Lambda functions. When AWS WAF integrates with an ALB, it can inspect and filter traffic before it reaches the targets. This integration allows AWS WAF to protect against common web exploits such as SQL injection, cross-site scripting (XSS), and HTTP flooding.\nB. Internet Gateway An internet gateway (IGW) is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in a VPC and the internet. However, AWS WAF cannot integrate with an internet gateway to protect web applications. Instead, AWS WAF is integrated with a web application hosted on an EC2 instance or behind an ALB.\nC. Web server hosted on Amazon EC2 instance. When a web application is hosted on an Amazon EC2 instance, AWS WAF can integrate with the web server to provide protection against common web exploits. AWS WAF can be deployed on a separate instance, which acts as a reverse proxy, or as a software module on the web server itself. This integration allows AWS WAF to protect against common web exploits such as SQL injection, cross-site scripting (XSS), and HTTP flooding.\nE. Static Website hosted on Amazon S3 bucket. When a static website is hosted on an Amazon S3 bucket, AWS WAF cannot integrate with the website to provide protection against common web exploits. AWS WAF is designed to integrate with web applications that are hosted on web servers or behind load balancers. However, S3 bucket can be protected from malicious access through S3 bucket policies and access control lists (ACLs).\nIn conclusion, AWS WAF can integrate with Amazon CloudFront and Application Load Balancer (ALB) to protect web applications against common web exploits such as SQL injection, cross-site scripting (XSS), and HTTP flooding.\n\n"
}, {
  "id" : 303,
  "question" : "A customer has expressed his intent to opt for a support plan to get access to the below 6 security checks.\nWhich support plans offer these security checks at a minimal price point? (Select TWO) Security check 1: S3 Bucket Permissions Security check 2: Security Groups - Specific Ports Unrestricted Security check 3: IAM Use Security check 4: MFA on Root Account Security check 5: EBS Public Snapshots Security check 6: RDS Public Snapshots.\n",
  "answers" : [ {
    "id" : "08b8fd631f9542d7ac2bcfa46ac1ffba",
    "option" : "Basic Plan",
    "isCorrect" : "true"
  }, {
    "id" : "2b8ca3a7771c4387bc16d8dba181988a",
    "option" : "Developer Plan",
    "isCorrect" : "true"
  }, {
    "id" : "24d2905a81cd433aa1183cc27b82f5f1",
    "option" : "Business Plan",
    "isCorrect" : "false"
  }, {
    "id" : "f1f5ac762b2b4152981fc15b718cf8da",
    "option" : "Enterprise Plan",
    "isCorrect" : "false"
  }, {
    "id" : "e08190f6390a4c9697d3b73dbdd24750",
    "option" : "Gold Plan.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A, B.\nOption A is CORRECT.\nCustomers of basic support plan gets access to the above listed 6 security checks.\nThe basic support plan is free.\nOption B is CORRECT.\nCustomers of developer support plan get access to the above listed 6 security checks.\nDeveloper support plan is cheaper than Business and Enterprise support plan.\nOption C is INCORRECT.\nCustomers with a Business support plan have access to all the trusted advisor checks, including the listed 6 security checks.\nBut this option is incorrect as this support plan is costlier than the Basic and Developer support plan.\nOption D is INCORRECT.\nAlthough customers with Enterprise support plans have access to all the trusted advisor checks, including the listed 6 security checks, this option is incorrect as this support plan is costlier than the Basic, Developer, and Business support plan.\nOption E is INCORRECT as this is not a valid option.\nhttps://aws.amazon.com/premiumsupport/technology/trusted-advisor/\n\nThe Basic and Developer support plans offer the Security checks 1, 2, 3, and 4 at a minimal price point.\nThe Basic support plan offers 24/7 customer service, documentation, whitepapers, and support forums. Additionally, it provides access to AWS Trusted Advisor, which offers the security checks 1, 2, 3, and 4.\nThe Developer support plan offers all the benefits of the Basic plan plus a Developer Support Center that provides one-on-one technical support. It also includes access to AWS Trusted Advisor, which offers the security checks 1, 2, 3, and 4.\nHowever, the Security checks 5 and 6 are not offered by AWS Trusted Advisor and require the Business, Enterprise, or Gold support plan to access.\nThe Business support plan offers all the benefits of the Basic and Developer plans plus personalized support, third-party software support, and infrastructure event management. It also includes access to the AWS Support API, which provides access to Security check 5 and 6.\nThe Enterprise and Gold support plans offer all the benefits of the lower tier plans plus a dedicated Technical Account Manager, a dedicated support team, and more advanced infrastructure event management. Both plans also include access to the AWS Support API, which provides access to Security check 5 and 6.\nTherefore, the correct answers to this question are A. Basic Plan and B. Developer Plan.\n\n"
}, {
  "id" : 304,
  "question" : "A growing food delivery start-up intends to implement DDoS protection for its applications on AWS.\nWhich fully managed service offering from AWS ensures customers protection from DDoS attack?\n",
  "answers" : [ {
    "id" : "d5ebc60b9962414cbaf57eb825e4e17e",
    "option" : "AWS Shield",
    "isCorrect" : "true"
  }, {
    "id" : "32876fe998434e4dbded0c7548988272",
    "option" : "AWS WAF - Web Application Firewall",
    "isCorrect" : "false"
  }, {
    "id" : "5c3a529c63bf4bbabf79d7f8baca889c",
    "option" : "AWS Firewall Manager",
    "isCorrect" : "false"
  }, {
    "id" : "cb4c572188ce47f082ec981827bfee8e",
    "option" : "Amazon GuardDuty.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAWS Shield is a fully managed service from AWS that ensures protection from DDoS attacks.\nOption B is INCORRECT.\nAWS WAF helps us protect web applications and APIs against attacks by configuring various rules and conditions (like allow, block etc., based on defined conditions).\nOption C is INCORRECT.\nAWS Firewall Manager enables management and configuration of firewall across AWS accounts and applications centrally.\nOption D is INCORRECT.\nAmazon GuardDuty performs continuous monitoring to protect AWS account, S3 data and workloads from any malicious, unauthorized activities.\nhttps://aws.amazon.com/shield/\nhttps://aws.amazon.com/waf/\nhttps://aws.amazon.com/firewall-manager/\nhttps://aws.amazon.com/guardduty/\n\nThe correct answer to the question is A. AWS Shield.\nExplanation: AWS Shield is a fully managed service offered by Amazon Web Services (AWS) that provides customers with protection from Distributed Denial of Service (DDoS) attacks. DDoS attacks are cyber-attacks that aim to disrupt the normal traffic of a website, application or network by overwhelming them with a large amount of traffic from multiple sources.\nAWS Shield offers two tiers of protection:\nStandard tier: It provides automatic DDoS mitigation for all AWS customers at no extra charge. This protection is applied to all Amazon Elastic Compute Cloud (EC2), Elastic Load Balancing, Amazon CloudFront, AWS Global Accelerator, AWS Route 53, and Amazon Virtual Private Cloud (VPC) resources. Advanced tier: It offers additional protection beyond the standard tier for customers who have higher requirements for availability and protection against attacks. It provides access to 24/7 support and protection against more sophisticated attacks, such as Network and Transport Layer attacks.\nAWS Shield is easy to use and can be enabled for any AWS resource with just a few clicks in the AWS Management Console or through the AWS Shield API. With AWS Shield, customers can protect their applications and workloads against DDoS attacks without any upfront costs or long-term commitments.\nAWS WAF (Web Application Firewall) is another AWS service that can be used to protect against DDoS attacks. However, it is primarily used for protecting web applications from common web exploits and vulnerabilities such as SQL injection and cross-site scripting (XSS).\nAWS Firewall Manager is a service that allows customers to centrally configure and manage firewall rules across multiple AWS accounts and resources. It is not designed specifically for DDoS protection.\nAmazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior in AWS accounts and workloads. It is not a DDoS protection service.\nTherefore, the correct answer to the question is A. AWS Shield.\n\n"
}, {
  "id" : 305,
  "question" : "Which AWS service offering uses machine learning and graph theory capability on automatically collected log data to help you conduct faster and efficient security investigations?\n",
  "answers" : [ {
    "id" : "5d4a32f011b6409b887a91ec41880a41",
    "option" : "Amazon Macie",
    "isCorrect" : "false"
  }, {
    "id" : "b2513aada92b4d79a2cab930585bb155",
    "option" : "Amazon Detective",
    "isCorrect" : "true"
  }, {
    "id" : "c66f43ca01fe42f3a27c6253af1d7a8c",
    "option" : "AWS Artifact",
    "isCorrect" : "false"
  }, {
    "id" : "b1956c1ad0a347398a0d4fcceb60dfd8",
    "option" : "Amazon GuardDuty.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nAmazon Macie is a fully managed service from AWS that provides data security and privacy by utilizing Amazon's machine learning and pattern matching capabilities.\nOption B is CORRECT.\nAmazon Detective is a security service that uses machine learning capabilities on the automatically collected log data to help customers perform efficient and fast security investigations.\nOption C is INCORRECT.\nAWS Artifact is a central resource for all the information about compliance.\nAWS artifact provides on-demand access to compliance reports at no additional cost.\nOption D is INCORRECT.\nAmazon GuardDuty performs continuous monitoring to protect AWS account, S3 data and workloads from any malicious, unauthorized activities.\nhttps://aws.amazon.com/macie/\nhttps://aws.amazon.com/detective/faqs/\nhttps://aws.amazon.com/artifact/\nhttps://aws.amazon.com/guardduty/\n\nThe correct answer is B. Amazon Detective.\nAmazon Detective is an AWS service that helps you to analyze, investigate, and quickly identify the root cause of potential security issues or suspicious activities across your AWS resources. It uses machine learning and graph theory algorithms to analyze automatically collected log data, such as CloudTrail, VPC Flow Logs, and DNS logs, to create interactive visualizations and graphs that provide a detailed picture of your AWS environment.\nWith Amazon Detective, you can easily analyze and investigate security issues without the need for any additional software, infrastructure, or expertise. The service provides visualizations of AWS resources, such as EC2 instances and security groups, that can help you to identify the source of suspicious activities, and it can automatically identify patterns and anomalies in log data that could indicate a security threat.\nFurthermore, Amazon Detective helps you to conduct faster and more efficient security investigations by reducing the time and effort required to analyze large volumes of log data. The service automatically aggregates and summarizes log data, and it provides a unified view of security-related events across your AWS environment.\nAmazon Detective also provides built-in collaboration features, such as sharing and commenting on findings, which can help you to work with your security team to quickly remediate security issues and improve your overall security posture.\nIn summary, Amazon Detective uses machine learning and graph theory capabilities on automatically collected log data to help you conduct faster and more efficient security investigations across your AWS resources.\n\n"
}, {
  "id" : 306,
  "question" : "A program manager wants: 1- To get an alert when the costs are forecasted to exceed. 2- Understand the usage and cost over time, preferably via visualization to identify areas wherein savings could be done. Which AWS products should be used to cater for these requirements?\n",
  "answers" : [ {
    "id" : "6bd6ca2183f245d2a9db3e95aef4c0c3",
    "option" : "AWS Budget and AWS Cost Explorer",
    "isCorrect" : "true"
  }, {
    "id" : "d8708b4d19b14e80a6491723da5fb748",
    "option" : "AWS Budget and AWS Savings plan",
    "isCorrect" : "false"
  }, {
    "id" : "5fc9255a22b34013bf772a4e8ce38850",
    "option" : "AWS Savings plan and AWS Cost Explorer",
    "isCorrect" : "false"
  }, {
    "id" : "c1da898f36fa45c594fe0d088adbf60d",
    "option" : "AWS Savings plan and AWS Organization.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nAWS Budget: AWS Budget enables setting alerts via setting up custom budgets so that we are notified when cost and usage are forecasted to exceed or if cost and usage exceeds.\nAWS Cost Explorer: AWS Cost Explorer helps analyze the usage and cost incurred over time by visualization through an easy to use interface.\nAWS Savings plan: AWS Savings plan is a pricing model designed to offer up to 72% of savings to customers on compute usage of Amazon EC2, Amazon Fargate, and AWS Lambda.\nAWS Organization: AWS Organization provides capabilities to centrally perform management and governance of billing, compliance adherence, security, resource sharing and access control.\nOption A is CORRECT.\nAWS Budget and AWS Cost Explorer should be used.\nOption B is INCORRECT.\nOption C is INCORRECT.\nOption D is INCORRECT.\nReference:\nhttps://aws.amazon.com/aws-cost-management/aws-budgets/\nhttps://aws.amazon.com/aws-cost-management/aws-cost-explorer/\nhttps://aws.amazon.com/savingsplans/\nhttps://aws.amazon.com/organizations/faqs/\n\nThe program manager wants to have visibility into AWS cost and usage to be able to control expenses and identify areas where savings could be done. To meet these requirements, two AWS products can be used: AWS Budget and AWS Cost Explorer.\nAWS Budget provides a way to set custom cost and usage budgets for AWS services. It allows the program manager to set alerts for when costs are forecasted to exceed the budget. AWS Budget also provides reports to view the actual costs incurred and compare them against the budget. By using AWS Budget, the program manager can proactively monitor their AWS costs and take actions to control them.\nAWS Cost Explorer provides cost and usage reports that enable program managers to visualize and analyze their AWS usage and cost over time. It provides an intuitive interface that allows program managers to identify areas where they could potentially reduce their expenses. AWS Cost Explorer allows program managers to filter data by service, account, and time period to get a comprehensive view of their AWS usage.\nTherefore, the correct answer is A. AWS Budget and AWS Cost Explorer. AWS Savings Plan is not necessary for the requirements mentioned in the question. AWS Organization is used to manage multiple AWS accounts, but it does not provide cost and usage visibility.\n\n"
}, {
  "id" : 307,
  "question" : "An administrator is tasked to configure privileges for the new joiners in the department.\nThe admin is selectively granting privileges and ensuring that not all the team members can access all the resources. Which principle is the administrator following?\n",
  "answers" : [ {
    "id" : "757b1976c3504445a8e8759abbe6751d",
    "option" : "Principle of privileged users",
    "isCorrect" : "false"
  }, {
    "id" : "55ad6f4edd9d433192068bd858d19ac5",
    "option" : "Least privilege of group principle",
    "isCorrect" : "false"
  }, {
    "id" : "8b1c6a6e64514485b163f28094857734",
    "option" : "Best practices of permission advisory",
    "isCorrect" : "false"
  }, {
    "id" : "be9689b5ec4745cd8b9b240ab5307bb4",
    "option" : "Principle of least privilege.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nThis is not a valid option.\nOption B is INCORRECT.\nThis is not a valid option.\nOption C is INCORRECT.\nThis is not a valid option.\nOption D is CORRECT.\nThe administrator follows the â€œPrinciple of least privilegeâ€ as not all the privileges are granted to all the new joiners.\nThe privileges are being selectively granted.\nReference:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege\n\nThe principle that the administrator is following is the Principle of Least Privilege, which is the practice of limiting user access rights to the minimum level necessary to perform their job functions. This principle is an essential part of any security strategy and helps to minimize the risk of a security breach.\nBy selectively granting privileges, the administrator is ensuring that each team member has access only to the resources they need to perform their job functions. This helps to prevent accidental or intentional misuse of resources and limits the potential damage that could occur in the event of a security breach.\nThe Principle of Least Privilege is often used in combination with other security principles and best practices, such as the Principle of Privileged Users, which involves restricting access to high-level privileges and closely monitoring privileged accounts. It is also related to the Best Practices of Permission Advisory, which involves regularly reviewing and auditing user permissions to ensure that they are appropriate and up-to-date.\nOverall, the Principle of Least Privilege is a fundamental security principle that helps organizations to minimize risk and protect their resources. By following this principle, administrators can ensure that each team member has access only to the resources they need to do their job, while limiting the potential damage that could occur in the event of a security breach.\n\n"
}, {
  "id" : 308,
  "question" : "A customer in Tokyo requires to transfer files (total of 1TB) to his S3 bucket in us-east-1\nWhich of the below AWS offerings can facilitate this in an easy, secured and fast way?\n",
  "answers" : [ {
    "id" : "ea7920d3086847c0b6586faec57b8873",
    "option" : "AWS Global accelerator",
    "isCorrect" : "false"
  }, {
    "id" : "3abf0be9b68f40ef894c547a47be8541",
    "option" : "AWS Edge Locations",
    "isCorrect" : "false"
  }, {
    "id" : "16954493cb224cf08d955f70881aa95c",
    "option" : "AWS S3 transfer acceleration",
    "isCorrect" : "true"
  }, {
    "id" : "4dc8297397674c478f90498c400a0414",
    "option" : "AWS Snowball family.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT.\nAWS Global accelerator provides static IP to enable fixed entry points to your applications reducing the overhead of managing specific IP addresses.\nAWS Global Accelerator helps by improving the application's availability and performance.\nIt does not help to upload files to S3.\nOption B is INCORRECT.\nEdge locations are the sites that are used by AWS CDN (Cloudfront) to serve contents to the users.\nOne of the characteristics of edge locations is that edge locations cache data.\nHence it helps serve the requests faster.\nIt is not used to help upload files to an S3 bucket.\nOption C is CORRECT.\nAWS S3 transfer acceleration can use optimized network protocols and the AWS edge infrastructure when uploading files to the S3 bucket.\nThis method is easy, secure and fast.\nOption D is INCORRECT.\nSnowball facilitates the transfer of huge batches of data in one go.\nThe typical turnaround for AWS Snowball is 5-7 days.\nIt is not suitable for 1TB files.\nReference:\nhttps://aws.amazon.com/global-accelerator/faqs/\nhttps://wa.aws.amazon.com/wat.concept.edge-location.en.html\nhttps://aws.amazon.com/s3/faqs/\n\nThe best option to transfer files to an S3 bucket in a different region in a fast, secure, and easy way would be through AWS S3 Transfer Acceleration (Option C).\nAWS S3 Transfer Acceleration is a service that enables fast, secure, and easy transfers of files over the internet to Amazon S3 buckets. It takes advantage of Amazon CloudFront's globally distributed edge locations to accelerate transfers over the public internet.\nTo use AWS S3 Transfer Acceleration, the customer can simply enable it on their S3 bucket in us-east-1, and then upload the files using a specially generated endpoint URL that includes the acceleration domain name. This endpoint URL can be easily shared with the customer in Tokyo.\nAWS Global Accelerator (Option A) is a service that improves the availability and performance of applications by routing traffic through the AWS global network infrastructure. While it can improve performance, it is not specifically designed for file transfers and may not be the most efficient option for this use case.\nAWS Edge Locations (Option B) are part of the Amazon CloudFront content delivery network and are used to cache content closer to end-users for faster access. They may be used in conjunction with AWS S3 Transfer Acceleration to further optimize performance, but alone, they are not the optimal solution for transferring large files across regions.\nAWS Snowball family (Option D) is a physical data transport solution that allows for the secure and efficient transfer of large amounts of data into and out of AWS. It involves physically shipping a device (Snowball or Snowball Edge) to the customer to load the data onto, which is then shipped back to AWS to be uploaded. While this is a secure and efficient option for large data sets, it may not be the most cost-effective or practical solution for a single transfer of 1TB.\n\n"
}, {
  "id" : 309,
  "question" : "Which below statement correctly represents the use of AWS Edge locations?\n",
  "answers" : [ {
    "id" : "c44b515872aa4534b87445efb75c9aaa",
    "option" : "Eases (reduces) load on origin server by caching the responses.",
    "isCorrect" : "true"
  }, {
    "id" : "05fd43fa02e447a0988a23f512f90e01",
    "option" : "Eases (reduces) the latency and network traffic by hosting applications at locations closer to the end-users.",
    "isCorrect" : "false"
  }, {
    "id" : "307796ff24ef4bb2a4297ba32100cc7a",
    "option" : "Eases (reduces) the latency and network traffic by hosting databases at locations closer to the end-users.",
    "isCorrect" : "false"
  }, {
    "id" : "23a6a385e078411fa374e1862db83ab4",
    "option" : "Eases (reduces) the latency and network traffic by hosting EC2 servers at locations closer to the end-users.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nResponses cached at the CloudFront edge locations reduce the load on origin servers as responses are served from the CloudFront edge location.\nOption B is INCORRECT.\nApplications cannot be hosted on edge locations.\nOption C is INCORRECT.\nDatabases cannot be hosted on edge locations.\nOption D is INCORRECT.\nEC2 servers cannot be hosted on edge locations.\nReference:\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio-explained.html\nhttps://wa.aws.amazon.com/wat.concept.edge-location.en.html\n\nAWS Edge locations are a part of Amazon Web Services' global infrastructure and are used to improve the performance and availability of web applications and content for end-users. These edge locations act as a cache and a proxy server between the user and the origin server, which is typically located in a centralized location.\nThe correct statement representing the use of AWS Edge locations is option B, which says that it eases (reduces) the latency and network traffic by hosting applications at locations closer to the end-users. This is achieved by replicating the content from the origin server to the edge location, which is located closer to the end-users. This replication is done using Amazon CloudFront, a global content delivery network that caches the content from the origin server and distributes it to the edge locations around the world.\nWhen a user requests a web application or content, the request is routed to the nearest edge location, which then serves the content from its cache, reducing the round trip time for the request and response. This reduces the latency and network traffic as the content is served from the edge location rather than from the origin server.\nOption A is incorrect as it refers to the use of a cache at the edge location to reduce the load on the origin server, which is a secondary effect of edge locations. Option C is incorrect as it refers to hosting databases at edge locations, which is not a typical use case for edge locations. Option D is incorrect as it refers to hosting EC2 servers at edge locations, which is not a typical use case for edge locations either.\nIn summary, AWS Edge locations are used to improve the performance and availability of web applications and content by caching the content at locations closer to the end-users, which reduces the latency and network traffic.\n\n"
}, {
  "id" : 310,
  "question" : "An Insurance organization must maintain a long-term archive of customer data to ensure compliance with The Health Insurance Portability and Accountability Act (HIPAA). If any department requires the archived data for any purpose, a request will be placed by the involved department at least 14 hours in advance to the IT department. Which of the below listed is the most appropriate S3 storage in this scenario?\n",
  "answers" : [ {
    "id" : "44dc4f197aaf4f548820cbb8bd20fd12",
    "option" : "Amazon S3 Glacier deep archive",
    "isCorrect" : "true"
  }, {
    "id" : "a676a549799e4cacb641ef5199c2867d",
    "option" : "Amazon S3 Standard",
    "isCorrect" : "false"
  }, {
    "id" : "18b00d090bf24ab094ac703fd92aa94c",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "00d1726588de4a9096730d4975977d10",
    "option" : "Amazon S3 Standard-Infrequent Access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAmazon S3 Glacier deep archive is best suited in this scenario as the window available to perform data retrieval is 14 hours.\nRetrieval of data from Amazon S3 Glacier deep archive can be done within 12 hours.\nAmazon S3 Glacier deep archive is up to 75% cheaper than Amazon S3 Glacier.\nOption B is INCORRECT.\nS3 standard is ideal for general-purpose storage of frequently accessed data.\nOption C is INCORRECT.\nAmazon S3 Glacier deep archive is best suited in this scenario as the window available to perform data retrieval is 14 hours.\nRetrieval of data from Amazon S3 Glacier deep archive can be done within 12 hours.\nAmazon S3 Glacier deep archive is up to 75% cheaper than Amazon S3 Glacier.\nOption D is INCORRECT.\nAmazon S3 Standard-Infrequent Access is better suited for less frequently accessed, long-lived data.\nReference:\nhttps://aws.amazon.com/s3/faqs/\nhttps://aws.amazon.com/about-aws/whats-new/2019/03/S3-glacier-deep-archive/\n\nIn this scenario, an Insurance organization must maintain a long-term archive of customer data to ensure compliance with The Health Insurance Portability and Accountability Act (HIPAA). The organization must also be able to respond to requests from any department for the archived data. The request will be placed by the involved department at least 14 hours in advance to the IT department.\nThe most appropriate S3 storage in this scenario would be Amazon S3 Glacier deep archive.\nAmazon S3 Glacier deep archive is a low-cost storage option designed for long-term data archiving. It offers the lowest storage costs of all the S3 storage classes but with longer retrieval times. Data can be archived for up to several decades, and once archived, data is stored on durable, tamper-evident media designed to last for generations.\nIn this scenario, the organization needs to maintain a long-term archive of customer data to ensure compliance with HIPAA. The data is not required to be accessed frequently, but it must be available for retrieval when needed. The request for data retrieval is placed at least 14 hours in advance, which means that longer retrieval times can be accommodated.\nAmazon S3 Glacier deep archive offers the most cost-effective and durable storage option for this scenario. The data can be archived for several decades and retrieved when required, with the retrieval time being suitable for the 14-hour advance notice period.\n\n"
}, {
  "id" : 311,
  "question" : "You are asked to suggest an appropriate Amazon S3 storage class for â€œdata with unknown/changing access patternâ€\nWhich one would you suggest?\n",
  "answers" : [ {
    "id" : "cc7eae3020504bbab4708069ebbf669d",
    "option" : "Amazon S3 Intelligent-Tiering",
    "isCorrect" : "true"
  }, {
    "id" : "58ee876daa2542b08689e6dae93ed343",
    "option" : "Amazon S3 Standard",
    "isCorrect" : "false"
  }, {
    "id" : "a533b58a1a734415b12552e8b3e045aa",
    "option" : "Amazon S3 Glacier",
    "isCorrect" : "false"
  }, {
    "id" : "9af4e3747b5e46bb8898c2bb5a8494c9",
    "option" : "Amazon S3 Standard-Infrequent Access.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAmazon S3 Intelligent-Tiering is best suited for data with â€œunknown/changing access patternâ€.\nOption B is INCORRECT.\nS3 standard is ideal for general-purpose storage of frequently accessed data.\nOption C is INCORRECT.\nAmazon S3 Glacier is preferable for archival of data for a long term.\nOption D is INCORRECT.\nAmazon S3 Standard-Infrequent Access is better suited for less frequently accessed, long-lived data.\nReference:\nhttps://aws.amazon.com/s3/storage-classes/\n\nFor data with an unknown or changing access pattern, the most appropriate Amazon S3 storage class would be Amazon S3 Intelligent-Tiering.\nAmazon S3 Intelligent-Tiering is designed to optimize costs for data with unknown or changing access patterns. It automatically moves objects between two access tiers: frequent access and infrequent access, based on changing access patterns and automatically optimizes costs for you. This means that data that is frequently accessed will be stored in the frequent access tier, while data that is not accessed as frequently will be moved to the infrequent access tier to save costs.\nOn the other hand, Amazon S3 Standard is suitable for frequently accessed data that requires low latency and high throughput. It is designed for use cases such as big data analytics, mobile and gaming applications, content distribution, and backup and recovery.\nAmazon S3 Glacier is a suitable storage class for data archiving and long-term storage. It is designed for data that is accessed infrequently, and where retrieval times of several hours are acceptable.\nAmazon S3 Standard-Infrequent Access is designed for data that is accessed less frequently than Amazon S3 Standard, but more frequently than Amazon S3 Glacier. It is designed for long-lived data that is less frequently accessed, but requires rapid access when needed.\nTherefore, for data with unknown or changing access patterns, Amazon S3 Intelligent-Tiering would be the most appropriate storage class as it automatically moves data between frequent access and infrequent access tiers based on changing access patterns, which optimizes costs for you.\n\n"
}, {
  "id" : 312,
  "question" : "Which feature of Amazon S3 could be utilized to save costs by automatically transitioning objects to appropriate low-cost storage classes at defined intervals?\n",
  "answers" : [ {
    "id" : "3b772a12ddeb412991ecacc7b474bf0b",
    "option" : "By configuring â€œAmazon S3 lifecycle policiesâ€",
    "isCorrect" : "true"
  }, {
    "id" : "e96388b058e040479e7240a705af949d",
    "option" : "Using Versioning",
    "isCorrect" : "false"
  }, {
    "id" : "73008306c03948ad8cd387a239e7e660",
    "option" : "Enabling replication",
    "isCorrect" : "false"
  }, {
    "id" : "94db843c0353449ea85ed01d91cba4a6",
    "option" : "Performing S3 Batch Operations.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nUsing â€œAmazon S3 lifecycle policiesâ€, we can set up rules to perform â€œTransition Actionsâ€ and â€œExpiration Actionsâ€\nTransition actions define the rules as to when an object transitions to different storage classes.\nOption B is INCORRECT.\nS3 versioning enables maintaining multiple versions of an object in one bucket.\nOption C is INCORRECT.\nReplication enables duplication/copy of objects from one S3 bucket to another S3 bucket, either in the same region or different region.\nOption D is INCORRECT.\nS3 Batch Operations are used to perform batch operations on objects in S3\nBatch operation can perform a single operation on a specified list of S3 objects.\nAutomatic transition of objects to different storage classes could not be done via S3 batch operations.\nReference:\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/ObjectVersioning.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/batch-ops.html\n\nThe correct answer to this question is A. By configuring â€œAmazon S3 lifecycle policies.â€\nAmazon S3 (Simple Storage Service) is an object storage service provided by Amazon Web Services (AWS) that allows customers to store and retrieve data from anywhere on the web. One of the key features of Amazon S3 is the ability to automatically transition objects to appropriate low-cost storage classes at defined intervals using lifecycle policies.\nLifecycle policies are a set of rules that define the lifecycle of objects in Amazon S3. These rules can be used to automate the transition of objects between storage classes or to expire objects after a specific period. By using lifecycle policies, customers can optimize their storage costs by moving less frequently accessed data to lower-cost storage tiers, such as Amazon S3 Standard-Infrequent Access (S3 Standard-IA) or Amazon S3 Glacier.\nLifecycle policies can be configured using the Amazon S3 Management Console, AWS CLI, or AWS SDKs. The policies are based on object age, object size, or a combination of both. Customers can also define the transition action, such as moving objects to S3 Standard-IA after 30 days or moving them to Glacier after 60 days.\nIn conclusion, by configuring Amazon S3 lifecycle policies, customers can automate the transition of objects between storage classes and optimize their storage costs.\n\n"
}, {
  "id" : 313,
  "question" : "Your company has setup EC2 Instances in a VPC for their application.\nThe IT Security department has advised that all traffic be monitored to the EC2 Instances.\nWhich of the following features can be used to capture information for outgoing and incoming IP traffic from network interfaces in a VPC.\n",
  "answers" : [ {
    "id" : "22502b8c70ac48888ecef442a328cad5",
    "option" : "AWS Cloudwatch",
    "isCorrect" : "false"
  }, {
    "id" : "2c6e742f9e7f4b47bcb8561a458089f3",
    "option" : "AWS EC2",
    "isCorrect" : "false"
  }, {
    "id" : "0e0d6268a79447db8d16147957fa42b2",
    "option" : "AWS SQS",
    "isCorrect" : "false"
  }, {
    "id" : "a09cbdaabc1344cc9dd226214103f44d",
    "option" : "AWS VPC Flow Logs.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nThe AWS Documentation mentions the following.\nVPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.\nFlow log data can be published to Amazon CloudWatch Logs and Amazon S3\nAfter you've created a flow log, you can retrieve and view its data in the chosen destination.\nOption A is incorrect since this is a monitoring service.\nOption B is incorrect since this is a compute service.\nOption C is incorrect since this is a messaging service.\nFor more information on VPC flow logs, please visit the below URL.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html\n\nThe correct answer is D. AWS VPC Flow Logs.\nVPC Flow Logs is a feature provided by AWS that allows users to capture information about the IP traffic going to and from network interfaces in a VPC. VPC Flow Logs are created at the VPC, Subnet, or Network Interface level and provide visibility into the traffic that is flowing through the network.\nWith VPC Flow Logs, you can capture information about the source and destination IP addresses, ports, protocols, and packets and bytes transferred. This information can be used for a variety of purposes, including security analysis, troubleshooting, and compliance reporting.\nAWS CloudWatch is a monitoring service provided by AWS that can be used to monitor various metrics and logs from different AWS resources, including EC2 instances. However, it does not provide the capability to capture IP traffic from network interfaces.\nAWS EC2 is a service provided by AWS that allows users to launch and manage virtual machines in the cloud. While EC2 instances can be configured to capture network traffic, it does not provide the capability to capture IP traffic from other network interfaces in a VPC.\nAWS SQS is a messaging service provided by AWS that allows users to send and receive messages between distributed application components. It is not related to network traffic monitoring in a VPC.\nTherefore, the correct answer is D. AWS VPC Flow Logs.\n\n"
}, {
  "id" : 314,
  "question" : "A â€œMember AWS accountâ€ in an AWS Organization (using consolidated billing) wants to receive a cost breakdown report (product-wise daily report) so that the analysis of cost and usage could be done. Where can this report be configured to be delivered?\n",
  "answers" : [ {
    "id" : "704a7d5d3cfb4f8eb54c0d0c787f89dd",
    "option" : "S3 bucket owned by the member account",
    "isCorrect" : "false"
  }, {
    "id" : "88d59ffe47a54b78bdebe2ef2dd78bbb",
    "option" : "S3 bucket owned by the master account",
    "isCorrect" : "true"
  }, {
    "id" : "70c86bb1b32a4948bad8fb1c46e858ea",
    "option" : "AWS Management Console",
    "isCorrect" : "false"
  }, {
    "id" : "e9a8efbfc73d4311b7c8cf07f287a0b7",
    "option" : "Amazon Athena.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nAs the consolidated billing feature is being used in AWS organizations, the S3 bucket where the report could be configured to be received should be owned by the master account in the organization.\nBilling reports cannot be received in S3 buckets owned by member accounts.\nThe report delivered to the S3 bucket owned by the master account could be ingested to Amazon Athena.\nAfter that, the data in the S3 bucket can be analyzed using standard SQL queries.\nAWS Management Console is a centralized management and governance console for all the AWS products.\nOption A is INCORRECT.\nOption B is CORRECT.\nOption C is INCORRECT.\nOption D is INCORRECT.\nReference:\nhttps://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html\nhttps://aws.amazon.com/athena/\nhttps://aws.amazon.com/console/\n\nIn an AWS Organization, the member accounts are linked to a master account, which enables the master account to consolidate billing and perform other management tasks across all the member accounts.\nTo receive a cost breakdown report (product-wise daily report), the member account can configure the report to be delivered to an S3 bucket owned by either the member account or the master account.\nOption A: S3 bucket owned by the member account If the member account chooses to configure the report to be delivered to an S3 bucket owned by the member account, they need to create an S3 bucket in their account and grant permission to the master account to access the bucket. This can be done by creating an IAM role in the member account and adding a policy that grants access to the master account. Then, the member account can configure the report to be delivered to this S3 bucket.\nOption B: S3 bucket owned by the master account If the member account chooses to configure the report to be delivered to an S3 bucket owned by the master account, they need to contact the master account administrator and request access to the S3 bucket. The master account administrator can then grant access to the member account by adding a bucket policy that allows the member account to write data to the bucket. Once access is granted, the member account can configure the report to be delivered to this S3 bucket.\nOption C: AWS Management Console The AWS Management Console is not the correct option to configure the delivery of a cost breakdown report. The console provides access to AWS services and resources, but it is not a destination for report delivery.\nOption D: Amazon Athena Amazon Athena is not the correct option to configure the delivery of a cost breakdown report. Athena is an interactive query service that enables users to analyze data in S3 using SQL. It is not a destination for report delivery.\nIn conclusion, the correct answers are A and B, as the member account can configure the report to be delivered to an S3 bucket owned by either the member account or the master account. The AWS Management Console and Amazon Athena are not the correct options for report delivery.\n\n"
}, {
  "id" : 315,
  "question" : "Which of the below is incorrect regarding AWS pricing?\n",
  "answers" : [ {
    "id" : "c29352f6d918483bb072fdf3a88853ee",
    "option" : "Offers pay-as-you-go",
    "isCorrect" : "false"
  }, {
    "id" : "f60f273d75ef4b599c6507d0a0dcdd56",
    "option" : "Enables savings upon reserving",
    "isCorrect" : "false"
  }, {
    "id" : "b898baee17164b5c9300964270c60a85",
    "option" : "Enables saving maximization upon using more",
    "isCorrect" : "false"
  }, {
    "id" : "a328b84a4bcf4312b5e124d33a75e985",
    "option" : "Reduced Inbound data transfer reduces costs incurred.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nThe statement is correct.\nCustomers have complete flexibility in opting for the required services and paying for the opted services.\nOption B is INCORRECT.\nThe statement is correct.\nFor certain services (example: EC2, RDS), savings of up to 75% could be realized vis-a-vis usage of similar on-demand capacity.\nOption C is INCORRECT.\nThe statement is correct.\nVolume-based discounts ensure that more savings are realized as the usage grows.\nOption D is CORRECT.\nThis statement is incorrect.\nInbound data transfer incurs no charges in all the regions across services.\nReference:\nhttps://aws.amazon.com/pricing/\nhttps://aws.amazon.com/blogs/aws/aws-lowers-its-pricing-again-free-inbound-data-transfer-and-lower-outbound-data-transfer-for-all-ser/\n\nOption A: Offers pay-as-you-go This statement is correct. AWS offers a pay-as-you-go pricing model, where customers only pay for the services they use on an hourly or monthly basis. This pricing model is flexible and allows customers to scale their usage up or down depending on their needs.\nOption B: Enables savings upon reserving This statement is also correct. AWS offers Reserved Instances (RIs), where customers can reserve capacity for a period of one or three years, in exchange for a lower hourly rate. This pricing model provides significant cost savings for customers who have a predictable workload and can commit to using the reserved capacity.\nOption C: Enables saving maximization upon using more This statement is incorrect. AWS pricing does not enable savings maximization upon using more. However, customers can take advantage of volume discounts for certain services by using more of them. For example, customers can receive a discount on Amazon S3 storage costs when they store more than a certain amount of data.\nOption D: Reduced Inbound data transfer reduces costs incurred This statement is correct. AWS charges for data transfer between its services, including inbound and outbound data transfer. However, customers can reduce costs by using AWS services in the same region, using CloudFront to cache content closer to end-users, and optimizing their network traffic to reduce data transfer.\nIn conclusion, the correct answer is Option C: Enables saving maximization upon using more, as it is incorrect regarding AWS pricing.\n\n"
}, {
  "id" : 316,
  "question" : "An organization is planning to implement protection for its applications on EC2, ELB and Route 53 resources.\nSo it can obtain near real-time visibility into the attacks and prevent sophisticated DDoS attacks. Which AWS Service would you suggest?\n",
  "answers" : [ {
    "id" : "5b5fd06515484a13a917c178adcc53f6",
    "option" : "AWS Shield Standard",
    "isCorrect" : "false"
  }, {
    "id" : "a18d7444199a4eefab7ae2c3d8f19a95",
    "option" : "AWS Shield Advanced",
    "isCorrect" : "true"
  }, {
    "id" : "1f96cc5e3edf4821acb1f2c2a2e047ea",
    "option" : "AWS Firewall Manager",
    "isCorrect" : "false"
  }, {
    "id" : "9efe097e279e4c22b8310c23df587f86",
    "option" : "AWS WAF (Web Application Firewall)",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nAWS Shield Standard helps prevent DDoS attacks that occur commonly and target websites/applications on the transport and network layer.\nWhen used in conjunction with Route 53 and Cloudfront, protection is assured against all known layer 3 and layer 4 attacks.\nAWS Shield Standard is free.\nOption B is CORRECT.\nAWS Shield Advanced protects applications on EC2, ELB, Cloudfront, global accelerator and Route 53 resources.\nSo near real-time visibility into the attacks could be obtained and sophisticated DDoS attacks could be prevented.\nOption C is INCORRECT.\nAWS Firewall Manager helps convenient implementation of compliance and simplifies the firewall rules management across AWS accounts.\nOption D is INCORRECT.AWS WAF (Web Application Firewall) offers prevention for APIs and Web Applications from unavailability, security compromise, increased resources consumption.\nReference:\nhttps://aws.amazon.com/shield/\nhttps://aws.amazon.com/waf/\nhttps://aws.amazon.com/firewall-manager/\n\nFor protecting applications on EC2, ELB and Route 53 resources, the suggested AWS service would be AWS Shield Advanced.\nAWS Shield is a managed DDoS (Distributed Denial of Service) protection service that safeguards applications running on AWS. It offers two levels of protection: AWS Shield Standard and AWS Shield Advanced.\nAWS Shield Standard is automatically enabled for all AWS customers at no additional cost. It provides basic DDoS protection for all AWS resources, including EC2 instances, Elastic Load Balancers (ELBs), Amazon CloudFront distributions, Amazon Route 53 hosted zones, and Amazon Global Accelerator.\nAWS Shield Advanced provides additional protection against more sophisticated DDoS attacks and offers near real-time visibility into attacks, which allows customers to quickly respond to threats. It provides 24/7 access to AWS DDoS experts and provides access to enhanced DDoS protection features, such as:\nAdvanced protection against larger and more sophisticated DDoS attacks Network traffic analysis and visibility Attack mitigation support by AWS DDoS response team Protection for non-AWS resources via AWS Global Accelerator.\nAWS Firewall Manager, on the other hand, is a security management service that allows you to centrally configure and manage firewall rules across multiple AWS accounts and resources. It provides a way to create and manage AWS WAF (Web Application Firewall) rules across multiple resources, but it does not provide DDoS protection.\nAWS WAF is a web application firewall that helps protect web applications from common web exploits that could affect application availability, compromise security, or consume excessive resources. While it provides protection against web application attacks, it does not provide DDoS protection.\nIn conclusion, AWS Shield Advanced is the suggested service for obtaining near real-time visibility into the attacks and prevent sophisticated DDoS attacks for the mentioned AWS resources (EC2, ELB, and Route 53).\n\n"
}, {
  "id" : 317,
  "question" : "Compliance team has mandated strict adherence to PCI DSS standards and Center for Internet Security (CIS) AWS Foundations Benchmark best practices.\nTo ensure compliance, it is decided that any deviations from the standards and best practices should be highlighted along with the recommended resolution steps. Which of the AWS offerings can help automate continuous, account/resource level security-related checks and configurations using standards and best practices?\n",
  "answers" : [ {
    "id" : "8a61082825144c499b0acbcd0a7c8826",
    "option" : "Amazon Macie",
    "isCorrect" : "false"
  }, {
    "id" : "ac8c5a40a793421aa0fcab427b9397bb",
    "option" : "AWS Security Hub",
    "isCorrect" : "true"
  }, {
    "id" : "94556a2e71fe47989d0cd695cf091300",
    "option" : "Amazon GuardDuty",
    "isCorrect" : "false"
  }, {
    "id" : "88007b4c123546209dec5637b26cabf2",
    "option" : "AWS Firewall Manager.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nAmazon Macie is a fully managed service from AWS that provides data security and privacy by utilizing Amazon's machine learning and pattern matching capabilities.\nOption B is CORRECT.\nAWS Security Hub provides a view of across-account security status and gives security alerts.\nSecurity hub identifies deviations from the standards and best practices and suggests recommended resolution steps.\nOption C is INCORRECT.\nAmazon GuardDuty performs continuous monitoring to protect AWS accounts, S3 data and workloads from any malicious, unauthorized activities.\nOption D is INCORRECT.\nAWS Firewall Manager enables management and configuration of firewalls across AWS accounts and applications centrally.\nReference:\nhttps://aws.amazon.com/macie/\nhttps://aws.amazon.com/security-hub/features/\nhttps://aws.amazon.com/guardduty/\nhttps://aws.amazon.com/firewall-manager/\n\nThe answer to this question is B. AWS Security Hub.\nAWS Security Hub is a service that provides a comprehensive view of security alerts and compliance status across an AWS account. It continuously monitors and assesses the security of AWS resources and provides detailed security findings and recommendations based on industry standards and best practices, including PCI DSS and CIS AWS Foundations Benchmark.\nAWS Security Hub automates security checks and configurations using security and compliance standards, including the ones mandated by the compliance team. It can aggregate findings from multiple security services, including Amazon GuardDuty, AWS Firewall Manager, and Amazon Macie.\nAWS Security Hub provides a centralized dashboard that presents a summary of security alerts and compliance status across multiple AWS accounts. It integrates with AWS CloudFormation and AWS Config to automate remediation of security findings and to maintain continuous compliance.\nBy using AWS Security Hub, organizations can ensure that they are adhering to the mandated security and compliance standards, including PCI DSS and CIS AWS Foundations Benchmark. They can also automate the identification of security vulnerabilities and the implementation of remediation actions to minimize the risk of security incidents.\n\n"
}, {
  "id" : 318,
  "question" : "AWS offers two savings plans to enable more savings and flexibility for its customers, namely, compute saving plans and EC2 Instance Savings plans. To which of the following services does the compute savings plans NOT apply?\n",
  "answers" : [ {
    "id" : "dbdfbb58e5284ae1b19ecb7a37de6371",
    "option" : "Amazon EC2",
    "isCorrect" : "false"
  }, {
    "id" : "2faa7d23954241a99d44f506fbefd97c",
    "option" : "AWS Fargate",
    "isCorrect" : "false"
  }, {
    "id" : "b0f102d02fe840b09abcddc6635f0790",
    "option" : "AWS Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "6b1d11dc346b4c24a7dd956b574fb16a",
    "option" : "AWS LightSail.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nCompute Savings Plan applies to EC2.\nOption B is INCORRECT.\nCompute Savings Plan applies to Fargate.\nOption C is INCORRECT.\nCompute Savings Plan applies to Lambda.\nOption D is CORRECT.\nCompute Savings Plan does NOT apply to AWS Lightsail.\nhttps://aws.amazon.com/savingsplans/faq/\n\nCompute Savings Plans and EC2 Instance Savings Plans are two types of cost-saving plans offered by AWS to provide cost savings and flexibility to customers. Compute Savings Plans offer flexibility to customers to save up to 66% on their compute usage across any AWS region. On the other hand, EC2 Instance Savings Plans provide discounts on the upfront costs of EC2 instances.\nCompute Savings Plans apply to the following services:\nA. Amazon EC2 B. AWS Fargate C. AWS Lambda\nCompute Savings Plans do not apply to AWS Lightsail. AWS Lightsail is a simpler version of Amazon EC2, and it is a bundled package of computing, storage, and networking resources for smaller projects. AWS Lightsail uses its pricing structure, which is different from Amazon EC2, and hence Compute Savings Plans do not apply to AWS Lightsail.\nTherefore, the correct answer is D. AWS LightSail.\n\n"
}, {
  "id" : 319,
  "question" : "AWS offers two savings plans to enable more savings and flexibility for its customers, namely, compute saving plans and EC2 Instance Savings plans. Which of the below statement is FALSE regarding Saving Plans?\n",
  "answers" : [ {
    "id" : "ab928b66439649b59d2f4bb7d4c7f8c0",
    "option" : "Capacity Reservations are not provided with Saving Plans.",
    "isCorrect" : "false"
  }, {
    "id" : "8a2c6895bf27470fb08b65f0695e684b",
    "option" : "Savings Plans are available for all the regions.",
    "isCorrect" : "true"
  }, {
    "id" : "162c593a159b4774b4f55fb20ee3a9ab",
    "option" : "Savings plans will apply on â€˜On-Demand Capacity Reservationsâ€™ that customers can allocate for their needs.",
    "isCorrect" : "false"
  }, {
    "id" : "8f22dcc6aed44c53b244cf2b381ed637",
    "option" : "The prices for Savings Plans do vary depending on the hourly commitment amount.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nThe given statement is True.\nOption B is CORRECT.\nThe given statement is False.\nFor China and Asia Pacific (Osaka-Local) Regions, currently the savings plans are not available.\nOption C is INCORRECT.\nThe given statement is True.\nOption D is INCORRECT.\nThe given statement is True.\nhttps://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html#sp-ris\n\nThe correct answer is C. Savings plans will apply on â€˜On-Demand Capacity Reservations' that customers can allocate for their needs.\nExplanation:\nAWS offers two savings plans to enable more savings and flexibility for its customers, namely, Compute Savings Plans and EC2 Instance Savings plans.\nCompute Savings Plans are flexible, commitment-based plans for compute usage in AWS, such as AWS Lambda, AWS Fargate, and Amazon EC2 instances. They provide significant discounts (up to 66%) on the On-Demand prices of the compute usage covered by the plan. Compute Savings Plans apply automatically to any EC2 instance usage that matches the plan's attributes, regardless of instance family, size, AZ, OS, or tenancy.\nEC2 Instance Savings Plans provide discounts (up to 72%) on EC2 instance usage in exchange for a commitment to use a specific instance family in a selected region. These plans provide the most significant discount on EC2 usage but require a commitment to use specific instance families, sizes, and regions.\nCapacity Reservations are a feature that enables customers to reserve capacity for specific instances in a particular Availability Zone for any duration, up to three years. The reserved capacity is billed at a lower hourly rate than On-Demand instances. However, capacity reservations are not provided with Savings Plans.\nSavings plans are available in all the regions where AWS offers compute services, and the prices for Savings Plans do vary depending on the hourly commitment amount. The hourly commitment amount represents the amount of compute usage the customer commits to use for the entire term of the plan.\n\n"
}, {
  "id" : 320,
  "question" : "Which of the below-listed services is a region-based AWS service?\n",
  "answers" : [ {
    "id" : "6f632d111a884d8ca785afb38ee1eb7a",
    "option" : "AWS IAM",
    "isCorrect" : "false"
  }, {
    "id" : "cfadcd08e91e49f18eb868da61d70086",
    "option" : "Amazon EFS",
    "isCorrect" : "true"
  }, {
    "id" : "f8dd72d3c4a142128f387b1830f4e91b",
    "option" : "Amazon Route 53",
    "isCorrect" : "false"
  }, {
    "id" : "9acb44ff9bd2471eaa09fa99d6c8dc9b",
    "option" : "Amazon CloudFront.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nAWS IAM is a global service.\nOption B is CORRECT.\nEFS is a regional service.\nOption C is INCORRECT.\nRoute 53 is a global service.\nOption D is INCORRECT.\nAmazon Cloudfront is a global service.\nReferences:\nhttps://aws.amazon.com/efs/\nhttps://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/\n\nThe correct answer is: C. Amazon Route 53.\nAmazon Route 53 is a Domain Name System (DNS) web service offered by Amazon Web Services (AWS). It is region-based, which means it operates within specific AWS regions.\nRoute 53 is a highly available and scalable service that is designed to route end users to Internet applications, such as websites or web services. It provides DNS resolution services for both public and private domain names, including internal Amazon EC2 instance names.\nIAM (A) - AWS Identity and Access Management (IAM) is a global service. It is not region-specific, and user accounts, policies, and permissions can be managed from any AWS region.\nAmazon EFS (B) - Amazon Elastic File System (EFS) is a regional service. It provides a file system that can be shared among multiple Amazon EC2 instances in the same region.\nCloudFront (D) - Amazon CloudFront is a global content delivery network (CDN) service. It distributes content globally from AWS edge locations, which are spread across the world. However, CloudFront does have regional configurations that allow for customizations to be applied to specific regions, but it still operates globally.\n\n"
}, {
  "id" : 321,
  "question" : "Which AWS service allows: 1- Running Docker containers 2- Simple API calls to launch and stop container-based applications.\n",
  "answers" : [ {
    "id" : "5f5e1d5dd9fb4d478a5a199704a6237e",
    "option" : "AWS Docker Manager",
    "isCorrect" : "false"
  }, {
    "id" : "e9775b5b309d44a6837196f3e752cc7f",
    "option" : "AWS EKS",
    "isCorrect" : "false"
  }, {
    "id" : "4bf4a72d342c4d1f8bb93a1757213da7",
    "option" : "Amazon Elastic Container Service",
    "isCorrect" : "true"
  }, {
    "id" : "c4bee8c1bd3d43adbf3b72e21fd80288",
    "option" : "AWS Fargate.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nAWS EKS (Elastic Kubernetes Service): AWS EKS is a managed service that simplifies Kubernetes deployment on AWS by eliminating the need to install, operate, and/or maintain its own Kubernetes control plane.\nAmazon Elastic Container Service: AWS ECS is a container management service that facilitates containers' management on the cluster, including running and stopping the containers.\nThe container-based applications could be launched/stopped using simple API calls.\nAWS Fargate: AWS Fargate is an â€œECS and EKS compatibleâ€ serverless compute engine for containers.\nOption A is INCORRECT.\nAWS Docker Manager is an invalid option.\nOption B is INCORRECT.\nOption C is CORRECT.\nOption D is INCORRECT.\nReference:\nhttps://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\nhttps://aws.amazon.com/fargate/\n\nThe correct answer is C. Amazon Elastic Container Service (Amazon ECS) allows running Docker containers and launching and stopping container-based applications via simple API calls.\nAmazon ECS is a fully-managed container orchestration service that supports Docker containers and allows running and managing containerized applications in a highly available, scalable, and secure environment. With Amazon ECS, you can easily deploy and manage Docker containers on AWS infrastructure without having to manage the underlying infrastructure.\nAmazon ECS provides a simple API for launching and stopping container-based applications. You can use the AWS Management Console, CLI, or SDKs to create, launch, and stop container instances, define task definitions, and manage containerized applications. You can also integrate Amazon ECS with other AWS services, such as Elastic Load Balancing, Amazon S3, AWS IAM, AWS CloudFormation, and Amazon CloudWatch, to build scalable, fault-tolerant, and highly available container-based applications.\nIn summary, Amazon ECS is the correct answer for this question because it provides a fully-managed container orchestration service that allows running Docker containers and launching and stopping container-based applications via simple API calls.\n\n"
}, {
  "id" : 322,
  "question" : "Which of the following tools can be used to enable authentication for a large number of users accessing an Amazon S3 bucket using a mobile app?\n",
  "answers" : [ {
    "id" : "a9789aa75ce347a484c02acd5fbe87e1",
    "option" : "AWS IAM user",
    "isCorrect" : "false"
  }, {
    "id" : "e91e8257b473477296ba3206f824220c",
    "option" : "Amazon Cognito",
    "isCorrect" : "true"
  }, {
    "id" : "998610915998424682da09ba28c5ae9f",
    "option" : "AWS IAM group",
    "isCorrect" : "false"
  }, {
    "id" : "1a73284b2c304acda4684057a48f7023",
    "option" : "AWS STS.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nUsing Amazon Cognito user pool &amp; identity pool, mobile app users can be authenticated to use AWS services like Amazon S3 bucket.\nOptions A &amp; C are incorrect as IAM users &amp; IAM group can create authentication &amp; manage access for users accessing AWS services.\nThese services are not suitable to provide authentication to millions of users accessing AWS services via mobile app.\nOption D is incorrect as AWS STS is a service that grants temporary access privilege for AWS IAM users or federated users.\nFor more information on Amazon Cognito, refer to the following URL:\nhttps://aws.amazon.com/cognito/faqs/\n\nThe recommended tool for enabling authentication for a large number of users accessing an Amazon S3 bucket using a mobile app is Amazon Cognito. Therefore, the correct answer is B.\nAmazon Cognito is a user authentication and management service that makes it easy to add sign-up, sign-in, and access control to mobile and web apps. With Amazon Cognito, you can easily create and manage user pools that scale to hundreds of millions of users, and provide secure access to AWS resources and other services.\nWhen using Amazon Cognito with Amazon S3, users can authenticate with the user pools provided by Amazon Cognito and be granted temporary AWS credentials that enable them to access the S3 bucket. This allows you to manage user access to your S3 bucket, and also provides a simple and secure way to authenticate your mobile app users.\nAWS IAM users and groups are primarily used for managing access to AWS services, and while it is possible to use them to grant access to an S3 bucket, this is not recommended for large numbers of users or mobile app users.\nAWS STS (Security Token Service) provides temporary security credentials that can be used to access AWS services. While it can be used to enable authentication for a large number of users accessing an S3 bucket, it is not as user-friendly or scalable as Amazon Cognito.\nIn summary, Amazon Cognito is the recommended tool for enabling authentication for a large number of users accessing an Amazon S3 bucket using a mobile app, as it provides a scalable, user-friendly solution for managing user access to your S3 bucket.\n\n"
}, {
  "id" : 323,
  "question" : "What is the feature that allows customers to â€œcreate copy of their LightSail instance in EC2â€?\n",
  "answers" : [ {
    "id" : "6cb2a0e90f1b4cf295ce3dc9334c7a5d",
    "option" : "LightSail Backup",
    "isCorrect" : "false"
  }, {
    "id" : "958f997da1c146f3b6379eecc99a29ef",
    "option" : "LightSail Copy",
    "isCorrect" : "false"
  }, {
    "id" : "f2f8f6a16da94cab931d1d49120a8677",
    "option" : "Upgrade to EC2",
    "isCorrect" : "true"
  }, {
    "id" : "139d61fc9a3a4ee7b2f66c24ec614369",
    "option" : "LightSail-EC2 snapshot.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT.\nLightSail Backup is an invalid option.\nOption B is INCORRECT.\nLightSail Copy is an invalid option.\nOption C is CORRECT.\nâ€œUpgrade to EC2â€ is the feature that allows customers to â€œcreate copy of their LightSail instance in EC2â€\nUsing this feature, customers can choose from the available set of various options, including the pricing model offered by EC2.\nOption D is INCORRECT.\nLightSail-EC2 snapshot is an invalid option.\nReference:\nhttps://aws.amazon.com/lightsail/faq/\nâ€œWhat is upgrade to EC2â€\n\nThe feature that allows customers to create a copy of their LightSail instance in EC2 is the LightSail-EC2 snapshot. This feature enables customers to migrate their LightSail instances to EC2 instances, which provides them with greater flexibility and scalability.\nTo use this feature, customers must create a snapshot of their LightSail instance, which is a backup of the instance's data at a specific point in time. They can then use the snapshot to create a new EC2 instance, which will be an exact copy of the LightSail instance, including all of its data and settings.\nThe LightSail-EC2 snapshot feature simplifies the migration process by eliminating the need to manually transfer data and settings between instances. Additionally, it allows customers to take advantage of the additional resources and capabilities offered by EC2, such as larger instance types, load balancing, and autoscaling.\nIn summary, the correct answer to the question is D. LightSail-EC2 snapshot.\n\n"
}, {
  "id" : 324,
  "question" : "Which AWS offering enables customers to speedily build a High-Performance Computing (HPC) environment in AWS?\n",
  "answers" : [ {
    "id" : "5a9ebfd58c6c44c5baefb415bd02badf",
    "option" : "AWS ECS",
    "isCorrect" : "false"
  }, {
    "id" : "0944c9ce1626431aac7e74c00d154f49",
    "option" : "AWS ParallelCluster",
    "isCorrect" : "true"
  }, {
    "id" : "c0d3eb6da8b6416bbafebf128c8918eb",
    "option" : "AWS Fargate",
    "isCorrect" : "false"
  }, {
    "id" : "e53cc6ff55d04a14802acf50fdec89a8",
    "option" : "EC2 Image Builder.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nAWS ECS is a container management service that facilitates containers' management on the cluster, including running and stopping the containers.\nOption B is CORRECT.\nAWS ParallelCluster is an open-source cluster management tool supported by AWS that helps customers deploy and manage HPC clusters in AWS.\nOption C is INCORRECT.\nAWS Fargate is an â€œECS and EKS compatibleâ€ serverless compute engine for containers.\nOption D is INCORRECT.EC2 Image Builder is AWS's fully managed offering that helps build secure images of operating system(s) for use on AWS.\nReference:\nhttps://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html\nhttps://aws.amazon.com/fargate/\nhttps://docs.aws.amazon.com/imagebuilder/\n\nThe correct answer is B. AWS ParallelCluster.\nAWS ParallelCluster is a fully-managed service that makes it easy to deploy and manage High-Performance Computing (HPC) clusters on AWS. It enables customers to quickly build and deploy HPC environments using Amazon EC2 instances and EBS volumes, and supports a wide range of popular HPC applications and frameworks such as OpenMPI, Intel MPI, and TensorFlow.\nAWS ParallelCluster simplifies the process of setting up and configuring HPC clusters, automating many of the tasks involved in configuring network settings, installing software, and managing cluster resources. It also provides customers with a range of customizable templates and configuration options, making it easy to tailor HPC environments to their specific needs.\nAWS ECS (A) is a container management service, which allows customers to run and manage containerized applications on AWS. It is not specifically designed for HPC workloads.\nAWS Fargate (C) is a compute engine for containers that allows customers to run containers without having to manage the underlying infrastructure. While it can be used to run containerized HPC workloads, it is not specifically designed for this purpose.\nEC2 Image Builder (D) is a service that simplifies the process of building and maintaining custom Amazon Machine Images (AMIs) for EC2 instances. It is not specifically designed for HPC workloads, but can be used to create custom AMIs for HPC applications.\n\n"
}, {
  "id" : 325,
  "question" : "One of our clients has outlined a Cloud adoption Journey and requires help for the transformation journey in moving to AWS cloud.\nWhich of the below services would assist them in their transformation journey?\n",
  "answers" : [ {
    "id" : "0f898d654f94477a853ec3d47253d141",
    "option" : "AWS Trusted Advisor",
    "isCorrect" : "false"
  }, {
    "id" : "bcd4d792e94642de9e953899d527f030",
    "option" : "AWS Whitepapers",
    "isCorrect" : "false"
  }, {
    "id" : "c92966865fbf4af48adf1a12bafaf7a5",
    "option" : "AWS Professional Services",
    "isCorrect" : "true"
  }, {
    "id" : "f204d95dd3334ff3b413f009ba4f4bc3",
    "option" : "AWS Managed Services.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: C.\nCloud adoption provides clients with sustainable business advantages.\nBringing in specialized skills, tools and experience form a cloud vendor can help clients in their cloud adoption journey extensively.\nAWS Professional Services organization provides a global team of experts for achieving specific business outcomes for clients during their journey to AWS Cloud.\nOne of the tools provided by AWS is the Cloud Adoption Readiness Tool (CART)\nCART helps access the cloud readiness of an Organization wanting to move to AWS Cloud using a survey for accessing cloud readiness across different perspectives like business, people, process, platform, operations &amp; security.\nAfter CART survey completion, a customized cloud migration assessment report charting the readiness of the Organization and improvement areas is suggested.\nOption A is incorrect since Trusted Advisor is an online tool for provisioning AWS resources using best practices to reduce cost, increase performance / fault tolerance, improve security by optimizing the AWS environment.\nOption B is incorrect since AWS whitepapers features a list of technical white papers covering various aspects of AWS ecosystem like architecture, security, service economics authored by AWS team.\nOption C is CORRECT.\nRefer to the above discussion on AWS Professional Services.\nOption D is incorrect.\nAWS managed services provides abstractions to the AWS infrastructure services layer &amp; ongoing management of AWS infrastructure that helps reduce operational overhead and risk.\nReference:\nhttps://aws.amazon.com/professional-services/\n\nOption C, AWS Professional Services, would assist the client in their transformation journey.\nAWS Professional Services is a team of AWS experts that provides consultancy and guidance to help customers adopt and operate AWS services. They have a range of services to help customers with their cloud adoption journey, including architecture and design, migration, and optimization.\nAWS Professional Services can help the client in the following ways:\nArchitecture and Design: AWS Professional Services can help the client in designing their AWS infrastructure, based on their business needs and requirements. They can also assist in reviewing the existing architecture and provide recommendations for improvement. Migration: AWS Professional Services can assist the client in migrating their existing applications and data to the AWS cloud. They can help in developing a migration plan, selecting the right migration approach, and executing the migration. Optimization: AWS Professional Services can help the client in optimizing their AWS infrastructure to ensure it is efficient, cost-effective, and meets their business needs. They can provide recommendations on optimizing resource utilization, improving performance, and reducing costs.\nOn the other hand, AWS Trusted Advisor (Option A) is a service that provides real-time guidance to help customers optimize their AWS infrastructure, improve security, and reduce costs. It does not provide consultancy or guidance on the cloud adoption journey.\nAWS Whitepapers (Option B) are detailed technical documents that provide in-depth information on various AWS services, best practices, and architectures. While they can be useful in understanding AWS services and how to use them, they do not provide guidance on the cloud adoption journey.\nAWS Managed Services (Option D) is a service that provides ongoing management and monitoring of AWS infrastructure for customers. It is not directly related to the cloud adoption journey, as it assumes that the customer has already adopted AWS services and requires ongoing management and support.\n\n"
}, {
  "id" : 326,
  "question" : "Which of the below listed 2 design principles relates to the â€œOperational Excellenceâ€ pillar of the Well-Architected framework? (Choose 2)\n",
  "answers" : [ {
    "id" : "5f684653fc00452e934b9cca7ea64214",
    "option" : "Implement a strong identity foundation",
    "isCorrect" : "false"
  }, {
    "id" : "96804e49ca07436388783a502f4e9c32",
    "option" : "Enable traceability",
    "isCorrect" : "false"
  }, {
    "id" : "ba6487886797415ea820f9b370cc76db",
    "option" : "Anticipate Failure",
    "isCorrect" : "true"
  }, {
    "id" : "831185734ca149a5a705f025dcda1283",
    "option" : "Manage change in automation",
    "isCorrect" : "false"
  }, {
    "id" : "b9258582069b4be1926921b434088ebd",
    "option" : "Perform operations as code.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: C, E.\nThe operational excellence pillar of a well-architected framework has below 5 design principles.\n-Perform operations as code.\n-Make frequent, small, reversible changes.\n-Refine operations procedures frequently.\n-Anticipate failure.\n-Learn from all operational failures.\nThe security pillar of a well-architected framework has below 7 design principles.\n-Implement a strong identity foundation.\n-Enable traceability.\n-Apply security at all layers.\n-Automate security best practices.\n-Protect data in transit and at rest.\n-Keep people away from data.\n-Prepare for security events.\nThe reliability pillar of a well-architected framework has below 5 design principles.\n-Automatically recover from failure.\n-Test recovery procedures.\n-Scale horizontally to increase aggregate workload availability.\n-Stop guessing capacity.\n-Manage change in automation.\nOption A is INCORRECT.\nImplement a strong identity foundation is the design principle relating to the security pillar.\nOption B is INCORRECT.\nEnable traceability is the design principle relating to the security pillar.\nOption C is CORRECT.\nOption D is INCORRECT.\nManage change in automation is the design principle relating to the reliability pillar.\nOption E is CORRECT.\nReference:\nhttps://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/\n\nThe AWS Well-Architected Framework is a methodology designed to help cloud architects build secure, efficient, and resilient infrastructure for their applications. The framework is based on five pillars: Operational Excellence, Security, Reliability, Performance Efficiency, and Cost Optimization.\nOperational Excellence is one of the five pillars of the AWS Well-Architected Framework. It focuses on ensuring that operations are efficient, reliable, and scalable. The following design principles relate to the Operational Excellence pillar:\nB. Enable traceability: This design principle ensures that you can track and monitor changes to your infrastructure and applications. Traceability helps you identify issues quickly and accurately and enables you to roll back changes if necessary. With AWS, you can enable traceability by using services like AWS CloudTrail, AWS Config, and Amazon CloudWatch to monitor and log activity in your environment.\nD. Manage change in automation: This design principle encourages you to automate as many operational tasks as possible. By automating tasks such as software deployments and configuration changes, you can reduce the risk of errors and ensure that your infrastructure is consistent and reliable. AWS offers several tools to help you manage change in automation, such as AWS CloudFormation and AWS OpsWorks.\nIn summary, two design principles that relate to the Operational Excellence pillar of the Well-Architected framework are \"Enable traceability\" and \"Manage change in automation.\" By enabling traceability and managing change in automation, you can build a more efficient, reliable, and scalable infrastructure for your applications on AWS.\n\n"
}, {
  "id" : 327,
  "question" : "What is the ability of AWS products and services to recover from disruptions and mitigate disruptions known as?\n",
  "answers" : [ {
    "id" : "5ede25e6601a4a3e9d1b845218f7221c",
    "option" : "Resiliency",
    "isCorrect" : "true"
  }, {
    "id" : "b36dbf695bb14f7fa0d0be7fa00982d5",
    "option" : "Consistency",
    "isCorrect" : "false"
  }, {
    "id" : "1374c1bc0f7c441f809bdd7511a178f7",
    "option" : "Durability",
    "isCorrect" : "false"
  }, {
    "id" : "eea2b90f2b8b44a1867c2f451dd2433a",
    "option" : "Latency.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nResiliency is the ability to recover from disruptions and mitigate disruptions.\nConsistency involves more than one system storing information, to return the same result when queried.\nDurability is the system's ability to perform even upon the occurrence of unexpected events.\nLatency is typically the measurement of delay between request and response.\nOption A is CORRECT as Resilience is the ability of AWS products to recover from disruptions and mitigate disruptions.\nOption B is INCORRECT because Consistency ensures that similar results are returned by more than one system storing information, when queried.\nOption C is INCORRECT because Durability is the ability of AWS product(s) to remain functional and perform despite unexpected events' occurrence.\nOption D is INCORRECT because Latency denotes the delay between getting a response after a request is made.\nReference:\nhttps://wa.aws.amazon.com/wat.concept.resiliency.en.html\nhttps://wa.aws.amazon.com/wat.concept.consistency.en.html\nhttps://wa.aws.amazon.com/wat.concept.durability.en.html\nhttps://wa.aws.amazon.com/wat.concept.latency.en.html\n\nThe ability of AWS products and services to recover from disruptions and mitigate disruptions is known as \"resiliency\".\nResiliency is the ability of a system to recover quickly and continue operating even when there is a disruption or failure. AWS has designed its products and services to be highly resilient, with multiple layers of redundancy and fault-tolerance built in to help ensure that disruptions are minimized and that the system remains available.\nAWS achieves resiliency through a number of techniques, including:\nMulti-Availability Zone (Multi-AZ) deployments: This involves deploying resources across multiple availability zones (AZs), which are geographically separated data centers. By deploying resources across multiple AZs, AWS can provide high availability and fault tolerance, even in the event of an AZ outage. Auto Scaling: AWS provides a service called Auto Scaling, which allows customers to automatically scale their resources up or down based on demand. By using Auto Scaling, customers can ensure that their applications can handle increased traffic without being overwhelmed, and can also save money by reducing resources during periods of low demand. Load balancing: AWS provides a range of load balancing services, such as Elastic Load Balancing (ELB), which can distribute traffic across multiple instances and AZs. By using load balancing, customers can ensure that their applications are highly available and can handle increased traffic. Data replication: AWS provides several data replication services, such as Amazon S3 (Simple Storage Service) and Amazon RDS (Relational Database Service), which replicate data across multiple AZs. By replicating data, AWS can help ensure that data remains available even in the event of a failure.\nIn summary, resiliency is the ability of AWS products and services to recover from disruptions and mitigate disruptions. AWS achieves resiliency through a variety of techniques, including multi-AZ deployments, auto scaling, load balancing, and data replication.\n\n"
}, {
  "id" : 328,
  "question" : "An architect is asked to design a solution for a distributed system in which the system's components operate in a way that components of one system do not negatively impact other components of the system. Which of the below listed architectural best practice can help to achieve this?\n",
  "answers" : [ {
    "id" : "f149ecd11aa546d08db85ef6249743c7",
    "option" : "Request Throttling",
    "isCorrect" : "false"
  }, {
    "id" : "c0d9d5aa9fb04c2b86e9cc3a1a843199",
    "option" : "Using stateless services",
    "isCorrect" : "false"
  }, {
    "id" : "13641a5d180d4fc6b8233eec63b69742",
    "option" : "Enabling automatic data backup",
    "isCorrect" : "false"
  }, {
    "id" : "adb33ad9af644f61a2b6fc8918b8c246",
    "option" : "Implement loose coupling.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nThe scenario in the question talks about a distributed system wherein there is minimal-to-no dependency amongst the components.\nThis can be achieved by implementing loose coupling amongst the components.\nRequest throttling, and, use of stateless services (Option A, B) help design resilient solution for distributed system that can withstand failures and can recover from failure quickly.\nEnabling automatic data backup (Option C) is the best practice for failure management, as backups aligned with requirements will ensure the required recovery time objectives (RTO) and recovery point objectives (RPO).\nOption A is INCORRECT because the scenario refers to ensure minimal dependency amongst the components.\nRequest throttling does not ensure minimal dependency but helps build a resilient solution.\nOption B is INCORRECT because using the stateless service ensures that the solution is resilient.\nHowever, loose coupling helps minimize the dependency amongst the various components in the solution.\nOption C is INCORRECT because implementing automatic data backup is a failure management best practice and does not contribute to minimize component dependency.\nOption D is CORRECT as loose coupling ensures that components are NOT tightly dependent on each other, assuring that if one component fails, it does not impact the working of other components.\nReference:\nhttps://d1.awsstatic.com/whitepapers/architecture/AWS_Well-Architected_Framework.pdf\n\nThe correct answer is D. Implement loose coupling.\nIn a distributed system, it is crucial to ensure that the components of the system operate in a way that one component does not negatively impact the others. This can be achieved by implementing loose coupling between the components.\nLoose coupling refers to the design principle where the components of a system are independent and do not rely on each other directly. In this design, the components can communicate with each other through well-defined interfaces or APIs. Loose coupling allows the components to evolve independently and enables the system to be more scalable and fault-tolerant.\nRequest throttling, which is a mechanism to limit the number of requests or connections to a system, can help to prevent overload on the system but it may not necessarily prevent negative impact on other components of the system.\nUsing stateless services is another important architectural best practice for distributed systems. Stateless services do not maintain any state information between requests, which makes them easier to scale and more resilient. However, this practice alone may not prevent negative impact on other components of the system.\nEnabling automatic data backup is a best practice for data protection but it does not directly address the issue of components impacting each other in a distributed system.\nTherefore, the best answer to the question is D. Implement loose coupling. This design principle ensures that the components of a distributed system are independent and communicate with each other through well-defined interfaces or APIs, which can help to prevent negative impact on other components of the system.\n\n"
}, {
  "id" : 329,
  "question" : "Which of the below statement is CORRECT regarding AWS Global infrastructure?\n",
  "answers" : [ {
    "id" : "45bf06f783514c5faa084c90be9dbb3a",
    "option" : "Each AWS region has multiple Availability Zones",
    "isCorrect" : "true"
  }, {
    "id" : "0bb21319e453495da95ee803047d7a6a",
    "option" : "Many AWS regions has single availability zone",
    "isCorrect" : "false"
  }, {
    "id" : "2dd4add1ca8c459ea3fa8bd5667e3029",
    "option" : "Availability zones are also known as AWS Local zones",
    "isCorrect" : "false"
  }, {
    "id" : "c8b8b7ed998b4d77a24d0649fbfe5a29",
    "option" : "To provide High Availability, AWS management console and control plane are isolated to a single region.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nThe statement is Correct.\nOption B is INCORRECT.\nThe statement is incorrect.\nOption A is correct.\nOption C is INCORRECT.\nThe statement is incorrect.\nAvailability zones and AWS Local Zones are different.\nOption D is INCORRECT.\nThe statement is incorrect.\nAWS management console and control plane utilize multi-AZs and are distributed across the AWS regions.\nReference:\nhttps://aws.amazon.com/about-aws/global-infrastructure/\n\nThe correct statement regarding AWS Global infrastructure is:\nA. Each AWS region has multiple Availability Zones.\nExplanation:\nAWS Global infrastructure is a collection of regions, each of which is a separate geographic area where AWS operates one or more data centers. AWS regions are composed of Availability Zones, which are distinct data centers within a region that are designed to be isolated from each other. Each Availability Zone is engineered to be highly resilient and fault-tolerant, with redundant power, networking, and connectivity.\nOption A is correct because each AWS region has multiple Availability Zones. The exact number of Availability Zones per region varies depending on the region, but most regions have at least three Availability Zones. Having multiple Availability Zones within a region provides customers with high availability and fault tolerance, enabling them to design and operate applications that are resilient to individual data center failures.\nOption B is incorrect because most AWS regions have multiple Availability Zones, not a single Availability Zone.\nOption C is incorrect because AWS Local Zones are a new type of AWS infrastructure deployment that brings AWS services to customers who need low latency access to compute and storage resources in a specific geographic location. Local Zones are not the same as Availability Zones, although they share some similarities.\nOption D is incorrect because AWS management console and control plane are not isolated to a single region. AWS management console and control plane are designed to be highly available and resilient, and they are distributed across multiple regions to ensure that they can continue to operate even in the event of a regional outage.\n\n"
}, {
  "id" : 330,
  "question" : "An online marketplace start-up dealing in real estate is planning to move to the cloud.\nWhich of the below is NOT a benefit of moving to the cloud?\n",
  "answers" : [ {
    "id" : "a37533d0cb2b4d4ea373ce7e29874fa7",
    "option" : "Install on a company`s own servers.",
    "isCorrect" : "true"
  }, {
    "id" : "1a92d02a2f09424da014f53ab6a14890",
    "option" : "Go global in minutes.",
    "isCorrect" : "false"
  }, {
    "id" : "effcbefaa737421db1be17f8d94619cc",
    "option" : "Stop guessing capacity.",
    "isCorrect" : "false"
  }, {
    "id" : "5cc816d1be0849babdde1afd35a66277",
    "option" : "Benefit from massive economies of scale.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nThis description belongs to on-premises and is not a benefit of moving to the cloud.\nOption B is INCORRECT.\nThis is a benefit of moving to the cloud.\nOption C is INCORRECT.\nThis is a benefit of moving to the cloud.\nOption D is INCORRECT.\nThis is a benefit of moving to the cloud.\nReference:\nhttps://docs.aws.amazon.com/whitepapers/latest/aws-overview/six-advantages-of-cloud-computing.html\n\nAnswer: A. Install on a company's own servers.\nExplanation: Moving to the cloud provides numerous benefits to companies, including the following:\nA. Go global in minutes: By moving to the cloud, companies can quickly expand their services to a global audience by deploying their application in multiple regions around the world.\nB. Stop guessing capacity: With the cloud, companies no longer have to guess how much capacity they need for their application. Instead, they can quickly scale their application up or down as needed to meet demand.\nC. Benefit from massive economies of scale: Cloud providers like AWS have massive infrastructure that is used by many customers. As a result, they can offer economies of scale that are not possible for individual companies to achieve.\nD. Install on a company's own servers: This is not a benefit of moving to the cloud. In fact, it is the opposite. Moving to the cloud means that companies no longer have to worry about maintaining their own servers or infrastructure. Instead, they can rely on the cloud provider to handle these tasks for them.\nTherefore, the correct answer to the question is A. Install on a company's own servers.\n\n"
}, {
  "id" : 331,
  "question" : "My On-Premise application's deployment cycle is roughly around 3-4 weeks.\nOn refactoring this huge application, its features can be deployed on AWS cloud in a matter of 2-3 days.\nWhat is the benefit achieved by moving to the AWS cloud?\n",
  "answers" : [ {
    "id" : "913801d005e94934bf920f0e395d1aa2",
    "option" : "Elasticity",
    "isCorrect" : "false"
  }, {
    "id" : "dd572bf42548478f9d338da4460d0689",
    "option" : "Flexibility",
    "isCorrect" : "false"
  }, {
    "id" : "19f239dc22914077abeb0b26af7783b2",
    "option" : "Agility",
    "isCorrect" : "true"
  }, {
    "id" : "6106e3c85f79453f957ebaeff5964932",
    "option" : "Resilience.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nDiagrams:\nThe scenario blends itself into the Business Agility value proposition where new features can be deployed faster to reduce errors.\nThe microservices architecture paradigm assists in Business Agility, where a large Monolith application is broken down into smaller functional units that can be developed &amp; deployed faster.\nAWS cloud, on the other hand, provides services like Lambda, Elastic Container Services, Elastic Kubernetes Service that assists in developing &amp; deploying microservices &amp; deployment tools (CI/CD) like CodeBuild, CodePipeline for automating the deployment process.\nCombined, they provide an environment supporting an Agile Business.\nOption A is incorrect.\nElasticity is the ability to scale resources on-demand whenever there is an increase in load on the existing infrastructure.\nThis Value proposition will benefit in substantial cost savings where I need not have to guess my capacity upfront.\nOption B is incorrect.\nFlexibility is the ability to utilize a broad range of products depending on the application &amp; infrastructure demands.\nFor example, there are a broad range of EC2 instance pricing models ranging from On-Demand instances to Reserved Instances to Spot Instances and EC2 instance types like General Purpose, Compute Optimized, Memory Optimized.\nThis will help with Low or No cost entries into the AWS cloud.\nOption C is CORRECT.\nUsing an Agile environment provided byAWS Cloud, I can release application features much faster than a traditional On-Premise environment.\nOption D is incorrect.\nOperational resiliency results in the improvement of defined SLA's and reducing unplanned outages that would result in downtime.\nThe features of High Availability, Fault Tolerance gives rise to a resilient system.\nReferences:\nhttps://www.youtube.com/watch?v=sA79G-zoSI0\nhttps://www.testpreptraining.com/tutorial/aws-cloud-practitioner/define-the-aws-cloud-and-its-value-proposition/\nhttps://youtu.be/Fl57qGYwdK4\n\n\nThe benefit achieved by moving the On-Premise application to the AWS cloud is agility.\nAgility refers to the ability to move quickly and easily with minimal disruption or downtime. By refactoring the application and deploying its features on AWS cloud, the deployment cycle time has been reduced from 3-4 weeks to 2-3 days. This reduction in deployment time allows organizations to respond faster to changing business requirements, customer needs, and market trends.\nMoreover, AWS offers a range of tools and services that enable organizations to scale up or down their resources as needed, providing elasticity. Elasticity is the ability to automatically scale computing resources up or down to meet changing demands. This means that organizations can quickly and easily adjust their resources to match their workload, without having to worry about the underlying infrastructure.\nFlexibility is another benefit of moving to the AWS cloud. AWS offers a wide range of services and tools, allowing organizations to choose the services that best fit their needs. This flexibility enables organizations to use the tools and services that are most appropriate for their specific application, resulting in improved performance and cost savings.\nResilience refers to the ability of a system to recover quickly from failures and disruptions. AWS provides several features and services, such as backup and disaster recovery, that enable organizations to maintain high levels of resilience.\nIn summary, while all of the options listed (elasticity, flexibility, agility, and resilience) are benefits of moving to the AWS cloud, the reduction in deployment time from 3-4 weeks to 2-3 days primarily highlights the agility that comes with cloud migration.\n\n"
}, {
  "id" : 332,
  "question" : "I need to migrate millions of customers' financial transaction data from the On-Premise Mainframe system to a non-relational database in AWS.\nThe database should also provide good performance for data retrieval and data analytics.\nWhich of the following Database services is the most suitable?\n",
  "answers" : [ {
    "id" : "8308210fae8a4508b659b1eb3e0305ae",
    "option" : "Amazon RDS",
    "isCorrect" : "false"
  }, {
    "id" : "d07af7cc842b48be82faa422705593b0",
    "option" : "Amazon RedShift",
    "isCorrect" : "false"
  }, {
    "id" : "9b590e8ceb15433a81cdb31a205b092f",
    "option" : "Amazon ElastiCache",
    "isCorrect" : "false"
  }, {
    "id" : "529b3c412f0546e6a05e96f6e39d9ecd",
    "option" : "Amazon DynamoDB.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nDiagrams:\nOn reading the scenario carefully, we notice that the Customer's Financial transaction data is huge.\nIt needs storage on the cloud.\nNoSQL databases like DynamoDB are designed to provide seamless scalability by automatically partitioning the database as it grows in size.\nSo a NoSQL database like DynamoDB will be the most appropriate database service that can be used for the scenario.\nOption A is incorrect.\nHere we are exclusively talking about Huge data volumes, Data retrieval and Data analytics.\nRDS databases are most useful for heavy transaction processing systems.\nThey also do not exhibit automatic partitioning capabilities with increased data volume &amp; stream processing capabilities like a NoSQL database like DynamoDB provides.\nOption B is incorrect.\nAmazon RedShift is a Data Warehousing solution primarily used for Operational analytics on business events.\nData Warehouse may comprise a big collection of an Enterprise's structured &amp; semi-structured data that can be used to build powerful reports &amp; dashboards using Business Intelligence tools.\nSince we only have the Customer's transactional data for our scenario, RedShift will not be a good fit here.\nOption C is incorrect.\nWe are talking about the scenario for a data migration of On-Premise Mainframe data, which will require a permanent, secure data store for storing the highly sensitive Customer's financial data.\nCaching solutions are typically in-memory data stores used for supporting applications requiring sub-milliseconds response times.\nCaching solutions usually maintain a subset of the data present in a data store that does not change frequently.\nAlso, caching solutions do not provide any facility for performing real-time data analytics, although they provide the best performance compared to any other data storage solutions.\nOption D is CORRECT.\nDynamoDB provides DynamoDB Accelerator (DAX) which is a fully managed, highly available in-memory cache.\nThis will help us speed up the performance of data retrieval that we require.\nDynamoDB also has a feature called DynamoDB streams that enables real-time capture of data changes using event notifications.\nThis helps applications to perform analytics on real-time streaming data to build dashboards without impacting database performance.\nThe stream events are asynchronous in nature to consuming applications like a Lambda function.\nSince the Customer's transactional data is highly confidential &amp; huge in volume, a robust, scalable, secure, performant data store like DynamoDB will be the best fit for our scenario.\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.Partitions.html\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html\n\n\nFor migrating millions of customers' financial transaction data from the On-Premise Mainframe system to a non-relational database in AWS, Amazon DynamoDB is the most suitable database service.\nAmazon DynamoDB is a fully managed NoSQL database service that provides fast and flexible performance for any scale of application. It is designed to store and retrieve any amount of data and can handle millions of requests per second with automatic scaling. It also supports key-value and document data models and is suitable for applications that require low latency and high throughput.\nIn this scenario, since the financial transaction data is non-relational and millions of records need to be stored and retrieved, a NoSQL database service like Amazon DynamoDB is the best choice. Additionally, DynamoDB supports fast and flexible data analytics with integrations with AWS services like Amazon EMR and AWS Glue.\nLet's compare the other options:\nAmazon RDS: Amazon RDS is a managed relational database service that provides SQL-based database engines like MySQL, PostgreSQL, Oracle, and SQL Server. It is optimized for transaction processing and is suitable for relational data with complex queries and transactions. However, it may not be the best choice for non-relational data like financial transaction data. Amazon RedShift: Amazon RedShift is a fully managed data warehouse service that is optimized for OLAP (Online Analytical Processing) workloads. It is designed for analytics queries on large datasets and is not suitable for transactional workloads. Therefore, it may not be the best choice for storing millions of transactional data. Amazon ElastiCache: Amazon ElastiCache is a managed in-memory data store service that supports popular open-source in-memory engines like Redis and Memcached. It is designed for high-performance, low-latency use cases like caching and session management. While it can be used to store transactional data, it may not be the most suitable option for storing millions of transactional data.\nTherefore, in summary, Amazon DynamoDB is the most suitable database service for migrating millions of customers' financial transaction data from the On-Premise Mainframe system to a non-relational database in AWS, providing good performance for data retrieval and data analytics.\n\n"
}, {
  "id" : 333,
  "question" : "I have certain applications On-Premise that run 24x7 &amp; have a consistent load.\nIplan to move to the AWS Cloud.\nWhat is the economic feature that will benefit me most referred to?\n",
  "answers" : [ {
    "id" : "9cf9151f564c46d58a76cd0f9c900f9a",
    "option" : "Pay-as-you-go",
    "isCorrect" : "false"
  }, {
    "id" : "b463ed1dc3c64c63a7ac0ef02211dbb1",
    "option" : "Save when you Reserve",
    "isCorrect" : "true"
  }, {
    "id" : "40f68708acc1475ba0a934d7b3ca0d46",
    "option" : "Pay less by using more",
    "isCorrect" : "false"
  }, {
    "id" : "5aa0ee1951b94cae900224b5ac441772",
    "option" : "Pay-per-compute-time.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nFrom the scenario, we can see that the On-Premises applications' workload is continuous &amp; stable.\nFor these kinds of applications, it is easy to predict upfront capacity.\nAlso, since the application runs continuously, I will benefit by reserving capacity for a certain period of time.\nOption A is incorrect.\nThe Pay-as-you-go model will be best used for workloads used for short durations &amp; have unpredictable load.\nOn-demand EC2 instances will be the best fit for this purpose.\nOption B is CORRECT.\nSince there is continuous usage of these applications with a predictable load, it will be best for me to reserve capacity upfront (period of 1 - 3 years) that will provide a substantial discount of 30 - 50% compared to its On Demand counterparts.\nOption C is incorrect.\nPay less by using more refers to volume discounts provided by AWS for increased usage.\nE.g., S3 Standard provides the following storage pricing, also referred to as Tiered-Pricing.\nData Storage.\nStorage Pricing.\nFirst TB / month.\n$0.025 per GB.\nNext 450 TB / month.\n$0.024 per GB.\nNext 500 TB / month.\n$0.023 per GB.\nOption D is incorrect.\nPay-per-compute-time refers to the use of serverless architectures like Lambda, where you pay only for the time when the compute resources are running.\nUnlike EC2 Pay-as-you-go pricing, AWS provisions resources for executing Lambda functions on the fly &amp; removes them immediately after execution.\nSo there is no idle utilization time that needs to be accounted for.\nSince our scenario consists of long-running applications, this option will be impractical for usage.\nReferences:\nhttps://aws.amazon.com/pricing/#:~:text=AWS%20offers%20you%20a%20pay,utilities%20like%20water%20and%20electricity.\nhttps://dzone.com/articles/the-cost-of-the-cloud-the-ultimate-aws-pricing-gui\nhttps://www.apptio.com/blog/aws-reserved-instances-cost-optimization/\nhttps://d1.awsstatic.com/whitepapers/aws_pricing_overview.pdf\n\nThe economic feature that will benefit you most in this scenario is \"Save when you Reserve,\" which is option B.\nWhen you move your applications to the cloud, you want to make sure you're using your resources as efficiently as possible. AWS offers several pricing models, but for consistent workloads like yours, the most cost-effective option is likely to be Reserved Instances (RIs).\nReserved Instances are a way to commit to using a certain amount of compute capacity in exchange for a discount on the hourly rate. You can purchase RIs for a 1- or 3-year term, and you can choose to pay all upfront, partial upfront, or no upfront. The more you pay upfront, the bigger the discount you'll receive.\nUsing Reserved Instances means that you'll have a guaranteed amount of capacity available to you at all times, without having to worry about fluctuations in demand or paying for more than you need. This can result in significant cost savings compared to paying the On-Demand rate (which is essentially pay-as-you-go) over the long term.\nOption A, Pay-as-you-go, means you pay only for what you use, with no upfront costs or commitments. This model can be useful if you have unpredictable workloads or if you're just starting out and want to experiment with different services.\nOption C, Pay less by using more, refers to AWS's tiered pricing structure. As you use more of a particular service, you'll move up to a higher usage tier and receive a lower per-unit cost. This can be a good option for services with variable usage patterns, but it's unlikely to be the most cost-effective choice for consistent workloads.\nOption D, Pay-per-compute-time, isn't a specific pricing model offered by AWS, but it could refer to the way that some services are priced, such as AWS Lambda. With Lambda, you're charged only for the time that your code is actually executing, rather than for the full duration of a server instance. This can be a cost-effective option for certain workloads, but it may not be the best choice if you need to run applications continuously.\n\n"
}, {
  "id" : 334,
  "question" : "A client who has adopted AWS cloud services would like to ensure that his systems always scale with increasing traffic for a great end-user experience.\nI have implemented the same by defining AutoScaling Scale-In &amp; Scale-Out policies &amp; CloudWatch alarms that trigger the AutoScaling.\nWhich Cloud Architecture Design principles have I implemented here? Select TWO most suitable options.\n",
  "answers" : [ {
    "id" : "a0034f0c2c3440009e44aaf2707e9d39",
    "option" : "Encryption",
    "isCorrect" : "false"
  }, {
    "id" : "ad919d6ba78b4421924a9882f1afe4cb",
    "option" : "Operational Excellence",
    "isCorrect" : "true"
  }, {
    "id" : "a7551801e04043dfada9a16986e27d4b",
    "option" : "Performance Efficiency",
    "isCorrect" : "true"
  }, {
    "id" : "03243f4b3d184daba136aadb9ecf438c",
    "option" : "Cost Optimization",
    "isCorrect" : "false"
  }, {
    "id" : "fffe9fb01cac45848b1aa551fa32dd3a",
    "option" : "Least privilege.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: B and C.\nLooking at the scenario, a good end-user experience is attached to systems being performant with increasing load on them.\nA combination of Load balancing &amp; AutoScaling enables a system to handle increase in load by spinning new instances to which the load will be distributed not to saturate resources like CPU &amp; Memory on a single server instance.\nFor the AutoScaling itself to work efficiently, there needs to be a good monitoring system that can track resource utilization and enable automation.\nThe scenario combines the Operational Excellence &amp; the Performance Efficiency Architecture design principles.\nOption A is incorrect.\nThis scenario does not describe anything about data encryptions so that this option is not selected.\nOption B is CORRECT.\nThe monitoring mechanism used here is CloudWatch for enabling AutoScaling to happen by tracking resource utilization metrics like CPU and Memory usage.\nThe Operational Excellence pillar focuses on Monitoring systems that will deliver continuous business value by providing automation, responding to events within the system.\nIn our scenario, automating the system's Scalability through monitoring will help maintain the desired performance levels.\nOption C is CORRECT.\nThe Performance Efficiency pillar focuses on monitoring the performance of a system.\nCloudWatch metrics help monitor a system's performance by using metrics like CPU, Memory, Disk utilization etc..\nAutomating tasks like AutoScaling using CloudWatch Alarms and defining scaling policies will ensure that the client's performance requirements will always be met with when there is an increase in traffic.\nOption D is incorrect.\nIn spite of using Scale-in &amp; Scale-out policies, I can be benefitted from cutting down idle resource utilization costs.\nThe scenario deals more with Performance &amp; Operational aspects.\nOption E is incorrect.\nLeast privilege is a security principle which is not mentioned in the question.\nDiagrams:\nReferences:\nhttps://aws.amazon.com/blogs/apn/the-5-pillars-of-the-aws-well-architected-framework/\n\n\nThe two Cloud Architecture Design principles that have been implemented in this scenario are:\nC. Performance Efficiency - This principle is focused on ensuring that the system is able to deliver the desired level of performance and availability to end-users. By defining AutoScaling Scale-In & Scale-Out policies and CloudWatch alarms that trigger the AutoScaling, the client is able to ensure that his systems always scale with increasing traffic for a great end-user experience. With the help of AutoScaling, the system can automatically adjust its capacity to maintain steady, predictable performance at the lowest possible cost.\nD. Cost Optimization - This principle is focused on optimizing costs without sacrificing performance or functionality. By implementing AutoScaling policies and CloudWatch alarms, the client can optimize the costs of his AWS infrastructure. When the traffic is low, the system can automatically reduce the number of instances to save costs, and when the traffic is high, the system can increase the number of instances to maintain performance. This results in cost savings as the client pays only for the resources he needs, and the system can handle sudden spikes in traffic without the need for manual intervention.\nA brief explanation of the other options:\nA. Encryption - Encryption is a security principle that ensures data is protected while in transit or at rest. While encryption may be important for data protection in an AWS environment, it is not relevant to the scenario described in the question.\nB. Operational Excellence - This principle is focused on ensuring that systems are reliable, efficient, and easy to operate. While the implementation of AutoScaling policies and CloudWatch alarms can contribute to operational excellence, this principle is more broadly focused on ensuring that the systems are well-architected, monitored, and maintained over time.\nE. Least privilege - This principle is focused on ensuring that access to resources is restricted to only those who need it to perform their jobs. While least privilege is an important security principle in AWS, it is not relevant to the scenario described in the question.\n\n"
}, {
  "id" : 335,
  "question" : "A customer's Data center and its applications are connected to AWS using a dedicated network (Direct Connect).What is the Cloud deployment model?\n",
  "answers" : [ {
    "id" : "a377d4012cee40c987d181111d012d8e",
    "option" : "Public Cloud",
    "isCorrect" : "false"
  }, {
    "id" : "1196eb89d0a34ca7b125503f4836c2c9",
    "option" : "Multi Cloud",
    "isCorrect" : "false"
  }, {
    "id" : "dd5a9abc673c49b8804f176a8cd70bd7",
    "option" : "Private Cloud",
    "isCorrect" : "false"
  }, {
    "id" : "1d19cae0a4f14c3dac8c15542ffd7aa2",
    "option" : "Hybrid Cloud.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect.\nWhen an application is fully deployed on a vendor's (AWS, Google Cloud) Cloud Infrastructure, it is termed as a Public Cloud.\nThe application features may have been either developed on the cloud (Cloud Native) or migrated from an existing Client's On-Premise infrastructure.\nOur scenario uses both the Customer's Data Center and AWS.\nOption B is incorrect.\nA multi-cloud leverages different Cloud Provider's environments.\nHe may deploy his applications on AWS and GoogleCloud-based on compute speed requirements, availability of managed services etc...With the use of Multi-Cloud, it is best to use Open Source tools like Terraform for Infrastructure as Code, Splunk for monitoring that are cloud vendor-agnostic to avoid vendor lockin.\nSince our scenario uses only the Customer's Data Center and AWS, there are no multiple cloud deployments.\nOption C is incorrect.\nA Private Cloud refers to a cloud environment that has been built In house on the Client's premises using Virtualization and resource management tools.\nPrivate clouds are usually built for specific security and regulatory requirements, specially hardware or configuration requirements, and extremely critical network latency that may not be possible on a vendor's public cloud platform.\nSince our scenario has both the Client's data center &amp; the AWS Cloud, it will not result in a PrivateCloud configuration.\nOption D is CORRECT.\nThe cloud deployment model is a Hybrid Cloud.\nHybrid clouds are often used where there is a requirement of burst capacity where workloads are â€œspilled overâ€ to a different cloud environment to meet capacity demands for a short period of time.\nHere, purchasing capacity may not be a good idea since it will be extremely ineffective from a cost standpoint resulting in under utilization of resources once the requirement for capacity ends.\nThe other compelling use case for a Hybrid cloud is a Highly Available / Disaster Recovery environment where the cloud model offers a lot of flexibility in making resources in the event of a Datacenter failure where the client may not need upfront investments for resources for an alternate site.\nDiagrams:\nReferences:\nhttps://youtu.be/X1qoKLL040A\nhttps://youtu.be/X_ZZO-oeNeg\nhttps://youtu.be/9KZL0_NuiUU\n\n\nThe cloud deployment model describes how an organization deploys its applications and data in the cloud. There are four primary cloud deployment models: public, private, hybrid, and multi-cloud.\nA public cloud is a deployment model where a third-party cloud service provider makes resources, such as virtual machines, storage, and applications, available to the general public over the internet. In this deployment model, the cloud provider owns and manages the infrastructure, including the hardware, software, and network components.\nA private cloud is a deployment model where the cloud resources are dedicated to a single organization, and the organization owns and manages the cloud infrastructure. The private cloud may be hosted on-premises or by a third-party service provider. The primary benefit of a private cloud is the increased control and security it provides.\nA hybrid cloud is a deployment model that combines public and private clouds to leverage the benefits of both. In a hybrid cloud, an organization can run its applications and store its data in either a public or private cloud and move workloads between the two environments as needed.\nMulti-cloud is a deployment model where an organization uses multiple public cloud service providers to distribute workloads across multiple clouds. In this deployment model, an organization may use different cloud providers for different applications or services, leveraging the strengths of each provider.\nIn the given scenario, the customer's data center and applications are connected to AWS using a dedicated network (Direct Connect), which means that the customer has a private connection to AWS. Therefore, the deployment model is a private cloud.\n\n"
}, {
  "id" : 336,
  "question" : "Your company has setup EC2 Instances in a VPC for their application.\nThe IT Security department needs to understand what the security mechanisms are available to protect the Instances when it comes to traffic going in and out of the instance.\nWhat are the two layers of security provided by AWS in the VPC? Choose 2 answers from the options given below.\n",
  "answers" : [ {
    "id" : "39f9e3f611cb4db1b17eacad5e23b4a0",
    "option" : "Security Groups",
    "isCorrect" : "true"
  }, {
    "id" : "701e38d9c0c84fd38d1e51eb02dfe66b",
    "option" : "NACLs",
    "isCorrect" : "true"
  }, {
    "id" : "94838dc6300b404ba459cfac7ddbc2ca",
    "option" : "DHCP Options",
    "isCorrect" : "false"
  }, {
    "id" : "9e7ae607890640b2817c14b13be6b125",
    "option" : "Route Tables.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answers: A and B.\nThe AWS Documentation mentions the following.\nA security group acts as a virtual firewall for your instance to control inbound and outbound traffic.\nWhen you launch an instance in a VPC, you can assign up to five security groups to the instance.\nA network access control list (ACL) is an optional layer of security for your VPC that acts as a firewall for controlling traffic in and out of one or more subnets.\nYou might set up network ACLs with rules similar to your security groups in order to add an additional layer of security to your VPC.Option C is incorrect since this is used to decide on the DNS servers for the VPC.\nOption D is incorrect since this is used for routing traffic in the VPC.\nFor more information on VPC security groups and NACL's, please visit the below URL.\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_SecurityGroups.html\nhttps://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_ACLs.html\n\nSure, I'd be happy to explain the two layers of security provided by AWS in a VPC.\nAmazon Virtual Private Cloud (VPC) is a logically isolated virtual network in the AWS Cloud where you can launch AWS resources such as Amazon Elastic Compute Cloud (EC2) instances, Amazon Relational Database Service (RDS) instances, and other services. VPC provides you with control over your virtual networking environment, including your own IP address range, subnets, route tables, and network gateways.\nWhen it comes to securing EC2 instances within a VPC, AWS provides two layers of security:\nSecurity Groups Network Access Control Lists (NACLs)\nA. Security Groups: Security Groups are the first layer of defense for your EC2 instances. They are virtual firewalls that control the inbound and outbound traffic for one or more instances. Security Groups are stateful, which means that if you allow inbound traffic, the return traffic is automatically allowed. You can create rules in Security Groups to allow traffic from specific IP addresses, CIDR blocks, or other Security Groups. You can also specify the protocols and ports to allow or deny traffic.\nFor example, you can create a Security Group that allows HTTP traffic (port 80) from the internet to your web server instances, but denies all other traffic. Similarly, you can create a Security Group that allows SSH traffic (port 22) only from your corporate IP address range to your database server instances.\nB. Network Access Control Lists (NACLs): Network Access Control Lists (NACLs) are the second layer of defense for your EC2 instances. They are also virtual firewalls, but they operate at the subnet level, not at the instance level. NACLs are stateless, which means that if you allow inbound traffic, you must also allow the return traffic explicitly. NACLs have separate inbound and outbound rules, and you can create rules to allow or deny traffic based on IP addresses, CIDR blocks, protocols, and ports.\nNACLs provide a coarse-grained control over traffic that flows in and out of your subnets. For example, you can create a NACL that allows HTTP traffic (port 80) from the internet to your web server subnet, but denies all other traffic. Similarly, you can create a NACL that allows SSH traffic (port 22) only from your corporate IP address range to your management subnet.\nC. DHCP Options: DHCP Options provide configuration settings for instances that are launched in your VPC, such as domain name servers and domain search lists. While DHCP Options are important for networking in general, they do not provide any additional security mechanisms for your instances.\nD. Route Tables: Route Tables are used to control the traffic between subnets in your VPC. While Route Tables are important for routing in general, they do not provide any additional security mechanisms for your instances.\nIn summary, AWS provides two layers of security for your EC2 instances within a VPC: Security Groups and Network Access Control Lists (NACLs). Security Groups operate at the instance level, while NACLs operate at the subnet level. Both Security Groups and NACLs allow you to control inbound and outbound traffic based on IP addresses, CIDR blocks, protocols, and ports.\n\n"
}, {
  "id" : 337,
  "question" : "Which of the following may NOT be an Economic benefit to a client using AWS cloud services?\n",
  "answers" : [ {
    "id" : "b3ab7f580bf74a2490c955db49e4051e",
    "option" : "The Client is running a dedicated MySQL Database Server on AWS with his own CPU bound license (BOYL).",
    "isCorrect" : "true"
  }, {
    "id" : "d3355cce40054fb3851746319fda019b",
    "option" : "The Client is running Spot Instances for batch data processing workloads.",
    "isCorrect" : "false"
  }, {
    "id" : "06e5220c3d33403db35a72c528b8eda1",
    "option" : "The client is running applications with a relatively predictable &amp; consistent resource Demand using AWS Reserved Instances.",
    "isCorrect" : "false"
  }, {
    "id" : "19e54d8f31064d1bbc2ac5e5a8902434",
    "option" : "The client is using S3 Intelligent Tiering storage class while uploading objects.",
    "isCorrect" : "false"
  }, {
    "id" : "e831c0bdcf1642009410527baef4aef8",
    "option" : "The client is using an Active - Passive failover routing strategy of his On - Premise Data Center to AWS cloud.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: A.\nOption A is CORRECT.\nCPU bound software licenses will require a Dedicated Host tenancy model rather than a Shared tenancy model for the EC2 instances hosting the MySQL Database software.\nDedicated hosts are the most expensive tenancy model when it comes to pricing.\nFor example, an m4 large On Demand dedicated host is 24 times more expensive than an On-demand shared host.\nA dedicated host tenancy may be used under exceptional circumstances of spiky traffic, while purchasing Hardware On-Premise may not be the best option.\nThen the client can take advantage of Cloud Elasticity.\nOption B is incorrect.\nBatch processing jobs rely more on accuracy rather than on speed which forms a good use case for using Spot Instances, providing economies of compute where intermittent disconnections may not be a real problem.\nOption C is incorrect.\nAWS Reserved Instances have discounts on the EC2 usage.\nThis method can provide an economic benefits.\nOption D is incorrect.\nS3 Intelligent Tiering is a smart solution for managing the lifecycle of S3 objects resulting in economies of cost.\nWith intelligent tiring, there are no retrieval fees nor there are any fees for moving objects between tiers unlike using S3 Lifecycle policies which incurs data transfer charges.\nS3 Lifecycle policies are also often challenging to define due to the unpredictable nature of application adoption and usage.\nEven in scenarios where access frequency is known, it may so happen that customers may not use proper storage class adjustments resulting in non-optimized budgets.\nOption E is incorrect.\nAn Active-Passive failover will always be economical to a client using the strategy for Disaster Recovery scenarios since he will not be investing in an entire redundant site with all resources running simultaneously.\nHe will have the flexibility to select his strategy depending on his Recovery Time objectives.\nThis will help him to save on costs.\nReferences:\nhttps://www.liquidweb.com/blog/private-cloud/\nhttps://www.nutanix.com/blog/s3-intelligent-tiering-aws-cost-saving\nhttps://www.xplenty.com/blog/the-5-types-of-data-processing/#batch\nhttps://aws.amazon.com/ec2/dedicated-hosts/pricing/\nhttps://aws.amazon.com/ec2/pricing/on-demand/\n\nOut of the given options, the answer is A.\nExplanation:\nAWS provides numerous benefits to its clients, including economic benefits. Economic benefits refer to cost savings that clients can experience by using AWS cloud services instead of building and maintaining their own infrastructure. Let's analyze each option to see which one may not be an economic benefit for the client.\nA. The client is running a dedicated MySQL Database Server on AWS with his own CPU bound license (BOYL).\nThis option may not be an economic benefit for the client because the client is using his own CPU bound license (BOYL). This means that the client is responsible for paying for the license, which may not be cost-effective compared to using AWS licenses.\nB. The client is running Spot Instances for batch data processing workloads.\nThis option is an economic benefit for the client because Spot Instances are significantly cheaper than On-Demand Instances. Spot Instances allow clients to bid on unused EC2 capacity and pay the current Spot price. Clients can save up to 90% on their EC2 costs by using Spot Instances.\nC. The client is running applications with a relatively predictable and consistent resource demand using AWS Reserved Instances.\nThis option is an economic benefit for the client because Reserved Instances allow clients to commit to using specific instance types in specific Availability Zones for a one- or three-year term. This commitment provides clients with significant cost savings compared to On-Demand Instances.\nD. The client is using S3 Intelligent Tiering storage class while uploading objects.\nThis option is an economic benefit for the client because S3 Intelligent Tiering is designed to optimize costs by automatically moving data between two access tiers based on changing access patterns. This feature helps clients save costs by reducing the need to manually move data between different storage classes.\nE. The client is using an Active-Passive failover routing strategy of his On-Premise Data Center to AWS cloud.\nThis option is an economic benefit for the client because it allows the client to use AWS resources as a disaster recovery site, which can be significantly cheaper than maintaining a separate disaster recovery site on-premise. Additionally, the client can use AWS resources only when needed, which reduces the overall costs of maintaining a disaster recovery site.\nIn summary, out of the given options, option A may not be an economic benefit for the client. The client is using his own CPU bound license (BOYL), which may not be cost-effective compared to using AWS licenses.\n\n"
}, {
  "id" : 338,
  "question" : "Which of the following indicates a Single Point Of Failure (SPOF) in an AWS Cloud Architecture design? Refer to the figure below.\n\n",
  "answers" : [ {
    "id" : "dfe5fea9557f47f4960ef5ba7dc71597",
    "option" : "Application Load Balancer",
    "isCorrect" : "false"
  }, {
    "id" : "830f7872c3774ee5bc38596e43dc59a0",
    "option" : "Bastion Host",
    "isCorrect" : "true"
  }, {
    "id" : "06adf778d341458da826ae0d57ad5830",
    "option" : "EC2 Instance",
    "isCorrect" : "false"
  }, {
    "id" : "96dc84b0a09340bd8d23181207b4da72",
    "option" : "NAT Gateway within an Availability Zone.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect.\nFor an Application Load Balancer, you need to specify at least two subnets each in a different AZ.\nThis ensures that the load balancer will redirect load to a different AZ if one of them goes down ensuring HA and redundancy.\nApplication Load balancer performs Health Checks on its registered targets and will route requests only to Healthy Targets.\nEven though the ALB is depicted as a single resource in the figure, multiple ALB instances are created for each AZ behind the scenes.\nIt ensures that the ALB is not a SPOF.Option B is CORRECT.\nWe can see in the figure that the Bastion Host has been deployed on only one of the AZ's causing a SPOF.\nFor redundancy, the Bastion Host needs to be created in Multiple AZ's where a failure in a single AZ will not impact access to Server instances in Private subnets through the Bastion Hosts.\nOption C is incorrect.\nWe can see EC2 instances being deployed on Private Subnets in multiple AZ's, ensuring redundancy &amp; avoiding SPOF.Option D is incorrect.\nNAT Gateway offers redundancy within a single AZ.\nHence it is not a SPOF within the AZ.\nReferences:\nhttps://packetswitch.co.uk/aws-nat-gateway-high-availability/#:~:text=NAT%20Gateway%20HA%20scenario,Availability%20Zones%20lose%20Internet%20access.\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html\nhttps://jayendrapatil.com/aws-bastion-host/\nhttps://stackoverflow.com/questions/46698011/are-amazon-elastic-load-balancer-elb-failure-proof\n"
}, {
  "id" : 339,
  "question" : "Among the AWS resources or the AWS features (cloud concepts) listed below, which option does NOT provide automation capabilities?\n",
  "answers" : [ {
    "id" : "4f3ee45d3e864bfa88321ebeac6b94c5",
    "option" : "Elastic BeanStalk",
    "isCorrect" : "false"
  }, {
    "id" : "0911341fe1434b84959b3e937f0fab6e",
    "option" : "DynamoDB",
    "isCorrect" : "false"
  }, {
    "id" : "b43d7c483f554ffdaa2cde6803b6e939",
    "option" : "Fault Tolerance",
    "isCorrect" : "false"
  }, {
    "id" : "28abf548aef1483bbff195e9ddd7cfcb",
    "option" : "RDS manual snapshot.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect.\nElastic Beanstalk provides a fast way to deploy a web application on AWS.\nBehind the scenes, it automatically handles resource provisioning, load balancing, autoscaling and monitoring when configured.\nOption B is incorrect.\nDynamoDB contains a feature called DynamoDB streams that provides change events for automatically capturing data during operations like CREATE, DELETE, UPDATE on its tables.\nThis relieves application developers from implementing naive methods like using a timer at regular intervals to scan the tables for specific data patterns for creating dashboards, which is non-real-time, performance-intensive and costly since it will use Read Capacity Units (RCU)\nThe events can be subscribed by a Lambda function, its resulting data patterns extracted in real-time without impacting Table performance or costs.\nOption C is incorrect.\nFault tolerance environment always ensures that catastrophic loss of resources like DataCenters or Availability zones will not result in application downtime resulting in 100% availability.\nFault tolerance mechanisms always have an Active-Active site failover mechanism where both the Primary and Backup resources are fully functional and operational.\nTraffic is sent to both the sites, and in the event of failover, traffic will be routed to the available site.\nFault-Tolerant setups may have different scenarios like Disaster Recovery,Multi-AZ database setups exhibiting redundancy with 2 copies of data to ensure write availability and 3 copies of data to ensure read availability.\nWe may compare Fault Tolerance to an Aircraft analogy where the loss of an engine will still allow the plane to function without rectifying it, which won't be possible mid-air.\nOption D is CORRECT.\nUnlike automated backups, manual snapshots are taken by users when needed.\nIt is not an automation method.\nReferences:\nhttps://medium.com/@crishantha/aws-disaster-recovery-scenarios-1e8234109e79\nhttps://aws.amazon.com/blogs/database/dynamodb-streams-use-cases-and-design-patterns/#:~:text=DynamoDB%20Streams%20is%20a%20powerful,for%20up%20to%2024%20hours.\nhttps://youtu.be/OjppS4RWWt8\n\nThe option that does NOT provide automation capabilities among the AWS resources or features listed is D. RDS manual snapshot.\nHere's a detailed explanation of each option:\nA. Elastic Beanstalk: Elastic Beanstalk is a fully managed service provided by AWS that helps deploy and manage applications in a variety of programming languages such as Java, .NET, PHP, Node.js, Python, Ruby, and Go. It provides automation capabilities for deploying, scaling, monitoring, and updating applications.\nB. DynamoDB: DynamoDB is a managed NoSQL database service provided by AWS. It provides automation capabilities for scaling, backup and recovery, and replication across multiple regions.\nC. Fault Tolerance: Fault Tolerance is a characteristic of a system that can continue to operate properly in the event of the failure of one or more components. AWS provides automation capabilities for achieving fault tolerance by using services like Elastic Load Balancing, Auto Scaling, and AWS Lambda.\nD. RDS manual snapshot: RDS (Relational Database Service) is a managed database service provided by AWS that supports various database engines such as MySQL, PostgreSQL, Oracle, and SQL Server. While RDS provides automation capabilities for many tasks such as database provisioning, backups, and software patching, manual snapshot creation is not automated.\nIn summary, all the options except for D provide automation capabilities. Elastic Beanstalk provides automation for deploying, scaling, monitoring, and updating applications. DynamoDB provides automation for scaling, backup and recovery, and replication. Fault Tolerance provides automation capabilities for achieving system reliability in the event of failures. RDS manual snapshots, on the other hand, do not provide automation capabilities.\n\n"
}, {
  "id" : 340,
  "question" : "I have certain applications On-Premises that experience times within a year where infrastructure takes a heavier load impact (e.g., Christmas, Thanksgiving, etc.) than other times in the year.\nYou do not want to decommission the on-premises infrastructure.\nWhat is the easiest and most cost-effective way in which I can handle this load?\n",
  "answers" : [ {
    "id" : "a41f2a4d19ed4b44b5cf679d5cb5ed74",
    "option" : "By moving all my infrastructure to AWS Cloud and using On-Demand capacity",
    "isCorrect" : "false"
  }, {
    "id" : "0138dd8d42cd4169a813c0d72d0c6206",
    "option" : "By creating a Private Cloud environment in my On-Premises data center that will provide me with the required elasticity",
    "isCorrect" : "false"
  }, {
    "id" : "e2e2f355c03d4909a80f1659a545ef8b",
    "option" : "By using Scheduled Reserved Instances to match capacity reservation for the load",
    "isCorrect" : "false"
  }, {
    "id" : "9d01f411d5404ff7962b226932124ddc",
    "option" : "By provisioning Burst Capacity on the AWS Cloud for the duration of the load.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is incorrect.\nOn looking at the scenario, we see that the variable load is only for a specific duration of time.\nSo moving the infrastructure entirely to a Public Cloud will not be the best solution to gain maximum benefit out of the elastic nature of a Public Cloud.\nOption B is incorrect.\nA Private Cloud will be more beneficial where there is a consistent load rather than a variable load.\nIt will be good to have a Private Cloud hosting the applications for economies of cost and agility rather than elasticity, which can be best obtained using a Public Cloud.\nOption C is incorrect.\nReserved Instances are usually best chosen where there is consistent usage and predictable load for a certain duration of time (1 - 3 Years)\nScheduled Reserved Instances have the ability to reserve capacity for a predictable recurring schedule that may be in a day, week, or month.\nThe advantage here is w.r.t the costs that I incur for Reserved Instances that can be managed without paying everything upfront rather than elasticity.\nOption D is CORRECT.\nThis is the best way to reduce the costs of purchasing Hardware and getting the benefits of the elasticity and On-Demand pricing provided by a Public Cloud environment.\nOn these specific occasions, I can demand a burst capacity by going to the AWS Public cloud.\nThat will certainly help me maintain the performance of my applications at heavy load.\nOnce the load reduces, I can then terminate the instances that are no longer used to save costs.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/burstable-performance-instances.html\nhttps://youtu.be/OzVpynd_2GM\n\nOption A: By moving all my infrastructure to AWS Cloud and using On-Demand capacity Moving all the infrastructure to AWS cloud and using On-Demand capacity could be an option, but it may not be the easiest and most cost-effective way. On-Demand instances are the most expensive instances in AWS, and they are not suitable for long-term use. They are suitable for short-term, irregular workloads.\nOption B: By creating a Private Cloud environment in my On-Premises data center that will provide me with the required elasticity Creating a Private Cloud environment in your On-Premises data center could provide the required elasticity. However, creating a private cloud environment requires a significant upfront investment, including hardware, software, and licensing costs, which may not be cost-effective. Also, maintaining a private cloud environment may be complex, and it may require a skilled IT team.\nOption C: By using Scheduled Reserved Instances to match capacity reservation for the load Using Scheduled Reserved Instances to match capacity reservation for the load could be a cost-effective option. Scheduled Reserved Instances allow you to reserve capacity for specific periods, which could match the expected heavy load period. By using Scheduled Reserved Instances, you could save up to 70% on your infrastructure costs compared to On-Demand instances.\nOption D: By provisioning Burst Capacity on the AWS Cloud for the duration of the load Provisioning Burst Capacity on the AWS Cloud for the duration of the load could be an option. AWS offers burstable instance types that provide a baseline level of CPU performance with the ability to burst CPU usage. However, bursting requires credits, and if you exhaust the credits, the instance's CPU performance will be throttled. Also, if the workload is sustained, bursting may not be a suitable option.\nIn conclusion, the most cost-effective and easiest way to handle the heavy load periods while keeping the On-Premises infrastructure is to use Scheduled Reserved Instances.\n\n"
}, {
  "id" : 341,
  "question" : "Which of the below can be configured to enhance the security at the subnet level?\n",
  "answers" : [ {
    "id" : "6f61c4280d7e4c0a9be78348c14ada8a",
    "option" : "Virtual Private Cloud (VPC)",
    "isCorrect" : "false"
  }, {
    "id" : "2d726f27cd074960972e175714f3617a",
    "option" : "Configure transitive VPC peering",
    "isCorrect" : "false"
  }, {
    "id" : "546f16b2a5cb486eb772b3ee1c3deaf7",
    "option" : "NACL (Network Access Control List)",
    "isCorrect" : "true"
  }, {
    "id" : "66a4c0606b1c4b138dfa872860272186",
    "option" : "Security Group.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT.\nVirtual Private Cloud (VPC) is a virtual network that lets us launch AWS resources in the defined virtual network.\nOption B is INCORRECT.\nConfigure transitive VPC peering is invalid as this is not supported in AWS.\nOption C is CORRECT.\nNACLs can be configured to enhance the security at the subnet level.\nOption D is INCORRECT.\nSecurity Group acts as a virtual firewall by controlling the traffic both inbound and outbound.\nSecurity group acts at the instance level.\nReference:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\nhttps://docs.aws.amazon.com/vpc/latest/peering/invalid-peering-configurations.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html\nhttps://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html\n\nThe correct answer is C. NACL (Network Access Control List).\nExplanation: In Amazon Web Services (AWS), a Virtual Private Cloud (VPC) allows customers to create their own virtual network within AWS. A VPC is a logically isolated section of the AWS cloud where customers can launch AWS resources like Amazon Elastic Compute Cloud (Amazon EC2) instances, Amazon Relational Database Service (Amazon RDS) instances, and others.\nA subnet is a range of IP addresses in the VPC. Each subnet must be associated with a route table, which controls the traffic between the subnets and the internet. When you create a VPC, AWS automatically creates a default subnet for you.\nTo enhance security at the subnet level, you can use a Network Access Control List (NACL). A NACL is an optional layer of security for your VPC that acts as a firewall for controlling inbound and outbound traffic at the subnet level.\nA NACL consists of a set of rules that allow or deny traffic to and from the subnet. Each rule is evaluated in order, starting with the lowest rule number. The rules can be configured to allow or deny traffic based on the source and destination IP addresses, the protocol used, and the port number.\nYou can configure a NACL to allow or deny traffic based on the IP address of the source or destination, the protocol used, and the port number. NACLs are stateless, which means that you must create a rule to allow inbound traffic and a separate rule to allow outbound traffic.\nSecurity Groups, on the other hand, are another important layer of security for your AWS resources. Security Groups act as a virtual firewall to control inbound and outbound traffic for your Amazon EC2 instances. Security Groups are stateful, which means that any traffic that is allowed to enter an instance is automatically allowed to leave that instance. Security Groups are associated with network interfaces, which means that you can apply different Security Groups to different instances.\nTherefore, Security Groups are not used to enhance the security at the subnet level, but to control the traffic for specific instances. Hence, the correct answer to the question is C. NACL.\n\n"
}, {
  "id" : 342,
  "question" : "Under the â€œShared Responsibility Model,â€ which of the listed below is Customer's Responsibility?\n",
  "answers" : [ {
    "id" : "eb3e86e3e33d4c2d941b1614075cd7be",
    "option" : "Hardware of the AWS underlying infrastructure",
    "isCorrect" : "false"
  }, {
    "id" : "e03c8fc7c33549a1b614ebf3a78f818f",
    "option" : "Client-side data encryption",
    "isCorrect" : "true"
  }, {
    "id" : "7921fa9915d14eb0a6787ba54e748493",
    "option" : "Database of the AWS infrastructure",
    "isCorrect" : "false"
  }, {
    "id" : "95eac0099a1b4ff3aeb3bbf9792d7e2d",
    "option" : "Networking of the AWS infrastructure.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nRefer to the link and diagram below.\nOption B is CORRECT.\nRefer to the link and diagram below.\nOption C is INCORRECT.\nRefer to the link and diagram below.\nOption D is INCORRECT.\nRefer to the link and diagram below.\nReference:\nhttps://aws.amazon.com/compliance/shared-responsibility-model/#:~:text=Security%20and%20Compliance%20is%20a%20shared%20responsibility%20between%20AWS%20and%20the%20customer.&amp;text=The%20customer%20assumes%20responsibility%20and,AWS%20provided%20security%20group%20firewall\n\n\nThe \"Shared Responsibility Model\" is a concept used in cloud computing that defines the responsibilities of both the cloud provider and the customer in terms of security and management of the infrastructure and data.\nAccording to this model, the cloud provider (in this case, AWS) is responsible for the security and maintenance of the underlying infrastructure, such as the physical data centers, network infrastructure, and server hardware.\nOn the other hand, the customer is responsible for managing and securing the applications, data, and operating systems that run on top of the cloud infrastructure. This includes setting up and maintaining security controls such as firewalls, encryption, and access controls, as well as configuring and monitoring the network and application infrastructure.\nGiven the options provided, the correct answer for the customer's responsibility under the Shared Responsibility Model is B. Client-side data encryption. This refers to the encryption of data by the customer before it is sent to AWS. While AWS provides server-side encryption for data at rest, it is the customer's responsibility to ensure that sensitive data is protected while in transit or while being processed by their applications.\nOption A, Hardware of the AWS underlying infrastructure, is the responsibility of AWS, as mentioned earlier.\nOption C, Database of the AWS infrastructure, is a shared responsibility between AWS and the customer, depending on the type of database service used. For example, with Amazon RDS, AWS manages the infrastructure and security of the database, while the customer is responsible for managing the database schema, data, and access controls.\nOption D, Networking of the AWS infrastructure, is also a shared responsibility. AWS is responsible for the security and management of the network infrastructure, including firewalls, load balancers, and routing, while the customer is responsible for configuring and managing their own network and security groups.\n\n"
}, {
  "id" : 343,
  "question" : "To make programmatic calls to AWS, a user was provided an access key ID and secret access key.\nHowever, the user has now forgotten the shared credentials and cannot make the required programmatic calls. How can an access key ID and secret access key be provided to the user?\n",
  "answers" : [ {
    "id" : "c6777ecd7b274e39915d10342fad2dd1",
    "option" : "Use the â€œForgot Passwordâ€ Option",
    "isCorrect" : "false"
  }, {
    "id" : "3fd2247f96ea40919ce1e288d17fc80b",
    "option" : "Use â€œCreate New Access Keyâ€ by logging in to AWS Management Console as the root user.",
    "isCorrect" : "true"
  }, {
    "id" : "34fdd1a918a6431f8f0129a343cc3f7e",
    "option" : "Credentials can not be generated",
    "isCorrect" : "false"
  }, {
    "id" : "24b8d921748f4d5bba7c76b69236dfd8",
    "option" : "Raise a ticket with AWS Support.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is INCORRECT.\nThis is an invalid option.\nOption B is CORRECT.\nOption C is INCORRECT.\nThis is an incorrect option.\nWe can create a new access key by logging in to Management Console as a root user.\nOption D is INCORRECT.\nThis is an incorrect option.\nWe can create a new access key by logging in to Management Console as a root user.\nReference:\nhttps://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html\n\nThe correct answer is B. Use â€œCreate New Access Keyâ€ by logging in to AWS Management Console as the root user.\nExplanation:\nWhen a user is provided with an access key ID and secret access key, it is their responsibility to keep these credentials secure. In case the user forgets the shared credentials, they can create new access keys by logging in to the AWS Management Console as the root user.\nOption A - Use the â€œForgot Passwordâ€ Option: This option is not applicable in this scenario as the user has forgotten their access key ID and secret access key, not their password.\nOption B - Use â€œCreate New Access Keyâ€ by logging in to AWS Management Console as the root user: This option is the correct solution to the problem. The root user can create new access keys for the user in question.\nOption C - Credentials can not be generated: This option is incorrect. As mentioned above, new access keys can be generated.\nOption D - Raise a ticket with AWS Support: This option is not required as the user can resolve the issue by creating new access keys themselves. It is always recommended to contact AWS Support if there is a critical issue or if the user is unable to resolve the issue on their own.\nIn summary, if a user forgets their access key ID and secret access key, they can create new access keys by logging in to the AWS Management Console as the root user.\n\n"
}, {
  "id" : 344,
  "question" : "To enable an application on an EC2 instance to perform some actions, the developer requires to grant access to the application for a few AWS resources.\nThe developer plans to provide his credential to the instance.\nHowever, as the developer's credentials are long-term, the developer is looking for an alternate to reduce the security risk. What can the developer do in this scenario to enable applications on EC2 to get access to the required AWS resources temporarily?\n",
  "answers" : [ {
    "id" : "76dd9ed5129244a4a2e62e3a0a77c21f",
    "option" : "Use â€œIAM Rolesâ€",
    "isCorrect" : "true"
  }, {
    "id" : "6ea71a863ff146b28973a3963fdb06b7",
    "option" : "Use â€œIAM Groupâ€",
    "isCorrect" : "false"
  }, {
    "id" : "6402d318e7bf4e928e3e1a65e73688b6",
    "option" : "Use â€œIAM Tagsâ€",
    "isCorrect" : "false"
  }, {
    "id" : "657e400d07bb4b84a217d0e0b0a24274",
    "option" : "There is no alternate way. A developer needs to give his credentials and revoke access when the required action is done.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nIAM roles facilitate access delegation to services/users that in general do not have access to AWS resources of your organization.\nUsers could assume the role (IAM Users) and/or services (AWS services) for getting temporary security credentials.\nThis can then be used to perform the required actions.\nAn IAM group is a service to grant/revoke/manage permissions on a collection of IAM users.\nIAM tags add custom attributes to IAM users or roles.\nIAM tags use key-value pairs.\nOption A is CORRECT as the IAM role can enable applications on EC2 to get access to the required AWS resources temporarily.\nOption B is INCORRECT because the IAM group is a collection of IAM Users and helps in access management.\nOption C is INCORRECT because IAM tags are simply â€œlabelsâ€ that add custom attributes to the users/roles.\nOption D is INCORRECT because we can use IAM Roles in the scenario.\nReference:\nhttps://aws.amazon.com/iam/features/manage-roles/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_groups.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_tags.html\n\nThe correct answer to this question is A. Use \"IAM Roles.\"\nAWS Identity and Access Management (IAM) is a service that enables you to control access to AWS resources. IAM provides a variety of features, such as users, groups, roles, and policies, that allow you to manage permissions to AWS resources.\nIAM roles are a secure way to grant permissions to entities that you trust. Roles are temporary credentials that are generated dynamically by AWS, and they allow an EC2 instance to access AWS services and resources without the need for long-term security credentials, such as access keys. IAM roles are assigned to an instance when it's launched, and the permissions associated with the role can be modified over time, without the need for manual updates to the instance.\nWhen a developer creates an IAM role, they can specify the permissions that the role will have, and then associate the role with the EC2 instance. The developer can then provide the application on the EC2 instance with the temporary security credentials that are associated with the role, allowing the application to perform the necessary actions on the AWS resources.\nUsing IAM roles provides several benefits:\nImproved security - IAM roles reduce the risk of security breaches by eliminating the need to store long-term security credentials, such as access keys, on the instance. Simplified management - IAM roles simplify the management of permissions by allowing you to modify permissions associated with a role instead of manually updating permissions on each instance. Reduced human error - IAM roles reduce the risk of human error by automating the process of granting and revoking permissions.\nIn conclusion, the best way for the developer to enable applications on EC2 to get access to the required AWS resources temporarily is to use IAM roles. IAM roles provide temporary credentials that are associated with a set of permissions, and they eliminate the need for long-term security credentials. This improves security, simplifies management, and reduces the risk of human error.\n\n"
}, {
  "id" : 345,
  "question" : "An application requires access to a database to retrieve certain information and this action would require the developer to hard-code the credentials. Hard-coding the credentials is not a best practice.\nHe can securely store encrypted credentials and retrieve them when required, eliminating the need of hard-coding credentials in the application.\nWhich AWS service would you suggest to the developer?\n",
  "answers" : [ {
    "id" : "d6731f6a955942c090e02236f475195f",
    "option" : "AWS Secrets Manager",
    "isCorrect" : "true"
  }, {
    "id" : "d7a1d1be5c5242c2896e53c981724f0e",
    "option" : "AWS Encryption SDK",
    "isCorrect" : "false"
  }, {
    "id" : "99ac46d5938c4e04b0bc335dde2d2e1f",
    "option" : "AWS Security Hub",
    "isCorrect" : "false"
  }, {
    "id" : "4edeb41963cd4e6e89d6fa898d0a3fe8",
    "option" : "AWS Artifact.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nAWS Secrets Manager helps in securely storing encrypted credentials and ensures retrieval when required.\nThe use of AWS Secrets Manager eliminates the need for hard-coding credentials in the application.\nAWS Encryption SDK is the encryption library that makes client-side encryption best-practice easier.\nThe encryption libraries facilitate cryptographic services and do not require AWS or any AWS service.\nAWS Security Hub gives you a comprehensive view of your high-priority security alerts and security posture across your AWS accounts.\nAWS Artifact is a central resource for all the information pertaining to compliance.\nAWS artifact provides on-demand access to compliance reports at no additional cost.\nOption A is CORRECT as AWS secrets Manager is an easy way to store encrypted credentials and perform on-demand retrieval safely.\nOption B is INCORRECT because AWS Encryption SDK does not facilitate storing and retrieving the credentials but makes implementation of the client-side encryption best practices easier.\nOption C is INCORRECT because AWS Security Hub facilitates the view of high-priority security alerts and provides a view of the security landscape across AWS accounts.\nOption D is INCORRECT as AWS artifacts does not help with the credentials storing and retrieval, but facilitates information pertaining to compliance centrally.\nReference:\nhttps://docs.aws.amazon.com/secretsmanager/\nhttps://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/introduction.html\nhttps://aws.amazon.com/security-hub/\nhttps://docs.aws.amazon.com/artifact/\n\nThe AWS service that would be most suitable for securely storing and retrieving encrypted credentials is AWS Secrets Manager.\nAWS Secrets Manager is a fully managed service that enables you to store and retrieve secrets such as database credentials, API keys, and other sensitive data. With AWS Secrets Manager, you can eliminate the need to hard-code secrets in your applications, and you can easily manage and rotate secrets without any downtime.\nWhen you use AWS Secrets Manager, you can store the secrets as key-value pairs, and you can use AWS Identity and Access Management (IAM) to control access to these secrets. You can also use AWS Key Management Service (KMS) to encrypt the secrets and ensure that they are only accessible to authorized users.\nTo use AWS Secrets Manager, you would need to create a secret, which can be done using the AWS Management Console, AWS CLI, or AWS SDKs. You would then configure your application to retrieve the secret when it needs to access the database or other resource.\nOverall, using AWS Secrets Manager would be the most secure and scalable solution for storing and retrieving sensitive information such as database credentials. It provides a central location for managing secrets, eliminates the need for hard-coded credentials, and enables you to easily rotate and manage secrets without any downtime.\n\n"
}, {
  "id" : 346,
  "question" : "When provisioning a security certificate from AWS Certificate Manager (ACM), which of the following statements is true? Choose TWO.\n",
  "answers" : [ {
    "id" : "fb24928ba3a04f59a08266724a3306df",
    "option" : "ACM-issued security certificate cannot be applied to an Application load balancer.",
    "isCorrect" : "false"
  }, {
    "id" : "29a4f65fba7847029c845125aeda7a71",
    "option" : "To verify a security certificate, a CNAME record would need to be created.",
    "isCorrect" : "true"
  }, {
    "id" : "5695b352ac6344038ef0cb815ffd9bca",
    "option" : "Third-party security certificates cannot be applied to AWS resources.",
    "isCorrect" : "false"
  }, {
    "id" : "597c81d43172420cb2f35746d7856025",
    "option" : "To verify a security certificate, the administrator would need to acknowledge a verification email sent to an address of their choice.",
    "isCorrect" : "true"
  }, {
    "id" : "04f087c6b2a14d5a89d3567fa1409da9",
    "option" : "A security certificate issued in ACM can only be applied to one AWS resource.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B, D.\nThere are two ways to validate and verify the issuance of a security certificate in AWS Certificate Manager.\nThese are creating a CNAME record in the hosted zone of the domain or email confirmation sent to the requester's email address.\nhttps://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-dns.html\nhttps://docs.aws.amazon.com/acm/latest/userguide/gs-acm-validate-email.html\nOption A is INCORRECT because it is possible to apply ACM-issued security certificates on the Application load balancer.\nOption C is INCORRECT because AWS customers can upload third-party security certificates into ACM and manage and apply them to AWS resources.\nOption E is INCORRECT because security certificates issued in ACM can be applied to multiple AWS resources.\n\nSure! Let's break down each option and identify the correct answers.\nA. ACM-issued security certificate cannot be applied to an Application load balancer. This statement is not true. ACM-issued security certificates can be applied to various AWS resources, including Application Load Balancers.\nB. To verify a security certificate, a CNAME record would need to be created. This statement is true. To verify a security certificate, you must create a CNAME record in your domain's DNS configuration that points to a unique URL provided by AWS.\nC. Third-party security certificates cannot be applied to AWS resources. This statement is not true. Third-party security certificates can be applied to AWS resources, including Elastic Load Balancers and CloudFront distributions.\nD. To verify a security certificate, the administrator would need to acknowledge a verification email sent to an address of their choice. This statement is not true. While some certificate authorities may use email verification, ACM uses DNS verification using a CNAME record.\nE. A security certificate issued in ACM can only be applied to one AWS resource. This statement is also not true. An ACM-issued certificate can be applied to multiple AWS resources, including Elastic Load Balancers and CloudFront distributions.\nTherefore, the correct answers are B and C.\n\n"
}, {
  "id" : 347,
  "question" : "An administrator would like VPCs in three different AWS accounts to access on-premise resources via a VPN connection terminating on a Transit Gateway.\nEach of the VPCs is in distinct AWS regions.\nHow can this be achieved?\n",
  "answers" : [ {
    "id" : "a158a599c0a5414ba8698e2c336ec2a7",
    "option" : "Use AWS Resource Access Manager (RAM) to share the Transit Gateway resource.",
    "isCorrect" : "true"
  }, {
    "id" : "2ef9e594ecf343bba573590f840febed",
    "option" : "Configure a Virtual Private Gateway (VGW) for each VPC and then extend the VPN tunnels to them.",
    "isCorrect" : "false"
  }, {
    "id" : "4d48788d5169469b986d77cc2d99541e",
    "option" : "Create VPC attachments from each of the VPCs to the Transit Gateway.",
    "isCorrect" : "false"
  }, {
    "id" : "cb1b35efb5d4422498cfc8393d94a88e",
    "option" : "Configure VPC peering connections between the VPCs and then route traffic from on-premise through the VPN to the Transit Gateway and then to each VPC peer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nAWS Resource Access Manager (AWS RAM) allows AWS customers to share resources between multiple AWS accounts.\nIn this scenario, it is possible to share the access to the Transit Gateway resource with the three AWS accounts, even if the VPCs are in distinct AWS regions.\nhttps://docs.aws.amazon.com/ram/latest/userguide/what-is.html\nOption B is INCORRECT because it is not possible to extend the VPN tunnels.\nIn the scenario, the VPN tunnels terminate on the Transit Gateway in one of the AWS accounts.\nOption C is INCORRECT because it cannot create VPC attachments without sharing the Transit Gateway resource.\nOption D is INCORRECT because it has transitive routing connotations.\nThis is not permissible in the AWS environment.\n\nTo enable VPCs in different AWS accounts to access on-premise resources through a VPN connection terminating on a Transit Gateway, you can follow the below steps:\nStep 1: Create a Transit Gateway First, create a Transit Gateway that will act as the hub for all your VPCs. Transit Gateway allows you to connect multiple VPCs and VPNs together and provide a centralized point to manage traffic.\nStep 2: Configure VPN Connection Next, set up a VPN connection between your on-premise network and the Transit Gateway. You can create a VPN connection using AWS VPN CloudHub or AWS VPN Gateway, depending on the number of VPN connections required.\nStep 3: Create VPC Attachments After setting up the VPN connection, create VPC attachments for each VPC in your three different AWS accounts. A VPC attachment is a logical connection between a VPC and the Transit Gateway. Each VPC attachment will have a unique identifier that you can use to configure routing.\nStep 4: Configure Route Tables To enable traffic flow between the VPCs and on-premise resources, configure the route tables in each VPC attachment to send traffic to the Transit Gateway. This will ensure that traffic from each VPC flows through the Transit Gateway and then to the on-premise network.\nStep 5: Enable Resource Sharing Finally, to allow VPCs in different AWS accounts to access the Transit Gateway, use AWS Resource Access Manager (RAM) to share the Transit Gateway resource with the other accounts. This will allow the VPCs in each account to create attachments to the Transit Gateway and enable traffic flow.\nSo the correct answer for this question would be option A, \"Use AWS Resource Access Manager (RAM) to share the Transit Gateway resource.\"\n\n"
}, {
  "id" : 348,
  "question" : "An administrator receives an alert and detailed report regarding credit card information that has been erroneously uploaded by a user into one of the S3 buckets during an online questionnaire exercise for a survey.\nWhich AWS service provided this detection and report?\n",
  "answers" : [ {
    "id" : "113422748a2c47d68f31a52ae89f0919",
    "option" : "Amazon Inspector",
    "isCorrect" : "false"
  }, {
    "id" : "2a2fedab8a9948c8a88bee9dcc6e2074",
    "option" : "Amazon EventBridge",
    "isCorrect" : "false"
  }, {
    "id" : "18fa41c043ec415cb528eb1b954fa556",
    "option" : "Amazon Detective",
    "isCorrect" : "false"
  }, {
    "id" : "1ae91c998c2b4c2caa332b925b0aba71",
    "option" : "Amazon Macie.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Correct Answer: D.\nAmazon Macie is a fully managed AWS service that provides data security and privacy using machine learning algorithms, artificial intelligence and pattern matching.\nThese mechanisms detect, discover, monitor, report and protect sensitive data stored in Amazon Simple Storage Service (Amazon S3)\nMacie can detect and alert sensitive data, such as bank credit card information.\nhttps://docs.aws.amazon.com/macie/latest/user/what-is-macie.html\nOption A is INCORRECT because Amazon Inspector does not assess actual data stored in S3\nIt primarily assesses applications for exposure and vulnerability.\nOption B is INCORRECT because Amazon EventBridge does not perform the function of detecting sensitive data.\nOption C is INCORRECT because primarily relevant in establishing the root cause of security incidencies or suspicious activities within the AWS environment.\n\nThe AWS service that is likely to have provided the detection and report for credit card information erroneously uploaded into an S3 bucket is Amazon Macie.\nAmazon Macie is a managed service that uses machine learning and pattern matching to automatically discover, classify, and protect sensitive data in AWS. It provides a comprehensive view of data access activity for security and compliance purposes, and it can detect and alert on anomalous activity related to sensitive data, such as unauthorized access, unusual data access patterns, and sensitive data being moved or deleted.\nIn this scenario, the credit card information uploaded into the S3 bucket would likely be classified as sensitive data by Amazon Macie, and it would have triggered an alert and a detailed report to the administrator based on its detection capabilities. Amazon Macie provides several pre-configured policies that detect common types of sensitive data, including credit card numbers, and it can also be configured to create custom policies to meet specific needs.\nAmazon Inspector is a service that analyzes the behavior of AWS resources and applications to identify potential security issues. It does not have specific capabilities to detect sensitive data in S3 buckets.\nAmazon EventBridge is a serverless event bus service that makes it easy to build event-driven applications at scale. It does not have specific capabilities to detect sensitive data in S3 buckets.\nAmazon Detective is a service that analyzes and visualizes data from AWS resources and applications to identify the root cause of security issues. It does not have specific capabilities to detect sensitive data in S3 buckets.\n\n"
}, {
  "id" : 349,
  "question" : "During an audit process, an organization is advised by the audit committee to centrally manage all the VPC security groups and WAF rules across their AWS environment.\nGiven that the organization has multiple AWS accounts, how can this be achieved?\n",
  "answers" : [ {
    "id" : "ae3a165aee3d468ab5d9e529c6027dd6",
    "option" : "AWS Identity &amp; Access Management (IAM)",
    "isCorrect" : "false"
  }, {
    "id" : "1313716ce77f4085be6d14610742e1f3",
    "option" : "AWS Firewall Manager",
    "isCorrect" : "true"
  }, {
    "id" : "695b1b0c177e41928e05cec08f676a26",
    "option" : "Amazon Cloud Directory",
    "isCorrect" : "false"
  }, {
    "id" : "6cac819b0d034f5ab55dce2bda43a608",
    "option" : "AWS Security Hub.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nAWS Firewall Manager makes it possible to manage VPC security groups, AWS Shield Advanced and WAF rules on one platform even across multiple AWS accounts.\nhttps://docs.aws.amazon.com/waf/latest/developerguide/fms-chapter.html\nOption A is INCORRECT because AWS Identity &amp; Access Management (IAM) does not allow for the management of VPC security groups or WAF rules.\nOption C is INCORRECT because Amazon Cloud Directory is a repository for developer objects.\nThe service does not have the functionality to centrally manage all the VPC security groups or WAF rules in the AWS environment.\nOption D is INCORRECT because AWS Security Hub is a full-view, single-look, comprehensive depiction of the security state of the customer's AWS environment.\nThe service collates security data across AWS accounts and facilitates the analysis of data security patterns.\nIt identifies the highest priority security areas in the customer's AWS environment.\n\nThe correct answer to the question is B. AWS Firewall Manager.\nExplanation:\nAWS Firewall Manager is a security management service that allows central management of firewall rules and policies across multiple AWS accounts and resources. Firewall Manager enables administrators to configure and manage AWS WAF rules and security group rules across multiple VPCs in different AWS accounts.\nTo achieve centralized management of VPC security groups and WAF rules across multiple AWS accounts using Firewall Manager, an administrator can create an AWS Firewall Manager policy. A policy is a set of rules that specify the desired configuration for security groups and WAF rules in a specific AWS environment.\nOnce the policy is created, it can be applied to multiple accounts or specific resources such as VPCs. The Firewall Manager policy ensures that the security groups and WAF rules are compliant with the organization's security policies and standards.\nAWS Identity & Access Management (IAM) is a service that manages user access to AWS resources. It is not designed for managing VPC security groups or WAF rules across multiple AWS accounts.\nAmazon Cloud Directory is a managed service for building directories for applications. It is not designed for managing VPC security groups or WAF rules across multiple AWS accounts.\nAWS Security Hub is a security service that provides a comprehensive view of security alerts and compliance status across an organization's AWS accounts. It is not designed for managing VPC security groups or WAF rules across multiple AWS accounts.\n\n"
}, {
  "id" : 350,
  "question" : "Which of the following statements accurately describe a function of AWS Secrets Manager? [Select Two]\n",
  "answers" : [ {
    "id" : "33129de1f17c41e2af38cc6b316db388",
    "option" : "Encrypts authentication information in code, ensuring that it is unreadable, that is, not in plain-text.",
    "isCorrect" : "false"
  }, {
    "id" : "550f684abbfa476c8ca02d0b5df789ba",
    "option" : "Replaces the need to hardcode authentication credentials in code.",
    "isCorrect" : "true"
  }, {
    "id" : "54469e5c7c06432da8eab7894f22be29",
    "option" : "Makes it possible to include an API call in code that retrieves authentication information from a central repository.",
    "isCorrect" : "true"
  }, {
    "id" : "4dbfdf7fe078474287c7e5bb8632156f",
    "option" : "Automatically rotates and updates the code in the application build, ensuring that repositories are kept up to date.",
    "isCorrect" : "false"
  }, {
    "id" : "888a969e61a0412c843d97eac574df12",
    "option" : "Facilitates the embedding of authentication information in code during runtime.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B, C.\nAWS Secrets Manager allows users to replace authentication information in code with an API call to Secrets Manager.\nThis API call then retrieves the secret programmatically.\nThis safeguards the secret from being compromised since the secret is removed from the code.\nAWS Secrets Manager automatically rotates the secret in accordance with specified schedules which allows the implementation of more secure short-term secrets.\nThese, in turn, reduce the risk of authentication information in code being compromised.\nhttps://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html\nOption A is INCORRECT because AWS Secrets Manager does not encrypt authentication information whilst it is in the code.\nOption D is INCORRECT because AWS Secrets Manager does not automatically rotate or update the application code.\nRather, it automatically rotates the secret in accordance with specified schedules.\nOption E is INCORRECT because AWS Secrets Manager does not facilitate embedding authentication information in code during runtime.\nDevelopers do not need to hard-code authentication information in code.\n\nAWS Secrets Manager is a fully managed service provided by Amazon Web Services (AWS) that helps you protect secrets such as database credentials, API keys, and other sensitive information. It enables you to easily store, distribute, and manage secrets throughout their lifecycle. Secrets Manager provides a secure and scalable solution to manage secrets, reducing the risk of unauthorized access to sensitive data.\nThe correct statements that describe the function of AWS Secrets Manager are:\nA. Encrypts authentication information in code, ensuring that it is unreadable, that is, not in plain-text.\nThis statement is accurate because AWS Secrets Manager encrypts the secrets using the industry-standard AES-256 encryption algorithm, ensuring that the data is secure both in transit and at rest. The encryption ensures that the authentication information is unreadable and inaccessible by unauthorized users, providing an added layer of security.\nB. Replaces the need to hardcode authentication credentials in code.\nThis statement is also accurate because AWS Secrets Manager eliminates the need to hardcode authentication credentials in code, which can be a security risk. Instead, the secrets are stored securely in the AWS Secrets Manager, which can be accessed by authorized users and applications through API calls.\nC. Makes it possible to include an API call in code that retrieves authentication information from a central repository is not a correct statement.\nAlthough AWS Secrets Manager enables applications to access secrets using API calls, it does not retrieve authentication information from a central repository. Instead, the secrets are stored securely in the AWS Secrets Manager, which can be accessed by authorized users and applications through API calls.\nD. Automatically rotates and updates the code in the application build, ensuring that repositories are kept up to date is not a correct statement.\nAWS Secrets Manager does support automatic rotation of secrets, but it does not automatically update the code in the application build. Instead, applications need to be updated manually to use the new secrets after rotation.\nE. Facilitates the embedding of authentication information in code during runtime is not a correct statement.\nEmbedding authentication information in code during runtime can be a security risk, and AWS Secrets Manager is designed to eliminate this practice. Instead, secrets are stored securely in the AWS Secrets Manager, and applications can access them through API calls.\n\n"
}, {
  "id" : 351,
  "question" : "A client has decided to go for a MySQL RDS database on the AWS cloud based on its Scalability &amp; High Availability features.\nWhen he does so, what role does he play in making the database secure? (Select TWO.)\n",
  "answers" : [ {
    "id" : "8dfa0920e67a42bcb516449640f6faa7",
    "option" : "He can restrict RDS access to the database by using a Security Group.",
    "isCorrect" : "true"
  }, {
    "id" : "00aaf12fe2c542a6a7e746a7343dd705",
    "option" : "He can provide the most recent updates of his database software installed on the EC2 Instance for preventing Security attacks.",
    "isCorrect" : "false"
  }, {
    "id" : "32e5ffb8144c4af8baa33e66553b49dd",
    "option" : "He can provide the most recent versions of his Operating System on the EC2 instance for preventing Security attacks.",
    "isCorrect" : "false"
  }, {
    "id" : "d3895823e46a4c8cac43cc7c6f54fa4e",
    "option" : "He can Encrypt database data at rest by using EBS volume storage encryption.",
    "isCorrect" : "false"
  }, {
    "id" : "9b75d696daf446069dba30977eeb0b85",
    "option" : "He can plan for backup &amp; recovery strategies for data that may be lost.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answers: A and E.\nRDS is a managed service (Database As a Service) that allows the user to ease administrative tasks like Database software updates and Operating System patch updates.\nThus it helps the user concentrate more on the design/development of the database.\nThe instance class types(EC2 VM's) that support the database instance configuration are abstracted from the user since it is provided as a service.\nAll in all RDS offers automatic DB installation process, storage disk provisioning, Database upgrades, Security patches and backups of SQL Server databases.\nIn this scenario, we would like to know the tasks that a user can perform as a part of the Shared responsibility model for security in an RDS database.\nOption A is CORRECT.\nSecurity Groups can be used to control Ingress / Egress traffic flowing in &amp; out of an RDS database instance.\nA user can configure an Ingress security group rule for restricting traffic to certain IP addresses of an RDS port such as 3306.\nOption B is incorrect.\nDatabase instances are abstracted from the user &amp; database software updates are managed by the service provider(AWS).\nOption C is incorrect.\nSince the instance types are abstracted from the user, the OS security patches are also controlled by the service provider (AWS).\nOption D is incorrect.\nData encryption at rest is possible in an RDS instance.\nHowever, using an EBS volume will not be possible since that will require much more control to the instance hosting the Database to mount an EBS volume.\nRDS automatically manages storage disk provisioning.\nIt allows a user to select the storage type during database creation/modification time from the following types: General purpose SSD, Provisioned IOPS, Magnetic.\nOption E is CORRECT.\nAlthough RDS provides an automated backup facility, the user needs to enable it &amp; plan for the window time where the backup process can be initiated.\nRDS also provides the user with a facility to do manual backups (Point in time DB snapshots) which can be planned.\nReferences:\nhttps://aws.amazon.com/blogs/database/common-administrator-responsibilities-on-amazon-rds-and-aurora-for-postgresql-databases/\nhttps://serverguy.com/comparison/pros-cons-rds-vs-ec2-mysql-aws/\nhttps://www.percona.com/blog/2018/05/08/how-to-enable-amazon-rds-remote-access/\n\nAs a client, you play a critical role in ensuring the security of your MySQL RDS database on the AWS cloud. The following are the two roles you play in making the database secure:\nA. Restrict RDS access to the database by using a Security Group: A security group acts as a virtual firewall that controls inbound and outbound traffic for your RDS instance. You can use security groups to restrict access to your RDS instance by IP addresses, ports, and protocols. By using a security group, you can ensure that only authorized users have access to your RDS instance. Therefore, it is essential to configure a security group that only permits necessary traffic to and from the RDS instance.\nD. Encrypt database data at rest by using EBS volume storage encryption: Encrypting data at rest helps prevent unauthorized access and makes it harder for attackers to steal data. Amazon RDS supports encryption of data at rest by using AWS Key Management Service (KMS) encryption keys. You can enable encryption when creating a new RDS instance or modify an existing instance to enable encryption. You can also use the Amazon RDS automated backup feature to encrypt backups of your RDS instance. EBS volume storage encryption provides an additional layer of encryption for data stored on the EBS volumes attached to the RDS instance.\nTherefore, the client can make the database secure by restricting RDS access to the database by using a Security Group and encrypting database data at rest by using EBS volume storage encryption. It's worth noting that providing the most recent updates of database software and Operating System on the EC2 instance is the responsibility of AWS, and they ensure that the database software and Operating System are up-to-date with the latest security patches. The client should plan for backup and recovery strategies for data that may be lost, but it does not have any direct impact on the security of the database.\n\n"
}, {
  "id" : 352,
  "question" : "I have a web application with the following VPC configuration as shown below.\nSubnet 1 (172.31.0.0/20) hosts a Web server(on a EC2 instance ) &amp; Subnet 2(172.31.16.0/20) hosts a Database Server (on a EC2 instance)\nWhich of the statements mentioned below does NOT define good practice from a Security &amp; Compliance perspective &amp; which may be modified in the Diagram? Select TWO.\n\n",
  "answers" : [ {
    "id" : "edfbf68417f64818bae82a20f68c2848",
    "option" : "The Subnet hosting the Web Serverhaving a route to the Internet Gateway is a good practice.",
    "isCorrect" : "false"
  }, {
    "id" : "4e984e578ffa4f52bdb49c886ebc55d9",
    "option" : "Hosting the Web Server on a Default subnet is a good practice",
    "isCorrect" : "false"
  }, {
    "id" : "d55f6c63abd04d37adc09443f3785a4d",
    "option" : "The Subnet hosting the Database Server having a route to a NAT Gateway is a good practice.",
    "isCorrect" : "false"
  }, {
    "id" : "b855dfee9dbf4513974789384cb7a2ac",
    "option" : "Hosting the Database Server on a Default subnet is a good practice",
    "isCorrect" : "true"
  }, {
    "id" : "f457d05cb48d427390cd3842398a5427",
    "option" : "The Subnet hosting the Database Server having a route to the Internet Gateway is a good practice.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answers: D and E.\nThe figure shown above shows that both the Database Server &amp; the Web Server have routes to the internet gateway using the Main Route table.\nDatabase servers are usually hosted in a non-DMZ zone which offers more protection from outside attack.\nAlso, since web servers are front-ending the application requests, it should not be necessary to expose the backend database directly to the internet.\nOption A is incorrect.\nSince the Web Server front-ends application requests, it should have a route to the Internet Gateway for accessing it from the Internet.\nOption B is incorrect.\nA default subnet provides both a Public Ipv4 &amp; a Private Ipv4 address.\nThe Web Server will require either a Public IP or Elastic IP to be accessible from the Internet.\nOption C is incorrect.\nA NAT gateway provides an Egress only access to the internet by allowing the DataBase server to connect to the internet to update software patches.\nThis is a good practice for disallowing direct Ingress access to the DataBase Server from the internet.\nThe diagram can be modified to reflect this change.\nOption D is CORRECT.\nSince the database does not require a Public IP, it is suitable to host the DB Server on a non-default subnet that does not allocate a Public IP by default.\nOption E is CORRECT.\nThe database server should not have a route to the Internet Gateway.\nDiagram:\nReferences:\nhttps://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\nhttps://www.testpreptraining.com/tutorial/aws-cloud-practitioner/aws-cloud-security/\n\n"
}, {
  "id" : 353,
  "question" : "I have a Mobile App that needs to access AWS resources like S3, DynamoDB.\nWhat is the best way to allow users of the mobile app access to these AWS resources?\n",
  "answers" : [ {
    "id" : "55fc15ece658404993c4ccbe80ba531d",
    "option" : "Keep the Security Credentialsassociated with the AWS resource access within the Mobile App",
    "isCorrect" : "false"
  }, {
    "id" : "81445ae68cd54083a568666a03832c67",
    "option" : "Use Security Token Service (STS) with Identity Federation thatwill allow an User access to resources within a session",
    "isCorrect" : "true"
  }, {
    "id" : "bdb3a81b1413459bbb6c2b7ce4ea9569",
    "option" : "Create Users &amp; Groups within IAM and assign IAM policies for accessing the resources",
    "isCorrect" : "false"
  }, {
    "id" : "48d3a187d96e4f81b7b92e80f2f690e8",
    "option" : "Have the mobile app connect toanother web application running on an EC2 instance that can assume a role for accessing the AWS resources.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nA mobile app that becomes popular can have a large user base.\nThe best way to provide access to AWS resources in this scenario will be to use Federated Identity access using External Identity Providers(IdP) like Amazon, Facebook, Google etc.\nThe mobile app can establish trust with these well-known IdPs and take advantage of the authentication mechanisms to validate user identity.\nAs shown in the figure below, the mobile app uses Amazon as the external IdP for accepting user credentials &amp; authenticating him.\nUsing Cognito &amp; the Security Token Service, the Mobile App then gets temporary Security Credentials for accessing the AWS resources required by the app.\nThe role associated with the STS token and its assigned policies determine what can be accessed.\nOption A is incorrect.\nDistributing long-term AWS Security Credentials with external applications is not recommended since the credentials may be compromised resulting in security breaches.\nOption B is CORRECT.\nThe STS token will contain temporary credentials with a Role indicating the access level that a mobile app user can have.\nThe security credentials will be valid for a specific user session only.\nOption C is incorrect.\nCreating an ever-growing set of Users within AWS IAM and assigning them permissions for accessing AWS resources will be impractical.\nOption D is incorrect.\nAlthough it is a viable option to connect to an EC2 instance running a similar web application, it will duplicate effort on the developer's part.\nReferences:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_oidc_cognito.html\nhttps://www.testpreptraining.com/tutorial/aws-cloud-practitioner/aws-access-management/\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/introduction.html\n\n\nThe best way to allow users of a mobile app to access AWS resources such as S3 and DynamoDB depends on several factors, such as security, scalability, and ease of management. However, there are several ways to accomplish this task.\nA. Keeping the Security Credentials within the Mobile App This option is not recommended as it poses a significant security risk. If the security credentials, such as AWS access keys or IAM user credentials, are stored in the mobile app, they can be easily compromised by malicious attackers. Once attackers gain access to the credentials, they can use them to access sensitive data or resources.\nB. Using Security Token Service (STS) with Identity Federation This option provides a more secure way of accessing AWS resources. STS is a web service that provides temporary security credentials for accessing AWS resources. Identity federation allows users to access AWS resources using their existing identity and credentials, such as Facebook, Google, or Amazon accounts.\nBy using STS with identity federation, the mobile app can obtain temporary security credentials that allow it to access the desired AWS resources within a session. Once the session expires, the credentials are automatically revoked, providing an additional layer of security. This option also simplifies management, as the app developer does not need to create and manage AWS credentials for each user.\nC. Creating Users & Groups within IAM This option involves creating IAM users and groups, assigning IAM policies to the users or groups, and providing the necessary credentials to the mobile app. While this option provides fine-grained access control, it can be challenging to manage, especially for a large number of users. Additionally, this option requires the app developer to create and manage AWS credentials for each user, which can be a time-consuming task.\nD. Connecting the Mobile App to another Web Application running on EC2 This option involves creating a web application that runs on an EC2 instance and assumes an IAM role for accessing the desired AWS resources. The mobile app can then connect to the web application and request the necessary data or resources. This option provides a secure way of accessing AWS resources, as the mobile app does not store any credentials. However, this option can be challenging to implement, especially for complex applications, and requires additional infrastructure and management overhead.\nIn summary, the best way to allow users of a mobile app to access AWS resources such as S3 and DynamoDB is by using Security Token Service (STS) with identity federation. This option provides a secure and scalable way of accessing AWS resources, simplifies management, and eliminates the need for app developers to create and manage AWS credentials for each user.\n\n"
}, {
  "id" : 354,
  "question" : "I have a compliance requirement for my application, stating that unrestricted SSH access to anyEC2 instance needs to be immediately notified to an admin.\nWhich services can I use to achieve the requirement?\n",
  "answers" : [ {
    "id" : "c4dc5f3925e8409eb96f11b3a93d3f8b",
    "option" : "AWS Trusted Advisor, Amazon SNS",
    "isCorrect" : "false"
  }, {
    "id" : "81a867c4886443dbb45321f363bc3849",
    "option" : "AWS Inspector, Amazon SNS",
    "isCorrect" : "false"
  }, {
    "id" : "a0442b2d8bdd41b58affb8bec6224e9b",
    "option" : "AWS Config, Amazon SNS",
    "isCorrect" : "false"
  }, {
    "id" : "89703dd1c2a9497f9cf0efae55e505a1",
    "option" : "Both B &amp; C.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nBoth AWS Inspector &amp; AWS Config can scan EC2 instances, access their network exposure, and then integrate with Amazon SNS to send notifications.\nTrusted Advisor also can check for overly permissive access of EC2 instances.\nStill, the notifications can be performed by monitoring the Trusted Advisor check results with AWS CloudWatch events that can use specific targets like Lambda, SNS etc.\nOption A is incorrect.\nTrusted Advisor results cannot be directly configured with SNS.\nThey need to be monitored using CloudWatch events.\nOption B is incorrect.\nFor the given scenario, both AWS Config &amp; AWS Inspector can be configured to send notifications to SNS when a compliance breach is observed.\nOption C is incorrect.\nThe same explanation is given in Option.\nB.Option D is CORRECT.\nThe Network Reachability rules package recently released for AWS Inspector helps analyze Amazon VPC network configuration to determine whether an EC2 instance can be reached from external networks like the Internet.\nIt does it by analyzing network configurations like Security Groups, NACL's, Route tables etc...The assessment that is run, its security findings can be published to an SNS topic.\nAWS Config's Configuration Streams can be configured with resources like Amazon SNS.\nWithin AWS Config, you can configure Managed rules or Custom rules that can detect compliance violations &amp; use the configuration stream for sending notifications.\nDiagrams:\nReferences:\nhttps://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\nhttps://aws.amazon.com/blogs/security/how-to-remediate-amazon-inspector-security-findings-automatically/\nhttps://aws.amazon.com/blogs/aws/trusted-advisor-console-basic/\nhttps://docs.aws.amazon.com/awssupport/latest/user/cloudwatch-events-ta.html\n\n\nThe correct answer is option D. Both AWS Inspector and AWS Config can be used in combination with Amazon SNS to achieve the compliance requirement of immediate notification to an admin when unrestricted SSH access is granted to any EC2 instance.\nHere is an explanation of each service and how they can be used to meet the requirement:\nAWS Inspector: It is a service that automatically assesses applications for vulnerabilities or deviations from best practices. It provides security assessments that help you test your applications and infrastructure for security issues. You can use AWS Inspector to set up rules packages that detect when SSH access is unrestricted to an EC2 instance. When a violation of the rule package is detected, Inspector generates a finding and sends it to AWS Config. AWS Config: It is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records the configurations of your resources, and provides you with a detailed inventory of the resources in your account, as well as the relationships between them. You can use AWS Config to create a rule that triggers an SNS notification whenever a new EC2 instance is launched with unrestricted SSH access. Amazon SNS: It is a fully managed notification service that lets you send messages or notifications from your applications or services to multiple recipients. You can use SNS to send an email or SMS message to an admin when a violation is detected by Inspector or AWS Config.\nTherefore, using AWS Inspector and AWS Config in combination with Amazon SNS, you can achieve the compliance requirement of immediately notifying an admin when unrestricted SSH access is granted to any EC2 instance.\n\n"
}, {
  "id" : 355,
  "question" : "I enable encryption on an S3 bucket that I have created with the following selections.\nRefer to the figure below. With the KMS encryption selected as (aws/s3), which of the following statement is NOT true?\n\n",
  "answers" : [ {
    "id" : "7db4b3ee66254508aef0970f7008693d",
    "option" : "The KMS key cannot be deleted.",
    "isCorrect" : "false"
  }, {
    "id" : "9d365cc92e554cacb6e4b2c1615507eb",
    "option" : "The KMS key can be rotated both automatically and manually.",
    "isCorrect" : "true"
  }, {
    "id" : "4bd4a355c50b4943942897b666d5fb5a",
    "option" : "The KMS key can be integrated with CloudTrail for auditing purposes.",
    "isCorrect" : "false"
  }, {
    "id" : "ebc726dcee314870943f86a165b173ce",
    "option" : "The KMS key cannot be baked into Custom Roles.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect.\nAWS managed KMS keys cannot be deleted unlike their Customer Managed counterparts.\nOption B is CORRECT.\nAWS managed KMS keys can only be rotated automatically compared to the Customer Managed KMS keys that can be rotated automatically or manually.\nManual rotation of keys provides greater control over the keys &amp; makes it more secure and difficult to compromise.\nOption C is incorrect.\nAWS KMS integrates CloudTrail that will record calls to KMS by various users, roles, and other AWS services.\nAll API calls to KMS are captured as events by CloudTrail that can be logged to destinations like S3 or send them to CloudWatch for analysis.\nOption D is incorrect.\nAWS managed KMS keys cannot be managed, rotated or their policies changed by a user.\nThey can only be viewed within the account.\nCustomer-managed KMS keys, on the other hand, can be fully controlled by a user for maintaining their key policies, IAM policies, Enabling/disabling them, rotating them, etc...\nReferences:\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html\nhttps://youtu.be/SOnJyqwGn1I\n"
}, {
  "id" : 356,
  "question" : "Which AWS service is a machine learning-based tool that analyzes metrics of historical utilization and makes recommendations of compute service(s) to be used for the workload?\n",
  "answers" : [ {
    "id" : "4997551e68c640779edab7fbd25188f1",
    "option" : "AWS Outposts",
    "isCorrect" : "false"
  }, {
    "id" : "0139b8a2976b436a9127e06caff7e9fa",
    "option" : "AWS Well-Architected Tool",
    "isCorrect" : "false"
  }, {
    "id" : "e13250e835404cf5af8b00db7095ea1e",
    "option" : "AWS Management Console",
    "isCorrect" : "false"
  }, {
    "id" : "961890bfc9254acf86903cb9143892e8",
    "option" : "AWS Compute Optimizer.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT because AWS Outpost is a fully managed service that provides a seamless hybrid experience by facilitating the running of AWS services and infrastructure on-premises.\nAWS outpost does not provide recommendations for using the compute services after analyzing the past utilization metrics.\nOption B is INCORRECT as AWS Well-Architected Tool is a tool that provides advice on architecting the workload in the cloud.\nThis tool also enables customers to review their architecture against the best practices.\nOption C is INCORRECT because AWS Management Console is a web-based user interface that helps users to access and manage all the aspects of all the available AWS services.\nThis is a management and governance tool.\nOption D is CORRECT.\nAWS Compute Optimizer is a machine learning-based tool that analyzes metrics of historical utilization and makes recommendations of compute service(s) to be used for the workload.\nReference:\nhttps://aws.amazon.com/outposts/\nhttps://aws.amazon.com/well-architected-tool/\nhttps://aws.amazon.com/console/\nhttps://aws.amazon.com/compute-optimizer/\n\nThe correct answer is D. AWS Compute Optimizer.\nAWS Compute Optimizer is a machine learning-based service that helps in making informed decisions about which compute services to use for your workload based on historical utilization metrics. Compute Optimizer analyzes the resource utilization data of your workloads such as CPU, memory, and network, to identify the optimal AWS compute resources that best match your workload requirements.\nCompute Optimizer provides recommendations for Amazon EC2 instance types, Amazon EC2 Auto Scaling groups, and Amazon ECS task definitions. It considers various factors such as performance, cost optimization, and availability to provide personalized recommendations. You can use these recommendations to optimize the performance and cost of your applications running on AWS.\nCompute Optimizer also provides an API that you can use to access the recommendations programmatically, and integrate with your existing systems.\nIn summary, AWS Compute Optimizer is a machine learning-based tool that analyzes metrics of historical utilization and makes recommendations of compute services to be used for the workload, based on performance, cost optimization, and availability.\n\n"
}, {
  "id" : 357,
  "question" : "Which of the below could be used to perform best practices aligned deployment of popular technologies on AWS, and eventually reduce the time taken for environment build and eventual usage of the environment?\n",
  "answers" : [ {
    "id" : "4a12be97f1a24247b1184c73a9574d0c",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "5b63522a72ce4c4ab0c955471325388c",
    "option" : "AWS OpsWorks",
    "isCorrect" : "false"
  }, {
    "id" : "c1f3e427410b4ac88fd34b1f2c195b16",
    "option" : "AWS Auto deploy",
    "isCorrect" : "false"
  }, {
    "id" : "6b9d9f8af896440d8834e2bef5561fbd",
    "option" : "AWS Quick Starts.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: D.\nOption A is INCORRECT.\nAWS Elastic Beanstalk helps in web applications and services scaling and deployment.\nHowever, we need to provide the code.\nOption B is INCORRECT.\nAimed specifically for chef and puppet, AWS OpsWorks helps facilitate managed instances of Chef and Puppet.\nOption C is INCORRECT.\nAWS Auto deploy is an invalid service.\nOption D is CORRECT.\nAWS Quick Starts: Built by AWS Architects and partners, quick start helps to automate deployments aligned with the best practice.\nCloudformation templates are included along with Quick Start for the automation of the deployment.\nReference:\nhttps://aws.amazon.com/elasticbeanstalk/\nhttps://aws.amazon.com/opsworks/\nhttps://aws.amazon.com/quickstart/\n\nAll of the options listed in the question are AWS services that can help with deployment of popular technologies on AWS. However, the service that is specifically designed for best practice aligned deployment is AWS Quick Starts.\nAWS Quick Starts are pre-built reference architectures designed by AWS Solutions Architects and partners that follow best practices for deploying popular technologies on AWS. These Quick Starts are designed to help reduce the time taken to build and deploy an environment on AWS by providing an automated and repeatable way to deploy the technology. Quick Starts are available for a variety of popular technologies, including databases, analytics, security, machine learning, and more.\nTo use AWS Quick Starts, users can select the Quick Start they want to deploy from the AWS Marketplace or the Quick Start website, and then follow the step-by-step deployment guide provided. The guide includes instructions for configuring the necessary AWS services, such as EC2 instances, RDS databases, and load balancers, as well as setting up security and networking.\nIn contrast, AWS Elastic Beanstalk and AWS OpsWorks are services that provide additional layers of abstraction on top of AWS services to simplify the deployment and management of applications. Elastic Beanstalk is a fully managed service that allows users to quickly deploy and manage web applications on AWS, while OpsWorks is a configuration management service that allows users to automate the deployment and management of applications using Chef or Puppet.\nAWS Auto Deploy, on the other hand, is not a valid AWS service name. It is possible that this was meant to refer to AWS CodeDeploy, which is a service that automates the deployment of applications to EC2 instances, on-premises instances, or serverless Lambda functions. However, CodeDeploy does not provide pre-built reference architectures like Quick Starts do.\n\n"
}, {
  "id" : 358,
  "question" : "A developer working on enhancing few applications in AWS requires an AWS service that can host git-based repositories securely. Which AWS service can the developer use?\n",
  "answers" : [ {
    "id" : "9762353e699944459ae349d5bbed45cf",
    "option" : "AWS CodeCommit",
    "isCorrect" : "true"
  }, {
    "id" : "95327a68aaa4462faf2828de16e1a0a3",
    "option" : "AWS CodeStar",
    "isCorrect" : "false"
  }, {
    "id" : "b45ba1b1d1b24df7b0c684fdbfdb4a2b",
    "option" : "Amazon CodeGuru",
    "isCorrect" : "false"
  }, {
    "id" : "386b595cdf0c4b72a4eb8211d452987c",
    "option" : "AWS CodePipeline.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nAWS CodeCommit helps in hosting git-based repositories securely.\nAWS CodeCommit is a fully managed service and provides source control services.\nOption B is INCORRECT.\nAWS CodeStar is a cloud-based AWS service that helps in swift and quick development and building and deploying applications in AWS.\nOption C is INCORRECT.\nAmazon CodeGuru is an ML-powered development tool that provides code quality improvement recommendations.\nOption D is INCORRECT.\nAWS CodePipeline is a fully managed workflow management tool that facilitates automation of various phases of the release process.\nReference:\nhttps://aws.amazon.com/codecommit/\nhttps://aws.amazon.com/codestar/\nhttps://aws.amazon.com/codeguru/\nhttps://aws.amazon.com/codepipeline/\n\nThe AWS service that the developer can use to host git-based repositories securely is AWS CodeCommit (option A).\nAWS CodeCommit is a fully managed source control service that makes it easy for companies to host secure and highly scalable private Git repositories. It eliminates the need for developers to manage and scale their own infrastructure, allowing them to focus on writing code.\nAWS CodeStar (option B) is a service that provides templates and pre-configured resources to help developers quickly build, test, and deploy applications on AWS. While it integrates with CodeCommit for source control, it is not primarily designed for hosting Git repositories.\nAmazon CodeGuru (option C) is an AI-powered developer tool that provides recommendations for improving code quality and identifying performance bottlenecks. It is not a source control service.\nAWS CodePipeline (option D) is a fully managed continuous delivery service that helps developers automate their software release process. While it integrates with CodeCommit for source control, it is not primarily designed for hosting Git repositories.\nIn summary, AWS CodeCommit is the best option for a developer who needs to host git-based repositories securely.\n\n"
}, {
  "id" : 359,
  "question" : "Which of the below statements are true with regards to Amazon S3 security and access management? Choose 2.\n",
  "answers" : [ {
    "id" : "afac961df8094b60a62c33a7d5a9872c",
    "option" : "Self-created S3 resources are only accessible to the user by default.",
    "isCorrect" : "true"
  }, {
    "id" : "09983179eb864d319febab7ea8227920",
    "option" : "Access Control Lists (ACLs) could be used to grant time bound access using temporary URLs.",
    "isCorrect" : "false"
  }, {
    "id" : "605f319fb4724d558bf2fb5a0eff1bc5",
    "option" : "By default S3 buckets are private, however, objects are public. The object owner needs to change the permissions upon creation of objects to make the objects private.",
    "isCorrect" : "false"
  }, {
    "id" : "b4fce6baaaab41ef998cc93a6732017e",
    "option" : "Amazon Macie can protect data in Amazon EC2.",
    "isCorrect" : "false"
  }, {
    "id" : "2b7ecf52442e4904a988e099769f6e79",
    "option" : "Server-side and client-side encryptions are supported by S3 for data uploads.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answer: A, E.\nOption A is CORRECT.\nUsers, by default, have access only to the S3 resources that they have created.\nOption B is INCORRECT.\nTime-bound access using temporary URLs can be provided using query string authentication.\nOption C is INCORRECT.\nBy default, both S3 buckets and objects are private.\nOption D is INCORRECT.\nAmazon Macie is a tool to protect the data in Amazon S3 instead of EC2.\nOption E is CORRECT.\nS3 supports both server-side and client-side encryptions.\nReference:\nhttps://aws.amazon.com/s3/security/\n\nThe two true statements regarding Amazon S3 security and access management are B and E.\nB. Access Control Lists (ACLs) could be used to grant time-bound access using temporary URLs: Access Control Lists (ACLs) in Amazon S3 are used to manage access to buckets and objects. ACLs allow the bucket owner to grant permissions to other AWS accounts or to the public. Temporary URLs can be generated by the bucket owner using the pre-signed URL feature to grant access to specific objects for a limited time period.\nE. Server-side and client-side encryptions are supported by S3 for data uploads: Amazon S3 provides the option to encrypt data both on the server-side and client-side. Server-side encryption allows S3 to encrypt data before storing it on disks in its data centers, while client-side encryption allows data to be encrypted before being sent to S3. Both types of encryption help to protect sensitive data from unauthorized access.\nA. Self-created S3 resources are only accessible to the user by default: This statement is false. By default, S3 resources are private and only accessible by the resource owner. However, the resource owner can grant permissions to other AWS accounts or to the public using ACLs or bucket policies.\nC. By default S3 buckets are private, however, objects are public. The object owner needs to change the permissions upon creation of objects to make the objects private: This statement is also false. By default, both S3 buckets and objects are private and only accessible by the resource owner. The object owner can choose to grant access to other AWS accounts or to the public using ACLs or bucket policies.\nD. Amazon Macie can protect data in Amazon EC2: This statement is false. Amazon Macie is a security service provided by AWS that helps to discover, classify, and protect sensitive data in Amazon S3. It does not provide protection for data in Amazon EC2. However, Amazon EC2 provides its own security features, such as security groups and network ACLs, to protect instances and data.\n\n"
}, {
  "id" : 360,
  "question" : "An e-commerce company has launched a new application and determined that it needs to perform load distribution for http and https traffic because of the increased traffic during the monthly discount days. Which Load Balancer would be suitable?\n",
  "answers" : [ {
    "id" : "a7f4d16a0a19440eae28639063433d07",
    "option" : "Classic Load Balancer",
    "isCorrect" : "false"
  }, {
    "id" : "47cc1ea0e36a43f29d84d963991a7710",
    "option" : "Legacy Load Balancer",
    "isCorrect" : "false"
  }, {
    "id" : "35fcf7c253334954b005f7aae1752395",
    "option" : "Application Load Balancer",
    "isCorrect" : "true"
  }, {
    "id" : "75f6f71738c54d2b94146210103cbc0e",
    "option" : "Network Load Balancer.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is INCORRECT.\nClassic Load Balancer operates at request and connection level and provides basic load balancing.\nOption B is INCORRECT.\nLegacy Load Balancer is an invalid option.\nOption C is CORRECT.\nApplication Load Balancer is apt for http and https traffic load balancing.\nApplication load balancer operates at layer 7.\nOption D is INCORRECT.\nNetwork Load Balancer is apt for TCP, UDP, TLS traffic load balancing and helps provide extreme performance.\nReference:\nhttps://aws.amazon.com/elasticloadbalancing/#:~:text=Elastic%20Load%20Balancing%20automatically%20distributes,or%20across%20multiple%20Availability%20Zones.\n\nFor an e-commerce company that needs to perform load distribution for http and https traffic due to increased traffic during monthly discount days, the suitable load balancer would be the Application Load Balancer (ALB).\nExplanation:\nClassic Load Balancer (CLB):\nIt is the oldest type of load balancer offered by AWS and is used for distributing traffic evenly across multiple EC2 instances. It operates at the transport layer (Layer 4) of the OSI model and supports only TCP and SSL/TLS protocols. It does not support HTTP/2 or WebSockets. Classic Load Balancer is not suitable for the given scenario as it is an outdated technology and does not provide advanced features like content-based routing.\nLegacy Load Balancer:\nThere is no such thing as a \"Legacy Load Balancer\" in AWS.\nApplication Load Balancer (ALB):\nIt is the recommended load balancer for HTTP/HTTPS traffic and is suitable for distributing traffic to multiple targets, such as EC2 instances, containers, and IP addresses. It operates at the application layer (Layer 7) of the OSI model and supports advanced features such as content-based routing, host- and path-based routing, and WebSocket traffic. The ALB can route traffic based on the content of the request, such as the URL path or headers, which makes it well-suited for use in microservices architectures.\nNetwork Load Balancer (NLB):\nIt is suitable for handling TCP/UDP traffic and is designed to handle millions of requests per second with low latency. NLB operates at the transport layer (Layer 4) of the OSI model and can handle extreme performance requirements. Network Load Balancer is not suitable for the given scenario as it does not support HTTP/HTTPS traffic, and it's an overkill for the requirements mentioned in the question.\nTherefore, the Application Load Balancer (ALB) is the most suitable load balancer for the given scenario. It is designed to handle HTTP/HTTPS traffic and provides advanced features like content-based routing, host- and path-based routing, and WebSocket traffic, making it well-suited for an e-commerce company that needs to handle increased traffic during monthly discount days.\n\n"
}, {
  "id" : 361,
  "question" : "Which of the following statements related to the AWS Global Infrastructure are correct? Select best TWO.\n",
  "answers" : [ {
    "id" : "0f31d0c904a146fe826a0033355ad21f",
    "option" : "For achieving High Availability and Performance, customers usually deploy their applications across multiple AWS Regions",
    "isCorrect" : "false"
  }, {
    "id" : "265bd247fa9448a7b2f59d424da3ab90",
    "option" : "Edge locations can be used todeploy infrastructures like EC2 instances, EBS storage",
    "isCorrect" : "false"
  }, {
    "id" : "415845622ff14608bd1290f79f1912e2",
    "option" : "Availability zones can contain one or more data centers",
    "isCorrect" : "true"
  }, {
    "id" : "d49e19f8d10944cdbe0e6ed24bf64620",
    "option" : "An EC2 instanceâ€™s AMI in a particular region can be copied to another region for using it in that Region.",
    "isCorrect" : "true"
  }, {
    "id" : "2dd48c0b098542ed9573c5a08af7dcf6",
    "option" : "An elastic IP address is allocated to an Availability Zone.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: C and D.\nOption A is incorrect because the first level of redundancy is always Availability Zones when it comes to High Availability &amp; Performance.\nWhen applications are deployed across multiple Availability Zones within the same region, a single Availability Zone's failure will not result in the application being unavailable as the failover mechanism will switch the request of a client to another availability zone.\nWhen requests are routed through a load-balancer, it will manage requests depending on the resource utilization of services within an availability zone.\nThis will ensure the good performance of the application.\nRegional replications are a second level of redundancy where data is replicated between Geographical Regions.\nOne of the reasons could be a Disaster Planning architecture for Business Continuity where a complete Region becomes unavailable &amp; a backup of the data can be maintained in another region for redundancy.\nOption B is incorrect.\nEdge locations are used by services like CloudFront to cache data &amp; reduce latency for end-user access by using those Edge locations as a Global Content Delivery Network (CDN)\nEC2 instances can be used as Origin servers to a CloudFront distribution for serving dynamic content.\nOption C is CORRECT.\nAn Availability Zone is a logical Data Center within a Region.\nEach zone in a Region has redundant and separate power, networking and connectivity to reduce the likelihood of two zones failing simultaneously.\nEach zone could be backed with one or more physical Data Centers, the largest being backed by 5.\nOption D is CORRECT.\nCopying an AMI from one region to another enables you to launch consistent instances based on the same AMI to different Regions.\nThe consistency can be observed from the fact that the AMI's can contain pre-installed software that can be used in different regions.\nFor example, if I have a redundant AMI copy as a backup in another Region, it becomes easy to launch an EC2 instance from that AMI in that Region without re-installing all those software again, thus improving downtime of my applications.\nOption E is incorrect.\nElastic IP (EIP) addresses are static IP addresses that can be created and assigned within an AWS Account rather than an Availability Zone.\nYou can have a maximum of 5 EIP's for an Account.\nIt can be assigned to an EC2 instance.\nAn EIP is most useful for an application hosted on a single EC2 instance with its IP mapped to a domain name.\nWhen the EC2 instance has a public IP &amp; getting restarted, it will be assigned a new public IP making the site unavailable unless it is remapped to the domain.\nAn EIP will help here by retaining the same public IP address at the time of a server restart so that the site does not incur downtime.\nDiagrams:\nReferences:\nhttps://aws.amazon.com/about-aws/global-infrastructure/?p=ngi&amp;loc=0\nhttps://digitalcloud.training/certification-training/aws-certified-cloud-practitioner/aws-global-infrastructure/\nhttps://aws.amazon.com/blogs/aws/ec2-ami-copy-between-regions/\nhttps://www.rackspace.com/blog/aws-101-regions-availability-zones#:~:text=An%20availability%20zone%20is%20a,use%20by%20any%20AWS%20customer.&amp;text=In%20each%20zone%2C%20participating%20data,over%20redundant%20private%20network%20links.\n\n\nThe correct statements related to the AWS Global Infrastructure are:\nA. For achieving High Availability and Performance, customers usually deploy their applications across multiple AWS Regions C. Availability zones can contain one or more data centers\nExplanation:\nAWS Global Infrastructure is a collection of physical data centers, availability zones, edge locations, and regions spread across the globe. Here are the explanations for each statement:\nA. For achieving High Availability and Performance, customers usually deploy their applications across multiple AWS Regions:\nAWS Regions are a physical location where AWS has multiple data centers in a specific geographic area. These data centers are isolated from each other to provide fault tolerance and high availability. By deploying applications across multiple regions, customers can achieve high availability and performance. This way, if there is an outage in one region, the application can still be accessed from another region.\nC. Availability zones can contain one or more data centers:\nAn Availability Zone (AZ) is a distinct location within an AWS Region that is engineered to be isolated from failures in other Availability Zones. Each Availability Zone consists of one or more data centers, each with redundant power, networking, and connectivity. By deploying applications across multiple Availability Zones, customers can achieve high availability and fault tolerance.\nB. Edge locations can be used to deploy infrastructures like EC2 instances, EBS storage:\nEdge locations are endpoints for AWS services like Amazon CloudFront, Amazon Route 53, and AWS Shield. These locations are designed to cache and deliver content from Amazon S3 buckets or EC2 instances to end-users at low latency. They can be used to deploy infrastructures like EC2 instances, EBS storage, and other AWS services to improve performance.\nD. An EC2 instance's AMI in a particular region can be copied to another region for using it in that Region:\nAn Amazon Machine Image (AMI) is a pre-configured virtual machine image used to create EC2 instances. An AMI can be copied from one region to another region to launch EC2 instances in a different region. This is useful when customers want to deploy their applications in different regions without having to create new AMIs.\nE. An elastic IP address is allocated to an Availability Zone:\nAn Elastic IP address is a static, public IPv4 address that can be associated with an EC2 instance or a network interface. Elastic IP addresses are allocated to AWS accounts and can be remapped to any EC2 instance within the account's VPC. Elastic IP addresses are not allocated to an Availability Zone; they are allocated to the account and can be used in any Availability Zone in the same region.\n\n"
}, {
  "id" : 362,
  "question" : "Which of the following AWS services does NOT provide the capability of provisioning IT infrastructure?\n",
  "answers" : [ {
    "id" : "5cb66aea5fc841c6b3cd30281fb8bcaf",
    "option" : "AWS CodePipeline",
    "isCorrect" : "true"
  }, {
    "id" : "b7bb9feef2444cdfa1d82cce0461d4c2",
    "option" : "AWS CloudFormation",
    "isCorrect" : "false"
  }, {
    "id" : "c3783144dd1e42c29758baa108dc44a3",
    "option" : "AWS Elastic Beanstalk",
    "isCorrect" : "false"
  }, {
    "id" : "c6413c01e6104559a308a46f6520ea3c",
    "option" : "All of them.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nCodePipeline is a Continuous Integration / Continuous Delivery service that builds, tests &amp; deploys code whenever there is a change in the source code.\nCodePipeline does not in itself provision IT infrastructure, instead, it uses available targets like ECS, S3, Elastic Beanstalk for deploying generated artifacts.\nOption B is incorrect.\nAWS CloudFormation provides an easy way to model a collection of AWS resources, provision them quickly and consistently and manage them throughout their lifecycle by treating infrastructure as code.\nTemplates can be used to create, update, and delete an entire stack as a single unit, as often as needed.\nYou can manage and provision stacks across multiple AWS accounts and AWS Regions.\nOption C is incorrect.\nElasticBeanstalk lets you provision resources &amp; deploy a web application in a matter of minutes.\nElasticBeanstalk handles your hosting environment's details and allows you to define Capacity provisioning, Load balancing, scaling &amp; application health monitoring for your applications.\nOption D is incorrect since CodePipeline does not provision resources.\nReferences:\nhttps://aws.amazon.com/codepipeline/\nhttps://aws.amazon.com/cloudformation/\nhttps://aws.amazon.com/elasticbeanstalk/\n\nThe correct answer is A. AWS CodePipeline does not provide the capability of provisioning IT infrastructure.\nHere's a detailed explanation of each option:\nA. AWS CodePipeline: AWS CodePipeline is a continuous delivery service that automates the build, test, and deploy phases of your release process for software applications. It does not provide the capability of provisioning IT infrastructure.\nB. AWS CloudFormation: AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so you can spend less time managing those resources and more time focusing on your applications that run in AWS. With AWS CloudFormation, you can use a template to describe the resources you want to provision and deploy them in a predictable and repeatable way.\nC. AWS Elastic Beanstalk: AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nD. All of them: This is an incorrect option because as mentioned above, AWS CodePipeline does not provide the capability of provisioning IT infrastructure, while AWS CloudFormation and AWS Elastic Beanstalk both provide this capability.\nTherefore, the correct answer is A. AWS CodePipeline does not provide the capability of provisioning IT infrastructure.\n\n"
}, {
  "id" : 363,
  "question" : "I have an application whose execution time is short but is very critical.\nFor keeping my costs minimum for running the application, what is the best AWS Compute service that I can use?\n",
  "answers" : [ {
    "id" : "4bc2dc740e6440a08a44525b6aba9da6",
    "option" : "Spot Instance",
    "isCorrect" : "false"
  }, {
    "id" : "296ce0adba964094a4b7d927266ad874",
    "option" : "Lambda function",
    "isCorrect" : "true"
  }, {
    "id" : "a5f9304fdedf4bc58dbef6c229e30a5c",
    "option" : "Reserved Instance",
    "isCorrect" : "false"
  }, {
    "id" : "5fe30ed32fad405ebf7baf12afad7c9c",
    "option" : "On demand EC2 Instance.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOption A is incorrect.\nAlthough spot instances offer very low compute prices, they are most useful in situations where you have flexible start &amp; end times for your applications.\nSince time criticality is an important aspect for application execution, spot instances may not be the best fit here which can be terminated within a short notice period.\nOption B is CORRECT.With AWS lambda, code gets executed only when needed &amp; AWS automatically takes care of dynamically provisioning / de-provisioning compute capacity to execute the Lambda function.\nAlso known as serverless computing technology, users pay only for the duration of the lambda function execution &amp; they do not need to provision or manage servers.\nThis is the best possible way to dramatically reduce costs without managing idle time of unused compute capacity.\nOption C is incorrect.\nAlthough Standard Reserved Instances provide up to 75% off on-demand price, they are more useful for steady-state or predictable applications.\nSince our scenario only talks about limited usage, RI may not be a good choice.\nOption D is incorrect.\nOn-demand EC2 instances need to be provisioned &amp; managed by the user.\nYou also need to account for idle compute time &amp; terminate instances that are idle which otherwise will have cost implications.\nDiagram:\nDepicts a serverless architecture using API Gateway, Lambda, Dynamo DB streams for an Employee Attendance system.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html\nhttps://aws.amazon.com/lambda/\nhttps://d1.awsstatic.com/whitepapers/serverless-architectures-with-aws-lambda.pdf\nhttps://aws.amazon.com/ecs/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc&amp;ecs-blogs.sort-by=item.additionalFields.createdDate&amp;ecs-blogs.sort-order=desc\n\n\nIf you have an application with a short execution time that is very critical and you want to keep your costs minimal, the best AWS Compute service that you can use is Lambda function.\nLambda function is a serverless compute service that allows you to run code without provisioning or managing servers. It automatically scales to handle the incoming requests and bills you only for the time your code runs.\nThis makes Lambda function a perfect fit for short-running, critical applications as it eliminates the need for long-term infrastructure management, reduces operational costs, and ensures high availability.\nOn the other hand, Spot Instances are a cost-effective option for workloads that can handle interruptions and have flexible start and end times. They are ideal for applications that can be interrupted and resumed, such as batch processing, scientific computing, and data analysis.\nReserved Instances are used for applications that have a steady-state usage, while On-Demand Instances are used for workloads that require consistent performance and low latency.\nIn conclusion, if you have a short-running, critical application, you should consider using Lambda function. It will help you keep your costs minimal while ensuring high availability and scalability.\n\n"
}, {
  "id" : 364,
  "question" : "I have some data that is not frequently accessed.\nBut when requested within six months, the data needs to be available immediately.\nAfter six months, the data is not accessed but needs to be maintained for historical purposes.\nWhat is the best S3 storage class lifecycle available to me with the lowest possible cost?\n",
  "answers" : [ {
    "id" : "2c035a2f0cb140249b46ffbd912e4e61",
    "option" : "Store data for the first 6 months in S3 Standard &amp; move data to Glacier after that.",
    "isCorrect" : "false"
  }, {
    "id" : "8215978a6288403f86f6cd65ead480a4",
    "option" : "Store data the first 6 months in S3 One Zone â€“ IA &amp; move data to Glacier after that.",
    "isCorrect" : "true"
  }, {
    "id" : "29dd82d9cfdd41a69482adf699f9ac25",
    "option" : "Store data the first 6 months in S3 Standard IA &amp; move data to Glacier after that.",
    "isCorrect" : "false"
  }, {
    "id" : "886d6d9319d44f5086ba146e5b20b5eb",
    "option" : "Store data in Glacier &amp; use expedited retrieval for accessing data immediately.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nOn analyzing the scenario carefully, we can see here that data is infrequently accessed for the first 6 months &amp; is then archived for long term storage.\nOption A is incorrect since it is not advisable to store data in S3 Standard that is not frequently accessed.\nS3 standard will incur higher costs for this scenario.\nOption B is CORRECT.\nS3 One Zone IA offers the best cost-effective solution for infrequently accessed data.\nSince the lowest cost is desired here, we can overlook the resilience model offered by High Availability storage solutions.\nAfter six months, data can be moved to Glacier for archival purposes.\nOption C is incorrect.\nS3 Standard IA is more cost-effective than its S3 standard counterpart for infrequently accessed data.\nBut we do not select it as the best available option compared to the pricing of S3 One Zone IA, which offers a 20% subsidized cost compared to S3 Standard IA.Option D is incorrect because Glacier offers a way to store archival data rather than storing it for frequent/infrequent access.\nAlso, expedited retrieval costs are greater (0.03 per GB) than the S3 One Zone IA cost which is 0.01 per GB.References:\nhttps://aws.amazon.com/s3/storage-classes/\nhttps://aws.amazon.com/s3/pricing/\nhttps://youtu.be/wYclDu6GhkU\n\nThe most cost-effective storage solution for data that is not frequently accessed is to use S3 storage classes. S3 provides several storage classes, each with different costs, durability, availability, and retrieval time characteristics. The choice of storage class depends on the access patterns and retention requirements of the data.\nIn this scenario, the data needs to be immediately available for the first six months but not accessed frequently afterward, requiring historical retention. Therefore, we need a storage class that can provide immediate access for the first six months and then move to a lower cost, less frequently accessed storage class that can maintain the data for historical purposes.\nOption A: Store data for the first 6 months in S3 Standard & move data to Glacier after that.\nS3 Standard is designed for frequently accessed data and provides low latency, high throughput, and high durability. It is the most expensive storage class and not suitable for data that is infrequently accessed. Therefore, this option is not the best choice for this scenario.\nOption B: Store data the first 6 months in S3 One Zone - IA & move data to Glacier after that.\nS3 One Zone - IA is a cost-effective storage class that provides low-cost, infrequently accessed storage for data that can be recreated if lost. It stores data in a single availability zone, which means it is less durable than S3 Standard or S3 Standard-IA. Therefore, it is not suitable for data that requires high durability. Additionally, S3 One Zone - IA is not designed for long-term storage, so it is not suitable for historical retention. Therefore, this option is not the best choice for this scenario.\nOption C: Store data the first 6 months in S3 Standard IA & move data to Glacier after that.\nS3 Standard-IA is designed for infrequently accessed data that requires immediate access when needed. It provides the same low latency and high throughput as S3 Standard but at a lower cost. S3 Standard-IA has a minimum storage duration of 30 days, which means that data must be stored for at least 30 days to avoid early deletion fees. After six months, the data can be moved to a lower-cost storage class like Glacier, which provides long-term, low-cost storage for data that is rarely accessed. Therefore, this option is the best choice for this scenario.\nOption D: Store data in Glacier & use expedited retrieval for accessing data immediately.\nGlacier is a low-cost storage class designed for long-term, infrequently accessed data. It provides high durability and low cost but has a longer retrieval time than S3 Standard or S3 Standard-IA. Expedited retrieval can be used to retrieve data within minutes, but it incurs additional fees. Therefore, this option is not the best choice for this scenario, as the data needs to be immediately available for the first six months, and using Glacier would result in a longer retrieval time and higher costs.\nIn conclusion, the best S3 storage class lifecycle available for this scenario is to store data in S3 Standard-IA for the first six months and move the data to Glacier after that to maintain historical retention.\n\n"
}, {
  "id" : 365,
  "question" : "I have a Virtual Private Cloud infrastructure environment hosting an Application &amp; a Database.\nWhat are the best practices that I can use to host them within the infrastructure? Select TWO.\n",
  "answers" : [ {
    "id" : "d434dff018d1452bb8f906012158a5f3",
    "option" : "Host the Application in a Public Subnet, Database in a Private Subnet with an ELB frontending the Application in a Public Subnet",
    "isCorrect" : "false"
  }, {
    "id" : "9d35598d8cfe4c75ad89af37125e3c1c",
    "option" : "Host both the Application &amp; Database in a Private subnet with an ELB frontending the Application in a public Subnet",
    "isCorrect" : "true"
  }, {
    "id" : "2cbd28006b2547da858bc8f5a3d28127",
    "option" : "Host both the Application &amp; Database in a Public subnet with an ELB frontending the Application in a public Subnet",
    "isCorrect" : "false"
  }, {
    "id" : "1aaefcc7a08d442fabcc16e4094ddc67",
    "option" : "Subnet configured for the Application should have a route to the Internet Gateway",
    "isCorrect" : "false"
  }, {
    "id" : "adb7cde9846d48c8b5a82eff24ca130b",
    "option" : "Subnet configured for the ELB should contain a NAT Gateway.",
    "isCorrect" : "true"
  } ],
  "explanations" : "Answers: B and E.\nOption A is incorrect.\nThe entry point to the Application is the ELB.\nIt's best to have only the ELB within the Public Subnet and have the Application &amp; Database in the Private subnet.\nThis way, a user can access the application through the ELB to provide High Availability &amp; failover.\nOption B is CORRECT.\nThis is the best possible configuration that can be defined for maximum Security, High Availability &amp; Failover.\nOption C is incorrect.\nThis configuration will be least Secure since users will be able to access all the Application, Database &amp; ELB within the Public Subnet.\nAlso, single points of failure may occur due to a lack of proper services structuring within the respective layers.\nOption D is incorrect since the Application should be placed within the Private Subnet that should not route the Internet Gateway.\nInstead, it should have a route to the NAT gateway for accessing the Internet in an Egress manner.\nOption E is CORRECT.\nA NAT gateway allows resources hosted within the Private Subnet to access the Internet for operations like OS updates or DB patch updates.\nSince the ELB is the only resource within a Public Subnet, it should ideally contain a NAT Gateway that will allow the Application or Database to access the Internet.\nDiagram:\nThe figure below shows a typical configuration of an ELB, EC2 instance, RDS, NAT Gateway, Bastion host &amp; route table.\nThe NAT Gateway &amp; Bastion Host is contained within the Public Subnet along with the ELB, while the EC2 instance &amp; RDS is contained within a Private Subnet.\nReferences:\nhttps://youtu.be/tD9vDv0uyI8\nhttps://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\n\n\nThe best practices for hosting an application and database in a Virtual Private Cloud (VPC) infrastructure environment are as follows:\nHost the Application in a Public Subnet, Database in a Private Subnet with an ELB frontending the Application in a Public Subnet This approach separates the application and database tiers by placing them in different subnets. The application tier is placed in a public subnet to allow access to the internet for communication with users, while the database tier is placed in a private subnet that is not accessible from the internet. An Elastic Load Balancer (ELB) is placed in the public subnet to front-end the application, and it distributes traffic across multiple instances to ensure high availability and scalability. This configuration provides a higher level of security for the database, as it is not directly accessible from the internet. Subnet configured for the Application should have a route to the Internet Gateway In this approach, the application and database tiers are placed in the same subnet, but the subnet is configured to have a route to the internet gateway. This allows the application to communicate with users over the internet, but it also exposes the database to potential security risks.\nOption A is the recommended approach as it follows the best practice of separating the application and database tiers in different subnets for improved security, while also using an ELB for scalability and high availability. Option B exposes the database to potential security risks, and options C and D do not follow best practices for separating the application and database tiers.\nOption E is not directly related to hosting the application and database, but rather relates to configuring the ELB subnet to allow outgoing traffic to the internet via a NAT Gateway. This is not a required step for hosting an application and database in a VPC, but it can be used to provide access to external resources, such as software updates or external APIs.\n\n"
}, {
  "id" : 366,
  "question" : "Consider the following scenario: Id First Name Last Name 1 John Abraham I have stored this data in a DynamoDB table.\nAfter running an update, I change the Last Name from â€œAbrahamâ€ to â€œMathewâ€\nWhen John reads his data immediately after the update, he still sees his Last Name as Abraham rather than Mathew.\nWhat is the possible reason?\n",
  "answers" : [ {
    "id" : "7ce693deb21c4fb2b0a6fc2c02946043",
    "option" : "DynamoDB takes 1 second to ensure consistency across all its copies of data.",
    "isCorrect" : "false"
  }, {
    "id" : "7c3a838d02884e84abe2d7025c7ed2fc",
    "option" : "By default DynamoDB reads are eventually consistent.",
    "isCorrect" : "true"
  }, {
    "id" : "bef456d4f4a648fe85bbc626a77d8377",
    "option" : "DynamoDB does not support strongly consistent reads.",
    "isCorrect" : "false"
  }, {
    "id" : "c8b1dfb74bb44dd3be4159149966abe8",
    "option" : "All ofA, B &amp; C.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nDynamoDB chooses Performance over Consistency while delivering reads.\nUsing this model, during a read, DynamoDB will look for the nearest &amp; most available storage location for fulfilling the read request.\nDynamoDB stores three geographically distributed replicas of each table.\nIt may so happen that the storage location where DynamoDB performs the read would not have been updated with the latest data.\nStrong consistency involves multiple reads to ensure that the latest update is always available sacrificing Performance.\nOption A is incorrect.\nConsistency across DynamoDB does reach within 1 second.\nBut it is the DynamoDB's read model rather than the lag that causes stale data to appear.\nOption B is CORRECT.\nThe default read model supported by DynamoDB is an â€œEventually Consistentâ€ one as described above.\nOption C is incorrect.\nDynamoDB supports strongly consistent reads but by default it uses eventually consistent reads.\nOption D is incorrect because of the reasons discussed above.\nReference:\nhttps://youtu.be/SqMevk3n3EQ\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html\n\nThe possible reason for John still seeing his Last Name as \"Abraham\" after an update could be due to the eventual consistency model used by DynamoDB, which is the default behavior for read operations. In other words, the data may not have been propagated to all copies of the data in DynamoDB at the time of John's read operation, resulting in a stale read.\nDynamoDB is a NoSQL database service offered by AWS that provides fast and flexible document and key-value store capabilities. To achieve high availability and durability, DynamoDB replicates data across multiple servers within an AWS Region or across multiple regions.\nDynamoDB offers two types of consistency models for read operations: eventual consistency and strong consistency. By default, DynamoDB uses eventual consistency, which means that the data read may not reflect the most recent write operation for a short period. This is because the data is replicated across multiple servers asynchronously, and it takes time to propagate the changes to all copies of the data. In other words, eventual consistency sacrifices consistency in favor of availability and performance.\nOn the other hand, strong consistency ensures that the data read reflects the most recent write operation. However, strong consistency may impact availability and performance, as it requires additional communication between the servers to ensure that the data is consistent across all copies.\nIn the given scenario, if the application requires strong consistency, the read operation should be configured to use strongly consistent reads, which are available as an option in DynamoDB. If the application can tolerate eventual consistency, the application should be designed to handle stale reads.\nTo summarize, the possible reason for John still seeing his Last Name as \"Abraham\" after an update is due to the eventual consistency model used by DynamoDB, which is the default behavior for read operations. The correct answer to the question is B, which states that by default, DynamoDB reads are eventually consistent.\n\n"
}, {
  "id" : 367,
  "question" : "I have two applications, â€œImage Processingâ€ &amp; â€œOrder Processing,â€ hosted on my website on different EC2 servers in an Auto Scaling Group.\nWhat is the best way to provide access to a user for any of these applications on this website?\n",
  "answers" : [ {
    "id" : "9a318ba709e04a3cb9139740ce122b60",
    "option" : "I can provide the public DNS URL of each of my servers where my application ishosted.",
    "isCorrect" : "false"
  }, {
    "id" : "25bde67d8aa847738c182b9535d95376",
    "option" : "I can use the Classic Load Balancer that will route requests to differentapplications depending on the userâ€™s request.",
    "isCorrect" : "false"
  }, {
    "id" : "3e3758cbb5934e72be0227d73e0a1dd5",
    "option" : "I can use the Application Load Balancer that will route requests to differentapplications depending on the userâ€™s request.",
    "isCorrect" : "true"
  }, {
    "id" : "365340fdb1f14c2fb5cee8a2c793a601",
    "option" : "I can use the Network Load Balancer that will route requests to differentapplications depending on the userâ€™s request.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: C.\nOption A is incorrect.\nWhile it is possible to provide a public DNS URL or a public IP address of your site to a user, it is not the best practice for several reasons.\nIt will be inefficient &amp; cumbersome having the users to know many URL's or IP addresses as the number of applications increase.\nLoad distribution will not be possible with this scenario resulting in possible Single Point of Failures &amp; unacceptable application performance as the load increases.\nHigh Availability &amp; failover will not be possible since we are exposing a static IP address or URL.\nOption B is incorrect.\nClassic Load Balancer provides basic load balancing capabilities that will distribute traffic equally among many servers under it.\nOption C is CORRECT.\nApplication Load Balancer supports a feature named Path-based routing that will route requests based on URL patterns provided in the request.\nApplication Load Balancer achieves this feature by using Target groups that hold a specific set of resources.\nEC2 instances, Auto Scaling groups, ECS tasks etc.\nIt is the responsibility of the Target group which keeps track of the instances of that particular class and intelligently route requests based on the load within a specific group.\nOption D is incorrect.\nNetwork Load balancers distribute load based on network variables like IP address, destination ports.\nIt is layer 4 (TCP) and below and is not designed to take into consideration anything at the application layer such as content type, cookie data, custom headers, user location, or the application behavior.\nSo Network Load balancer cannot ensure the availability of applications.\nDiagram:\nAs seen in the diagram below, the Application Load Balancer acts as a single point of entry to various applications hosted on a website.\nBased on the URL pattern, e.g., www.example.com/orders, the ALB will route the request based on the Path to a specific Target Group hosting the application.\nIn this case, the Orders Auto Scaling group will be held by the ALB's Target Group.\nWithin the Auto Scaling group, the ALB will intelligently route the request to different server instances depending on the load.\nReferences:\nhttps://youtu.be/-hFAWk6hyZA\nhttps://aws.amazon.com/elasticloadbalancing/#:~:text=Elastic%20Load%20Balancing%20automatically%20distributes,or%20across%20multiple%20Availability%20Zones.\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html\nhttps://docs.aws.amazon.com/autoscaling/plans/userguide/what-is-aws-auto-scaling.html\n\n\nThe best way to provide access to a user for any of these applications on the website is to use a load balancer. A load balancer is a tool that distributes incoming network traffic across multiple servers to ensure that no single server is overwhelmed. This helps to improve the availability and scalability of the website.\nThere are three types of load balancers in AWS:\nClassic Load Balancer (CLB): It routes traffic based on either Layer 4 (TCP/UDP) or Layer 7 (HTTP/HTTPS) information. It can balance traffic across EC2 instances in one or more Availability Zones. Application Load Balancer (ALB): It is designed to route traffic based on application-level content, such as URL or HTTP headers. It can balance traffic across multiple services or EC2 instances in one or more Availability Zones. Network Load Balancer (NLB): It is designed to handle TCP/UDP traffic at high speed, routing traffic based on IP protocol data. It can balance traffic across EC2 instances in one or more Availability Zones.\nConsidering the given scenario, as we have two applications, we should choose the Application Load Balancer (ALB) because it is specifically designed to route traffic based on application-level content. ALB allows routing traffic based on HTTP requests' contents, such as URL, headers, and cookies, to different EC2 instances running the two applications. ALB can also perform path-based routing, redirecting traffic to different servers based on the URL path specified in the request.\nUsing the public DNS URL of each server where applications are hosted may work, but it is not the best solution as it does not distribute traffic evenly, leading to potential overload of one server and underutilization of another.\nTherefore, the correct answer to the given question is option C: I can use the Application Load Balancer that will route requests to different applications depending on the user's request.\n\n"
}, {
  "id" : 368,
  "question" : "I need to keep track of all invalid login attempts of a user when he tried to SSH to an EC2 instance.\nHow can I achieve that?\n",
  "answers" : [ {
    "id" : "7b6a01ec09ca4099aba08e656a234fb4",
    "option" : "Collecting log data from the EC2 instance and delivering them to a CloudWatch Logs log stream.",
    "isCorrect" : "true"
  }, {
    "id" : "a9720ad9e491428588603697758f965c",
    "option" : "Integrating CloudTrail with CloudWatch Logs to deliver data events captured by the login activity to a CloudWatch Logs log stream.",
    "isCorrect" : "false"
  }, {
    "id" : "ad48a1fc55da4be4b43093c56a3fc1c7",
    "option" : "Running a log utility on the EC2 instance periodically and check the server logs.",
    "isCorrect" : "false"
  }, {
    "id" : "23dac516363d43aeb397bbea1703b83e",
    "option" : "Both B &amp;",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: A.\nOption A is CORRECT.\nLog data can be collected from EC2 instances by installing &amp; configuring a CloudWatch Log Agent on the EC2 server.\nThese logs can then be delivered to CloudWatch log group streams, where they can be analyzed using Metric Filters.\nActions like notifying an admin on the invalid login attempt can then be done by defining CloudWatch alarms on the associated Log metrics.\nOption B is incorrect.\nCloudTrail tracks API requests made by users.\nIts logs will be more useful when operations on resources are performed like creating an EC2 instance or terminating an EC2 instance.\nThose logs can be integrated with CloudWatch for detecting abnormal operations on different AWS resources.\nThe user login scenario is captured as logs on the EC2 instance &amp; sent to CloudWatch by the Log Agent.\nWhen a user tries to SSH to an EC2 instance, the activity is not recorded in AWS CloudTrail as it is not an AWS API call.\nOption C is incorrect.\nAlthough it is possible to run a log utility to report failed login attempts by the user, it defeats the advantages that a centralized Monitoring &amp; Logging system offers.\nAlso, by doing so, real-time monitoring will not happen due to the absence of streaming data resulting in delayed incident detection &amp; resolution.\nOption D is incorrect since the best way to track log data is to push it to a Log stream destination where it can be quickly monitored resulting in faster incident resolution.\nDiagram:\nReferences:\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html\nhttps://aws.amazon.com/cloudwatch/\nhttps://aws.amazon.com/cloudtrail/\nhttps://www.tests.com/aws-monitoring-and-auditing/\n\n\nThe correct answer is D. Both A and B options are correct.\nTo keep track of all invalid login attempts of a user when he tried to SSH to an EC2 instance, you can use AWS CloudWatch Logs and AWS CloudTrail.\nAWS CloudWatch Logs allows you to monitor, store, and access log files from Amazon EC2 instances, AWS CloudTrail, and other AWS services. You can use CloudWatch Logs to collect and store log data from your EC2 instances, and then analyze and retrieve the data using CloudWatch Logs queries.\nAWS CloudTrail is a service that provides a record of actions taken by a user, role, or an AWS service in your AWS account. You can use CloudTrail to log all user activity, including SSH login attempts to your EC2 instances.\nTo set up monitoring for invalid SSH login attempts, you can follow the below steps:\nCreate a CloudWatch Logs log group: This log group will store all the log data collected from your EC2 instances. Install the CloudWatch Logs agent on your EC2 instances: The CloudWatch Logs agent allows you to collect and send log data to your CloudWatch Logs log group. Configure the CloudWatch Logs agent to monitor SSH logins: You can configure the CloudWatch Logs agent to monitor the /var/log/secure log file on your EC2 instance. This file contains information about all SSH login attempts, including successful and unsuccessful ones. Integrate CloudTrail with CloudWatch Logs: CloudTrail captures all SSH login attempts to your EC2 instances, and you can configure it to deliver data events captured by the login activity to a CloudWatch Logs log stream. This will allow you to monitor and analyze all SSH login attempts in one place.\nBy setting up CloudWatch Logs and CloudTrail in this way, you can monitor and track all SSH login attempts to your EC2 instances, including failed attempts. You can also set up alarms to notify you if there are too many failed login attempts or if there are suspicious login attempts from unknown IP addresses.\n\n"
}, {
  "id" : 369,
  "question" : "I am using the Amazon Simple Notification Service to send notifications to alert admins whenever the CPU utilization of an EC2 instance crosses 70%\nWhich of the following can be subscribers to an SNS Topic? (Select TWO)\n",
  "answers" : [ {
    "id" : "f2926f8487eb4c1d9a418698adcc1980",
    "option" : "Email",
    "isCorrect" : "true"
  }, {
    "id" : "5de5f826037f4524b0fd68709eabf677",
    "option" : "Amazon S3",
    "isCorrect" : "false"
  }, {
    "id" : "0a7764518535431397d37c5235896d0d",
    "option" : "AWS Lambda",
    "isCorrect" : "true"
  }, {
    "id" : "6adde749962d42c2b10a2d9146914089",
    "option" : "Amazon CloudWatch",
    "isCorrect" : "false"
  }, {
    "id" : "22b4c30ea51e47bfa3032b1918e17c8d",
    "option" : "Amazon DynamoDB streams.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answers: A and C.\nSNS is extremely useful for the fan-out types of applications, i.e., multiple clients that push messages to an SNS topic &amp; multiple listeners can be notified when a message arrives at the Topic.\nOption A is CORRECT.\nSNS messages can be sent to registered addresses as Email (text-based or Object) who act as subscribers to the notification.\nOption B is incorrect.\nS3 acts as a publisher of SNS notifications.\nWhen a file is uploaded to S3, it can publish an event that can then be subscribed to &amp; acted upon.\nOption C is CORRECT.\nA lambda function can subscribe to an SNS Topic and can act on any events that are published to that Topic.\nAn S3 PUT or CREATE event for uploading documents can have a Lambda subscriber that can pull out metadata information contained within the documents &amp; store it in a Dynamo DB database.\nOption D is incorrect.\nCloudWatch will act as a publisher of events using alarms.\nGetting back to our scenario, we can set CloudWatch alarms on the CPU utilization metrics of the EC2 instance.\nThe alarms can then be published to an SNS Topic for notifying users.\nOption E is incorrect.\nDynamo DB streams are events that are emitted when record modifications occur on a Dynamo DB table like INSERT, UPDATE, etc.\nThey are extremely useful to create informative dashboards in real-time.\nDynamo DB streams can trigger a lambda function that can publish a message to an SNS Topic.\nSo we can see here that Dynamo DB stream acts as a publisher of events.\nReferences:\nhttps://docs.aws.amazon.com/sns/latest/dg/welcome.html\nhttps://docs.aws.amazon.com/sns/latest/dg/sns-create-subscribe-endpoint-to-topic.html\n\n\nAmazon Simple Notification Service (SNS) is a fully managed messaging service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SNS provides topics for high-throughput, push-based, many-to-many messaging.\nTo answer the question, SNS allows you to send messages to various endpoints or subscribers that can receive notifications through supported protocols, such as HTTP/S, Email, SMS, Lambda, and more.\nIn this scenario, the requirement is to send notifications to alert admins whenever the CPU utilization of an EC2 instance crosses 70%. Therefore, we need to select the subscribers to an SNS Topic that can receive and process the alert message.\nLet's review the answer options:\nA. Email: This is a valid option as it allows an email subscriber to receive the notification via email. The email recipient can then act accordingly based on the message received.\nB. Amazon S3: This is not a valid option as Amazon S3 is an object storage service and is not designed to receive notifications from SNS topics.\nC. AWS Lambda: This is a valid option as AWS Lambda is a compute service that can receive the notification and perform a set of customized actions based on the message received.\nD. Amazon CloudWatch: This is not a valid option as Amazon CloudWatch is a monitoring service that can send alerts to SNS topics but cannot be a subscriber to an SNS topic.\nE. Amazon DynamoDB streams: This is not a valid option as Amazon DynamoDB streams is a feature of Amazon DynamoDB that captures a time-ordered sequence of item-level modifications in a DynamoDB table and can send the stream records to AWS Lambda, Amazon Kinesis Data Streams, or Amazon Simple Queue Service (SQS). It is not designed to receive notifications from SNS topics.\nTherefore, the correct answers are A. Email and C. AWS Lambda.\n\n"
}, {
  "id" : 370,
  "question" : "I have a website that hosts mission-critical applications and requires 99.999% uptime.\nWhat routing policy will I apply while using Route 53?\n",
  "answers" : [ {
    "id" : "a488793a8bb7438a96bbffd479e90886",
    "option" : "Multi Value Answer Routing",
    "isCorrect" : "false"
  }, {
    "id" : "da8ba962a68548338ba81437fc3ff29c",
    "option" : "Failover Routing",
    "isCorrect" : "true"
  }, {
    "id" : "79ab9a4d802b4d09a461d16dec7ba671",
    "option" : "Weighted Routing",
    "isCorrect" : "false"
  }, {
    "id" : "891f09b48fe3467faee1985058b48b8d",
    "option" : "Simple Routing.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Answer: B.\nSince the mission-critical applications require 99.999% uptime, I would need an Active-Active site replication of resources.\nHere one site's failure will result in Route 53 automatically switching to the other site, thus maintaining the uptime requirement.\nOption A is incorrect.\nMultivalve answer routing provides the ability to return multiple health-checkable IP addresses, which is a way to use DNS to improve availability and load balancing.\nThe critical point here is that these IP addresses may not point to servers at multiple site locations.\nRather they may be servers in different availability zones within the same region.\nSince we add a higher level of resiliency for the critical requirement, it's always advisable to provide an entire region failure.\nOption B is CORRECT.\nFailover routing is usually used in Disaster Recovery scenarios where an Active-Passive or Active-Active Disaster recovery configuration is required.\nOption C is incorrect.\nA weighted routing policy is usually used to route traffic in proportions that are specified.\nE.g., if there is a new version of software that needs to be tested, 20% of the traffic can be sent to that site for getting user feedback rather than sending 100% of the traffic to that site.\nOption D is incorrect.\nA simple routing policy is used for routing traffic to a single resource, e.g., mapping an URL to a web server.\nDiagram:\nReference:\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html\n\n\nThe routing policy that should be applied while using Route 53 to ensure 99.999% uptime for a mission-critical application is Multi Value Answer (MVA) Routing.\nMVA Routing is a routing policy provided by Route 53 that is designed to improve the availability and latency of your applications. It allows you to specify multiple IP addresses or multiple values for a single resource record set, and Route 53 responds to DNS queries with up to eight healthy IP addresses or values for the same record. This allows Route 53 to return multiple IP addresses for a given hostname, improving availability and fault tolerance by making it easier to route traffic to the healthy endpoints.\nIn addition, MVA Routing also includes health checks to monitor the health of the endpoints associated with the record set. If an endpoint becomes unhealthy, Route 53 will stop including that endpoint in the response to DNS queries, and will only include the healthy endpoints. This ensures that traffic is only routed to healthy endpoints, improving the availability of your application.\nFailover Routing is another routing policy that can be used to improve availability, but it is typically used in scenarios where there are two different endpoints that are serving traffic, such as a primary and secondary site. Weighted Routing is used when you want to route traffic to different resources in proportions specified by you. Simple Routing is the default routing policy, which is used when you only have one resource that serves traffic.\nTherefore, Multi Value Answer (MVA) Routing is the most appropriate routing policy to ensure 99.999% uptime for a mission-critical application hosted on AWS.\n\n"
}, {
  "id" : 371,
  "question" : "Refer to the following figure below A user wishing to access an application installed in â€œEC2-Bâ€ instance in â€œVPC Bâ€ can be reached by.\n(Choose the best answer)\n\n",
  "answers" : [ {
    "id" : "03d352df0ac74660b422b368e499ef5a",
    "option" : "Using an Internet Gateway that can be attached to VPC B",
    "isCorrect" : "false"
  }, {
    "id" : "5d85f7df58004558b4f800911f70bf9e",
    "option" : "Through EC2-A Public subnet &amp; EC2-A Private subnet using a VPC Peeringconnection to EC2-B",
    "isCorrect" : "true"
  }, {
    "id" : "91eb98538d934766b4e50744097a6679",
    "option" : "Through EC2-A Public subnet &amp; EC2-A Private subnet using a VPN connection to EC2-B",
    "isCorrect" : "false"
  }, {
    "id" : "98928247298e4607bb331d7f4031f52a",
    "option" : "Instancesusing Private IP address cannot be connected using VPC Peering.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nInstances within a VPC's subnets can communicate with each other by default.\nIn order to seamlessly connect between different networks, a concept called VPC Peering is introduced by AWS.\nUsing a VPC Peering connection, instances in private subnets in two different networks can talk to each other.\nIn the above diagram, since VPC B does not have an Internet Gateway, the only way that an User can reach the EC2-B instance is through a VPC peering connection between EC2-A &amp; EC2-B.\nOption A is incorrect since this will involve modifications to the existing Architecture and dilute existing security configurations.\nOption B is CORRECT.\nThe user can connect to VPC A, EC2-A Public, EC2-A Private, EC2-B Private to access applications installed on EC2-B.\nOption C is incorrect since a VPN connection is normally used to connect On premises Servers to AWS services using the Internet.\nOption D is incorrect since the only way to connect instances using Private IP's in different networks is through VPC Peering.\nReferences:\nhttps://youtu.be/HMInA8yMw1k\nhttps://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html\n"
}, {
  "id" : 372,
  "question" : "My containerized application requires me to define and manage capacity(CPU, Memory, Instance type) explicitly for optimizing costs while using AWS infrastructure.\nWhich of the following technologies will I use while defining my Elastic Container Services compute instances.\n",
  "answers" : [ {
    "id" : "73a28b6db2de47c489145999125f57d9",
    "option" : "EC2",
    "isCorrect" : "true"
  }, {
    "id" : "885405ff55e8491fb06d430cc1701078",
    "option" : "Fargate",
    "isCorrect" : "false"
  }, {
    "id" : "682b17494db5439b89d019669d44da9c",
    "option" : "Lambda",
    "isCorrect" : "false"
  }, {
    "id" : "b9c00667f89b465fbf9c53fdbb01a5b9",
    "option" : "Either of B or C since they are serverless technologies that are cost effective.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nElastic Container Services supports two launch types for deploying containers mainly the EC2 type &amp; the Fargate type.\nFargate provides infrastructure automation for provisioning compute instances as required.\nSince our scenario requires explicit control of CPU &amp; Memory resources for launching the container instances, EC2 will be the most appropriate type.\nOption A is CORRECT since the user will have complete control over the VM configuration &amp; the OS.\nOption B is incorrect since Fargate automates infrastructure provisioning where a user may have limited control.\nOption C is incorrect since Lambda is a serverless technology &amp; is not used within a service containerization context.\nOption D is incorrect since serverless technologies although may be lucrative from a cost standpoint will not have complete user control over configuring the compute instances.\nDiagram:\nReference:\nhttps://aws.amazon.com/ecs/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc&amp;ecs-blogs.sort-by=item.additionalFields.createdDate&amp;ecs-blogs.sort-order=desc\n\n\nIf you want to define and manage the capacity explicitly for your containerized application, you will need to use Elastic Container Service (ECS) on AWS. ECS is a container orchestration service that allows you to run Docker containers on AWS infrastructure.\nECS offers two compute options for running your containers: EC2 and Fargate. EC2 is a traditional compute option that provides you with the ability to specify and manage the capacity for your instances. Fargate, on the other hand, is a serverless compute option that allows you to run containers without having to manage the underlying infrastructure.\nIf you choose to use EC2, you will have to provision and manage the instances that your containers run on. This means that you will need to choose the appropriate instance type, specify the amount of CPU and memory required for your containers, and manage the scaling of your instances.\nIf you choose to use Fargate, you will not have to manage the underlying infrastructure. Fargate automatically provisions and scales the compute resources required for your containers based on the CPU and memory requirements that you specify.\nIn both cases, you will be able to optimize costs by choosing the appropriate instance types, specifying the amount of CPU and memory required for your containers, and scaling your instances based on demand. However, Fargate may be more cost-effective if you do not want to manage the underlying infrastructure yourself.\nLambda is not a suitable option for running containerized applications since it is a serverless compute option that only supports running code in response to events. Therefore, the correct answer is either A or B, depending on your preference for managing the underlying infrastructure.\n\n"
}, {
  "id" : 373,
  "question" : "I require different levels of access for my application that is installed on an EC2 instance.\nI have configured an ENI for the same purpose.\nWhich of the following statement is incorrect?\n",
  "answers" : [ {
    "id" : "68d6b2d9e84a48d6b80822a834301350",
    "option" : "I can detach the primary ENI of my EC2 instance and connect it to another instance for moving its Elastic IP",
    "isCorrect" : "true"
  }, {
    "id" : "67d1d7f9973240babc27cf9c00137e1b",
    "option" : "I can configure a Security Group for my ENI and restrict traffic to the EC2 instance",
    "isCorrect" : "false"
  }, {
    "id" : "32bc25ac80c940679005c7364922fca4",
    "option" : "I can detach a secondary ENI containing a Private IP from one EC2 instance and attach it to another",
    "isCorrect" : "false"
  }, {
    "id" : "7542890bb4004251b4524d28714afa0d",
    "option" : "I can attach an Elastic IP to an EC2 instance in another subnet by releasing it from the ENI in the current subnet to which it is currently attached to.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT.\nThe primary ENI of an instance cannot be detached from the instance.\nBy default, the primary ENI is created with the creation of the EC2 instance &amp; deleted when the instance is terminated.\nOption B is incorrect since an EC2 instance may require restricted access to certain IP addresses.\nThis can be achieved by creating a new ENI &amp; attaching a Public IP &amp; Security Group restricting permissions.\nOption C is incorrect.\nSecondary ENI's that are created can be detached from the instance to which it is attached to &amp; attached to another instance within the same subnet.\nThe Private IP then gets allocated to the second instance to which it is attached currently.\nOption D is incorrect.\nENI's are subnet specific.\nSo for attaching an Elastic IP to an instance in a different subnet, I need to first release it to the pool by dissociating it from an attached instance.\nThis way, I can attach the Elastic IP to an instance in a different subnet.\nReferences:\nhttps://youtu.be/Zg8rMLE88mg\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html\n\nAs per the given scenario, you have an EC2 instance and you have configured an ENI (Elastic Network Interface) to provide different levels of access for your application. An ENI is a logical networking component in a VPC (Virtual Private Cloud) that represents a virtual network interface card that you can attach to an instance in a VPC.\nLet's review each answer option:\nA. I can detach the primary ENI of my EC2 instance and connect it to another instance for moving its Elastic IP\nThis statement is correct. An Elastic IP is a static, public IP address that you can allocate to your AWS account and associate with your instance's ENI. If you want to move the Elastic IP address from one instance to another, you can detach the primary ENI from the first instance and attach it to the second instance. This will move the Elastic IP address along with the ENI to the new instance.\nB. I can configure a Security Group for my ENI and restrict traffic to the EC2 instance\nThis statement is also correct. You can configure a security group for your ENI and control the inbound and outbound traffic to and from your instance. By configuring the security group for your ENI, you can restrict the traffic to your EC2 instance at a more granular level than by just configuring the security group for your instance.\nC. I can detach a secondary ENI containing a Private IP from one EC2 instance and attach it to another\nThis statement is also correct. You can detach a secondary ENI containing a private IP address from one instance and attach it to another instance in the same VPC. This can be useful in scenarios where you want to move a network interface to a different instance without having to reconfigure the IP addresses and networking settings.\nD. I can attach an Elastic IP to an EC2 instance in another subnet by releasing it from the ENI in the current subnet to which it is currently attached to.\nThis statement is incorrect. An Elastic IP can only be associated with an ENI in the same subnet as the instance. You cannot attach an Elastic IP to an instance in a different subnet by releasing it from the ENI in the current subnet. If you want to associate an Elastic IP with an instance in a different subnet, you need to allocate a new Elastic IP and associate it with an ENI in the same subnet as the instance.\nTherefore, the incorrect statement is option D.\n\n"
}, {
  "id" : 374,
  "question" : "I need to check whether my EC2 instances are running properly.\nI have written a script that will help me obtain the instance's ID and then send a notification to an SNS topic.\nHow can I obtain the instance's ID?\n",
  "answers" : [ {
    "id" : "f8301f48ded442cf90125276c848bd42",
    "option" : "By querying the instanceâ€™s Meta Data",
    "isCorrect" : "true"
  }, {
    "id" : "98515147f7594d588ebc17591ac0ba58",
    "option" : "By querying the instances User Data",
    "isCorrect" : "false"
  }, {
    "id" : "d2e06ce2d59749a2be30d9cb4274d0ac",
    "option" : "I need to get authorized with an IAM role prior to accessing the instanceâ€™s ID using Metadata",
    "isCorrect" : "false"
  }, {
    "id" : "c97d7fa248e84c74afed898e5b795bf1",
    "option" : "I cannot use a script. I need to login to the AWS console &amp; manually check the instanceâ€™s ID &amp; its status.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: A.\nOption A is CORRECT.\nAn instance's Meta data provides me with information about the instance like Instance ID, Local IP, Instance Type etc..I can query the instances Meta data using an internal IP http://169.254.169.254/latest/meta-data/ which can be accessed only from within an EC2 instance.\nOption B is incorrect.\nUser Data is a section used for providing startup configuration to an EC2 instance.\nEg if I need to have an Apache web server running on an EC2 instance after it is provisioned, I can use the User Data section &amp; provide scripts for installing the software during instance creation.\nOption C is incorrect.\nMeta data information is available to EC2 instances by default without the need to provide an IAM role for accessing it.\nOption D is incorrect.\nConsole login or manual intervention is not required to achieve this task.\nI can write a script &amp; configure it to run at a scheduled time of the day wherein the EC2 instance can express itself to provide its health status.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/instancedata-data-retrieval.html\nhttps://youtu.be/tPQsI8n6er0\n\nTo obtain an instance's ID, you can use the instance's metadata service. This service provides a unique URL that you can use to access the instance's metadata, including its ID.\nYou can access the metadata service by making an HTTP GET request to the following URL: http://169.254.169.254/latest/meta-data/instance-id\nThis URL will return the instance's ID as plain text. You can then use this value in your script to send notifications to an SNS topic or perform other actions.\nNote that you don't need to be authorized with an IAM role to access an instance's metadata. The metadata service is available to all instances, regardless of their IAM permissions.\nIn summary, the correct answer to this question is A: By querying the instance's Meta Data.\n\n"
}, {
  "id" : 375,
  "question" : "EC2 User Data provides a feature wherein I can do bootstrapping activities when the instance is created.\nWhich of the following statement regarding User Data is incorrect?\n",
  "answers" : [ {
    "id" : "0b22b87b2e004307859467d988f40358",
    "option" : "User data for an instance can be modified, if the instance is in the stopped state and the root volume is an EBS volume.",
    "isCorrect" : "false"
  }, {
    "id" : "bfa2fc46ca4b45eeab4c9d116c05f6db",
    "option" : "User data for an instance can be modified if it is in the running state and the root volume is an EBS volume.",
    "isCorrect" : "true"
  }, {
    "id" : "ae4638d98e924337913756dabf39319e",
    "option" : "No need to use `sudo` in the User Data script to run as the root user.",
    "isCorrect" : "false"
  }, {
    "id" : "27b83b5af1334ce6a61ac29a67c1b4c2",
    "option" : "User Data script can be modified using both the AWS console &amp; the AWS CLI.",
    "isCorrect" : "false"
  } ],
  "explanations" : "Correct Answer: B.\nOption A is incorrect because it is a true statement.\nYou can modify user data for an instance in the stopped state if the root volume is an EBS volume.\nOption B is correct because it is a false statement.\nIf the instance is in the stopped state, user data can be modified, not in the running state.\nOption C is incorrect because it is a true statement.\nThe script commands within User Data executes as root only.\nSo no need to use Sudo explicitly for running them.\nOption D is incorrect because it is a true statement.\nUser Data script can be modified usingAWS CLI also.\nReferences:\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-add-user-data.html\n\nUser data is a feature of Amazon Elastic Compute Cloud (EC2) instances that enables you to execute scripts or run commands when an instance boots up. This feature is used to perform various bootstrapping activities like installing software, configuring settings, and setting up custom security rules, among other things.\nAnswer B is incorrect. User data for an instance cannot be modified if it is in the running state, regardless of the root volume's type. Once an instance is running, its configuration is fixed, and the user data cannot be changed. If you need to make changes to the user data, you will need to stop the instance first.\nAnswer A is correct. If the instance is in the stopped state and the root volume is an EBS volume, you can modify the user data by creating a snapshot of the root volume, attaching the snapshot to a different instance, mounting the root volume, and then modifying the user data.\nAnswer C is correct. When you execute user data on an EC2 instance, it runs as the root user by default, so there is no need to use sudo in the user data script to run as the root user.\nAnswer D is correct. You can modify the user data script using both the AWS console and the AWS Command Line Interface (CLI). In the console, you can modify the user data in the advanced details section of the instance launch wizard. With the AWS CLI, you can use the aws ec2 modify-instance-attribute command to modify the user data of a running or stopped instance.\n\n"
} ]